{
  "Spiking Neural Network": {
    "2408.12407": {
      "paper_id": "2408.12407v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.12407v1",
      "paper_key": "2408.12407",
      "paper_title": "Adaptive Spiking Neural Networks with Hybrid Coding",
      "paper_url": "http://arxiv.org/abs/2408.12407v1",
      "paper_abstract": "The Spiking Neural Network (SNN), due to its unique spiking-driven nature, is a more energy-efficient and effective neural network compared to Artificial Neural Networks (ANNs). The encoding method directly influences the overall performance of the network, and currently, direct encoding is primarily used for directly trained SNNs. When working with static image datasets, direct encoding inputs the same feature map at every time step, failing to fully exploit the spatiotemporal properties of SNNs. While temporal encoding converts input data into spike trains with spatiotemporal characteristics, traditional SNNs utilize the same neurons when processing input data across different time steps, limiting their ability to integrate and utilize spatiotemporal information effectively.To address this, this paper employs temporal encoding and proposes the Adaptive Spiking Neural Network (ASNN), enhancing the utilization of temporal encoding in conventional SNNs. Additionally, temporal encoding is less frequently used because short time steps can lead to significant loss of input data information, often necessitating a higher number of time steps in practical applications. However, training large SNNs with long time steps is challenging due to hardware constraints. To overcome this, this paper introduces a hybrid encoding approach that not only reduces the required time steps for training but also continues to improve the overall network performance.Notably, significant improvements in classification performance are observed on both Spikformer and Spiking ResNet architectures.our code is available at https://github.com/hhx0320/ASNN",
      "paper_authors": [
        "Huaxu He"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-08-22",
      "update_time": "2024-08-22",
      "comments": null,
      "repo_url": "https://github.com/hhx0320/asnn"
    },
    "2408.12293": {
      "paper_id": "2408.12293v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.12293v1",
      "paper_key": "2408.12293",
      "paper_title": "AT-SNN: Adaptive Tokens for Vision Transformer on Spiking Neural Network",
      "paper_url": "http://arxiv.org/abs/2408.12293v1",
      "paper_abstract": "In the training and inference of spiking neural networks (SNNs), direct training and lightweight computation methods have been orthogonally developed, aimed at reducing power consumption. However, only a limited number of approaches have applied these two mechanisms simultaneously and failed to fully leverage the advantages of SNN-based vision transformers (ViTs) since they were originally designed for convolutional neural networks (CNNs). In this paper, we propose AT-SNN designed to dynamically adjust the number of tokens processed during inference in SNN-based ViTs with direct training, wherein power consumption is proportional to the number of tokens. We first demonstrate the applicability of adaptive computation time (ACT), previously limited to RNNs and ViTs, to SNN-based ViTs, enhancing it to discard less informative spatial tokens selectively. Also, we propose a new token-merge mechanism that relies on the similarity of tokens, which further reduces the number of tokens while enhancing accuracy. We implement AT-SNN to Spikformer and show the effectiveness of AT-SNN in achieving high energy efficiency and accuracy compared to state-of-the-art approaches on the image classification tasks, CIFAR10, CIFAR-100, and TinyImageNet. For example, our approach uses up to 42.4% fewer tokens than the existing best-performing method on CIFAR-100, while conserving higher accuracy.",
      "paper_authors": [
        "Donghwa Kang",
        "Youngmoon Lee",
        "Eun-Kyu Lee",
        "Brent Kang",
        "Jinkyu Lee",
        "Hyeongboo Baek"
      ],
      "primary_category": "cs.AI",
      "publish_time": "2024-08-22",
      "update_time": "2024-08-22",
      "comments": "8 pages",
      "repo_url": "#"
    },
    "2408.10900": {
      "paper_id": "2408.10900v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.10900v1",
      "paper_key": "2408.10900",
      "paper_title": "Towards Efficient Formal Verification of Spiking Neural Network",
      "paper_url": "http://arxiv.org/abs/2408.10900v1",
      "paper_abstract": "Recently, AI research has primarily focused on large language models (LLMs), and increasing accuracy often involves scaling up and consuming more power. The power consumption of AI has become a significant societal issue; in this context, spiking neural networks (SNNs) offer a promising solution. SNNs operate event-driven, like the human brain, and compress information temporally. These characteristics allow SNNs to significantly reduce power consumption compared to perceptron-based artificial neural networks (ANNs), highlighting them as a next-generation neural network technology. However, societal concerns regarding AI go beyond power consumption, with the reliability of AI models being a global issue. For instance, adversarial attacks on AI models are a well-studied problem in the context of traditional neural networks. Despite their importance, the stability and property verification of SNNs remains in the early stages of research. Most SNN verification methods are time-consuming and barely scalable, making practical applications challenging. In this paper, we introduce temporal encoding to achieve practical performance in verifying the adversarial robustness of SNNs. We conduct a theoretical analysis of this approach and demonstrate its success in verifying SNNs at previously unmanageable scales. Our contribution advances SNN verification to a practical level, facilitating the safer application of SNNs.",
      "paper_authors": [
        "Baekryun Seong",
        "Jieung Kim",
        "Sang-Ki Ko"
      ],
      "primary_category": "cs.AI",
      "publish_time": "2024-08-20",
      "update_time": "2024-08-20",
      "comments": null,
      "repo_url": "#"
    },
    "2408.09403": {
      "paper_id": "2408.09403v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.09403v2",
      "paper_key": "2408.09403",
      "paper_title": "Obtaining Optimal Spiking Neural Network in Sequence Learning via CRNN-SNN Conversion",
      "paper_url": "http://arxiv.org/abs/2408.09403v2",
      "paper_abstract": "Spiking neural networks (SNNs) are becoming a promising alternative to conventional artificial neural networks (ANNs) due to their rich neural dynamics and the implementation of energy-efficient neuromorphic chips. However, the non-differential binary communication mechanism makes SNN hard to converge to an ANN-level accuracy. When SNN encounters sequence learning, the situation becomes worse due to the difficulties in modeling long-range dependencies. To overcome these difficulties, researchers developed variants of LIF neurons and different surrogate gradients but still failed to obtain good results when the sequence became longer (e.g., $>$500). Unlike them, we obtain an optimal SNN in sequence learning by directly mapping parameters from a quantized CRNN. We design two sub-pipelines to support the end-to-end conversion of different structures in neural networks, which is called CNN-Morph (CNN $\\rightarrow$ QCNN $\\rightarrow$ BIFSNN) and RNN-Morph (RNN $\\rightarrow$ QRNN $\\rightarrow$ RBIFSNN). Using conversion pipelines and the s-analog encoding method, the conversion error of our framework is zero. Furthermore, we give the theoretical and experimental demonstration of the lossless CRNN-SNN conversion. Our results show the effectiveness of our method over short and long timescales tasks compared with the state-of-the-art learning- and conversion-based methods. We reach the highest accuracy of 99.16% (0.46 $\\uparrow$) on S-MNIST, 94.95% (3.95 $\\uparrow$) on PS-MNIST (sequence length of 784) respectively, and the lowest loss of 0.057 (0.013 $\\downarrow$) within 8 time-steps in collision avoidance dataset.",
      "paper_authors": [
        "Jiahao Su",
        "Kang You",
        "Zekai Xu",
        "Weizhi Xu",
        "Zhezhi He"
      ],
      "primary_category": "cs.AI",
      "publish_time": "2024-08-18",
      "update_time": "2024-08-26",
      "comments": "Accepted by 33rd International Conference on Artificial Neural\n  Networks",
      "repo_url": "#"
    },
    "2408.11067": {
      "paper_id": "2408.11067v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.11067v1",
      "paper_key": "2408.11067",
      "paper_title": "Toward End-to-End Bearing Fault Diagnosis for Industrial Scenarios with Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2408.11067v1",
      "paper_abstract": "Spiking neural networks (SNNs) transmit information via low-power binary spikes and have received widespread attention in areas such as computer vision and reinforcement learning. However, there have been very few explorations of SNNs in more practical industrial scenarios. In this paper, we focus on the application of SNNs in bearing fault diagnosis to facilitate the integration of high-performance AI algorithms and real-world industries. In particular, we identify two key limitations of existing SNN fault diagnosis methods: inadequate encoding capacity that necessitates cumbersome data preprocessing, and non-spike-oriented architectures that constrain the performance of SNNs. To alleviate these problems, we propose a Multi-scale Residual Attention SNN (MRA-SNN) to simultaneously improve the efficiency, performance, and robustness of SNN methods. By incorporating a lightweight attention mechanism, we have designed a multi-scale attention encoding module to extract multiscale fault features from vibration signals and encode them as spatio-temporal spikes, eliminating the need for complicated preprocessing. Then, the spike residual attention block extracts high-dimensional fault features and enhances the expressiveness of sparse spikes with the attention mechanism for end-to-end diagnosis. In addition, the performance and robustness of MRA-SNN is further enhanced by introducing the lightweight attention mechanism within the spiking neurons to simulate the biological dendritic filtering effect. Extensive experiments on MFPT and JNU benchmark datasets demonstrate that MRA-SNN significantly outperforms existing methods in terms of accuracy, energy consumption and noise robustness, and is more feasible for deployment in real-world industrial scenarios.",
      "paper_authors": [
        "Yongqi Ding",
        "Lin Zuo",
        "Mengmeng Jing",
        "Kunshan Yang",
        "Biao Chen",
        "Yunqian Yu"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-08-17",
      "update_time": "2024-08-17",
      "comments": "13 pages, 10 figures",
      "repo_url": "#"
    },
    "2408.09108": {
      "paper_id": "2408.09108v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.09108v1",
      "paper_key": "2408.09108",
      "paper_title": "Temporal Reversed Training for Spiking Neural Networks with Generalized Spatio-Temporal Representation",
      "paper_url": "http://arxiv.org/abs/2408.09108v1",
      "paper_abstract": "Spiking neural networks (SNNs) have received widespread attention as an ultra-low energy computing paradigm. Recent studies have focused on improving the feature extraction capability of SNNs, but they suffer from inefficient inference and suboptimal performance. In this paper, we propose a simple yet effective temporal reversed training (TRT) method to optimize the spatio-temporal performance of SNNs and circumvent these problems. We perturb the input temporal data by temporal reversal, prompting the SNN to produce original-reversed consistent output logits and to learn perturbation-invariant representations. For static data without temporal dimension, we generalize this strategy by exploiting the inherent temporal property of spiking neurons for spike feature temporal reversal. In addition, we utilize the lightweight ``star operation\" (element-wise multiplication) to hybridize the original and temporally reversed spike firing rates and expand the implicit dimensions, which serves as spatio-temporal regularization to further enhance the generalization of the SNN. Our method involves only an additional temporal reversal operation and element-wise multiplication during training, thus incurring negligible training overhead and not affecting the inference efficiency at all. Extensive experiments on static/neuromorphic object/action recognition, and 3D point cloud classification tasks demonstrate the effectiveness and generalizability of our method. In particular, with only two timesteps, our method achieves 74.77\\% and 90.57\\% accuracy on ImageNet and ModelNet40, respectively.",
      "paper_authors": [
        "Lin Zuo",
        "Yongqi Ding",
        "Wenwei Luo",
        "Mengmeng Jing",
        "Xianlong Tian",
        "Kunshan Yang"
      ],
      "primary_category": "cs.AI",
      "publish_time": "2024-08-17",
      "update_time": "2024-08-17",
      "comments": "15 pages, 8 figures",
      "repo_url": "#"
    },
    "2408.08794": {
      "paper_id": "2408.08794v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.08794v1",
      "paper_key": "2408.08794",
      "paper_title": "Xpikeformer: Hybrid Analog-Digital Hardware Acceleration for Spiking Transformers",
      "paper_url": "http://arxiv.org/abs/2408.08794v1",
      "paper_abstract": "This paper introduces Xpikeformer, a hybrid analog-digital hardware architecture designed to accelerate spiking neural network (SNN)-based transformer models. By combining the energy efficiency and temporal dynamics of SNNs with the powerful sequence modeling capabilities of transformers, Xpikeformer leverages mixed analog-digital computing techniques to enhance performance and energy efficiency. The architecture integrates analog in-memory computing (AIMC) for feedforward and fully connected layers, and a stochastic spiking attention (SSA) engine for efficient attention mechanisms. We detail the design, implementation, and evaluation of Xpikeformer, demonstrating significant improvements in energy consumption and computational efficiency. Through an image classification task and a wireless communication symbol detection task, we show that Xpikeformer can achieve software-comparable inference accuracy. Energy evaluations reveal that Xpikeformer achieves up to a $17.8$--$19.2\\times$ reduction in energy consumption compared to state-of-the-art digital ANN transformers and up to a $5.9$--$6.8\\times$ reduction compared to fully digital SNN transformers. Xpikeformer also achieves a $12.0\\times$ speedup compared to the GPU implementation of spiking transformers.",
      "paper_authors": [
        "Zihang Song",
        "Prabodh Katti",
        "Osvaldo Simeone",
        "Bipin Rajendran"
      ],
      "primary_category": "cs.AR",
      "publish_time": "2024-08-16",
      "update_time": "2024-08-16",
      "comments": null,
      "repo_url": "#"
    },
    "2408.07734": {
      "paper_id": "2408.07734v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.07734v1",
      "paper_key": "2408.07734",
      "paper_title": "Analog Spiking Neuron in CMOS 28 nm Towards Large-Scale Neuromorphic Processors",
      "paper_url": "http://arxiv.org/abs/2408.07734v1",
      "paper_abstract": "The computational complexity of deep learning algorithms has given rise to significant speed and memory challenges for the execution hardware. In energy-limited portable devices, highly efficient processing platforms are indispensable for reproducing the prowess afforded by much bulkier processing platforms.   In this work, we present a low-power Leaky Integrate-and-Fire (LIF) neuron design fabricated in TSMC's 28 nm CMOS technology as proof of concept to build an energy-efficient mixed-signal Neuromorphic System-on-Chip (NeuroSoC). The fabricated neuron consumes 1.61 fJ/spike and occupies an active area of 34 $\\mu m^{2}$, leading to a maximum spiking frequency of 300 kHz at 250 mV power supply.   These performances are used in a software model to emulate the dynamics of a Spiking Neural Network (SNN). Employing supervised backpropagation and a surrogate gradient technique, the resulting accuracy on the MNIST dataset, using 4-bit post-training quantization stands at 82.5\\%. The approach underscores the potential of such ASIC implementation of quantized SNNs to deliver high-performance, energy-efficient solutions to various embedded machine-learning applications.",
      "paper_authors": [
        "Marwan Besrour",
        "Jacob Lavoie",
        "Takwa Omrani",
        "Gabriel Martin-Hardy",
        "Esmaeil Ranjbar Koleibi",
        "Jeremy Menard",
        "Konin Koua",
        "Philippe Marcoux",
        "Mounir Boukadoum",
        "Rejean Fontaine"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-08-14",
      "update_time": "2024-08-14",
      "comments": null,
      "repo_url": "#"
    },
    "2408.07517": {
      "paper_id": "2408.07517v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.07517v1",
      "paper_key": "2408.07517",
      "paper_title": "Advancing Spatio-Temporal Processing in Spiking Neural Networks through Adaptation",
      "paper_url": "http://arxiv.org/abs/2408.07517v1",
      "paper_abstract": "Efficient implementations of spiking neural networks on neuromorphic hardware promise orders of magnitude less power consumption than their non-spiking counterparts. The standard neuron model for spike-based computation on such neuromorphic systems has long been the leaky integrate-and-fire (LIF) neuron. As a promising advancement, a computationally light augmentation of the LIF neuron model with an adaptation mechanism experienced a recent upswing in popularity, caused by demonstrations of its superior performance on spatio-temporal processing tasks. The root of the superiority of these so-called adaptive LIF neurons however, is not well understood. In this article, we thoroughly analyze the dynamical, computational, and learning properties of adaptive LIF neurons and networks thereof. We find that the frequently observed stability problems during training of such networks can be overcome by applying an alternative discretization method that results in provably better stability properties than the commonly used Euler-Forward method. With this discretization, we achieved a new state-of-the-art performance on common event-based benchmark datasets. We also show that the superiority of networks of adaptive LIF neurons extends to the prediction and generation of complex time series. Our further analysis of the computational properties of networks of adaptive LIF neurons shows that they are particularly well suited to exploit the spatio-temporal structure of input sequences. Furthermore, these networks are surprisingly robust to shifts of the mean input strength and input spike rate, even when these shifts were not observed during training. As a consequence, high-performance networks can be obtained without any normalization techniques such as batch normalization or batch-normalization through time.",
      "paper_authors": [
        "Maximilian Baronig",
        "Romain Ferrand",
        "Silvester Sabathiel",
        "Robert Legenstein"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-08-14",
      "update_time": "2024-08-14",
      "comments": null,
      "repo_url": "https://github.com/IGITUGraz/SE-adlif"
    },
    "2408.07388": {
      "paper_id": "2408.07388v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.07388v1",
      "paper_key": "2408.07388",
      "paper_title": "DPSNN: Spiking Neural Network for Low-Latency Streaming Speech Enhancement",
      "paper_url": "http://arxiv.org/abs/2408.07388v1",
      "paper_abstract": "Speech enhancement (SE) improves communication in noisy environments, affecting areas such as automatic speech recognition, hearing aids, and telecommunications. With these domains typically being power-constrained and event-based while requiring low latency, neuromorphic algorithms in the form of spiking neural networks (SNNs) have great potential. Yet, current effective SNN solutions require a contextual sampling window imposing substantial latency, typically around 32ms, too long for many applications. Inspired by Dual-Path Spiking Neural Networks (DPSNNs) in classical neural networks, we develop a two-phase time-domain streaming SNN framework -- the Dual-Path Spiking Neural Network (DPSNN). In the DPSNN, the first phase uses Spiking Convolutional Neural Networks (SCNNs) to capture global contextual information, while the second phase uses Spiking Recurrent Neural Networks (SRNNs) to focus on frequency-related features. In addition, the regularizer suppresses activation to further enhance energy efficiency of our DPSNNs. Evaluating on the VCTK and Intel DNS Datasets, we demonstrate that our approach achieves the very low latency (approximately 5ms) required for applications like hearing aids, while demonstrating excellent signal-to-noise ratio (SNR), perceptual quality, and energy efficiency.",
      "paper_authors": [
        "Tao Sun",
        "Sander Boht\u00e9"
      ],
      "primary_category": "cs.SD",
      "publish_time": "2024-08-14",
      "update_time": "2024-08-14",
      "comments": null,
      "repo_url": "#"
    },
    "2408.07150": {
      "paper_id": "2408.07150v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.07150v1",
      "paper_key": "2408.07150",
      "paper_title": "The Potential of Combined Learning Strategies to Enhance Energy Efficiency of Spiking Neuromorphic Systems",
      "paper_url": "http://arxiv.org/abs/2408.07150v1",
      "paper_abstract": "Ensuring energy-efficient design in neuromorphic computing systems necessitates a tailored architecture combined with algorithmic approaches. This manuscript focuses on enhancing brain-inspired perceptual computing machines through a novel combined learning approach for Convolutional Spiking Neural Networks (CSNNs). CSNNs present a promising alternative to traditional power-intensive and complex machine learning methods like backpropagation, offering energy-efficient spiking neuron processing inspired by the human brain. The proposed combined learning method integrates Pair-based Spike Timing-Dependent Plasticity (PSTDP) and power law-dependent Spike-timing-dependent plasticity (STDP) to adjust synaptic efficacies, enabling the utilization of stochastic elements like memristive devices to enhance energy efficiency and improve perceptual computing accuracy. By reducing learning parameters while maintaining accuracy, these systems consume less energy and have reduced area overhead, making them more suitable for hardware implementation. The research delves into neuromorphic design architectures, focusing on CSNNs to provide a general framework for energy-efficient computing hardware. Various CSNN architectures are evaluated to assess how less trainable parameters can maintain acceptable accuracy in perceptual computing systems, positioning them as viable candidates for neuromorphic architecture. Comparisons with previous work validate the achievements and methodology of the proposed architecture.",
      "paper_authors": [
        "Ali Shiri Sichani",
        "Sai Kankatala"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-08-13",
      "update_time": "2024-08-13",
      "comments": null,
      "repo_url": "#"
    },
    "2408.06968": {
      "paper_id": "2408.06968v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.06968v1",
      "paper_key": "2408.06968",
      "paper_title": "Event-Stream Super Resolution using Sigma-Delta Neural Network",
      "paper_url": "http://arxiv.org/abs/2408.06968v1",
      "paper_abstract": "This study introduces a novel approach to enhance the spatial-temporal resolution of time-event pixels based on luminance changes captured by event cameras. These cameras present unique challenges due to their low resolution and the sparse, asynchronous nature of the data they collect. Current event super-resolution algorithms are not fully optimized for the distinct data structure produced by event cameras, resulting in inefficiencies in capturing the full dynamism and detail of visual scenes with improved computational complexity. To bridge this gap, our research proposes a method that integrates binary spikes with Sigma Delta Neural Networks (SDNNs), leveraging spatiotemporal constraint learning mechanism designed to simultaneously learn the spatial and temporal distributions of the event stream. The proposed network is evaluated using widely recognized benchmark datasets, including N-MNIST, CIFAR10-DVS, ASL-DVS, and Event-NFS. A comprehensive evaluation framework is employed, assessing both the accuracy, through root mean square error (RMSE), and the computational efficiency of our model. The findings demonstrate significant improvements over existing state-of-the-art methods, specifically, the proposed method outperforms state-of-the-art performance in computational efficiency, achieving a 17.04-fold improvement in event sparsity and a 32.28-fold increase in synaptic operation efficiency over traditional artificial neural networks, alongside a two-fold better performance over spiking neural networks.",
      "paper_authors": [
        "Waseem Shariff",
        "Joe Lemley",
        "Peter Corcoran"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2024-08-13",
      "update_time": "2024-08-13",
      "comments": "ECCV: The 18th European Conference on Computer Vision ECCV 2024 NeVi\n  Workshop",
      "repo_url": "#"
    },
    "2408.05845": {
      "paper_id": "2408.05845v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.05845v1",
      "paper_key": "2408.05845",
      "paper_title": "On the Solvability of the {XOR} Problem by Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2408.05845v1",
      "paper_abstract": "The linearly inseparable XOR problem and the related problem of representing binary logical gates is revisited from the point of view of temporal encoding and its solvability by spiking neural networks with minimal configurations of leaky integrate-and-fire (LIF) neurons. We use this problem as an example to study the effect of different hyper parameters such as information encoding, the number of hidden units in a fully connected reservoir, the choice of the leaky parameter and the reset mechanism in terms of reset-to-zero and reset-by-subtraction based on different refractory times. The distributions of the weight matrices give insight into the difficulty, respectively the probability, to find a solution. This leads to the observation that zero refractory time together with graded spikes and an adapted reset mechanism, reset-to-mod, makes it possible to realize sparse solutions of a minimal configuration with only two neurons in the hidden layer to resolve all binary logic gate constellations with XOR as a special case.",
      "paper_authors": [
        "Bernhard A. Moser",
        "Michael Lunglmayr"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-08-11",
      "update_time": "2024-08-11",
      "comments": null,
      "repo_url": "#"
    },
    "2408.06383": {
      "paper_id": "2408.06383v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.06383v1",
      "paper_key": "2408.06383",
      "paper_title": "Dilated Convolution with Learnable Spacings",
      "paper_url": "http://arxiv.org/abs/2408.06383v1",
      "paper_abstract": "This thesis presents and evaluates the Dilated Convolution with Learnable Spacings (DCLS) method. Through various supervised learning experiments in the fields of computer vision, audio, and speech processing, the DCLS method proves to outperform both standard and advanced convolution techniques. The research is organized into several steps, starting with an analysis of the literature and existing convolution techniques that preceded the development of the DCLS method. We were particularly interested in the methods that are closely related to our own and that remain essential to capture the nuances and uniqueness of our approach. The cornerstone of our study is the introduction and application of the DCLS method to convolutional neural networks (CNNs), as well as to hybrid architectures that rely on both convolutional and visual attention approaches. DCLS is shown to be particularly effective in tasks such as classification, semantic segmentation, and object detection. Initially using bilinear interpolation, the study also explores other interpolation methods, finding that Gaussian interpolation slightly improves performance. The DCLS method is further applied to spiking neural networks (SNNs) to enable synaptic delay learning within a neural network that could eventually be transferred to so-called neuromorphic chips. The results show that the DCLS method stands out as a new state-of-the-art technique in SNN audio classification for certain benchmark tasks in this field. These tasks involve datasets with a high temporal component. In addition, we show that DCLS can significantly improve the accuracy of artificial neural networks for the multi-label audio classification task. We conclude with a discussion of the chosen experimental setup, its limitations, the limitations of our method, and our results.",
      "paper_authors": [
        "Ismail Khalfaoui-Hassani"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-08-10",
      "update_time": "2024-08-10",
      "comments": "PhD Thesis",
      "repo_url": "#"
    },
    "2408.05156": {
      "paper_id": "2408.05156v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.05156v1",
      "paper_key": "2408.05156",
      "paper_title": "Neuromorphic Keyword Spotting with Pulse Density Modulation MEMS Microphones",
      "paper_url": "http://arxiv.org/abs/2408.05156v1",
      "paper_abstract": "The Keyword Spotting (KWS) task involves continuous audio stream monitoring to detect predefined words, requiring low energy devices for continuous processing. Neuromorphic devices effectively address this energy challenge. However, the general neuromorphic KWS pipeline, from microphone to Spiking Neural Network (SNN), entails multiple processing stages. Leveraging the popularity of Pulse Density Modulation (PDM) microphones in modern devices and their similarity to spiking neurons, we propose a direct microphone-to-SNN connection. This approach eliminates intermediate stages, notably reducing computational costs. The system achieved an accuracy of 91.54\\% on the Google Speech Command (GSC) dataset, surpassing the state-of-the-art for the Spiking Speech Command (SSC) dataset which is a bio-inspired encoded GSC. Furthermore, the observed sparsity in network activity and connectivity indicates potential for remarkably low energy consumption in a neuromorphic device implementation.",
      "paper_authors": [
        "Sidi Yaya Arnaud Yarga",
        "Sean U. N. Wood"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-08-09",
      "update_time": "2024-08-09",
      "comments": "Accepted at INTERSPEECH 2024",
      "repo_url": "https://github.com/NECOTIS/Keyword-Spotting-with-PDM"
    },
    "2408.05098": {
      "paper_id": "2408.05098v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.05098v1",
      "paper_key": "2408.05098",
      "paper_title": "Overcoming the Limitations of Layer Synchronization in Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2408.05098v1",
      "paper_abstract": "Currently, neural-network processing in machine learning applications relies on layer synchronization, whereby neurons in a layer aggregate incoming currents from all neurons in the preceding layer, before evaluating their activation function. This is practiced even in artificial Spiking Neural Networks (SNNs), which are touted as consistent with neurobiology, in spite of processing in the brain being, in fact asynchronous. A truly asynchronous system however would allow all neurons to evaluate concurrently their threshold and emit spikes upon receiving any presynaptic current. Omitting layer synchronization is potentially beneficial, for latency and energy efficiency, but asynchronous execution of models previously trained with layer synchronization may entail a mismatch in network dynamics and performance. We present a study that documents and quantifies this problem in three datasets on our simulation environment that implements network asynchrony, and we show that models trained with layer synchronization either perform sub-optimally in absence of the synchronization, or they will fail to benefit from any energy and latency reduction, when such a mechanism is in place. We then \"make ends meet\" and address the problem with unlayered backprop, a novel backpropagation-based training method, for learning models suitable for asynchronous processing. We train with it models that use different neuron execution scheduling strategies, and we show that although their neurons are more reactive, these models consistently exhibit lower overall spike density (up to 50%), reach a correct decision faster (up to 2x) without integrating all spikes, and achieve superior accuracy (up to 10% higher). Our findings suggest that asynchronous event-based (neuromorphic) AI computing is indeed more efficient, but we need to seriously rethink how we train our SNN models, to benefit from it.",
      "paper_authors": [
        "Roel Koopman",
        "Amirreza Yousefzadeh",
        "Mahyar Shahsavari",
        "Guangzhi Tang",
        "Manolis Sifalakis"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-08-09",
      "update_time": "2024-08-09",
      "comments": null,
      "repo_url": "#"
    },
    "2408.02961": {
      "paper_id": "2408.02961v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.02961v1",
      "paper_key": "2408.02961",
      "paper_title": "Synaptic Modulation using Interspike Intervals Increases Energy Efficiency of Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2408.02961v1",
      "paper_abstract": "Despite basic differences between Spiking Neural Networks (SNN) and Artificial Neural Networks (ANN), most research on SNNs involve adapting ANN-based methods for SNNs. Pruning (dropping connections) and quantization (reducing precision) are often used to improve energy efficiency of SNNs. These methods are very effective for ANNs whose energy needs are determined by signals transmitted on synapses. However, the event-driven paradigm in SNNs implies that energy is consumed by spikes. In this paper, we propose a new synapse model whose weights are modulated by Interspike Intervals (ISI) i.e. time difference between two spikes. SNNs composed of this synapse model, termed ISI Modulated SNNs (IMSNN), can use gradient descent to estimate how the ISI of a neuron changes after updating its synaptic parameters. A higher ISI implies fewer spikes and vice-versa. The learning algorithm for IMSNNs exploits this information to selectively propagate gradients such that learning is achieved by increasing the ISIs resulting in a network that generates fewer spikes. The performance of IMSNNs with dense and convolutional layers have been evaluated in terms of classification accuracy and the number of spikes using the MNIST and FashionMNIST datasets. The performance comparison with conventional SNNs shows that IMSNNs exhibit upto 90% reduction in the number of spikes while maintaining similar classification accuracy.",
      "paper_authors": [
        "Dylan Adams",
        "Magda Zajaczkowska",
        "Ashiq Anjum",
        "Andrea Soltoggio",
        "Shirin Dora"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-08-06",
      "update_time": "2024-08-06",
      "comments": "9 pages, 3 figures",
      "repo_url": "#"
    },
    "2408.02125": {
      "paper_id": "2408.02125v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.02125v1",
      "paper_key": "2408.02125",
      "paper_title": "Abstraction in Neural Networks",
      "paper_url": "http://arxiv.org/abs/2408.02125v1",
      "paper_abstract": "We show how brain networks, modeled as Spiking Neural Networks, can be viewed at different levels of abstraction. Lower levels include complications such as failures of neurons and edges. Higher levels are more abstract, making simplifying assumptions to avoid these complications. We show precise relationships between executions of networks at different levels, which enables us to understand the behavior of lower-level networks in terms of the behavior of higher-level networks.   We express our results using two abstract networks, A1 and A2, one to express firing guarantees and the other to express non-firing guarantees, and one detailed network D. The abstract networks contain reliable neurons and edges, whereas the detailed network has neurons and edges that may fail, subject to some constraints. Here we consider just initial stopping failures. To define these networks, we begin with abstract network A1 and modify it systematically to obtain the other two networks. To obtain A2, we simply lower the firing thresholds of the neurons. To obtain D, we introduce failures of neurons and edges, and incorporate redundancy in the neurons and edges in order to compensate for the failures. We also define corresponding inputs for the networks, and corresponding executions of the networks.   We prove two main theorems, one relating corresponding executions of A1 and D and the other relating corresponding executions of A2 and D. Together, these give both firing and non-firing guarantees for the detailed network D. We also give a third theorem, relating the effects of D on an external reliable actuator neuron to the effects of the abstract networks on the same actuator neuron.",
      "paper_authors": [
        "Nancy Lynch"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-08-04",
      "update_time": "2024-08-04",
      "comments": null,
      "repo_url": "#"
    },
    "2408.11823": {
      "paper_id": "2408.11823v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.11823v1",
      "paper_key": "2408.11823",
      "paper_title": "Mamba-Spike: Enhancing the Mamba Architecture with a Spiking Front-End for Efficient Temporal Data Processing",
      "paper_url": "http://arxiv.org/abs/2408.11823v1",
      "paper_abstract": "The field of neuromorphic computing has gained significant attention in recent years, aiming to bridge the gap between the efficiency of biological neural networks and the performance of artificial intelligence systems. This paper introduces Mamba-Spike, a novel neuromorphic architecture that integrates a spiking front-end with the Mamba backbone to achieve efficient and robust temporal data processing. The proposed approach leverages the event-driven nature of spiking neural networks (SNNs) to capture and process asynchronous, time-varying inputs, while harnessing the power of the Mamba backbone's selective state spaces and linear-time sequence modeling capabilities to model complex temporal dependencies effectively. The spiking front-end of Mamba-Spike employs biologically inspired neuron models, along with adaptive threshold and synaptic dynamics. These components enable efficient spatiotemporal feature extraction and encoding of the input data. The Mamba backbone, on the other hand, utilizes a hierarchical structure with gated recurrent units and attention mechanisms to capture long-term dependencies and selectively process relevant information. To evaluate the efficacy of the proposed architecture, a comprehensive empirical study is conducted on both neuromorphic datasets, including DVS Gesture and TIDIGITS, and standard datasets, such as Sequential MNIST and CIFAR10-DVS. The results demonstrate that Mamba-Spike consistently outperforms state-of-the-art baselines, achieving higher accuracy, lower latency, and improved energy efficiency. Moreover, the model exhibits robustness to various input perturbations and noise levels, highlighting its potential for real-world applications. The code will be available at https://github.com/ECNU-Cross-Innovation-Lab/Mamba-Spike.",
      "paper_authors": [
        "Jiahao Qin",
        "Feng Liu"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-08-04",
      "update_time": "2024-08-04",
      "comments": "12 pages, 5 figures, accepted by CGI 2024",
      "repo_url": "https://github.com/ecnu-cross-innovation-lab/mamba-spike"
    },
    "2408.01996": {
      "paper_id": "2408.01996v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.01996v1",
      "paper_key": "2408.01996",
      "paper_title": "Configuring Safe Spiking Neural Controllers for Cyber-Physical Systems through Formal Verification",
      "paper_url": "http://arxiv.org/abs/2408.01996v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) are a subclass of neuromorphic models that have great potential to be used as controllers in Cyber-Physical Systems (CPSs) due to their energy efficiency. They can benefit from the prevalent approach of first training an Artificial Neural Network (ANN) and then translating to an SNN with subsequent hyperparameter tuning. The tuning is required to ensure that the resulting SNN is accurate with respect to the ANN in terms of metrics like Mean Squared Error (MSE). However, SNN controllers for safety-critical CPSs must also satisfy safety specifications, which are not guaranteed by the conversion approach. In this paper, we propose a solution which tunes the $temporal$ $window$ hyperparameter of the translated SNN to ensure both accuracy and compliance with the safe range specification that requires the SNN outputs to remain within a safe range. The core verification problem is modelled using mixed-integer linear programming (MILP) and is solved with Gurobi. When the controller fails to meet the range specification, we compute tight bounds on the SNN outputs as feedback for the CPS developer. To mitigate the high computational cost of verification, we integrate data-driven steps to minimize verification calls. Our approach provides designers with the confidence to safely integrate energy-efficient SNN controllers into modern CPSs. We demonstrate our approach with experimental results on five different benchmark neural controllers.",
      "paper_authors": [
        "Arkaprava Gupta",
        "Sumana Ghosh",
        "Ansuman Banerjee",
        "Swarup Kumar Mohalik"
      ],
      "primary_category": "cs.ET",
      "publish_time": "2024-08-04",
      "update_time": "2024-08-04",
      "comments": "This is the complete version of a paper with the same title that\n  appeared at MEMOCODE 2024",
      "repo_url": "#"
    },
    "2408.01701": {
      "paper_id": "2408.01701v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.01701v1",
      "paper_key": "2408.01701",
      "paper_title": "Signal-SGN: A Spiking Graph Convolutional Network for Skeletal Action Recognition via Learning Temporal-Frequency Dynamics",
      "paper_url": "http://arxiv.org/abs/2408.01701v1",
      "paper_abstract": "In skeletal-based action recognition, Graph Convolutional Networks (GCNs) based methods face limitations due to their complexity and high energy consumption. Spiking Neural Networks (SNNs) have gained attention in recent years for their low energy consumption, but existing methods combining GCNs and SNNs fail to fully utilize the temporal characteristics of skeletal sequences, leading to increased storage and computational costs. To address this issue, we propose a Signal-SGN(Spiking Graph Convolutional Network), which leverages the temporal dimension of skeletal sequences as the spiking timestep and treats features as discrete stochastic signals. The core of the network consists of a 1D Spiking Graph Convolutional Network (1D-SGN) and a Frequency Spiking Convolutional Network (FSN). The SGN performs graph convolution on single frames and incorporates spiking network characteristics to capture inter-frame temporal relationships, while the FSN uses Fast Fourier Transform (FFT) and complex convolution to extract temporal-frequency features. We also introduce a multi-scale wavelet transform feature fusion module(MWTF) to capture spectral features of temporal signals, enhancing the model's classification capability. We propose a pluggable temporal-frequency spatial semantic feature extraction module(TFSM) to enhance the model's ability to distinguish features without increasing inference-phase consumption. Our numerous experiments on the NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets demonstrate that the proposed models not only surpass existing SNN-based methods in accuracy but also reduce computational and storage costs during training. Furthermore, they achieve competitive accuracy compared to corresponding GCN-based methods, which is quite remarkable.",
      "paper_authors": [
        "Naichuan Zheng",
        "Hailun Xia",
        "Dapeng Liu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-03",
      "update_time": "2024-08-03",
      "comments": null,
      "repo_url": "#"
    },
    "2408.00611": {
      "paper_id": "2408.00611v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.00611v1",
      "paper_key": "2408.00611",
      "paper_title": "Using CSNNs to Perform Event-based Data Processing & Classification on ASL-DVS",
      "paper_url": "http://arxiv.org/abs/2408.00611v1",
      "paper_abstract": "Recent advancements in bio-inspired visual sensing and neuromorphic computing have led to the development of various highly efficient bio-inspired solutions with real-world applications. One notable application integrates event-based cameras with spiking neural networks (SNNs) to process event-based sequences that are asynchronous and sparse, making them difficult to handle. In this project, we develop a convolutional spiking neural network (CSNN) architecture that leverages convolutional operations and recurrent properties of a spiking neuron to learn the spatial and temporal relations in the ASL-DVS gesture dataset. The ASL-DVS gesture dataset is a neuromorphic dataset containing hand gestures when displaying 24 letters (A to Y, excluding J and Z due to the nature of their symbols) from the American Sign Language (ASL). We performed classification on a pre-processed subset of the full ASL-DVS dataset to identify letter signs and achieved 100\\% training accuracy. Specifically, this was achieved by training in the Google Cloud compute platform while using a learning rate of 0.0005, batch size of 25 (total of 20 batches), 200 iterations, and 10 epochs.",
      "paper_authors": [
        "Ria Patel",
        "Sujit Tripathy",
        "Zachary Sublett",
        "Seoyoung An",
        "Riya Patel"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-08-01",
      "update_time": "2024-08-01",
      "comments": "8 pages, 14 figures",
      "repo_url": "#"
    },
    "2408.00516": {
      "paper_id": "2408.00516v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.00516v1",
      "paper_key": "2408.00516",
      "paper_title": "Low-Power Vibration-Based Predictive Maintenance for Industry 4.0 using Neural Networks: A Survey",
      "paper_url": "http://arxiv.org/abs/2408.00516v1",
      "paper_abstract": "The advancements in smart sensors for Industry 4.0 offer ample opportunities for low-powered predictive maintenance and condition monitoring. However, traditional approaches in this field rely on processing in the cloud, which incurs high costs in energy and storage. This paper investigates the potential of neural networks for low-power on-device computation of vibration sensor data for predictive maintenance. We review the literature on Spiking Neural Networks (SNNs) and Artificial Neuronal Networks (ANNs) for vibration-based predictive maintenance by analyzing datasets, data preprocessing, network architectures, and hardware implementations. Our findings suggest that no satisfactory standard benchmark dataset exists for evaluating neural networks in predictive maintenance tasks. Furthermore frequency domain transformations are commonly employed for preprocessing. SNNs mainly use shallow feed forward architectures, whereas ANNs explore a wider range of models and deeper networks. Finally, we highlight the need for future research on hardware implementations of neural networks for low-power predictive maintenance applications and the development of a standardized benchmark dataset.",
      "paper_authors": [
        "Alexandru Vasilache",
        "Sven Nitzsche",
        "Daniel Floegel",
        "Tobias Schuermann",
        "Stefan von Dosky",
        "Thomas Bierweiler",
        "Marvin Mu\u00dfler",
        "Florian K\u00e4lber",
        "Soeren Hohmann",
        "Juergen Becker"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-08-01",
      "update_time": "2024-08-01",
      "comments": "The final version will be published at the ECML-PKDD 2024 joint\n  post-workshop proceeding in Springer Communications in Computer and\n  Information Science",
      "repo_url": "#"
    },
    "2408.00280": {
      "paper_id": "2408.00280v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.00280v1",
      "paper_key": "2408.00280",
      "paper_title": "Towards Scalable GPU-Accelerated SNN Training via Temporal Fusion",
      "paper_url": "http://arxiv.org/abs/2408.00280v1",
      "paper_abstract": "Drawing on the intricate structures of the brain, Spiking Neural Networks (SNNs) emerge as a transformative development in artificial intelligence, closely emulating the complex dynamics of biological neural networks. While SNNs show promising efficiency on specialized sparse-computational hardware, their practical training often relies on conventional GPUs. This reliance frequently leads to extended computation times when contrasted with traditional Artificial Neural Networks (ANNs), presenting significant hurdles for advancing SNN research. To navigate this challenge, we present a novel temporal fusion method, specifically designed to expedite the propagation dynamics of SNNs on GPU platforms, which serves as an enhancement to the current significant approaches for handling deep learning tasks with SNNs. This method underwent thorough validation through extensive experiments in both authentic training scenarios and idealized conditions, confirming its efficacy and adaptability for single and multi-GPU systems. Benchmarked against various existing SNN libraries/implementations, our method achieved accelerations ranging from $5\\times$ to $40\\times$ on NVIDIA A100 GPUs. Publicly available experimental codes can be found at https://github.com/EMI-Group/snn-temporal-fusion.",
      "paper_authors": [
        "Yanchen Li",
        "Jiachun Li",
        "Kebin Sun",
        "Luziwei Leng",
        "Ran Cheng"
      ],
      "primary_category": "cs.AI",
      "publish_time": "2024-08-01",
      "update_time": "2024-08-01",
      "comments": "International Conference on Artificial Neural Networks (ICANN) 2024",
      "repo_url": "#"
    },
    "2407.20947": {
      "paper_id": "2407.20947v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.20947v1",
      "paper_key": "2407.20947",
      "paper_title": "An Asynchronous Multi-core Accelerator for SNN inference",
      "paper_url": "http://arxiv.org/abs/2407.20947v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) are extensively utilized in brain-inspired computing and neuroscience research. To enhance the speed and energy efficiency of SNNs, several many-core accelerators have been developed. However, maintaining the accuracy of SNNs often necessitates frequent explicit synchronization among all cores, which presents a challenge to overall efficiency. In this paper, we propose an asynchronous architecture for Spiking Neural Networks (SNNs) that eliminates the need for inter-core synchronization, thus enhancing speed and energy efficiency. This approach leverages the pre-determined dependencies of neuromorphic cores established during compilation. Each core is equipped with a scheduler that monitors the status of its dependencies, allowing it to safely advance to the next timestep without waiting for other cores. This eliminates the necessity for global synchronization and minimizes core waiting time despite inherent workload imbalances. Comprehensive evaluations using five different SNN workloads show that our architecture achieves a 1.86x speedup and a 1.55x increase in energy efficiency compared to state-of-the-art synchronization architectures.",
      "paper_authors": [
        "Zhuo Chen",
        "De Ma",
        "Xiaofei Jin",
        "Qinghui Xing",
        "Ouwen Jin",
        "Xin Du",
        "Shuibing He",
        "Gang Pan"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-07-30",
      "update_time": "2024-07-30",
      "comments": null,
      "repo_url": "#"
    },
    "2407.20708": {
      "paper_id": "2407.20708v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.20708v3",
      "paper_key": "2407.20708",
      "paper_title": "Integer-Valued Training and Spike-Driven Inference Spiking Neural Network for High-performance and Energy-efficient Object Detection",
      "paper_url": "http://arxiv.org/abs/2407.20708v3",
      "paper_abstract": "Brain-inspired Spiking Neural Networks (SNNs) have bio-plausibility and low-power advantages over Artificial Neural Networks (ANNs). Applications of SNNs are currently limited to simple classification tasks because of their poor performance. In this work, we focus on bridging the performance gap between ANNs and SNNs on object detection. Our design revolves around network architecture and spiking neuron. First, the overly complex module design causes spike degradation when the YOLO series is converted to the corresponding spiking version. We design a SpikeYOLO architecture to solve this problem by simplifying the vanilla YOLO and incorporating meta SNN blocks. Second, object detection is more sensitive to quantization errors in the conversion of membrane potentials into binary spikes by spiking neurons. To address this challenge, we design a new spiking neuron that activates Integer values during training while maintaining spike-driven by extending virtual timesteps during inference. The proposed method is validated on both static and neuromorphic object detection datasets. On the static COCO dataset, we obtain 66.2% mAP@50 and 48.9% mAP@50:95, which is +15.0% and +18.7% higher than the prior state-of-the-art SNN, respectively. On the neuromorphic Gen1 dataset, we achieve 67.2% mAP@50, which is +2.5% greater than the ANN with equivalent architecture, and the energy efficiency is improved by 5.7*. Code: https://github.com/BICLab/SpikeYOLO",
      "paper_authors": [
        "Xinhao Luo",
        "Man Yao",
        "Yuhong Chou",
        "Bo Xu",
        "Guoqi Li"
      ],
      "primary_category": "cs.AI",
      "publish_time": "2024-07-30",
      "update_time": "2024-08-05",
      "comments": "Accepted by ECCV2024; 19 pages, 4 figures",
      "repo_url": "https://github.com/biclab/spikeyolo"
    },
    "2407.20633": {
      "paper_id": "2407.20633v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.20633v1",
      "paper_key": "2407.20633",
      "paper_title": "Spiking-DD: Neuromorphic Event Camera based Driver Distraction Detection with Spiking Neural Network",
      "paper_url": "http://arxiv.org/abs/2407.20633v1",
      "paper_abstract": "Event camera-based driver monitoring is emerging as a pivotal area of research, driven by its significant advantages such as rapid response, low latency, power efficiency, enhanced privacy, and prevention of undersampling. Effective detection of driver distraction is crucial in driver monitoring systems to enhance road safety and reduce accident rates. The integration of an optimized sensor such as Event Camera with an optimized network is essential for maximizing these benefits. This paper introduces the innovative concept of sensing without seeing to detect driver distraction, leveraging computationally efficient spiking neural networks (SNN). To the best of our knowledge, this study is the first to utilize event camera data with spiking neural networks for driver distraction. The proposed Spiking-DD network not only achieve state of the art performance but also exhibit fewer parameters and provides greater accuracy than current event-based methodologies.",
      "paper_authors": [
        "Waseem Shariff",
        "Paul Kielty",
        "Joseph Lemley",
        "Peter Corcoran"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-30",
      "update_time": "2024-07-30",
      "comments": "Irish Machine Vision and Image Processing Conference (IMVIP) 2024",
      "repo_url": "#"
    },
    "2407.20547": {
      "paper_id": "2407.20547v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.20547v1",
      "paper_key": "2407.20547",
      "paper_title": "Neuromorphic on-chip reservoir computing with spiking neural network architectures",
      "paper_url": "http://arxiv.org/abs/2407.20547v1",
      "paper_abstract": "Reservoir computing is a promising approach for harnessing the computational power of recurrent neural networks while dramatically simplifying training. This paper investigates the application of integrate-and-fire neurons within reservoir computing frameworks for two distinct tasks: capturing chaotic dynamics of the H\\'enon map and forecasting the Mackey-Glass time series. Integrate-and-fire neurons can be implemented in low-power neuromorphic architectures such as Intel Loihi. We explore the impact of network topologies created through random interactions on the reservoir's performance. Our study reveals task-specific variations in network effectiveness, highlighting the importance of tailored architectures for distinct computational tasks. To identify optimal network configurations, we employ a meta-learning approach combined with simulated annealing. This method efficiently explores the space of possible network structures, identifying architectures that excel in different scenarios. The resulting networks demonstrate a range of behaviors, showcasing how inherent architectural features influence task-specific capabilities. We study the reservoir computing performance using a custom integrate-and-fire code, Intel's Lava neuromorphic computing software framework, and via an on-chip implementation in Loihi. We conclude with an analysis of the energy performance of the Loihi architecture.",
      "paper_authors": [
        "Samip Karki",
        "Diego Chavez Arana",
        "Andrew Sornborger",
        "Francesco Caravelli"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-07-30",
      "update_time": "2024-07-30",
      "comments": "19 pages, 9 figures; single column",
      "repo_url": "#"
    },
    "2407.20508": {
      "paper_id": "2407.20508v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.20508v1",
      "paper_key": "2407.20508",
      "paper_title": "Unveiling the Potential of Spiking Dynamics in Graph Representation Learning through Spatial-Temporal Normalization and Coding Strategies",
      "paper_url": "http://arxiv.org/abs/2407.20508v1",
      "paper_abstract": "In recent years, spiking neural networks (SNNs) have attracted substantial interest due to their potential to replicate the energy-efficient and event-driven processing of biological neurons. Despite this, the application of SNNs in graph representation learning, particularly for non-Euclidean data, remains underexplored, and the influence of spiking dynamics on graph learning is not yet fully understood. This work seeks to address these gaps by examining the unique properties and benefits of spiking dynamics in enhancing graph representation learning. We propose a spike-based graph neural network model that incorporates spiking dynamics, enhanced by a novel spatial-temporal feature normalization (STFN) technique, to improve training efficiency and model stability. Our detailed analysis explores the impact of rate coding and temporal coding on SNN performance, offering new insights into their advantages for deep graph networks and addressing challenges such as the oversmoothing problem. Experimental results demonstrate that our SNN models can achieve competitive performance with state-of-the-art graph neural networks (GNNs) while considerably reducing computational costs, highlighting the potential of SNNs for efficient neuromorphic computing applications in complex graph-based scenarios.",
      "paper_authors": [
        "Mingkun Xu",
        "Huifeng Yin",
        "Yujie Wu",
        "Guoqi Li",
        "Faqiang Liu",
        "Jing Pei",
        "Shuai Zhong",
        "Lei Deng"
      ],
      "primary_category": "cs.AI",
      "publish_time": "2024-07-30",
      "update_time": "2024-07-30",
      "comments": null,
      "repo_url": "#"
    },
    "2407.20421": {
      "paper_id": "2407.20421v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.20421v1",
      "paper_key": "2407.20421",
      "paper_title": "Event-based Optical Flow on Neuromorphic Processor: ANN vs. SNN Comparison based on Activation Sparsification",
      "paper_url": "http://arxiv.org/abs/2407.20421v1",
      "paper_abstract": "Spiking neural networks (SNNs) for event-based optical flow are claimed to be computationally more efficient than their artificial neural networks (ANNs) counterparts, but a fair comparison is missing in the literature. In this work, we propose an event-based optical flow solution based on activation sparsification and a neuromorphic processor, SENECA. SENECA has an event-driven processing mechanism that can exploit the sparsity in ANN activations and SNN spikes to accelerate the inference of both types of neural networks. The ANN and the SNN for comparison have similar low activation/spike density (~5%) thanks to our novel sparsification-aware training. In the hardware-in-loop experiments designed to deduce the average time and energy consumption, the SNN consumes 44.9ms and 927.0 microjoules, which are 62.5% and 75.2% of the ANN's consumption, respectively. We find that SNN's higher efficiency attributes to its lower pixel-wise spike density (43.5% vs. 66.5%) that requires fewer memory access operations for neuron states.",
      "paper_authors": [
        "Yingfu Xu",
        "Guangzhi Tang",
        "Amirreza Yousefzadeh",
        "Guido de Croon",
        "Manolis Sifalakis"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-07-29",
      "update_time": "2024-07-29",
      "comments": "18 pages, 12 figures, 4 tables",
      "repo_url": "https://github.com/yingfuxu/ann_vs_snn_evflow"
    },
    "2407.20099": {
      "paper_id": "2407.20099v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.20099v1",
      "paper_key": "2407.20099",
      "paper_title": "RSC-SNN: Exploring the Trade-off Between Adversarial Robustness and Accuracy in Spiking Neural Networks via Randomized Smoothing Coding",
      "paper_url": "http://arxiv.org/abs/2407.20099v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) have received widespread attention due to their unique neuronal dynamics and low-power nature. Previous research empirically shows that SNNs with Poisson coding are more robust than Artificial Neural Networks (ANNs) on small-scale datasets. However, it is still unclear in theory how the adversarial robustness of SNNs is derived, and whether SNNs can still maintain its adversarial robustness advantage on large-scale dataset tasks. This work theoretically demonstrates that SNN's inherent adversarial robustness stems from its Poisson coding. We reveal the conceptual equivalence of Poisson coding and randomized smoothing in defense strategies, and analyze in depth the trade-off between accuracy and adversarial robustness in SNNs via the proposed Randomized Smoothing Coding (RSC) method. Experiments demonstrate that the proposed RSC-SNNs show remarkable adversarial robustness, surpassing ANNs and achieving state-of-the-art robustness results on large-scale dataset ImageNet. Our open-source implementation code is available at this https URL: https://github.com/KemingWu/RSC-SNN.",
      "paper_authors": [
        "Keming Wu",
        "Man Yao",
        "Yuhong Chou",
        "Xuerui Qiu",
        "Rui Yang",
        "Bo Xu",
        "Guoqi Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-29",
      "update_time": "2024-07-29",
      "comments": "Accepted by ACM MM 2024",
      "repo_url": "https://github.com/KemingWu/RSC-SNN"
    },
    "2407.19566": {
      "paper_id": "2407.19566v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.19566v1",
      "paper_key": "2407.19566",
      "paper_title": "Rouser: Robust SNN training using adaptive threshold learning",
      "paper_url": "http://arxiv.org/abs/2407.19566v1",
      "paper_abstract": "In Spiking Neural Networks (SNNs), learning rules are based on neuron spiking behavior, that is, if and when spikes are generated due to a neuron's membrane potential exceeding that neuron's firing threshold, and this spike timing encodes vital information. However, the threshold is generally treated as a hyperparameter, and incorrect selection can lead to neurons that do not spike for large portions of the training process, hindering the effective rate of learning. Inspired by homeostatic mechanisms in biological neurons, this work (Rouser) presents a study to rouse training-inactive neurons and improve the SNN training by using an in-loop adaptive threshold learning mechanism. Rouser's adaptive threshold allows for dynamic adjustments based on input data and network hyperparameters, influencing spike timing and improving training. This study focuses primarily on investigating the significance of learning neuron thresholds alongside weights in SNNs. We evaluate the performance of Rouser on the spatiotemporal datasets NMNIST, DVS128 and Spiking Heidelberg Digits (SHD), compare our results with state-of-the-art SNN training techniques, and discuss the strengths and limitations of our approach. Our results suggest that promoting threshold from a hyperparameter to a parameter can effectively address the issue of dead neurons during training, resulting in a more robust training algorithm that leads to improved training convergence, increased test accuracy, and substantial reductions in the number of training epochs needed to achieve viable accuracy. Rouser achieves up to 70% lower training latency while providing up to 2% higher accuracy over state-of-the-art SNNs with similar network architecture on the neuromorphic datasets NMNIST, DVS128 and SHD.",
      "paper_authors": [
        "Sanaz Mahmoodi Takaghaj",
        "Jack Sampson"
      ],
      "primary_category": "cs.ET",
      "publish_time": "2024-07-28",
      "update_time": "2024-07-28",
      "comments": null,
      "repo_url": "#"
    },
    "2407.18838": {
      "paper_id": "2407.18838v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.18838v1",
      "paper_key": "2407.18838",
      "paper_title": "The Role of Temporal Hierarchy in Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2407.18838v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) have the potential for rich spatio-temporal signal processing thanks to exploiting both spatial and temporal parameters. The temporal dynamics such as time constants of the synapses and neurons and delays have been recently shown to have computational benefits that help reduce the overall number of parameters required in the network and increase the accuracy of the SNNs in solving temporal tasks. Optimizing such temporal parameters, for example, through gradient descent, gives rise to a temporal architecture for different problems. As has been shown in machine learning, to reduce the cost of optimization, architectural biases can be applied, in this case in the temporal domain. Such inductive biases in temporal parameters have been found in neuroscience studies, highlighting a hierarchy of temporal structure and input representation in different layers of the cortex. Motivated by this, we propose to impose a hierarchy of temporal representation in the hidden layers of SNNs, highlighting that such an inductive bias improves their performance. We demonstrate the positive effects of temporal hierarchy in the time constants of feed-forward SNNs applied to temporal tasks (Multi-Time-Scale XOR and Keyword Spotting, with a benefit of up to 4.1% in classification accuracy). Moreover, we show that such architectural biases, i.e. hierarchy of time constants, naturally emerge when optimizing the time constants through gradient descent, initialized as homogeneous values. We further pursue this proposal in temporal convolutional SNNs, by introducing the hierarchical bias in the size and dilation of temporal kernels, giving rise to competitive results in popular temporal spike-based datasets.",
      "paper_authors": [
        "Filippo Moro",
        "Pau Vilimelis Aceituno",
        "Laura Kriener",
        "Melika Payvand"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-07-26",
      "update_time": "2024-07-26",
      "comments": "16 pages, 9 figures, pre-print",
      "repo_url": "#"
    },
    "2407.18625": {
      "paper_id": "2407.18625v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.18625v1",
      "paper_key": "2407.18625",
      "paper_title": "Topology Optimization of Random Memristors for Input-Aware Dynamic SNN",
      "paper_url": "http://arxiv.org/abs/2407.18625v1",
      "paper_abstract": "There is unprecedented development in machine learning, exemplified by recent large language models and world simulators, which are artificial neural networks running on digital computers. However, they still cannot parallel human brains in terms of energy efficiency and the streamlined adaptability to inputs of different difficulties, due to differences in signal representation, optimization, run-time reconfigurability, and hardware architecture. To address these fundamental challenges, we introduce pruning optimization for input-aware dynamic memristive spiking neural network (PRIME). Signal representation-wise, PRIME employs leaky integrate-and-fire neurons to emulate the brain's inherent spiking mechanism. Drawing inspiration from the brain's structural plasticity, PRIME optimizes the topology of a random memristive spiking neural network without expensive memristor conductance fine-tuning. For runtime reconfigurability, inspired by the brain's dynamic adjustment of computational depth, PRIME employs an input-aware dynamic early stop policy to minimize latency during inference, thereby boosting energy efficiency without compromising performance. Architecture-wise, PRIME leverages memristive in-memory computing, mirroring the brain and mitigating the von Neumann bottleneck. We validated our system using a 40 nm 256 Kb memristor-based in-memory computing macro on neuromorphic image classification and image inpainting. Our results demonstrate the classification accuracy and Inception Score are comparable to the software baseline, while achieving maximal 62.50-fold improvements in energy efficiency, and maximal 77.0% computational load savings. The system also exhibits robustness against stochastic synaptic noise of analogue memristors. Our software-hardware co-designed model paves the way to future brain-inspired neuromorphic computing with brain-like energy efficiency and adaptivity.",
      "paper_authors": [
        "Bo Wang",
        "Shaocong Wang",
        "Ning Lin",
        "Yi Li",
        "Yifei Yu",
        "Yue Zhang",
        "Jichang Yang",
        "Xiaoshan Wu",
        "Yangu He",
        "Songqi Wang",
        "Rui Chen",
        "Guoqi Li",
        "Xiaojuan Qi",
        "Zhongrui Wang",
        "Dashan Shang"
      ],
      "primary_category": "cs.ET",
      "publish_time": "2024-07-26",
      "update_time": "2024-07-26",
      "comments": "15 pages, 5 figures",
      "repo_url": "#"
    },
    "2408.05360": {
      "paper_id": "2408.05360v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.05360v1",
      "paper_key": "2408.05360",
      "paper_title": "On Noise Resiliency of Neuromorphic Inferential Communication in Microgrids",
      "paper_url": "http://arxiv.org/abs/2408.05360v1",
      "paper_abstract": "Neuromorphic computing leveraging spiking neural network has emerged as a promising solution to tackle the security and reliability challenges with the conventional cyber-physical infrastructure of microgrids. Its event-driven paradigm facilitates promising prospect in resilient and energy-efficient coordination among power electronic converters. However, different from biological neurons that are focused in the literature, microgrids exhibit distinct architectures and features, implying potentially diverse adaptability in its capabilities to dismiss information transfer, which remains largely unrevealed. One of the biggest drawbacks in the information transfer theory is the impact of noise in the signaling accuracy. Hence, this article hereby explores the noise resiliency of neuromorphic inferential communication in microgrids through case studies and underlines potential challenges and solutions as extensions beyond the results, thus offering insights for its implementation in real-world scenarios.",
      "paper_authors": [
        "Yubo Song",
        "Subham Sahoo",
        "Xiaoguang Diao"
      ],
      "primary_category": "cs.ET",
      "publish_time": "2024-07-25",
      "update_time": "2024-07-25",
      "comments": "This manuscript has been accepted for publication in 2024 IEEE Energy\n  Conversion Congress and Exposition (ECCE)",
      "repo_url": "#"
    },
    "2407.17672": {
      "paper_id": "2407.17672v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.17672v2",
      "paper_key": "2407.17672",
      "paper_title": "Spiking Neural Networks in Vertical Federated Learning: Performance Trade-offs",
      "paper_url": "http://arxiv.org/abs/2407.17672v2",
      "paper_abstract": "Federated machine learning enables model training across multiple clients while maintaining data privacy. Vertical Federated Learning (VFL) specifically deals with instances where the clients have different feature sets of the same samples. As federated learning models aim to improve efficiency and adaptability, innovative neural network architectures like Spiking Neural Networks (SNNs) are being leveraged to enable fast and accurate processing at the edge. SNNs, known for their efficiency over Artificial Neural Networks (ANNs), have not been analyzed for their applicability in VFL, thus far. In this paper, we investigate the benefits and trade-offs of using SNN models in a vertical federated learning setting. We implement two different federated learning architectures -- with model splitting and without model splitting -- that have different privacy and performance implications. We evaluate the setup using CIFAR-10 and CIFAR-100 benchmark datasets along with SNN implementations of VGG9 and ResNET classification models. Comparative evaluations demonstrate that the accuracy of SNN models is comparable to that of traditional ANNs for VFL applications, albeit significantly more energy efficient.",
      "paper_authors": [
        "Maryam Abbasihafshejani",
        "Anindya Maiti",
        "Murtuza Jadliwala"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-07-24",
      "update_time": "2024-08-13",
      "comments": null,
      "repo_url": "#"
    },
    "2407.17305": {
      "paper_id": "2407.17305v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.17305v1",
      "paper_key": "2407.17305",
      "paper_title": "Continual Learning in Bio-plausible Spiking Neural Networks with Hebbian and Spike Timing Dependent Plasticity: A Survey and Perspective",
      "paper_url": "http://arxiv.org/abs/2407.17305v1",
      "paper_abstract": "Recently, the use bio-plausible learning techniques such as Hebbian and Spike-Timing-Dependent Plasticity (STDP) have drawn significant attention for the design of compute-efficient AI systems that can continuously learn on-line at the edge. A key differentiating factor regarding this emerging class of neuromorphic continual learning system lies in the fact that learning must be carried using a data stream received in its natural order, as opposed to conventional gradient-based offline training where a static training dataset is assumed available a priori and randomly shuffled to make the training set independent and identically distributed (i.i.d). In contrast, the emerging class of neuromorphic continual learning systems covered in this survey must learn to integrate new information on the fly in a non-i.i.d manner, which makes these systems subject to catastrophic forgetting. In order to build the next generation of neuromorphic AI systems that can continuously learn at the edge, a growing number of research groups are studying the use of bio-plausible Hebbian neural network architectures and Spiking Neural Networks (SNNs) equipped with STDP learning. However, since this research field is still emerging, there is a need for providing a holistic view of the different approaches proposed in literature so far. To this end, this survey covers a number of recent works in the field of neuromorphic continual learning; provides background theory to help interested researchers to quickly learn the key concepts; and discusses important future research questions in light of the different works covered in this paper. It is hoped that this survey will contribute towards future research in the field of neuromorphic continual learning.",
      "paper_authors": [
        "Ali Safa"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-07-24",
      "update_time": "2024-07-24",
      "comments": null,
      "repo_url": "#"
    },
    "2407.16398": {
      "paper_id": "2407.16398v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.16398v1",
      "paper_key": "2407.16398",
      "paper_title": "A Quantum Leaky Integrate-and-Fire Spiking Neuron and Network",
      "paper_url": "http://arxiv.org/abs/2407.16398v1",
      "paper_abstract": "Quantum machine learning is in a period of rapid development and discovery, however it still lacks the resources and diversity of computational models of its classical complement. With the growing difficulties of classical models requiring extreme hardware and power solutions, and quantum models being limited by noisy intermediate-scale quantum (NISQ) hardware, there is an emerging opportunity to solve both problems together. Here we introduce a new software model for quantum neuromorphic computing -- a quantum leaky integrate-and-fire (QLIF) neuron, implemented as a compact high-fidelity quantum circuit, requiring only 2 rotation gates and no CNOT gates. We use these neurons as building blocks in the construction of a quantum spiking neural network (QSNN), and a quantum spiking convolutional neural network (QSCNN), as the first of their kind. We apply these models to the MNIST, Fashion-MNIST, and KMNIST datasets for a full comparison with other classical and quantum models. We find that the proposed models perform competitively, with comparative accuracy, with efficient scaling and fast computation in classical simulation as well as on quantum devices.",
      "paper_authors": [
        "Dean Brand",
        "Francesco Petruccione"
      ],
      "primary_category": "quant-ph",
      "publish_time": "2024-07-23",
      "update_time": "2024-07-23",
      "comments": "9 pages, 6 figures, 1 table",
      "repo_url": "https://github.com/deanbrand/QSNN"
    },
    "2408.03951": {
      "paper_id": "2408.03951v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.03951v1",
      "paper_key": "2408.03951",
      "paper_title": "Harmonic fractal transformation, 4R-regeneration and noise shaping for ultra wide-band reception in FitzHugh-Nagumo neuronal model",
      "paper_url": "http://arxiv.org/abs/2408.03951v1",
      "paper_abstract": "Human hearing range significantly surpasses the typical neuronal spiking frequency. Yet, neurons with their modest frequency range not only efficiently receive and process multiple orders higher frequency signals, but also demonstrate remarkable stability and adaptability to frequency variations in brain functional connectivity. Ability to process signals beyond the limitations of the receiver temporal or frequency (bandwidth) resolution is highly desirable yet requires complex design architectures. Using the FitzHugh-Nagumo model we reveal the harmonic fractal transformation of frequency and bandwidth, which enables the Nyquist rate integer (for low frequencies) and sub-integer (for high frequencies) multiplication. We also demonstrate for the first time that noise shaping can be achieved in a simple RLC-circuit without a requirement of a delay line. The discovered effect presents a novel regeneration type - 4R: re-amplifying, re-shaping, re-timing, and re-modulating and due to the fractal nature of transformation offers a remarkable regenerative efficiency. The effect is a generalization of phase locking to non-periodic encoded signals. The discovered physical mechanism explains how using neuronal functionality one can receive and process signals over an ultra-wide band (below or higher the spiking neuronal range by multiple orders) and below the noise floor.",
      "paper_authors": [
        "Mariia Sorokina"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2024-07-22",
      "update_time": "2024-07-22",
      "comments": null,
      "repo_url": "#"
    },
    "2408.03336": {
      "paper_id": "2408.03336v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.03336v1",
      "paper_key": "2408.03336",
      "paper_title": "Few-Shot Transfer Learning for Individualized Braking Intent Detection on Neuromorphic Hardware",
      "paper_url": "http://arxiv.org/abs/2408.03336v1",
      "paper_abstract": "Objective: This work explores use of a few-shot transfer learning method to train and implement a convolutional spiking neural network (CSNN) on a BrainChip Akida AKD1000 neuromorphic system-on-chip for developing individual-level, instead of traditionally used group-level, models using electroencephalographic data. The efficacy of the method is studied on an advanced driver assist system related task of predicting braking intention. Main Results: Efficacy of the above methodology to develop individual specific braking intention predictive models by rapidly adapting the group-level model in as few as three training epochs while achieving at least 90% accuracy, true positive rate and true negative rate is presented. Further, results show an energy reduction of over 97% with only a 1.3x increase in latency when using the Akida AKD1000 processor for network inference compared to an Intel Xeon CPU. Similar results were obtained in a subsequent ablation study using a subset of five out of 19 channels. Significance: Especially relevant to real-time applications, this work presents an energy-efficient, few-shot transfer learning method that is implemented on a neuromorphic processor capable of training a CSNN as new data becomes available, operating conditions change, or to customize group-level models to yield personalized models unique to each individual.",
      "paper_authors": [
        "Nathan Lutes",
        "Venkata Sriram Siddhardh Nadendla",
        "K. Krishnamurthy"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-07-21",
      "update_time": "2024-07-21",
      "comments": "Journal of NeuroEngineering Submission",
      "repo_url": "#"
    },
    "2407.15152": {
      "paper_id": "2407.15152v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.15152v2",
      "paper_key": "2407.15152",
      "paper_title": "SNNGX: Securing Spiking Neural Networks with Genetic XOR Encryption on RRAM-based Neuromorphic Accelerator",
      "paper_url": "http://arxiv.org/abs/2407.15152v2",
      "paper_abstract": "Biologically plausible Spiking Neural Networks (SNNs), characterized by spike sparsity, are growing tremendous attention over intellectual edge devices and critical bio-medical applications as compared to artificial neural networks (ANNs). However, there is a considerable risk from malicious attempts to extract white-box information (i.e., weights) from SNNs, as attackers could exploit well-trained SNNs for profit and white-box adversarial concerns. There is a dire need for intellectual property (IP) protective measures. In this paper, we present a novel secure software-hardware co-designed RRAM-based neuromorphic accelerator for protecting the IP of SNNs. Software-wise, we design a tailored genetic algorithm with classic XOR encryption to target the least number of weights that need encryption. From a hardware perspective, we develop a low-energy decryption module, meticulously designed to provide zero decryption latency. Extensive results from various datasets, including NMNIST, DVSGesture, EEGMMIDB, Braille Letter, and SHD, demonstrate that our proposed method effectively secures SNNs by encrypting a minimal fraction of stealthy weights, only 0.00005% to 0.016% weight bits. Additionally, it achieves a substantial reduction in energy consumption, ranging from x59 to x6780, and significantly lowers decryption latency, ranging from x175 to x4250. Moreover, our method requires as little as one sample per class in dataset for encryption and addresses hessian/gradient-based search insensitive problems. This strategy offers a highly efficient and flexible solution for securing SNNs in diverse applications.",
      "paper_authors": [
        "Kwunhang Wong",
        "Songqi Wang",
        "Wei Huang",
        "Xinyuan Zhang",
        "Yangu He",
        "Karl M. H. Lai",
        "Yuzhong Jiao",
        "Ning Lin",
        "Xiaojuan Qi",
        "Xiaoming Chen",
        "Zhongrui Wang"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-07-21",
      "update_time": "2024-08-26",
      "comments": "International Conference on Computer-Aided Design 2024",
      "repo_url": "https://github.com/u3556440/SNNGX_qSNN_encryption"
    },
    "2407.14883": {
      "paper_id": "2407.14883v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.14883v2",
      "paper_key": "2407.14883",
      "paper_title": "Inferring Ingrained Remote Information in AC Power Flows Using Neuromorphic Modality Regime",
      "paper_url": "http://arxiv.org/abs/2407.14883v2",
      "paper_abstract": "In this paper, we infer remote measurements such as remote voltages and currents online with change in AC power flows using spiking neural network (SNN) as grid-edge technology for efficient coordination of power electronic converters. This work unifies power and information as a means of data normalization using a multi-modal regime in the form of spikes using energy-efficient neuromorphic learning and event-driven asynchronous data collection. Firstly, we organize the synchronous real-valued measurements at each edge and translate them into asynchronous spike-based events to collect sparse data for training of SNN at each edge. Instead of relying on error-dependent supervised data-driven learning theory, we exploit the latency-driven unsupervised Hebbian learning rule to obtain modulation pulses for switching of power electronic converters that can now comprehend grid disturbances locally and adapt their operation without requiring explicit infrastructure for global coordination. Not only does this philosophy block exogenous path arrival for cyber attackers by dismissing the cyber layer, it also entails converter adaptation to system reconfiguration and parameter mismatch issues. We conclude this work by validating its energy-efficient and effective online learning performance under various scenarios in different system sizes, including modified IEEE 14-bus system and under experimental conditions.",
      "paper_authors": [
        "Xiaoguang Diao",
        "Yubo Song",
        "Subham Sahoo"
      ],
      "primary_category": "eess.SY",
      "publish_time": "2024-07-20",
      "update_time": "2024-08-09",
      "comments": "The manuscript has been accepted for publication in the Proceedings\n  of 2024 IEEE International Conference on Communications, Control, and\n  Computing Technologies for Smart Grids (SmartGridComm 2024)",
      "repo_url": "#"
    },
    "2407.14097": {
      "paper_id": "2407.14097v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.14097v1",
      "paper_key": "2407.14097",
      "paper_title": "On the Robustness of Fully-Spiking Neural Networks in Open-World Scenarios using Forward-Only Learning Algorithms",
      "paper_url": "http://arxiv.org/abs/2407.14097v1",
      "paper_abstract": "In the last decade, Artificial Intelligence (AI) models have rapidly integrated into production pipelines propelled by their excellent modeling performance. However, the development of these models has not been matched by advancements in algorithms ensuring their safety, failing to guarantee robust behavior against Out-of-Distribution (OoD) inputs outside their learning domain. Furthermore, there is a growing concern with the sustainability of AI models and their required energy consumption in both training and inference phases. To mitigate these issues, this work explores the use of the Forward-Forward Algorithm (FFA), a biologically plausible alternative to Backpropagation, adapted to the spiking domain to enhance the overall energy efficiency of the model. By capitalizing on the highly expressive topology emerging from the latent space of models trained with FFA, we develop a novel FF-SCP algorithm for OoD Detection. Our approach measures the likelihood of a sample belonging to the in-distribution (ID) data by using the distance from the latent representation of samples to class-representative manifolds. Additionally, to provide deeper insights into our OoD pipeline, we propose a gradient-free attribution technique that highlights the features of a sample pushing it away from the distribution of any class. Multiple experiments using our spiking FFA adaptation demonstrate that the achieved accuracy levels are comparable to those seen in analog networks trained via back-propagation. Furthermore, OoD detection experiments on multiple datasets prove that FF-SCP outperforms avant-garde OoD detectors within the spiking domain in terms of several metrics used in this area. We also present a qualitative analysis of our explainability technique, exposing the precision by which the method detects OoD features, such as embedded artifacts or missing regions.",
      "paper_authors": [
        "Erik B. Terres-Escudero",
        "Javier Del Ser",
        "Aitor Mart\u00ednez-Seras",
        "Pablo Garcia-Bringas"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-07-19",
      "update_time": "2024-07-19",
      "comments": null,
      "repo_url": "https://github.com/AnonymousSquirrel316/FFA_OOD"
    },
    "2407.14073": {
      "paper_id": "2407.14073v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.14073v3",
      "paper_key": "2407.14073",
      "paper_title": "LoAS: Fully Temporal-Parallel Dataflow for Dual-Sparse Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2407.14073v3",
      "paper_abstract": "Spiking Neural Networks (SNNs) have gained significant research attention in the last decade due to their potential to drive resource-constrained edge devices. Though existing SNN accelerators offer high efficiency in processing sparse spikes with dense weights, opportunities are less explored in SNNs with sparse weights, i.e., dual-sparsity. In this work, we study the acceleration of dual-sparse SNNs, focusing on their core operation, sparse-matrix-sparse-matrix multiplication (spMspM). We observe that naively running a dual-sparse SNN on existing spMspM accelerators designed for dual-sparse Artificial Neural Networks (ANNs) exhibits sub-optimal efficiency. The main challenge is that processing timesteps, a natural property of SNNs, introduces an extra loop to ANN spMspM, leading to longer latency and more memory traffic. To address the problem, we propose a fully temporal-parallel (FTP) dataflow, which minimizes both data movement across timesteps and the end-to-end latency of dual-sparse SNNs. To maximize the efficiency of FTP dataflow, we propose an FTP-friendly spike compression mechanism that efficiently compresses single-bit spikes and ensures contiguous memory access. We further propose an FTP-friendly inner-join circuit that can lower the cost of the expensive prefix-sum circuits with almost no throughput penalty. All the above techniques for FTP dataflow are encapsulated in LoAS, a Low-latency inference Accelerator for dual-sparse SNNs. With FTP dataflow, compression, and inner-join, running dual-sparse SNN workloads on LoAS demonstrates significant speedup (up to $8.51\\times$) and energy reduction (up to $3.68\\times$) compared to running it on prior dual-sparse accelerators.",
      "paper_authors": [
        "Ruokai Yin",
        "Youngeun Kim",
        "Di Wu",
        "Priyadarshini Panda"
      ],
      "primary_category": "cs.AR",
      "publish_time": "2024-07-19",
      "update_time": "2024-09-01",
      "comments": "Accepted to MICRO 2024. Will update with the camera-ready version\n  once ready. (Github: https://github.com/RuokaiYin/LoAS)",
      "repo_url": "https://github.com/ruokaiyin/loas"
    },
    "2407.13534": {
      "paper_id": "2407.13534v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.13534v1",
      "paper_key": "2407.13534",
      "paper_title": "Accurate Mapping of RNNs on Neuromorphic Hardware with Adaptive Spiking Neurons",
      "paper_url": "http://arxiv.org/abs/2407.13534v1",
      "paper_abstract": "Thanks to their parallel and sparse activity features, recurrent neural networks (RNNs) are well-suited for hardware implementation in low-power neuromorphic hardware. However, mapping rate-based RNNs to hardware-compatible spiking neural networks (SNNs) remains challenging. Here, we present a ${\\Sigma}{\\Delta}$-low-pass RNN (lpRNN): an RNN architecture employing an adaptive spiking neuron model that encodes signals using ${\\Sigma}{\\Delta}$-modulation and enables precise mapping. The ${\\Sigma}{\\Delta}$-neuron communicates analog values using spike timing, and the dynamics of the lpRNN are set to match typical timescales for processing natural signals, such as speech. Our approach integrates rate and temporal coding, offering a robust solution for the efficient and accurate conversion of RNNs to SNNs. We demonstrate the implementation of the lpRNN on Intel's neuromorphic research chip Loihi, achieving state-of-the-art classification results on audio benchmarks using 3-bit weights. These results call for a deeper investigation of recurrency and adaptation in event-based systems, which may lead to insights for edge computing applications where power-efficient real-time inference is required.",
      "paper_authors": [
        "Gauthier Boeshertz",
        "Giacomo Indiveri",
        "Manu Nair",
        "Alpha Renner"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-07-18",
      "update_time": "2024-07-18",
      "comments": "5 pages, 3 figures, accepted at ICONS 2024",
      "repo_url": "#"
    },
    "2408.00794": {
      "paper_id": "2408.00794v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.00794v1",
      "paper_key": "2408.00794",
      "paper_title": "CCSRP: Robust Pruning of Spiking Neural Networks through Cooperative Coevolution",
      "paper_url": "http://arxiv.org/abs/2408.00794v1",
      "paper_abstract": "Spiking neural networks (SNNs) have shown promise in various dynamic visual tasks, yet those ready for practical deployment often lack the compactness and robustness essential in resource-limited and safety-critical settings. Prior research has predominantly concentrated on enhancing the compactness or robustness of artificial neural networks through strategies like network pruning and adversarial training, with little exploration into similar methodologies for SNNs. Robust pruning of SNNs aims to reduce computational overhead while preserving both accuracy and robustness. Current robust pruning approaches generally necessitate expert knowledge and iterative experimentation to establish suitable pruning criteria or auxiliary modules, thus constraining their broader application. Concurrently, evolutionary algorithms (EAs) have been employed to automate the pruning of artificial neural networks, delivering remarkable outcomes yet overlooking the aspect of robustness. In this work, we propose CCSRP, an innovative robust pruning method for SNNs, underpinned by cooperative co-evolution. Robust pruning is articulated as a tri-objective optimization challenge, striving to balance accuracy, robustness, and compactness concurrently, resolved through a cooperative co-evolutionary pruning framework that independently prunes filters across layers using EAs. Our experiments on CIFAR-10 and SVHN demonstrate that CCSRP can match or exceed the performance of the latest methodologies.",
      "paper_authors": [
        "Zichen Song",
        "Jiakang Li",
        "Songning Lai",
        "Sitan Huang"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-07-18",
      "update_time": "2024-07-18",
      "comments": null,
      "repo_url": "#"
    },
    "2408.00788": {
      "paper_id": "2408.00788v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.00788v1",
      "paper_key": "2408.00788",
      "paper_title": "SpikeVoice: High-Quality Text-to-Speech Via Efficient Spiking Neural Network",
      "paper_url": "http://arxiv.org/abs/2408.00788v1",
      "paper_abstract": "Brain-inspired Spiking Neural Network (SNN) has demonstrated its effectiveness and efficiency in vision, natural language, and speech understanding tasks, indicating their capacity to \"see\", \"listen\", and \"read\". In this paper, we design \\textbf{SpikeVoice}, which performs high-quality Text-To-Speech (TTS) via SNN, to explore the potential of SNN to \"speak\". A major obstacle to using SNN for such generative tasks lies in the demand for models to grasp long-term dependencies. The serial nature of spiking neurons, however, leads to the invisibility of information at future spiking time steps, limiting SNN models to capture sequence dependencies solely within the same time step. We term this phenomenon \"partial-time dependency\". To address this issue, we introduce Spiking Temporal-Sequential Attention STSA in the SpikeVoice. To the best of our knowledge, SpikeVoice is the first TTS work in the SNN field. We perform experiments using four well-established datasets that cover both Chinese and English languages, encompassing scenarios with both single-speaker and multi-speaker configurations. The results demonstrate that SpikeVoice can achieve results comparable to Artificial Neural Networks (ANN) with only 10.5 energy consumption of ANN.",
      "paper_authors": [
        "Kexin Wang",
        "Jiahong Zhang",
        "Yong Ren",
        "Man Yao",
        "Di Shang",
        "Bo Xu",
        "Guoqi Li"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-07-17",
      "update_time": "2024-07-17",
      "comments": "9 pages",
      "repo_url": "#"
    },
    "2407.12516": {
      "paper_id": "2407.12516v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.12516v1",
      "paper_key": "2407.12516",
      "paper_title": "Online Pseudo-Zeroth-Order Training of Neuromorphic Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2407.12516v1",
      "paper_abstract": "Brain-inspired neuromorphic computing with spiking neural networks (SNNs) is a promising energy-efficient computational approach. However, successfully training SNNs in a more biologically plausible and neuromorphic-hardware-friendly way is still challenging. Most recent methods leverage spatial and temporal backpropagation (BP), not adhering to neuromorphic properties. Despite the efforts of some online training methods, tackling spatial credit assignments by alternatives with comparable performance as spatial BP remains a significant problem. In this work, we propose a novel method, online pseudo-zeroth-order (OPZO) training. Our method only requires a single forward propagation with noise injection and direct top-down signals for spatial credit assignment, avoiding spatial BP's problem of symmetric weights and separate phases for layer-by-layer forward-backward propagation. OPZO solves the large variance problem of zeroth-order methods by the pseudo-zeroth-order formulation and momentum feedback connections, while having more guarantees than random feedback. Combining online training, OPZO can pave paths to on-chip SNN training. Experiments on neuromorphic and static datasets with fully connected and convolutional networks demonstrate the effectiveness of OPZO with similar performance compared with spatial BP, as well as estimated low training costs.",
      "paper_authors": [
        "Mingqing Xiao",
        "Qingyan Meng",
        "Zongpeng Zhang",
        "Di He",
        "Zhouchen Lin"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-07-17",
      "update_time": "2024-07-17",
      "comments": null,
      "repo_url": "#"
    },
    "2407.12261": {
      "paper_id": "2407.12261v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.12261v1",
      "paper_key": "2407.12261",
      "paper_title": "Voltage-Controlled Magnetoelectric Devices for Neuromorphic Diffusion Process",
      "paper_url": "http://arxiv.org/abs/2407.12261v1",
      "paper_abstract": "Stochastic diffusion processes are pervasive in nature, from the seemingly erratic Brownian motion to the complex interactions of synaptically-coupled spiking neurons. Recently, drawing inspiration from Langevin dynamics, neuromorphic diffusion models were proposed and have become one of the major breakthroughs in the field of generative artificial intelligence. Unlike discriminative models that have been well developed to tackle classification or regression tasks, diffusion models as well as other generative models such as ChatGPT aim at creating content based upon contexts learned. However, the more complex algorithms of these models result in high computational costs using today's technologies, creating a bottleneck in their efficiency, and impeding further development. Here, we develop a spintronic voltage-controlled magnetoelectric memory hardware for the neuromorphic diffusion process. The in-memory computing capability of our spintronic devices goes beyond current Von Neumann architecture, where memory and computing units are separated. Together with the non-volatility of magnetic memory, we can achieve high-speed and low-cost computing, which is desirable for the increasing scale of generative models in the current era. We experimentally demonstrate that the hardware-based true random diffusion process can be implemented for image generation and achieve comparable image quality to software-based training as measured by the Frechet inception distance (FID) score, achieving ~10^3 better energy-per-bit-per-area over traditional hardware.",
      "paper_authors": [
        "Yang Cheng",
        "Qingyuan Shu",
        "Albert Lee",
        "Haoran He",
        "Ivy Zhu",
        "Haris Suhail",
        "Minzhang Chen",
        "Renhe Chen",
        "Zirui Wang",
        "Hantao Zhang",
        "Chih-Yao Wang",
        "Shan-Yi Yang",
        "Yu-Chen Hsin",
        "Cheng-Yi Shih",
        "Hsin-Han Lee",
        "Ran Cheng",
        "Sudhakar Pamarti",
        "Xufeng Kou",
        "Kang L. Wang"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-07-17",
      "update_time": "2024-07-17",
      "comments": null,
      "repo_url": "#"
    },
    "2407.10028": {
      "paper_id": "2407.10028v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.10028v1",
      "paper_key": "2407.10028",
      "paper_title": "Time-Integrated Spike-Timing-Dependent-Plasticity",
      "paper_url": "http://arxiv.org/abs/2407.10028v1",
      "paper_abstract": "In this work, we propose time-integrated spike-timing-dependent plasticity (TI-STDP), a mathematical model of synaptic plasticity that allows spiking neural networks to continuously adapt to sensory input streams in an unsupervised fashion. Notably, we theoretically establish and formally prove key properties related to the synaptic adjustment mechanics that underwrite TI-STDP. Empirically, we demonstrate the efficacy of TI-STDP in simulations of jointly learning deeper spiking neural networks that process input digit pixel patterns, at both full image and patch-levels, comparing to two powerful historical instantations of STDP; trace-based STDP (TR-STDP) and event-based post-synaptic STDP (EV-STDP). Usefully, we demonstrate that not only are all forms of STDP capable of meaningfully adapting the synaptic efficacies of a multi-layer biophysical architecture, but that TI-STDP is notably able to do so without requiring the tracking of a large window of pre- and post-synaptic spike timings, the maintenance of additional parameterized traces, or the restriction of synaptic plasticity changes to occur within very narrow windows of time. This means that our findings show that TI-STDP can efficiently embody the benefits of models such as canonical STDP, TR-STDP, and EV-STDP without their costs or drawbacks. Usefully, our results further demonstrate the promise of using a spike-correlation scheme such as TI-STDP in conducting credit assignment in discrete pulse-based neuromorphic models, particularly those than acquire a lower-level distributed representation jointly with an upper-level, more abstract representation that self-organizes to cluster based on inherent cross-pattern similarities. We further demonstrate TI-STDP's effectiveness in adapting a simple neuronal circuit that learns a simple bi-level, part-whole hierarchy from sensory input patterns.",
      "paper_authors": [
        "William Gebhardt",
        "Alexander G. Ororbia"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2024-07-13",
      "update_time": "2024-07-13",
      "comments": null,
      "repo_url": "#"
    },
    "2407.09260": {
      "paper_id": "2407.09260v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.09260v1",
      "paper_key": "2407.09260",
      "paper_title": "Evaluation of Encoding Schemes on Ubiquitous Sensor Signal for Spiking Neural Network",
      "paper_url": "http://arxiv.org/abs/2407.09260v1",
      "paper_abstract": "Spiking neural networks (SNNs), a brain-inspired computing paradigm, are emerging for their inference performance, particularly in terms of energy efficiency and latency attributed to the plasticity in signal processing. To deploy SNNs in ubiquitous computing systems, signal encoding of sensors is crucial for achieving high accuracy and robustness. Using inertial sensor readings for gym activity recognition as a case study, this work comprehensively evaluates four main encoding schemes and deploys the corresponding SNN on the neuromorphic processor Loihi2 for post-deployment encoding assessment. Rate encoding, time-to-first-spike encoding, binary encoding, and delta modulation are evaluated using metrics like average fire rate, signal-to-noise ratio, classification accuracy, robustness, and inference latency and energy. In this case study, the time-to-first-spike encoding required the lowest firing rate (2%) and achieved a comparative accuracy (89%), although it was the least robust scheme against error spikes (over 20% accuracy drop with 0.1 noisy spike rate). Rate encoding with optimal value-to-probability mapping achieved the highest accuracy (91.7%). Binary encoding provided a balance between information reconstruction and noise resistance. Multi-threshold delta modulation showed the best robustness, with only a 0.7% accuracy drop at a 0.1 noisy spike rate. This work serves researchers in selecting the best encoding scheme for SNN-based ubiquitous sensor signal processing, tailored to specific performance requirements.",
      "paper_authors": [
        "Sizhen Bian",
        "Elisa Donati",
        "Michele Magno"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2024-07-12",
      "update_time": "2024-07-12",
      "comments": null,
      "repo_url": "#"
    },
    "2407.09083": {
      "paper_id": "2407.09083v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.09083v2",
      "paper_key": "2407.09083",
      "paper_title": "BKDSNN: Enhancing the Performance of Learning-based Spiking Neural Networks Training with Blurred Knowledge Distillation",
      "paper_url": "http://arxiv.org/abs/2407.09083v2",
      "paper_abstract": "Spiking neural networks (SNNs), which mimic biological neural system to convey information via discrete spikes, are well known as brain-inspired models with excellent computing efficiency. By utilizing the surrogate gradient estimation for discrete spikes, learning-based SNN training methods that can achieve ultra-low inference latency (number of time-step) emerge recently. Nevertheless, due to the difficulty in deriving precise gradient estimation for discrete spikes using learning-based method, a distinct accuracy gap persists between SNN and its artificial neural networks (ANNs) counterpart. To address the aforementioned issue, we propose a blurred knowledge distillation (BKD) technique, which leverages random blurred SNN feature to restore and imitate the ANN feature. Note that, our BKD is applied upon the feature map right before the last layer of SNN, which can also mix with prior logits-based knowledge distillation for maximized accuracy boost. To our best knowledge, in the category of learning-based methods, our work achieves state-of-the-art performance for training SNNs on both static and neuromorphic datasets. On ImageNet dataset, BKDSNN outperforms prior best results by 4.51% and 0.93% with the network topology of CNN and Transformer respectively.",
      "paper_authors": [
        "Zekai Xu",
        "Kang You",
        "Qinghai Guo",
        "Xiang Wang",
        "Zhezhi He"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-07-12",
      "update_time": "2024-07-15",
      "comments": "accepted by European Conference on Computer Vision (ECCV) 2024",
      "repo_url": "https://github.com/intelligent-computing-research-group/bkdsnn"
    },
    "2407.08704": {
      "paper_id": "2407.08704v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.08704v1",
      "paper_key": "2407.08704",
      "paper_title": "Towards Efficient Deployment of Hybrid SNNs on Neuromorphic and Edge AI Hardware",
      "paper_url": "http://arxiv.org/abs/2407.08704v1",
      "paper_abstract": "This paper explores the synergistic potential of neuromorphic and edge computing to create a versatile machine learning (ML) system tailored for processing data captured by dynamic vision sensors. We construct and train hybrid models, blending spiking neural networks (SNNs) and artificial neural networks (ANNs) using PyTorch and Lava frameworks. Our hybrid architecture integrates an SNN for temporal feature extraction and an ANN for classification. We delve into the challenges of deploying such hybrid structures on hardware. Specifically, we deploy individual components on Intel's Neuromorphic Processor Loihi (for SNN) and Jetson Nano (for ANN). We also propose an accumulator circuit to transfer data from the spiking to the non-spiking domain. Furthermore, we conduct comprehensive performance analyses of hybrid SNN-ANN models on a heterogeneous system of neuromorphic and edge AI hardware, evaluating accuracy, latency, power, and energy consumption. Our findings demonstrate that the hybrid spiking networks surpass the baseline ANN model across all metrics and outperform the baseline SNN model in accuracy and latency.",
      "paper_authors": [
        "James Seekings",
        "Peyton Chandarana",
        "Mahsa Ardakani",
        "MohammadReza Mohammadi",
        "Ramtin Zand"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-07-11",
      "update_time": "2024-07-11",
      "comments": null,
      "repo_url": "#"
    },
    "2407.08362": {
      "paper_id": "2407.08362v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.08362v1",
      "paper_key": "2407.08362",
      "paper_title": "STAL: Spike Threshold Adaptive Learning Encoder for Classification of Pain-Related Biosignal Data",
      "paper_url": "http://arxiv.org/abs/2407.08362v1",
      "paper_abstract": "This paper presents the first application of spiking neural networks (SNNs) for the classification of chronic lower back pain (CLBP) using the EmoPain dataset. Our work has two main contributions. We introduce Spike Threshold Adaptive Learning (STAL), a trainable encoder that effectively converts continuous biosignals into spike trains. Additionally, we propose an ensemble of Spiking Recurrent Neural Network (SRNN) classifiers for the multi-stream processing of sEMG and IMU data. To tackle the challenges of small sample size and class imbalance, we implement minority over-sampling with weighted sample replacement during batch creation. Our method achieves outstanding performance with an accuracy of 80.43%, AUC of 67.90%, F1 score of 52.60%, and Matthews Correlation Coefficient (MCC) of 0.437, surpassing traditional rate-based and latency-based encoding methods. The STAL encoder shows superior performance in preserving temporal dynamics and adapting to signal characteristics. Importantly, our approach (STAL-SRNN) outperforms the best deep learning method in terms of MCC, indicating better balanced class prediction. This research contributes to the development of neuromorphic computing for biosignal analysis. It holds promise for energy-efficient, wearable solutions in chronic pain management.",
      "paper_authors": [
        "Freek Hens",
        "Mohammad Mahdi Dehshibi",
        "Leila Bagheriye",
        "Mahyar Shahsavari",
        "Ana Tajadura-Jim\u00e9nez"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-07-11",
      "update_time": "2024-07-11",
      "comments": null,
      "repo_url": "https://github.com/freek1/emopain-stl"
    },
    "2407.08356": {
      "paper_id": "2407.08356v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.08356v1",
      "paper_key": "2407.08356",
      "paper_title": "Event-based vision on FPGAs -- a survey",
      "paper_url": "http://arxiv.org/abs/2407.08356v1",
      "paper_abstract": "In recent years there has been a growing interest in event cameras, i.e. vision sensors that record changes in illumination independently for each pixel. This type of operation ensures that acquisition is possible in very adverse lighting conditions, both in low light and high dynamic range, and reduces average power consumption. In addition, the independent operation of each pixel results in low latency, which is desirable for robotic solutions. Nowadays, Field Programmable Gate Arrays (FPGAs), along with general-purpose processors (GPPs/CPUs) and programmable graphics processing units (GPUs), are popular architectures for implementing and accelerating computing tasks. In particular, their usefulness in the embedded vision domain has been repeatedly demonstrated over the past 30 years, where they have enabled fast data processing (even in real-time) and energy efficiency. Hence, the combination of event cameras and reconfigurable devices seems to be a good solution, especially in the context of energy-efficient real-time embedded systems. This paper gives an overview of the most important works, where FPGAs have been used in different contexts to process event data. It covers applications in the following areas: filtering, stereovision, optical flow, acceleration of AI-based algorithms (including spiking neural networks) for object classification, detection and tracking, and applications in robotics and inspection systems. Current trends and challenges for such systems are also discussed.",
      "paper_authors": [
        "Tomasz Kryjak"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-11",
      "update_time": "2024-07-11",
      "comments": "Accepted for the 2024 27th Euromicro Conference on Digital System\n  Design (DSD)",
      "repo_url": "#"
    },
    "2407.08130": {
      "paper_id": "2407.08130v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.08130v1",
      "paper_key": "2407.08130",
      "paper_title": "Spiking Tucker Fusion Transformer for Audio-Visual Zero-Shot Learning",
      "paper_url": "http://arxiv.org/abs/2407.08130v1",
      "paper_abstract": "The spiking neural networks (SNNs) that efficiently encode temporal sequences have shown great potential in extracting audio-visual joint feature representations. However, coupling SNNs (binary spike sequences) with transformers (float-point sequences) to jointly explore the temporal-semantic information still facing challenges. In this paper, we introduce a novel Spiking Tucker Fusion Transformer (STFT) for audio-visual zero-shot learning (ZSL). The STFT leverage the temporal and semantic information from different time steps to generate robust representations. The time-step factor (TSF) is introduced to dynamically synthesis the subsequent inference information. To guide the formation of input membrane potentials and reduce the spike noise, we propose a global-local pooling (GLP) which combines the max and average pooling operations. Furthermore, the thresholds of the spiking neurons are dynamically adjusted based on semantic and temporal cues. Integrating the temporal and semantic information extracted by SNNs and Transformers are difficult due to the increased number of parameters in a straightforward bilinear model. To address this, we introduce a temporal-semantic Tucker fusion module, which achieves multi-scale fusion of SNN and Transformer outputs while maintaining full second-order interactions. Our experimental results demonstrate the effectiveness of the proposed approach in achieving state-of-the-art performance in three benchmark datasets. The harmonic mean (HM) improvement of VGGSound, UCF101 and ActivityNet are around 15.4\\%, 3.9\\%, and 14.9\\%, respectively.",
      "paper_authors": [
        "Wenrui Li",
        "Penghong Wang",
        "Ruiqin Xiong",
        "Xiaopeng Fan"
      ],
      "primary_category": "cs.MM",
      "publish_time": "2024-07-11",
      "update_time": "2024-07-11",
      "comments": "Accepted by TIP",
      "repo_url": "#"
    },
    "2407.07020": {
      "paper_id": "2407.07020v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.07020v1",
      "paper_key": "2407.07020",
      "paper_title": "Less is More: Efficient Brain-Inspired Learning for Autonomous Driving Trajectory Prediction",
      "paper_url": "http://arxiv.org/abs/2407.07020v1",
      "paper_abstract": "Accurately and safely predicting the trajectories of surrounding vehicles is essential for fully realizing autonomous driving (AD). This paper presents the Human-Like Trajectory Prediction model (HLTP++), which emulates human cognitive processes to improve trajectory prediction in AD. HLTP++ incorporates a novel teacher-student knowledge distillation framework. The \"teacher\" model equipped with an adaptive visual sector, mimics the dynamic allocation of attention human drivers exhibit based on factors like spatial orientation, proximity, and driving speed. On the other hand, the \"student\" model focuses on real-time interaction and human decision-making, drawing parallels to the human memory storage mechanism. Furthermore, we improve the model's efficiency by introducing a new Fourier Adaptive Spike Neural Network (FA-SNN), allowing for faster and more precise predictions with fewer parameters. Evaluated using the NGSIM, HighD, and MoCAD benchmarks, HLTP++ demonstrates superior performance compared to existing models, which reduces the predicted trajectory error with over 11% on the NGSIM dataset and 25% on the HighD datasets. Moreover, HLTP++ demonstrates strong adaptability in challenging environments with incomplete input data. This marks a significant stride in the journey towards fully AD systems.",
      "paper_authors": [
        "Haicheng Liao",
        "Yongkang Li",
        "Zhenning Li",
        "Chengyue Wang",
        "Chunlin Tian",
        "Yuming Huang",
        "Zilin Bian",
        "Kaiqun Zhu",
        "Guofa Li",
        "Ziyuan Pu",
        "Jia Hu",
        "Zhiyong Cui",
        "Chengzhong Xu"
      ],
      "primary_category": "cs.AI",
      "publish_time": "2024-07-09",
      "update_time": "2024-07-09",
      "comments": "arXiv admin note: substantial text overlap with arXiv:2402.19251",
      "repo_url": "#"
    },
    "2407.07014": {
      "paper_id": "2407.07014v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.07014v2",
      "paper_key": "2407.07014",
      "paper_title": "An Attempt to Devise a Pairwise Ising-Type Maximum Entropy Model Integrated Cost Function for Optimizing SNN Deployment",
      "paper_url": "http://arxiv.org/abs/2407.07014v2",
      "paper_abstract": "The deployment process of a spiking neural network (SNN) often involves partitioning the neural network and mapping these partitions onto processing units within the neuromorphic hardware. Finding optimal deployment schemes is an NP-hard problem. Optimizing these schemes presents challenges, particular in devising computationally effective cost functions optimization objectives such as communication time consumption and energy efficiency. These objectives require consideration of network dynamics shaped by neuron activity patterns, demanding intricate mathematical analyses or simulations for integrating them into a cost model for SNN development.   Our approach focuses on network dynamics, which are hardware-independent and can be modeled separately from specific hardware configurations. We employ a pairwise Ising-type maximum entropy model, which is a model show effective in accurately capturing pairwise correlations among system components in a collaborative system. On top of this model, we incorporates hardware and network structure-specific factors to devise a cost function.   We conducted an extremely preliminary investigation using the SpiNNaker machine. We show that the ising model training can also be computationally complex. Currently, we lack sufficient evidence to substantiate the effectiveness of our proposed methods. Further efforts is needed to explore integrating network dynamics into SNN deployment.",
      "paper_authors": [
        "Wanhong Huang"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-07-09",
      "update_time": "2024-07-11",
      "comments": null,
      "repo_url": "#"
    },
    "2407.06452": {
      "paper_id": "2407.06452v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.06452v1",
      "paper_key": "2407.06452",
      "paper_title": "Exploiting Heterogeneity in Timescales for Sparse Recurrent Spiking Neural Networks for Energy-Efficient Edge Computing",
      "paper_url": "http://arxiv.org/abs/2407.06452v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) represent the forefront of neuromorphic computing, promising energy-efficient and biologically plausible models for complex tasks. This paper weaves together three groundbreaking studies that revolutionize SNN performance through the introduction of heterogeneity in neuron and synapse dynamics. We explore the transformative impact of Heterogeneous Recurrent Spiking Neural Networks (HRSNNs), supported by rigorous analytical frameworks and novel pruning methods like Lyapunov Noise Pruning (LNP). Our findings reveal how heterogeneity not only enhances classification performance but also reduces spiking activity, leading to more efficient and robust networks. By bridging theoretical insights with practical applications, this comprehensive summary highlights the potential of SNNs to outperform traditional neural networks while maintaining lower computational costs. Join us on a journey through the cutting-edge advancements that pave the way for the future of intelligent, energy-efficient neural computing.",
      "paper_authors": [
        "Biswadeep Chakraborty",
        "Saibal Mukhopadhyay"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-07-08",
      "update_time": "2024-07-08",
      "comments": "20 pages, 12 figures, 5 tables. arXiv admin note: text overlap with\n  arXiv:2211.04297, arXiv:2302.11618, arXiv:2403.03409",
      "repo_url": "#"
    },
    "2407.05739": {
      "paper_id": "2407.05739v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.05739v1",
      "paper_key": "2407.05739",
      "paper_title": "Multi-Bit Mechanism: A Novel Information Transmission Paradigm for Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2407.05739v1",
      "paper_abstract": "Since proposed, spiking neural networks (SNNs) gain recognition for their high performance, low power consumption and enhanced biological interpretability. However, while bringing these advantages, the binary nature of spikes also leads to considerable information loss in SNNs, ultimately causing performance degradation. We claim that the limited expressiveness of current binary spikes, resulting in substantial information loss, is the fundamental issue behind these challenges. To alleviate this, our research introduces a multi-bit information transmission mechanism for SNNs. This mechanism expands the output of spiking neurons from the original single bit to multiple bits, enhancing the expressiveness of the spikes and reducing information loss during the forward process, while still maintaining the low energy consumption advantage of SNNs. For SNNs, this represents a new paradigm of information transmission. Moreover, to further utilize the limited spikes, we extract effective signals from the previous layer to re-stimulate the neurons, thus encouraging full spikes emission across various bit levels. We conducted extensive experiments with our proposed method using both direct training method and ANN-SNN conversion method, and the results show consistent performance improvements.",
      "paper_authors": [
        "Yongjun Xiao",
        "Xianlong Tian",
        "Yongqi Ding",
        "Pei He",
        "Mengmeng Jing",
        "Lin Zuo"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-07-08",
      "update_time": "2024-07-08",
      "comments": "Under review",
      "repo_url": "#"
    },
    "2407.18917": {
      "paper_id": "2407.18917v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.18917v1",
      "paper_key": "2407.18917",
      "paper_title": "Spatio-temporal Structure of Excitation and Inhibition Emerges in Spiking Neural Networks with and without Biologically Plausible Constraints",
      "paper_url": "http://arxiv.org/abs/2407.18917v1",
      "paper_abstract": "We present a Spiking Neural Network (SNN) model that incorporates learnable synaptic delays using Dilated Convolution with Learnable Spacings (DCLS). We train this model on the Raw Heidelberg Digits keyword spotting benchmark using Backpropagation Through Time with surrogate gradients. Analysing the spatio-temporal structure of synaptic interactions in the network we observe that after training excitation and inhibition are grouped together both in space and time. To further enhance the efficiency and biological realism of our model, we implemented a dynamic pruning strategy that combines DEEP R for connection removal and RigL for connection reintroduction, ensuring that the network maintains optimal connectivity throughout training. Additionally, we incorporated Dale's Principle, enforcing each neuron to be exclusively excitatory or inhibitory -- aligning our model closer to biological neural networks. We observed that, after training, the spatio-temporal patterns of excitation and inhibition appeared in the more biologically plausible model as well. Our research demonstrates the potential of integrating learnable delays, dynamic pruning, and biological constraints to develop efficient SNN models for temporal data processing. Furthermore, our results enhance the understanding of spatio-temporal dynamics in SNNs -- suggesting that the spatio-temporal features which emerge from training are robust to both pruning and rewiring processes -- providing a solid foundation for future work in neuromorphic computing applications.",
      "paper_authors": [
        "Bal\u00e1zs M\u00e9sz\u00e1ros",
        "James Knight",
        "Thomas Nowotny"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-07-07",
      "update_time": "2024-07-07",
      "comments": "10 pages, 4 figures, submitted to Frontiers in Computational\n  Neuroscience",
      "repo_url": "#"
    },
    "2407.05310": {
      "paper_id": "2407.05310v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.05310v1",
      "paper_key": "2407.05310",
      "paper_title": "Ternary Spike-based Neuromorphic Signal Processing System",
      "paper_url": "http://arxiv.org/abs/2407.05310v1",
      "paper_abstract": "Deep Neural Networks (DNNs) have been successfully implemented across various signal processing fields, resulting in significant enhancements in performance. However, DNNs generally require substantial computational resources, leading to significant economic costs and posing challenges for their deployment on resource-constrained edge devices. In this study, we take advantage of spiking neural networks (SNNs) and quantization technologies to develop an energy-efficient and lightweight neuromorphic signal processing system. Our system is characterized by two principal innovations: a threshold-adaptive encoding (TAE) method and a quantized ternary SNN (QT-SNN). The TAE method can efficiently encode time-varying analog signals into sparse ternary spike trains, thereby reducing energy and memory demands for signal processing. QT-SNN, compatible with ternary spike trains from the TAE method, quantifies both membrane potentials and synaptic weights to reduce memory requirements while maintaining performance. Extensive experiments are conducted on two typical signal-processing tasks: speech and electroencephalogram recognition. The results demonstrate that our neuromorphic signal processing system achieves state-of-the-art (SOTA) performance with a 94% reduced memory requirement. Furthermore, through theoretical energy consumption analysis, our system shows 7.5x energy saving compared to other SNN works. The efficiency and efficacy of the proposed system highlight its potential as a promising avenue for energy-efficient signal processing.",
      "paper_authors": [
        "Shuai Wang",
        "Dehao Zhang",
        "Ammar Belatreche",
        "Yichen Xiao",
        "Hongyu Qing",
        "Wenjie We",
        "Malu Zhang",
        "Yang Yang"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2024-07-07",
      "update_time": "2024-07-07",
      "comments": null,
      "repo_url": "#"
    },
    "2407.05262": {
      "paper_id": "2407.05262v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.05262v2",
      "paper_key": "2407.05262",
      "paper_title": "FastSpiker: Enabling Fast Training for Spiking Neural Networks on Event-based Data through Learning Rate Enhancements for Autonomous Embedded Systems",
      "paper_url": "http://arxiv.org/abs/2407.05262v2",
      "paper_abstract": "Autonomous embedded systems (e.g., robots) typically necessitate intelligent computation with low power/energy processing for completing their tasks. Such requirements can be fulfilled by embodied neuromorphic intelligence with spiking neural networks (SNNs) because of their high learning quality (e.g., accuracy) and sparse computation. Here, the employment of event-based data is preferred to ensure seamless connectivity between input and processing parts. However, state-of-the-art SNNs still face a long training time to achieve high accuracy, thereby incurring high energy consumption and producing a high rate of carbon emission. Toward this, we propose FastSpiker, a novel methodology that enables fast SNN training on event-based data through learning rate enhancements targeting autonomous embedded systems. In FastSpiker, we first investigate the impact of different learning rate policies and their values, then select the ones that quickly offer high accuracy. Afterward, we explore different settings for the selected learning rate policies to find the appropriate policies through a statistical-based decision. Experimental results show that our FastSpiker offers up to 10.5x faster training time and up to 88.39% lower carbon emission to achieve higher or comparable accuracy to the state-of-the-art on the event-based automotive dataset (i.e., NCARS). In this manner, our FastSpiker methodology paves the way for green and sustainable computing in realizing embodied neuromorphic intelligence for autonomous embedded systems.",
      "paper_authors": [
        "Iqra Bano",
        "Rachmad Vidya Wicaksana Putra",
        "Alberto Marchisio",
        "Muhammad Shafique"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-07-07",
      "update_time": "2024-09-12",
      "comments": "To appear at the 18th International Conference on Control,\n  Automation, Robotics and Vision (ICARCV), December 2024, Dubai, UAE",
      "repo_url": "#"
    },
    "2407.04525": {
      "paper_id": "2407.04525v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.04525v2",
      "paper_key": "2407.04525",
      "paper_title": "Enhancing learning in spiking neural networks through neuronal heterogeneity and neuromodulatory signaling",
      "paper_url": "http://arxiv.org/abs/2407.04525v2",
      "paper_abstract": "Recent progress in artificial intelligence (AI) has been driven by insights from neuroscience, particularly with the development of artificial neural networks (ANNs). This has significantly enhanced the replication of complex cognitive tasks such as vision and natural language processing. Despite these advances, ANNs struggle with continual learning, adaptable knowledge transfer, robustness, and resource efficiency - capabilities that biological systems handle seamlessly. Specifically, ANNs often overlook the functional and morphological diversity of the brain, hindering their computational capabilities. Furthermore, incorporating cell-type specific neuromodulatory effects into ANNs with neuronal heterogeneity could enable learning at two spatial scales: spiking behavior at the neuronal level, and synaptic plasticity at the circuit level, thereby potentially enhancing their learning abilities. In this article, we summarize recent bio-inspired models, learning rules and architectures and propose a biologically-informed framework for enhancing ANNs. Our proposed dual-framework approach highlights the potential of spiking neural networks (SNNs) for emulating diverse spiking behaviors and dendritic compartments to simulate morphological and functional diversity of neuronal computations. Finally, we outline how the proposed approach integrates brain-inspired compartmental models and task-driven SNNs, balances bioinspiration and complexity, and provides scalable solutions for pressing AI challenges, such as continual learning, adaptability, robustness, and resource-efficiency.",
      "paper_authors": [
        "Alejandro Rodriguez-Garcia",
        "Jie Mei",
        "Srikanth Ramaswamy"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2024-07-05",
      "update_time": "2024-09-16",
      "comments": "32 pages, 4 figures, 3 boxes",
      "repo_url": "#"
    },
    "2407.04752": {
      "paper_id": "2407.04752v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.04752v1",
      "paper_key": "2407.04752",
      "paper_title": "SpikeLLM: Scaling up Spiking Neural Network to Large Language Models via Saliency-based Spiking",
      "paper_url": "http://arxiv.org/abs/2407.04752v1",
      "paper_abstract": "The recent advancements in large language models (LLMs) with billions of parameters have significantly boosted their performance across various real-world applications. However, the inference processes for these models require substantial energy and computational resources, presenting considerable deployment challenges. In contrast, human brains, which contain approximately 86 billion biological neurons, exhibit significantly greater energy efficiency compared to LLMs with a similar number of parameters. Inspired by this, we redesign 7 to 70 billion parameter LLMs using bio-plausible spiking mechanisms, emulating the efficient behavior of the human brain. We propose the first spiking large language model as recent LLMs termed SpikeLLM. Coupled with the proposed model, a novel spike-driven quantization framework named Optimal Brain Spiking is introduced to reduce the energy cost and accelerate inference speed via two essential approaches: first (second)-order differentiation-based salient channel detection, and per-channel salient outlier expansion with Generalized Integrate-and-Fire neurons. Our proposed spike-driven quantization can plug in main streams of quantization training methods. In the OmniQuant pipeline, SpikeLLM significantly reduces 25.51% WikiText2 perplexity and improves 3.08% average accuracy of 6 zero-shot datasets on a LLAMA2-7B 4A4W model. In the GPTQ pipeline, SpikeLLM realizes a sparse ternary quantization, which achieves additive in all linear layers. Compared with PB-LLM with similar operations, SpikeLLM also exceeds significantly. We will release our code on GitHub.",
      "paper_authors": [
        "Xingrun Xing",
        "Boyan Gao",
        "Zheng Zhang",
        "David A. Clifton",
        "Shitao Xiao",
        "Li Du",
        "Guoqi Li",
        "Jiajun Zhang"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-07-05",
      "update_time": "2024-07-05",
      "comments": null,
      "repo_url": "#"
    },
    "2407.04076": {
      "paper_id": "2407.04076v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.04076v2",
      "paper_key": "2407.04076",
      "paper_title": "Natively neuromorphic LMU architecture for encoding-free SNN-based HAR on commercial edge devices",
      "paper_url": "http://arxiv.org/abs/2407.04076v2",
      "paper_abstract": "Neuromorphic models take inspiration from the human brain by adopting bio-plausible neuron models to build alternatives to traditional Machine Learning (ML) and Deep Learning (DL) solutions. The scarce availability of dedicated hardware able to actualize the emulation of brain-inspired computation, which is otherwise only simulated, yet still hinders the wide adoption of neuromorphic computing for edge devices and embedded systems. With this premise, we adopt the perspective of neuromorphic computing for conventional hardware and we present the L2MU, a natively neuromorphic Legendre Memory Unit (LMU) which entirely relies on Leaky Integrate-and-Fire (LIF) neurons. Specifically, the original recurrent architecture of LMU has been redesigned by modelling every constituent element with neural populations made of LIF or Current-Based (CuBa) LIF neurons. To couple neuromorphic computing and off-the-shelf edge devices, we equipped the L2MU with an input module for the conversion of real values into spikes, which makes it an encoding-free implementation of a Recurrent Spiking Neural Network (RSNN) able to directly work with raw sensor signals on non-dedicated hardware. As a use case to validate our network, we selected the task of Human Activity Recognition (HAR). We benchmarked our L2MU on smartwatch signals from hand-oriented activities, deploying it on three different commercial edge devices in compressed versions too. The reported results remark the possibility of considering neuromorphic models not only in an exclusive relationship with dedicated hardware but also as a suitable choice to work with common sensors and devices.",
      "paper_authors": [
        "Vittorio Fra",
        "Benedetto Leto",
        "Andrea Pignata",
        "Enrico Macii",
        "Gianvito Urgese"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-07-04",
      "update_time": "2024-07-24",
      "comments": "Paper accepted for the 33rd International Conference on Artificial\n  Neural Networks (ICANN 2024)",
      "repo_url": "#"
    },
    "2407.00931": {
      "paper_id": "2407.00931v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.00931v1",
      "paper_key": "2407.00931",
      "paper_title": "Real-Time Neuromorphic Navigation: Integrating Event-Based Vision and Physics-Driven Planning on a Parrot Bebop2 Quadrotor",
      "paper_url": "http://arxiv.org/abs/2407.00931v1",
      "paper_abstract": "In autonomous aerial navigation, real-time and energy-efficient obstacle avoidance remains a significant challenge, especially in dynamic and complex indoor environments. This work presents a novel integration of neuromorphic event cameras with physics-driven planning algorithms implemented on a Parrot Bebop2 quadrotor. Neuromorphic event cameras, characterized by their high dynamic range and low latency, offer significant advantages over traditional frame-based systems, particularly in poor lighting conditions or during high-speed maneuvers. We use a DVS camera with a shallow Spiking Neural Network (SNN) for event-based object detection of a moving ring in real-time in an indoor lab. Further, we enhance drone control with physics-guided empirical knowledge inside a neural network training mechanism, to predict energy-efficient flight paths to fly through the moving ring. This integration results in a real-time, low-latency navigation system capable of dynamically responding to environmental changes while minimizing energy consumption. We detail our hardware setup, control loop, and modifications necessary for real-world applications, including the challenges of sensor integration without burdening the flight capabilities. Experimental results demonstrate the effectiveness of our approach in achieving robust, collision-free, and energy-efficient flight paths, showcasing the potential of neuromorphic vision and physics-driven planning in enhancing autonomous navigation systems.",
      "paper_authors": [
        "Amogh Joshi",
        "Sourav Sanyal",
        "Kaushik Roy"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-07-01",
      "update_time": "2024-07-01",
      "comments": null,
      "repo_url": "https://github.com/amoghj98/neuronav"
    },
    "2407.01645": {
      "paper_id": "2407.01645v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.01645v1",
      "paper_key": "2407.01645",
      "paper_title": "Sign Gradient Descent-based Neuronal Dynamics: ANN-to-SNN Conversion Beyond ReLU Network",
      "paper_url": "http://arxiv.org/abs/2407.01645v1",
      "paper_abstract": "Spiking neural network (SNN) is studied in multidisciplinary domains to (i) enable order-of-magnitudes energy-efficient AI inference and (ii) computationally simulate neuro-scientific mechanisms. The lack of discrete theory obstructs the practical application of SNN by limiting its performance and nonlinearity support. We present a new optimization-theoretic perspective of the discrete dynamics of spiking neurons. We prove that a discrete dynamical system of simple integrate-and-fire models approximates the sub-gradient method over unconstrained optimization problems. We practically extend our theory to introduce a novel sign gradient descent (signGD)-based neuronal dynamics that can (i) approximate diverse nonlinearities beyond ReLU and (ii) advance ANN-to-SNN conversion performance in low time steps. Experiments on large-scale datasets show that our technique achieves (i) state-of-the-art performance in ANN-to-SNN conversion and (ii) is the first to convert new DNN architectures, e.g., ConvNext, MLP-Mixer, and ResMLP. We publicly share our source code at https://github.com/snuhcs/snn_signgd .",
      "paper_authors": [
        "Hyunseok Oh",
        "Youngki Lee"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-07-01",
      "update_time": "2024-07-01",
      "comments": "37 pages, 41 figures, to be published as an ICML 2024 paper",
      "repo_url": "https://github.com/snuhcs/snn_signgd"
    },
    "2407.00641": {
      "paper_id": "2407.00641v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.00641v1",
      "paper_key": "2407.00641",
      "paper_title": "HASNAS: A Hardware-Aware Spiking Neural Architecture Search Framework for Neuromorphic Compute-in-Memory Systems",
      "paper_url": "http://arxiv.org/abs/2407.00641v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) have shown capabilities for solving diverse machine learning tasks with ultra-low-power/energy computation. To further improve the performance and efficiency of SNN inference, the Compute-in-Memory (CIM) paradigm with emerging device technologies such as resistive random access memory is employed. However, most of SNN architectures are developed without considering constraints from the application and the underlying CIM hardware (e.g., memory, area, latency, and energy consumption). Moreover, most of SNN designs are derived from the Artificial Neural Networks, whose network operations are different from SNNs. These limitations hinder SNNs from reaching their full potential in accuracy and efficiency. Toward this, we propose HASNAS, a novel hardware-aware spiking neural architecture search (NAS) framework for neuromorphic CIM systems that finds an SNN that offers high accuracy under the given memory, area, latency, and energy constraints. To achieve this, HASNAS employs the following key steps: (1) optimizing SNN operations to achieve high accuracy, (2) developing an SNN architecture that facilitates an effective learning process, and (3) devising a systematic hardware-aware search algorithm to meet the constraints. The experimental results show that our HASNAS quickly finds an SNN that maintains high accuracy compared to the state-of-the-art by up to 11x speed-up, and meets the given constraints: 4x10^6 parameters of memory, 100mm^2 of area, 400ms of latency, and 120uJ energy consumption for CIFAR10 and CIFAR100; while the state-of-the-art fails to meet the constraints. In this manner, our HASNAS can enable efficient design automation for providing high-performance and energy-efficient neuromorphic CIM systems for diverse applications.",
      "paper_authors": [
        "Rachmad Vidya Wicaksana Putra",
        "Muhammad Shafique"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-30",
      "update_time": "2024-06-30",
      "comments": "9 pages, 13 figures, 2 tables",
      "repo_url": "#"
    },
    "2406.19708": {
      "paper_id": "2406.19708v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.19708v2",
      "paper_key": "2406.19708",
      "paper_title": "A Differentiable Approach to Multi-scale Brain Modeling",
      "paper_url": "http://arxiv.org/abs/2406.19708v2",
      "paper_abstract": "We present a multi-scale differentiable brain modeling workflow utilizing BrainPy, a unique differentiable brain simulator that combines accurate brain simulation with powerful gradient-based optimization. We leverage this capability of BrainPy across different brain scales. At the single-neuron level, we implement differentiable neuron models and employ gradient methods to optimize their fit to electrophysiological data. On the network level, we incorporate connectomic data to construct biologically constrained network models. Finally, to replicate animal behavior, we train these models on cognitive tasks using gradient-based learning rules. Experiments demonstrate that our approach achieves superior performance and speed in fitting generalized leaky integrate-and-fire and Hodgkin-Huxley single neuron models. Additionally, training a biologically-informed network of excitatory and inhibitory spiking neurons on working memory tasks successfully replicates observed neural activity and synaptic weight distributions. Overall, our differentiable multi-scale simulation approach offers a promising tool to bridge neuroscience data across electrophysiological, anatomical, and behavioral scales.",
      "paper_authors": [
        "Chaoming Wang",
        "Muyang Lyu",
        "Tianqiu Zhang",
        "Sichao He",
        "Si Wu"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-28",
      "update_time": "2024-07-01",
      "comments": "2nd Differentiable Almost Everything Workshop at ICML 2024",
      "repo_url": "https://github.com/chaoming0625/differentiable-brain-modeling-workflow"
    },
    "2406.19645": {
      "paper_id": "2406.19645v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.19645v1",
      "paper_key": "2406.19645",
      "paper_title": "Directly Training Temporal Spiking Neural Network with Sparse Surrogate Gradient",
      "paper_url": "http://arxiv.org/abs/2406.19645v1",
      "paper_abstract": "Brain-inspired Spiking Neural Networks (SNNs) have attracted much attention due to their event-based computing and energy-efficient features. However, the spiking all-or-none nature has prevented direct training of SNNs for various applications. The surrogate gradient (SG) algorithm has recently enabled spiking neural networks to shine in neuromorphic hardware. However, introducing surrogate gradients has caused SNNs to lose their original sparsity, thus leading to the potential performance loss. In this paper, we first analyze the current problem of direct training using SGs and then propose Masked Surrogate Gradients (MSGs) to balance the effectiveness of training and the sparseness of the gradient, thereby improving the generalization ability of SNNs. Moreover, we introduce a temporally weighted output (TWO) method to decode the network output, reinforcing the importance of correct timesteps. Extensive experiments on diverse network structures and datasets show that training with MSG and TWO surpasses the SOTA technique.",
      "paper_authors": [
        "Yang Li",
        "Feifei Zhao",
        "Dongcheng Zhao",
        "Yi Zeng"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-28",
      "update_time": "2024-06-28",
      "comments": null,
      "repo_url": "#"
    },
    "2406.19230": {
      "paper_id": "2406.19230v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.19230v1",
      "paper_key": "2406.19230",
      "paper_title": "Spiking Convolutional Neural Networks for Text Classification",
      "paper_url": "http://arxiv.org/abs/2406.19230v1",
      "paper_abstract": "Spiking neural networks (SNNs) offer a promising pathway to implement deep neural networks (DNNs) in a more energy-efficient manner since their neurons are sparsely activated and inferences are event-driven. However, there have been very few works that have demonstrated the efficacy of SNNs in language tasks partially because it is non-trivial to represent words in the forms of spikes and to deal with variable-length texts by SNNs. This work presents a \"conversion + fine-tuning\" two-step method for training SNNs for text classification and proposes a simple but effective way to encode pre-trained word embeddings as spike trains. We show empirically that after fine-tuning with surrogate gradients, the converted SNNs achieve comparable results to their DNN counterparts with much less energy consumption across multiple datasets for both English and Chinese. We also show that such SNNs are more robust to adversarial attacks than DNNs.",
      "paper_authors": [
        "Changze Lv",
        "Jianhan Xu",
        "Xiaoqing Zheng"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-27",
      "update_time": "2024-06-27",
      "comments": null,
      "repo_url": "https://github.com/Lvchangze/snn"
    },
    "2406.18350": {
      "paper_id": "2406.18350v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.18350v1",
      "paper_key": "2406.18350",
      "paper_title": "On Reducing Activity with Distillation and Regularization for Energy Efficient Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2406.18350v1",
      "paper_abstract": "Interest in spiking neural networks (SNNs) has been growing steadily, promising an energy-efficient alternative to formal neural networks (FNNs), commonly known as artificial neural networks (ANNs). Despite increasing interest, especially for Edge applications, these event-driven neural networks suffered from their difficulty to be trained compared to FNNs. To alleviate this problem, a number of innovative methods have been developed to provide performance more or less equivalent to that of FNNs. However, the spiking activity of a network during inference is usually not considered. While SNNs may usually have performance comparable to that of FNNs, it is often at the cost of an increase of the network's activity, thus limiting the benefit of using them as a more energy-efficient solution.   In this paper, we propose to leverage Knowledge Distillation (KD) for SNNs training with surrogate gradient descent in order to optimize the trade-off between performance and spiking activity. Then, after understanding why KD led to an increase in sparsity, we also explored Activations regularization and proposed a novel method with Logits Regularization. These approaches, validated on several datasets, clearly show a reduction in network spiking activity (-26.73% on GSC and -14.32% on CIFAR-10) while preserving accuracy.",
      "paper_authors": [
        "Thomas Louis",
        "Benoit Miramond",
        "Alain Pegatoquet",
        "Adrien Girard"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-26",
      "update_time": "2024-06-26",
      "comments": null,
      "repo_url": "#"
    },
    "2406.17617": {
      "paper_id": "2406.17617v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.17617v1",
      "paper_key": "2406.17617",
      "paper_title": "Embedded event based object detection with spiking neural network",
      "paper_url": "http://arxiv.org/abs/2406.17617v1",
      "paper_abstract": "The complexity of event-based object detection (OD) poses considerable challenges. Spiking Neural Networks (SNNs) show promising results and pave the way for efficient event-based OD. Despite this success, the path to efficient SNNs on embedded devices remains a challenge. This is due to the size of the networks required to accomplish the task and the ability of devices to take advantage of SNNs benefits. Even when \"edge\" devices are considered, they typically use embedded GPUs that consume tens of watts. In response to these challenges, our research introduces an embedded neuromorphic testbench that utilizes the SPiking Low-power Event-based ArchiTecture (SPLEAT) accelerator. Using an extended version of the Qualia framework, we can train, evaluate, quantize, and deploy spiking neural networks on an FPGA implementation of SPLEAT. We used this testbench to load a state-of-the-art SNN solution, estimate the performance loss associated with deploying the network on dedicated hardware, and run real-world event-based OD on neuromorphic hardware specifically designed for low-power spiking neural networks. Remarkably, our embedded spiking solution, which includes a model with 1.08 million parameters, operates efficiently with 490 mJ per prediction.",
      "paper_authors": [
        "Jonathan Courtois",
        "Pierre-Emmanuel Novac",
        "Edgar Lemaire",
        "Alain Pegatoquet",
        "Benoit Miramond"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-25",
      "update_time": "2024-06-25",
      "comments": "Result link: https://youtu.be/TsolUDaMY7Y",
      "repo_url": "#"
    },
    "2406.17285": {
      "paper_id": "2406.17285v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.17285v1",
      "paper_key": "2406.17285",
      "paper_title": "EON-1: A Brain-Inspired Processor for Near-Sensor Extreme Edge Online Feature Extraction",
      "paper_url": "http://arxiv.org/abs/2406.17285v1",
      "paper_abstract": "For Edge AI applications, deploying online learning and adaptation on resource-constrained embedded devices can deal with fast sensor-generated streams of data in changing environments. However, since maintaining low-latency and power-efficient inference is paramount at the Edge, online learning and adaptation on the device should impose minimal additional overhead for inference. With this goal in mind, we explore energy-efficient learning and adaptation on-device for streaming-data Edge AI applications using Spiking Neural Networks (SNNs), which follow the principles of brain-inspired computing, such as high-parallelism, neuron co-located memory and compute, and event-driven processing. We propose EON-1, a brain-inspired processor for near-sensor extreme edge online feature extraction, that integrates a fast online learning and adaptation algorithm. We report results of only 1% energy overhead for learning, by far the lowest overhead when compared to other SoTA solutions, while attaining comparable inference accuracy. Furthermore, we demonstrate that EON-1 is up for the challenge of low-latency processing of HD and UHD streaming video in real-time, with learning enabled.",
      "paper_authors": [
        "Alexandra Dobrita",
        "Amirreza Yousefzadeh",
        "Simon Thorpe",
        "Kanishkan Vadivel",
        "Paul Detterer",
        "Guangzhi Tang",
        "Gert-Jan van Schaik",
        "Mario Konijnenburg",
        "Anteneh Gebregiorgis",
        "Said Hamdioui",
        "Manolis Sifalakis"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-25",
      "update_time": "2024-06-25",
      "comments": null,
      "repo_url": "#"
    },
    "2406.17049": {
      "paper_id": "2406.17049v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.17049v2",
      "paper_key": "2406.17049",
      "paper_title": "Fast Switching Serial and Parallel Paradigms of SNN Inference on Multi-core Heterogeneous Neuromorphic Platform SpiNNaker2",
      "paper_url": "http://arxiv.org/abs/2406.17049v2",
      "paper_abstract": "With serial and parallel processors introduced into Spiking Neural Networks (SNNs) execution, more and more researchers are dedicated to improving the performance of the computing paradigms by taking full advantage of the strengths of the available processor. In this paper, we compare and integrate serial and parallel paradigms into one SNN compiling system. For a faster switching between them in the layer granularity, we train the classifier to prejudge a better paradigm before compiling instead of making the decision afterward, saving a great amount of compiling time and RAM space on the host PC. The classifier Adaptive Boost, with the highest accuracy (91.69%) among 12 classifiers, is integrated into the switching system, which utilizes less memory and processors on the multi-core neuromorphic hardware backend SpiNNaker2 than two individual paradigms. To the best of our knowledge, it is the first fast-switching compiling system for SNN simulation.",
      "paper_authors": [
        "Jiaxin Huang",
        "Bernhard Vogginger",
        "Florian Kelber",
        "Hector Gonzalez",
        "Klaus Knobloch",
        "Christian Georg Mayr"
      ],
      "primary_category": "cs.DC",
      "publish_time": "2024-06-24",
      "update_time": "2024-07-12",
      "comments": null,
      "repo_url": "#"
    },
    "2406.16479": {
      "paper_id": "2406.16479v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.16479v1",
      "paper_key": "2406.16479",
      "paper_title": "Emerging NeoHebbian Dynamics in Forward-Forward Learning: Implications for Neuromorphic Computing",
      "paper_url": "http://arxiv.org/abs/2406.16479v1",
      "paper_abstract": "Advances in neural computation have predominantly relied on the gradient backpropagation algorithm (BP). However, the recent shift towards non-stationary data modeling has highlighted the limitations of this heuristic, exposing that its adaptation capabilities are far from those seen in biological brains. Unlike BP, where weight updates are computed through a reverse error propagation path, Hebbian learning dynamics provide synaptic updates using only information within the layer itself. This has spurred interest in biologically plausible learning algorithms, hypothesized to overcome BP's shortcomings. In this context, Hinton recently introduced the Forward-Forward Algorithm (FFA), which employs local learning rules for each layer and has empirically proven its efficacy in multiple data modeling tasks. In this work we argue that when employing a squared Euclidean norm as a goodness function driving the local learning, the resulting FFA is equivalent to a neo-Hebbian Learning Rule. To verify this result, we compare the training behavior of FFA in analog networks with its Hebbian adaptation in spiking neural networks. Our experiments demonstrate that both versions of FFA produce similar accuracy and latent distributions. The findings herein reported provide empirical evidence linking biological learning rules with currently used training algorithms, thus paving the way towards extrapolating the positive outcomes from FFA to Hebbian learning rules. Simultaneously, our results imply that analog networks trained under FFA could be directly applied to neuromorphic computing, leading to reduced energy usage and increased computational speed.",
      "paper_authors": [
        "Erik B. Terres-Escudero",
        "Javier Del Ser",
        "Pablo Garc\u00eda-Bringas"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-24",
      "update_time": "2024-06-24",
      "comments": null,
      "repo_url": "https://github.com/erikberter/hebbian_ffa"
    },
    "2406.15112": {
      "paper_id": "2406.15112v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.15112v1",
      "paper_key": "2406.15112",
      "paper_title": "Micro-power spoken keyword spotting on Xylo Audio 2",
      "paper_url": "http://arxiv.org/abs/2406.15112v1",
      "paper_abstract": "For many years, designs for \"Neuromorphic\" or brain-like processors have been motivated by achieving extreme energy efficiency, compared with von-Neumann and tensor processor devices. As part of their design language, Neuromorphic processors take advantage of weight, parameter, state and activity sparsity. In the extreme case, neural networks based on these principles mimic the sparse activity oof biological nervous systems, in ``Spiking Neural Networks'' (SNNs). Few benchmarks are available for Neuromorphic processors, that have been implemented for a range of Neuromorphic and non-Neuromorphic platforms, which can therefore demonstrate the energy benefits of Neuromorphic processor designs. Here we describes the implementation of a spoken audio keyword-spotting (KWS) benchmark \"Aloha\" on the Xylo Audio 2 (SYNS61210) Neuromorphic processor device. We obtained high deployed quantized task accuracy, (95%), exceeding the benchmark task accuracy. We measured real continuous power of the deployed application on Xylo. We obtained best-in-class dynamic inference power ($291\\mu$W) and best-in-class inference efficiency ($6.6\\mu$J / Inf). Xylo sets a new minimum power for the Aloha KWS benchmark, and highlights the extreme energy efficiency achievable with Neuromorphic processor designs. Our results show that Neuromorphic designs are well-suited for real-time near- and in-sensor processing on edge devices.",
      "paper_authors": [
        "Hannah Bos",
        "Dylan R. Muir"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-21",
      "update_time": "2024-06-21",
      "comments": null,
      "repo_url": "#"
    },
    "2406.15034": {
      "paper_id": "2406.15034v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.15034v1",
      "paper_key": "2406.15034",
      "paper_title": "SVFormer: A Direct Training Spiking Transformer for Efficient Video Action Recognition",
      "paper_url": "http://arxiv.org/abs/2406.15034v1",
      "paper_abstract": "Video action recognition (VAR) plays crucial roles in various domains such as surveillance, healthcare, and industrial automation, making it highly significant for the society. Consequently, it has long been a research spot in the computer vision field. As artificial neural networks (ANNs) are flourishing, convolution neural networks (CNNs), including 2D-CNNs and 3D-CNNs, as well as variants of the vision transformer (ViT), have shown impressive performance on VAR. However, they usually demand huge computational cost due to the large data volume and heavy information redundancy introduced by the temporal dimension. To address this challenge, some researchers have turned to brain-inspired spiking neural networks (SNNs), such as recurrent SNNs and ANN-converted SNNs, leveraging their inherent temporal dynamics and energy efficiency. Yet, current SNNs for VAR also encounter limitations, such as nontrivial input preprocessing, intricate network construction/training, and the need for repetitive processing of the same video clip, hindering their practical deployment. In this study, we innovatively propose the directly trained SVFormer (Spiking Video transFormer) for VAR. SVFormer integrates local feature extraction, global self-attention, and the intrinsic dynamics, sparsity, and spike-driven nature of SNNs, to efficiently and effectively extract spatio-temporal features. We evaluate SVFormer on two RGB datasets (UCF101, NTU-RGBD60) and one neuromorphic dataset (DVS128-Gesture), demonstrating comparable performance to the mainstream models in a more efficient way. Notably, SVFormer achieves a top-1 accuracy of 84.03% with ultra-low power consumption (21 mJ/video) on UCF101, which is state-of-the-art among directly trained deep SNNs, showcasing significant advantages over prior models.",
      "paper_authors": [
        "Liutao Yu",
        "Liwei Huang",
        "Chenlin Zhou",
        "Han Zhang",
        "Zhengyu Ma",
        "Huihui Zhou",
        "Yonghong Tian"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-21",
      "update_time": "2024-06-21",
      "comments": "Accepted by IJCAI 2024 workshop - Human Brain and Artificial\n  Intelligence",
      "repo_url": "#"
    },
    "2406.14801": {
      "paper_id": "2406.14801v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.14801v1",
      "paper_key": "2406.14801",
      "paper_title": "Model Predictive Control of the Neural Manifold",
      "paper_url": "http://arxiv.org/abs/2406.14801v1",
      "paper_abstract": "Neural manifolds are an attractive theoretical framework for characterizing the complex behaviors of neural populations. However, many of the tools for identifying these low-dimensional subspaces are correlational and provide limited insight into the underlying dynamics. The ability to precisely control this latent activity would allow researchers to investigate the structure and function of neural manifolds. Employing techniques from the field of optimal control, we simulate controlling the latent dynamics of a neural population using closed-loop, dynamically generated sensory inputs. Using a spiking neural network (SNN) as a model of a neural circuit, we find low-dimensional representations of both the network activity (the neural manifold) and a set of salient visual stimuli. With a data-driven latent dynamics model, we apply model predictive control (MPC) to provide anticipatory, optimal control over the trajectory of the circuit in a latent space. We are able to control the latent dynamics of the SNN to follow several reference trajectories despite observing only a subset of neurons and with a substantial amount of unknown noise injected into the network. These results provide a framework to experimentally test for causal relationships between manifold dynamics and other variables of interest such as organismal behavior and BCI performance.",
      "paper_authors": [
        "Christof Fehrman",
        "C. Daniel Meliza"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2024-06-21",
      "update_time": "2024-06-21",
      "comments": null,
      "repo_url": "#"
    },
    "2406.14180": {
      "paper_id": "2406.14180v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.14180v1",
      "paper_key": "2406.14180",
      "paper_title": "RTFormer: Re-parameter TSBN Spiking Transformer",
      "paper_url": "http://arxiv.org/abs/2406.14180v1",
      "paper_abstract": "The Spiking Neural Networks (SNNs), renowned for their bio-inspired operational mechanism and energy efficiency, mirror the human brain's neural activity. Yet, SNNs face challenges in balancing energy efficiency with the computational demands of advanced tasks. Our research introduces the RTFormer, a novel architecture that embeds Re-parameterized Temporal Sliding Batch Normalization (TSBN) within the Spiking Transformer framework. This innovation optimizes energy usage during inference while ensuring robust computational performance. The crux of RTFormer lies in its integration of reparameterized convolutions and TSBN, achieving an equilibrium between computational prowess and energy conservation.",
      "paper_authors": [
        "Hongzhi Wang",
        "Xiubo Liang",
        "Mengjian Li",
        "Tao Zhang"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-20",
      "update_time": "2024-06-20",
      "comments": null,
      "repo_url": "#"
    },
    "2406.14178": {
      "paper_id": "2406.14178v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.14178v1",
      "paper_key": "2406.14178",
      "paper_title": "EvSegSNN: Neuromorphic Semantic Segmentation for Event Data",
      "paper_url": "http://arxiv.org/abs/2406.14178v1",
      "paper_abstract": "Semantic segmentation is an important computer vision task, particularly for scene understanding and navigation of autonomous vehicles and UAVs. Several variations of deep neural network architectures have been designed to tackle this task. However, due to their huge computational costs and their high memory consumption, these models are not meant to be deployed on resource-constrained systems. To address this limitation, we introduce an end-to-end biologically inspired semantic segmentation approach by combining Spiking Neural Networks (SNNs, a low-power alternative to classical neural networks) with event cameras whose output data can directly feed these neural network inputs. We have designed EvSegSNN, a biologically plausible encoder-decoder U-shaped architecture relying on Parametric Leaky Integrate and Fire neurons in an objective to trade-off resource usage against performance. The experiments conducted on DDD17 demonstrate that EvSegSNN outperforms the closest state-of-the-art model in terms of MIoU while reducing the number of parameters by a factor of $1.6$ and sparing a batch normalization stage.",
      "paper_authors": [
        "Dalia Hareb",
        "Jean Martinet"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-20",
      "update_time": "2024-06-20",
      "comments": null,
      "repo_url": "#"
    },
    "2407.09521": {
      "paper_id": "2407.09521v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.09521v1",
      "paper_key": "2407.09521",
      "paper_title": "Apprenticeship-Inspired Elegance: Synergistic Knowledge Distillation Empowers Spiking Neural Networks for Efficient Single-Eye Emotion Recognition",
      "paper_url": "http://arxiv.org/abs/2407.09521v1",
      "paper_abstract": "We introduce a novel multimodality synergistic knowledge distillation scheme tailored for efficient single-eye motion recognition tasks. This method allows a lightweight, unimodal student spiking neural network (SNN) to extract rich knowledge from an event-frame multimodal teacher network. The core strength of this approach is its ability to utilize the ample, coarser temporal cues found in conventional frames for effective emotion recognition. Consequently, our method adeptly interprets both temporal and spatial information from the conventional frame domain, eliminating the need for specialized sensing devices, e.g., event-based camera. The effectiveness of our approach is thoroughly demonstrated using both existing and our compiled single-eye emotion recognition datasets, achieving unparalleled performance in accuracy and efficiency over existing state-of-the-art methods.",
      "paper_authors": [
        "Yang Wang",
        "Haiyang Mei",
        "Qirui Bao",
        "Ziqi Wei",
        "Mike Zheng Shou",
        "Haizhou Li",
        "Bo Dong",
        "Xin Yang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-20",
      "update_time": "2024-06-20",
      "comments": "Accepted by IJCAI 2024",
      "repo_url": "#"
    },
    "2406.13672": {
      "paper_id": "2406.13672v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.13672v1",
      "paper_key": "2406.13672",
      "paper_title": "Q-SNNs: Quantized Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2406.13672v1",
      "paper_abstract": "Brain-inspired Spiking Neural Networks (SNNs) leverage sparse spikes to represent information and process them in an asynchronous event-driven manner, offering an energy-efficient paradigm for the next generation of machine intelligence. However, the current focus within the SNN community prioritizes accuracy optimization through the development of large-scale models, limiting their viability in resource-constrained and low-power edge devices. To address this challenge, we introduce a lightweight and hardware-friendly Quantized SNN (Q-SNN) that applies quantization to both synaptic weights and membrane potentials. By significantly compressing these two key elements, the proposed Q-SNNs substantially reduce both memory usage and computational complexity. Moreover, to prevent the performance degradation caused by this compression, we present a new Weight-Spike Dual Regulation (WS-DR) method inspired by information entropy theory. Experimental evaluations on various datasets, including static and neuromorphic, demonstrate that our Q-SNNs outperform existing methods in terms of both model size and accuracy. These state-of-the-art results in efficiency and efficacy suggest that the proposed method can significantly improve edge intelligent computing.",
      "paper_authors": [
        "Wenjie Wei",
        "Yu Liang",
        "Ammar Belatreche",
        "Yichen Xiao",
        "Honglin Cao",
        "Zhenbang Ren",
        "Guoqing Wang",
        "Malu Zhang",
        "Yang Yang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-19",
      "update_time": "2024-06-19",
      "comments": "8 pages, 5 figures",
      "repo_url": "#"
    },
    "2406.13568": {
      "paper_id": "2406.13568v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.13568v1",
      "paper_key": "2406.13568",
      "paper_title": "Trapezoidal Gradient Descent for Effective Reinforcement Learning in Spiking Networks",
      "paper_url": "http://arxiv.org/abs/2406.13568v1",
      "paper_abstract": "With the rapid development of artificial intelligence technology, the field of reinforcement learning has continuously achieved breakthroughs in both theory and practice. However, traditional reinforcement learning algorithms often entail high energy consumption during interactions with the environment. Spiking Neural Network (SNN), with their low energy consumption characteristics and performance comparable to deep neural networks, have garnered widespread attention. To reduce the energy consumption of practical applications of reinforcement learning, researchers have successively proposed the Pop-SAN and MDC-SAN algorithms. Nonetheless, these algorithms use rectangular functions to approximate the spike network during the training process, resulting in low sensitivity, thus indicating room for improvement in the training effectiveness of SNN. Based on this, we propose a trapezoidal approximation gradient method to replace the spike network, which not only preserves the original stable learning state but also enhances the model's adaptability and response sensitivity under various signal dynamics. Simulation results show that the improved algorithm, using the trapezoidal approximation gradient to replace the spike network, achieves better convergence speed and performance compared to the original algorithm and demonstrates good training stability.",
      "paper_authors": [
        "Yuhao Pan",
        "Xiucheng Wang",
        "Nan Cheng",
        "Qi Qiu"
      ],
      "primary_category": "cs.AI",
      "publish_time": "2024-06-19",
      "update_time": "2024-06-19",
      "comments": null,
      "repo_url": "#"
    },
    "2406.13179": {
      "paper_id": "2406.13179v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.13179v1",
      "paper_key": "2406.13179",
      "paper_title": "Global-Local Convolution with Spiking Neural Networks for Energy-efficient Keyword Spotting",
      "paper_url": "http://arxiv.org/abs/2406.13179v1",
      "paper_abstract": "Thanks to Deep Neural Networks (DNNs), the accuracy of Keyword Spotting (KWS) has made substantial progress. However, as KWS systems are usually implemented on edge devices, energy efficiency becomes a critical requirement besides performance. Here, we take advantage of spiking neural networks' energy efficiency and propose an end-to-end lightweight KWS model. The model consists of two innovative modules: 1) Global-Local Spiking Convolution (GLSC) module and 2) Bottleneck-PLIF module. Compared to the hand-crafted feature extraction methods, the GLSC module achieves speech feature extraction that is sparser, more energy-efficient, and yields better performance. The Bottleneck-PLIF module further processes the signals from GLSC with the aim to achieve higher accuracy with fewer parameters. Extensive experiments are conducted on the Google Speech Commands Dataset (V1 and V2). The results show our method achieves competitive performance among SNN-based KWS models with fewer parameters.",
      "paper_authors": [
        "Shuai Wang",
        "Dehao Zhang",
        "Kexin Shi",
        "Yuchen Wang",
        "Wenjie Wei",
        "Jibin Wu",
        "Malu Zhang"
      ],
      "primary_category": "cs.SD",
      "publish_time": "2024-06-19",
      "update_time": "2024-06-19",
      "comments": null,
      "repo_url": "#"
    },
    "2406.12552": {
      "paper_id": "2406.12552v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.12552v1",
      "paper_key": "2406.12552",
      "paper_title": "Evolutionary Spiking Neural Networks: A Survey",
      "paper_url": "http://arxiv.org/abs/2406.12552v1",
      "paper_abstract": "Spiking neural networks (SNNs) are gaining increasing attention as potential computationally efficient alternatives to traditional artificial neural networks(ANNs). However, the unique information propagation mechanisms and the complexity of SNN neuron models pose challenges for adopting traditional methods developed for ANNs to SNNs. These challenges include both weight learning and architecture design. While surrogate gradient learning has shown some success in addressing the former challenge, the latter remains relatively unexplored. Recently, a novel paradigm utilizing evolutionary computation methods has emerged to tackle these challenges. This approach has resulted in the development of a variety of energy-efficient and high-performance SNNs across a wide range of machine learning benchmarks. In this paper, we present a survey of these works and initiate discussions on potential challenges ahead.",
      "paper_authors": [
        "Shuaijie Shen",
        "Rui Zhang",
        "Chao Wang",
        "Renzhuo Huang",
        "Aiersi Tuerhong",
        "Qinghai Guo",
        "Zhichao Lu",
        "Jianguo Zhang",
        "Luziwei Leng"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-18",
      "update_time": "2024-06-18",
      "comments": null,
      "repo_url": "#"
    },
    "2406.12200": {
      "paper_id": "2406.12200v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.12200v1",
      "paper_key": "2406.12200",
      "paper_title": "SFedCA: Credit Assignment-Based Active Client Selection Strategy for Spiking Federated Learning",
      "paper_url": "http://arxiv.org/abs/2406.12200v1",
      "paper_abstract": "Spiking federated learning is an emerging distributed learning paradigm that allows resource-constrained devices to train collaboratively at low power consumption without exchanging local data. It takes advantage of both the privacy computation property in federated learning (FL) and the energy efficiency in spiking neural networks (SNN). Thus, it is highly promising to revolutionize the efficient processing of multimedia data. However, existing spiking federated learning methods employ a random selection approach for client aggregation, assuming unbiased client participation. This neglect of statistical heterogeneity affects the convergence and accuracy of the global model significantly. In our work, we propose a credit assignment-based active client selection strategy, the SFedCA, to judiciously aggregate clients that contribute to the global sample distribution balance. Specifically, the client credits are assigned by the firing intensity state before and after local model training, which reflects the local data distribution difference from the global model. Comprehensive experiments are conducted on various non-identical and independent distribution (non-IID) scenarios. The experimental results demonstrate that the SFedCA outperforms the existing state-of-the-art spiking federated learning methods, and requires fewer communication rounds.",
      "paper_authors": [
        "Qiugang Zhan",
        "Jinbo Cao",
        "Xiurui Xie",
        "Malu Zhang",
        "Huajin Tang",
        "Guisong Liu"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-06-18",
      "update_time": "2024-06-18",
      "comments": "9 pages",
      "repo_url": "#"
    },
    "2406.11778": {
      "paper_id": "2406.11778v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.11778v1",
      "paper_key": "2406.11778",
      "paper_title": "Brain-inspired Computational Modeling of Action Recognition with Recurrent Spiking Neural Networks Equipped with Reinforcement Delay Learning",
      "paper_url": "http://arxiv.org/abs/2406.11778v1",
      "paper_abstract": "The growing interest in brain-inspired computational models arises from the remarkable problem-solving efficiency of the human brain. Action recognition, a complex task in computational neuroscience, has received significant attention due to both its intricate nature and the brain's exceptional performance in this area. Nevertheless, current solutions for action recognition either exhibit limitations in effectively addressing the problem or lack the necessary biological plausibility. Deep neural networks, for instance, demonstrate acceptable performance but deviate from biological evidence, thereby restricting their suitability for brain-inspired computational studies. On the other hand, the majority of brain-inspired models proposed for action recognition exhibit significantly lower effectiveness compared to deep models and fail to achieve human-level performance. This deficiency can be attributed to their disregard for the underlying mechanisms of the brain. In this article, we present an effective brain-inspired computational model for action recognition. We equip our model with novel biologically plausible mechanisms for spiking neural networks that are crucial for learning spatio-temporal patterns. The key idea behind these new mechanisms is to bridge the gap between the brain's capabilities and action recognition tasks by integrating key biological principles into our computational framework. Furthermore, we evaluate the performance of our model against other models using a benchmark dataset for action recognition, DVS-128 Gesture. The results show that our model outperforms previous biologically plausible models and competes with deep supervised models.",
      "paper_authors": [
        "Alireza Nadafian",
        "Milad Mozafari",
        "Timoth\u00e9e Masquelier",
        "Mohammad Ganjtabesh"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-17",
      "update_time": "2024-06-17",
      "comments": null,
      "repo_url": "#"
    },
    "2406.10066": {
      "paper_id": "2406.10066v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.10066v1",
      "paper_key": "2406.10066",
      "paper_title": "Exemplar LCA-Decoder: A Scalable Framework for On-Chip Learning",
      "paper_url": "http://arxiv.org/abs/2406.10066v1",
      "paper_abstract": "Neuromorphic computing has recently gained significant attention as a promising combined approach for developing energy-efficient, parallel computing systems inspired by the human brain. Efficient training algorithms are imperative for the effective processing of data on neuromorphic platforms; however, their absence remains a notable gap in the field. In this paper, we reduce the gap by proposing an innovative encoder-decoder technique that leverages sparse coding and the Locally Competitive Algorithm (LCA) to provide a computationally efficient and power-conscious algorithm specifically designed for neuromorphic platforms. Using Exemplar LCA-Decoder we reduce the computational demands and memory requirements associated with training Spiking Neural Networks (SNNs) using error backpropagation methods. Our results show notable test accuracy on ImageNet and CIFAR10/100 datasets, surpassing the previously achieved SNN accuracy on these datasets. Additionally, Exemplar LCA-Decoder is scalable and allows expanding the model and adding new data points and classes cost-effectively.",
      "paper_authors": [
        "Sanaz Mahmoodi Takaghaj",
        "Jack Sampson"
      ],
      "primary_category": "cs.ET",
      "publish_time": "2024-06-14",
      "update_time": "2024-06-14",
      "comments": null,
      "repo_url": "#"
    },
    "2407.04718": {
      "paper_id": "2407.04718v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.04718v2",
      "paper_key": "2407.04718",
      "paper_title": "Event-Based Simulation of Stochastic Memristive Devices for Neuromorphic Computing",
      "paper_url": "http://arxiv.org/abs/2407.04718v2",
      "paper_abstract": "In this paper, we build a general model of memristors suitable for the simulation of event-based systems, such as hardware spiking neural networks, and more generally, neuromorphic computing systems. We extend an existing general model of memristors - the Generalised Metastable Switch Model - to an event-driven setting, eliminating errors associated discrete time approximation, as well as offering potential improvements in terms of computational efficiency for simulation. We introduce the notion of a volatility state variable, to allow for the modelling of memory-dependent and dynamic switching behaviour, succinctly capturing and unifying a variety of volatile phenomena present in memristive devices, including state relaxation, structural disruption, Joule heating, and drift acceleration phenomena. We supply a drift dataset for titanium dioxide memristors and introduce a linear conductance model to simulate the drift characteristics, motivated by a proposed physical model of filament growth. We then demonstrate an approach for fitting the parameters of the event-based model to the drift model.",
      "paper_authors": [
        "Waleed El-Geresy",
        "Christos Papavassiliou",
        "Deniz G\u00fcnd\u00fcz"
      ],
      "primary_category": "cs.ET",
      "publish_time": "2024-06-14",
      "update_time": "2024-07-23",
      "comments": "15 pages, 14 figures",
      "repo_url": "#"
    },
    "2406.12726": {
      "paper_id": "2406.12726v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.12726v1",
      "paper_key": "2406.12726",
      "paper_title": "ED-sKWS: Early-Decision Spiking Neural Networks for Rapid,and Energy-Efficient Keyword Spotting",
      "paper_url": "http://arxiv.org/abs/2406.12726v1",
      "paper_abstract": "Keyword Spotting (KWS) is essential in edge computing requiring rapid and energy-efficient responses. Spiking Neural Networks (SNNs) are well-suited for KWS for their efficiency and temporal capacity for speech. To further reduce the latency and energy consumption, this study introduces ED-sKWS, an SNN-based KWS model with an early-decision mechanism that can stop speech processing and output the result before the end of speech utterance. Furthermore, we introduce a Cumulative Temporal (CT) loss that can enhance prediction accuracy at both the intermediate and final timesteps. To evaluate early-decision performance, we present the SC-100 dataset including 100 speech commands with beginning and end timestamp annotation. Experiments on the Google Speech Commands v2 and our SC-100 datasets show that ED-sKWS maintains competitive accuracy with 61% timesteps and 52% energy consumption compared to SNN models without early-decision mechanism, ensuring rapid response and energy efficiency.",
      "paper_authors": [
        "Zeyang Song",
        "Qianhui Liu",
        "Qu Yang",
        "Yizhou Peng",
        "Haizhou Li"
      ],
      "primary_category": "cs.SD",
      "publish_time": "2024-06-14",
      "update_time": "2024-06-14",
      "comments": "Accepted by INTERSPEECH2024",
      "repo_url": "#"
    },
    "2406.09680": {
      "paper_id": "2406.09680v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.09680v1",
      "paper_key": "2406.09680",
      "paper_title": "Heterogeneous Federated Learning with Convolutional and Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2406.09680v1",
      "paper_abstract": "Federated learning (FL) has emerged as a promising paradigm for training models on decentralized data while safeguarding data privacy. Most existing FL systems, however, assume that all machine learning models are of the same type, although it becomes more likely that different edge devices adopt different types of AI models, including both conventional analogue artificial neural networks (ANNs) and biologically more plausible spiking neural networks (SNNs). This diversity empowers the efficient handling of specific tasks and requirements, showcasing the adaptability and versatility of edge computing platforms. One main challenge of such heterogeneous FL system lies in effectively aggregating models from the local devices in a privacy-preserving manner. To address the above issue, this work benchmarks FL systems containing both convoluntional neural networks (CNNs) and SNNs by comparing various aggregation approaches, including federated CNNs, federated SNNs, federated CNNs for SNNs, federated SNNs for CNNs, and federated CNNs with SNN fusion. Experimental results demonstrate that the CNN-SNN fusion framework exhibits the best performance among the above settings on the MNIST dataset. Additionally, intriguing phenomena of competitive suppression are noted during the convergence process of multi-model FL.",
      "paper_authors": [
        "Yingchao Yu",
        "Yuping Yan",
        "Jisong Cai",
        "Yaochu Jin"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-06-14",
      "update_time": "2024-06-14",
      "comments": "8 pages, 5 figures, FL@FM-IJCAI'24",
      "repo_url": "#"
    },
    "2406.07927": {
      "paper_id": "2406.07927v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.07927v1",
      "paper_key": "2406.07927",
      "paper_title": "ExoSpikeNet: A Light Curve Analysis Based Spiking Neural Network for Exoplanet Detection",
      "paper_url": "http://arxiv.org/abs/2406.07927v1",
      "paper_abstract": "Exoplanets are celestial bodies orbiting stars beyond our Solar System. Although historically they posed detection challenges, Kepler's data has revolutionized our understanding. By analyzing flux values from the Kepler Mission, we investigate the intricate patterns in starlight that may indicate the presence of exoplanets. This study investigates a novel approach for exoplanet classification using Spiking Neural Networks (SNNs) applied to data obtained from the NASA Kepler mission. SNNs offer a unique advantage by mimicking the spiking behavior of neurons in the brain, allowing for more nuanced and biologically inspired processing of temporal data. Experimental results demonstrate the efficacy of the proposed SNN architecture, excelling in various performance metrics such as accuracy, F1 score, precision, and recall.",
      "paper_authors": [
        "Maneet Chatterjee",
        "Anuvab Sen",
        "Subhabrata Roy"
      ],
      "primary_category": "astro-ph.IM",
      "publish_time": "2024-06-12",
      "update_time": "2024-06-12",
      "comments": "6 Pages, 10 Figures, 2 Tables, Accepted by the 13th IEEE\n  International Conference on Communication Systems and Network\n  Technologies(CSNT 2024), April 06-07,2024,India",
      "repo_url": "#"
    },
    "2406.07862": {
      "paper_id": "2406.07862v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.07862v1",
      "paper_key": "2406.07862",
      "paper_title": "Self-Distillation Learning Based on Temporal-Spatial Consistency for Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2406.07862v1",
      "paper_abstract": "Spiking neural networks (SNNs) have attracted considerable attention for their event-driven, low-power characteristics and high biological interpretability. Inspired by knowledge distillation (KD), recent research has improved the performance of the SNN model with a pre-trained teacher model. However, additional teacher models require significant computational resources, and it is tedious to manually define the appropriate teacher network architecture. In this paper, we explore cost-effective self-distillation learning of SNNs to circumvent these concerns. Without an explicit defined teacher, the SNN generates pseudo-labels and learns consistency during training. On the one hand, we extend the timestep of the SNN during training to create an implicit temporal ``teacher\" that guides the learning of the original ``student\", i.e., the temporal self-distillation. On the other hand, we guide the output of the weak classifier at the intermediate stage by the final output of the SNN, i.e., the spatial self-distillation. Our temporal-spatial self-distillation (TSSD) learning method does not introduce any inference overhead and has excellent generalization ability. Extensive experiments on the static image datasets CIFAR10/100 and ImageNet as well as the neuromorphic datasets CIFAR10-DVS and DVS-Gesture validate the superior performance of the TSSD method. This paper presents a novel manner of fusing SNNs with KD, providing insights into high-performance SNN learning methods.",
      "paper_authors": [
        "Lin Zuo",
        "Yongqi Ding",
        "Mengmeng Jing",
        "Kunshan Yang",
        "Yunqian Yu"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-06-12",
      "update_time": "2024-06-12",
      "comments": "17 pages, 6 figures",
      "repo_url": "#"
    },
    "2406.06879": {
      "paper_id": "2406.06879v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.06879v1",
      "paper_key": "2406.06879",
      "paper_title": "SpikePipe: Accelerated Training of Spiking Neural Networks via Inter-Layer Pipelining and Multiprocessor Scheduling",
      "paper_url": "http://arxiv.org/abs/2406.06879v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) have gained popularity due to their high energy efficiency. Prior works have proposed various methods for training SNNs, including backpropagation-based methods. Training SNNs is computationally expensive compared to their conventional counterparts and would benefit from multiprocessor hardware acceleration. This is the first paper to propose inter-layer pipelining to accelerate training in SNNs using systolic array-based processors and multiprocessor scheduling. The impact of training using delayed gradients is observed using three networks training on different datasets, showing no degradation for small networks and < 10% degradation for large networks. The mapping of various training tasks of the SNN onto systolic arrays is formulated, and the proposed scheduling method is evaluated on the three networks. The results are compared against standard pipelining algorithms. The results show that the proposed method achieves an average speedup of 1.6X compared to standard pipelining algorithms, with an upwards of 2X improvement in some cases. The incurred communication overhead due to the proposed method is less than 0.5% of the total required communication of training.",
      "paper_authors": [
        "Sai Sanjeet",
        "Bibhu Datta Sahoo",
        "Keshab K. Parhi"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2024-06-11",
      "update_time": "2024-06-11",
      "comments": null,
      "repo_url": "#"
    },
    "2406.06305": {
      "paper_id": "2406.06305v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.06305v1",
      "paper_key": "2406.06305",
      "paper_title": "NeuroMoCo: A Neuromorphic Momentum Contrast Learning Method for Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2406.06305v1",
      "paper_abstract": "Recently, brain-inspired spiking neural networks (SNNs) have attracted great research attention owing to their inherent bio-interpretability, event-triggered properties and powerful perception of spatiotemporal information, which is beneficial to handling event-based neuromorphic datasets. In contrast to conventional static image datasets, event-based neuromorphic datasets present heightened complexity in feature extraction due to their distinctive time series and sparsity characteristics, which influences their classification accuracy. To overcome this challenge, a novel approach termed Neuromorphic Momentum Contrast Learning (NeuroMoCo) for SNNs is introduced in this paper by extending the benefits of self-supervised pre-training to SNNs to effectively stimulate their potential. This is the first time that self-supervised learning (SSL) based on momentum contrastive learning is realized in SNNs. In addition, we devise a novel loss function named MixInfoNCE tailored to their temporal characteristics to further increase the classification accuracy of neuromorphic datasets, which is verified through rigorous ablation experiments. Finally, experiments on DVS-CIFAR10, DVS128Gesture and N-Caltech101 have shown that NeuroMoCo of this paper establishes new state-of-the-art (SOTA) benchmarks: 83.6% (Spikformer-2-256), 98.62% (Spikformer-2-256), and 84.4% (SEW-ResNet-18), respectively.",
      "paper_authors": [
        "Yuqi Ma",
        "Huamin Wang",
        "Hangchi Shen",
        "Xuemei Chen",
        "Shukai Duan",
        "Shiping Wen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-10",
      "update_time": "2024-06-10",
      "comments": "32 pages,4 figures,4 tables",
      "repo_url": "#"
    },
    "2406.06075": {
      "paper_id": "2406.06075v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.06075v1",
      "paper_key": "2406.06075",
      "paper_title": "Supervised Radio Frequency Interference Detection with SNNs",
      "paper_url": "http://arxiv.org/abs/2406.06075v1",
      "paper_abstract": "Radio Frequency Interference (RFI) poses a significant challenge in radio astronomy, arising from terrestrial and celestial sources, disrupting observations conducted by radio telescopes. Addressing RFI involves intricate heuristic algorithms, manual examination, and, increasingly, machine learning methods. Given the dynamic and temporal nature of radio astronomy observations, Spiking Neural Networks (SNNs) emerge as a promising approach. In this study, we cast RFI detection as a supervised multi-variate time-series segmentation problem. Notably, our investigation explores the encoding of radio astronomy visibility data for SNN inference, considering six encoding schemes: rate, latency, delta-modulation, and three variations of the step-forward algorithm. We train a small two-layer fully connected SNN on simulated data derived from the Hydrogen Epoch of Reionization Array (HERA) telescope and perform extensive hyper-parameter optimization. Results reveal that latency encoding exhibits superior performance, achieving a per-pixel accuracy of 98.8% and an f1-score of 0.761. Remarkably, these metrics approach those of contemporary RFI detection algorithms, notwithstanding the simplicity and compactness of our proposed network architecture. This study underscores the potential of RFI detection as a benchmark problem for SNN researchers, emphasizing the efficacy of SNNs in addressing complex time-series segmentation tasks in radio astronomy.",
      "paper_authors": [
        "Nicholas J. Pritchard",
        "Andreas Wicenec",
        "Mohammed Bennamoun",
        "Richard Dodson"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-10",
      "update_time": "2024-06-10",
      "comments": "7 pages, 2 figures, 4 tables. International Conference on\n  Neuromorphic Systems (ICONS) 2024, Accepted",
      "repo_url": "#"
    },
    "2406.05371": {
      "paper_id": "2406.05371v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.05371v1",
      "paper_key": "2406.05371",
      "paper_title": "Spiking Neural Networks with Consistent Mapping Relations Allow High-Accuracy Inference",
      "paper_url": "http://arxiv.org/abs/2406.05371v1",
      "paper_abstract": "Spike-based neuromorphic hardware has demonstrated substantial potential in low energy consumption and efficient inference. However, the direct training of deep spiking neural networks is challenging, and conversion-based methods still require substantial time delay owing to unresolved conversion errors. We determine that the primary source of the conversion errors stems from the inconsistency between the mapping relationship of traditional activation functions and the input-output dynamics of spike neurons. To counter this, we introduce the Consistent ANN-SNN Conversion (CASC) framework. It includes the Consistent IF (CIF) neuron model, specifically contrived to minimize the influence of the stable point's upper bound, and the wake-sleep conversion (WSC) method, synergistically ensuring the uniformity of neuron behavior. This method theoretically achieves a loss-free conversion, markedly diminishing time delays and improving inference performance in extensive classification and object detection tasks. Our approach offers a viable pathway toward more efficient and effective neuromorphic systems.",
      "paper_authors": [
        "Yang Li",
        "Xiang He",
        "Qingqun Kong",
        "Yi Zeng"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-08",
      "update_time": "2024-06-08",
      "comments": null,
      "repo_url": "#"
    },
    "2406.03762": {
      "paper_id": "2406.03762v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.03762v1",
      "paper_key": "2406.03762",
      "paper_title": "CORTEX: Large-Scale Brain Simulator Utilizing Indegree Sub-Graph Decomposition on Fugaku Supercomputer",
      "paper_url": "http://arxiv.org/abs/2406.03762v1",
      "paper_abstract": "We introduce CORTEX, an algorithmic framework designed for large-scale brain simulation. Leveraging the computational capacity of the Fugaku Supercomputer, CORTEX maximizes available problem size and processing performance. Our primary innovation, Indegree Sub-Graph Decomposition, along with a suite of parallel algorithms, facilitates efficient domain decomposition by segmenting the global graph structure into smaller, identically structured sub-graphs. This segmentation allows for parallel processing of synaptic interactions without inter-process dependencies, effectively eliminating data racing at the thread level without necessitating mutexes or atomic operations. Additionally, this strategy enhances the overlap of communication and computation. Benchmark tests conducted on spiking neural networks, characterized by biological parameters, have demonstrated significant enhancements in both problem size and simulation performance, surpassing the capabilities of the current leading open-source solution, the NEST Simulator. Our work offers a powerful new tool for the field of neuromorphic computing and understanding brain function.",
      "paper_authors": [
        "Tianxiang Lyu",
        "Mitsuhisa Sato",
        "Shigeki Aoki",
        "Ryutaro Himeno",
        "Zhe Sun"
      ],
      "primary_category": "cs.DC",
      "publish_time": "2024-06-06",
      "update_time": "2024-06-06",
      "comments": null,
      "repo_url": "#"
    },
    "2406.03470": {
      "paper_id": "2406.03470v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.03470v1",
      "paper_key": "2406.03470",
      "paper_title": "SpikeZIP-TF: Conversion is All You Need for Transformer-based SNN",
      "paper_url": "http://arxiv.org/abs/2406.03470v1",
      "paper_abstract": "Spiking neural network (SNN) has attracted great attention due to its characteristic of high efficiency and accuracy. Currently, the ANN-to-SNN conversion methods can obtain ANN on-par accuracy SNN with ultra-low latency (8 time-steps) in CNN structure on computer vision (CV) tasks. However, as Transformer-based networks have achieved prevailing precision on both CV and natural language processing (NLP), the Transformer-based SNNs are still encounting the lower accuracy w.r.t the ANN counterparts. In this work, we introduce a novel ANN-to-SNN conversion method called SpikeZIP-TF, where ANN and SNN are exactly equivalent, thus incurring no accuracy degradation. SpikeZIP-TF achieves 83.82% accuracy on CV dataset (ImageNet) and 93.79% accuracy on NLP dataset (SST-2), which are higher than SOTA Transformer-based SNNs. The code is available in GitHub: https://github.com/Intelligent-Computing-Research-Group/SpikeZIP_transformer",
      "paper_authors": [
        "Kang You",
        "Zekai Xu",
        "Chen Nie",
        "Zhijie Deng",
        "Qinghai Guo",
        "Xiang Wang",
        "Zhezhi He"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-05",
      "update_time": "2024-06-05",
      "comments": "* These authors contributed equally to this work",
      "repo_url": "https://github.com/Intelligent-Computing-Research-Group/SpikeZIP-TF"
    },
    "2406.03287": {
      "paper_id": "2406.03287v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.03287v1",
      "paper_key": "2406.03287",
      "paper_title": "SpikeLM: Towards General Spike-Driven Language Modeling via Elastic Bi-Spiking Mechanisms",
      "paper_url": "http://arxiv.org/abs/2406.03287v1",
      "paper_abstract": "Towards energy-efficient artificial intelligence similar to the human brain, the bio-inspired spiking neural networks (SNNs) have advantages of biological plausibility, event-driven sparsity, and binary activation. Recently, large-scale language models exhibit promising generalization capability, making it a valuable issue to explore more general spike-driven models. However, the binary spikes in existing SNNs fail to encode adequate semantic information, placing technological challenges for generalization. This work proposes the first fully spiking mechanism for general language tasks, including both discriminative and generative ones. Different from previous spikes with {0,1} levels, we propose a more general spike formulation with bi-directional, elastic amplitude, and elastic frequency encoding, while still maintaining the addition nature of SNNs. In a single time step, the spike is enhanced by direction and amplitude information; in spike frequency, a strategy to control spike firing rate is well designed. We plug this elastic bi-spiking mechanism in language modeling, named SpikeLM. It is the first time to handle general language tasks with fully spike-driven models, which achieve much higher accuracy than previously possible. SpikeLM also greatly bridges the performance gap between SNNs and ANNs in language modeling. Our code is available at https://github.com/Xingrun-Xing/SpikeLM.",
      "paper_authors": [
        "Xingrun Xing",
        "Zheng Zhang",
        "Ziyi Ni",
        "Shitao Xiao",
        "Yiming Ju",
        "Siqi Fan",
        "Yequan Wang",
        "Jiajun Zhang",
        "Guoqi Li"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-05",
      "update_time": "2024-06-05",
      "comments": null,
      "repo_url": "https://github.com/xingrun-xing/spikelm"
    },
    "2407.04714": {
      "paper_id": "2407.04714v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.04714v1",
      "paper_key": "2407.04714",
      "paper_title": "Efficient Hybrid Neuromorphic-Bayesian Model for Olfaction Sensing: Detection and Classification",
      "paper_url": "http://arxiv.org/abs/2407.04714v1",
      "paper_abstract": "Olfaction sensing in autonomous robotics faces challenges in dynamic operations, energy efficiency, and edge processing. It necessitates a machine learning algorithm capable of managing real-world odor interference, ensuring resource efficiency for mobile robotics, and accurately estimating gas features for critical tasks such as odor mapping, localization, and alarm generation. This paper introduces a hybrid approach that exploits neuromorphic computing in combination with probabilistic inference to address these demanding requirements. Our approach implements a combination of a convolutional spiking neural network for feature extraction and a Bayesian spiking neural network for odor detection and identification. The developed algorithm is rigorously tested on a dataset for sensor drift compensation for robustness evaluation. Additionally, for efficiency evaluation, we compare the energy consumption of our model with a non-spiking machine learning algorithm under identical dataset and operating conditions. Our approach demonstrates superior efficiency alongside comparable accuracy outcomes.",
      "paper_authors": [
        "Rizwana Kausar",
        "Fakhreddine Zayer",
        "Jaime Viegas",
        "Jorge Dias"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2024-06-05",
      "update_time": "2024-06-05",
      "comments": "7 Pages, Conference",
      "repo_url": "#"
    },
    "2406.03054": {
      "paper_id": "2406.03054v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.03054v1",
      "paper_key": "2406.03054",
      "paper_title": "Spiking representation learning for associative memories",
      "paper_url": "http://arxiv.org/abs/2406.03054v1",
      "paper_abstract": "Networks of interconnected neurons communicating through spiking signals offer the bedrock of neural computations. Our brains spiking neural networks have the computational capacity to achieve complex pattern recognition and cognitive functions effortlessly. However, solving real-world problems with artificial spiking neural networks (SNNs) has proved to be difficult for a variety of reasons. Crucially, scaling SNNs to large networks and processing large-scale real-world datasets have been challenging, especially when compared to their non-spiking deep learning counterparts. The critical operation that is needed of SNNs is the ability to learn distributed representations from data and use these representations for perceptual, cognitive and memory operations. In this work, we introduce a novel SNN that performs unsupervised representation learning and associative memory operations leveraging Hebbian synaptic and activity-dependent structural plasticity coupled with neuron-units modelled as Poisson spike generators with sparse firing (~1 Hz mean and ~100 Hz maximum firing rate). Crucially, the architecture of our model derives from the neocortical columnar organization and combines feedforward projections for learning hidden representations and recurrent projections for forming associative memories. We evaluated the model on properties relevant for attractor-based associative memories such as pattern completion, perceptual rivalry, distortion resistance, and prototype extraction.",
      "paper_authors": [
        "Naresh Ravichandran",
        "Anders Lansner",
        "Pawel Herman"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-05",
      "update_time": "2024-06-05",
      "comments": null,
      "repo_url": "#"
    },
    "2406.03046": {
      "paper_id": "2406.03046v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.03046v1",
      "paper_key": "2406.03046",
      "paper_title": "When Spiking neural networks meet temporal attention image decoding and adaptive spiking neuron",
      "paper_url": "http://arxiv.org/abs/2406.03046v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) are capable of encoding and processing temporal information in a biologically plausible way. However, most existing SNN-based methods for image tasks do not fully exploit this feature. Moreover, they often overlook the role of adaptive threshold in spiking neurons, which can enhance their dynamic behavior and learning ability. To address these issues, we propose a novel method for image decoding based on temporal attention (TAID) and an adaptive Leaky-Integrate-and-Fire (ALIF) neuron model. Our method leverages the temporal information of SNN outputs to generate high-quality images that surpass the state-of-the-art (SOTA) in terms of Inception score, Fr\\'echet Inception Distance, and Fr\\'echet Autoencoder Distance. Furthermore, our ALIF neuron model achieves remarkable classification accuracy on MNIST (99.78\\%) and CIFAR-10 (93.89\\%) datasets, demonstrating the effectiveness of learning adaptive thresholds for spiking neurons. The code is available at https://github.com/bollossom/ICLR_TINY_SNN.",
      "paper_authors": [
        "Xuerui Qiu",
        "Zheng Luan",
        "Zhaorui Wang",
        "Rui-Jie Zhu"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-05",
      "update_time": "2024-06-05",
      "comments": "Accepted by ICLR23 tiny paper",
      "repo_url": "https://github.com/bollossom/iclr_tiny_snn"
    },
    "2406.02923": {
      "paper_id": "2406.02923v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.02923v1",
      "paper_key": "2406.02923",
      "paper_title": "Rethinking Spiking Neural Networks as State Space Models",
      "paper_url": "http://arxiv.org/abs/2406.02923v1",
      "paper_abstract": "Spiking neural networks (SNNs) are posited as a biologically plausible alternative to conventional neural architectures, with their core computational framework resting on the extensively studied leaky integrate-and-fire (LIF) neuron design. The stateful nature of LIF neurons has spurred ongoing discussions about the ability of SNNs to process sequential data, akin to recurrent neural networks (RNNs). Despite this, there remains a significant gap in the exploration of current SNNs within the realm of long-range dependency tasks. In this study, to extend the analysis of neuronal dynamics beyond simplistic LIF mechanism, we present a novel class of stochastic spiking neuronal model grounded in state space models. We expand beyond the scalar hidden state representation of LIF neurons, which traditionally comprises only the membrane potential, by proposing an n-dimensional hidden state. Additionally, we enable fine-tuned formulation of neuronal dynamics across each layer by introducing learnable parameters, as opposed to the fixed dynamics in LIF neurons. We also develop a robust framework for scaling these neuronal models to deep SNN-based architectures, ensuring efficient parallel training while also adeptly addressing the challenge of non-differentiability of stochastic spiking operation during the backward phase. Our models attain state-of-the-art performance among SNN models across diverse long-range dependency tasks, encompassing the Long Range Arena benchmark, permuted sequential MNIST, and the Speech Command dataset. Moreover, we provide an analysis of the energy efficiency advantages, emphasizing the sparse activity pattern intrinsic to this spiking model.",
      "paper_authors": [
        "Malyaban Bal",
        "Abhronil Sengupta"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-05",
      "update_time": "2024-06-05",
      "comments": "Under Review",
      "repo_url": "#"
    },
    "2406.02349": {
      "paper_id": "2406.02349v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.02349v1",
      "paper_key": "2406.02349",
      "paper_title": "CADE: Cosine Annealing Differential Evolution for Spiking Neural Network",
      "paper_url": "http://arxiv.org/abs/2406.02349v1",
      "paper_abstract": "Spiking neural networks (SNNs) have gained prominence for their potential in neuromorphic computing and energy-efficient artificial intelligence, yet optimizing them remains a formidable challenge for gradient-based methods due to their discrete, spike-based computation. This paper attempts to tackle the challenges by introducing Cosine Annealing Differential Evolution (CADE), designed to modulate the mutation factor (F) and crossover rate (CR) of differential evolution (DE) for the SNN model, i.e., Spiking Element Wise (SEW) ResNet. Extensive empirical evaluations were conducted to analyze CADE. CADE showed a balance in exploring and exploiting the search space, resulting in accelerated convergence and improved accuracy compared to existing gradient-based and DE-based methods. Moreover, an initialization method based on a transfer learning setting was developed, pretraining on a source dataset (i.e., CIFAR-10) and fine-tuning the target dataset (i.e., CIFAR-100), to improve population diversity. It was found to further enhance CADE for SNN. Remarkably, CADE elevates the performance of the highest accuracy SEW model by an additional 0.52 percentage points, underscoring its effectiveness in fine-tuning and enhancing SNNs. These findings emphasize the pivotal role of a scheduler for F and CR adjustment, especially for DE-based SNN. Source Code on Github: https://github.com/Tank-Jiang/CADE4SNN.",
      "paper_authors": [
        "Runhua Jiang",
        "Guodong Du",
        "Shuyang Yu",
        "Yifei Guo",
        "Sim Kuan Goh",
        "Ho-Kin Tang"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-04",
      "update_time": "2024-06-04",
      "comments": null,
      "repo_url": "https://github.com/tank-jiang/cade4snn"
    },
    "2406.01883": {
      "paper_id": "2406.01883v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.01883v1",
      "paper_key": "2406.01883",
      "paper_title": "Context Gating in Spiking Neural Networks: Achieving Lifelong Learning through Integration of Local and Global Plasticity",
      "paper_url": "http://arxiv.org/abs/2406.01883v1",
      "paper_abstract": "Humans learn multiple tasks in succession with minimal mutual interference, through the context gating mechanism in the prefrontal cortex (PFC). The brain-inspired models of spiking neural networks (SNN) have drawn massive attention for their energy efficiency and biological plausibility. To overcome catastrophic forgetting when learning multiple tasks in sequence, current SNN models for lifelong learning focus on memory reserving or regularization-based modification, while lacking SNN to replicate human experimental behavior. Inspired by biological context-dependent gating mechanisms found in PFC, we propose SNN with context gating trained by the local plasticity rule (CG-SNN) for lifelong learning. The iterative training between global and local plasticity for task units is designed to strengthen the connections between task neurons and hidden neurons and preserve the multi-task relevant information. The experiments show that the proposed model is effective in maintaining the past learning experience and has better task-selectivity than other methods during lifelong learning. Our results provide new insights that the CG-SNN model can extend context gating with good scalability on different SNN architectures with different spike-firing mechanisms. Thus, our models have good potential for parallel implementation on neuromorphic hardware and model human's behavior.",
      "paper_authors": [
        "Jiangrong Shen",
        "Wenyao Ni",
        "Qi Xu",
        "Gang Pan",
        "Huajin Tang"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-04",
      "update_time": "2024-06-04",
      "comments": null,
      "repo_url": "#"
    },
    "2407.08746": {
      "paper_id": "2407.08746v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.08746v1",
      "paper_key": "2407.08746",
      "paper_title": "Iteration over event space in time-to-first-spike spiking neural networks for Twitter bot classification",
      "paper_url": "http://arxiv.org/abs/2407.08746v1",
      "paper_abstract": "This study proposes a framework that extends existing time-coding time-to-first-spike spiking neural network (SNN) models to allow processing information changing over time. We explain spike propagation through a model with multiple input and output spikes at each neuron, as well as design training rules for end-to-end backpropagation. This strategy enables us to process information changing over time. The model is trained and evaluated on a Twitter bot detection task where the time of events (tweets and retweets) is the primary carrier of information. This task was chosen to evaluate how the proposed SNN deals with spike train data composed of hundreds of events occurring at timescales differing by almost five orders of magnitude. The impact of various parameters on model properties, performance and training-time stability is analyzed.",
      "paper_authors": [
        "Mateusz Pabian",
        "Dominik Rzepka",
        "Miros\u0142aw Pawlak"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-03",
      "update_time": "2024-06-03",
      "comments": null,
      "repo_url": "#"
    },
    "2407.08744": {
      "paper_id": "2407.08744v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.08744v1",
      "paper_key": "2407.08744",
      "paper_title": "Toward Efficient Deep Spiking Neuron Networks:A Survey On Compression",
      "paper_url": "http://arxiv.org/abs/2407.08744v1",
      "paper_abstract": "With the rapid development of deep learning, Deep Spiking Neural Networks (DSNNs) have emerged as promising due to their unique spike event processing and asynchronous computation. When deployed on neuromorphic chips, DSNNs offer significant power advantages over Deep Artificial Neural Networks (DANNs) and eliminate time and energy consuming multiplications due to the binary nature of spikes (0 or 1). Additionally, DSNNs excel in processing temporal information, making them potentially superior for handling temporal data compared to DANNs. However, their deep network structure and numerous parameters result in high computational costs and energy consumption, limiting real-life deployment. To enhance DSNNs efficiency, researchers have adapted methods from DANNs, such as pruning, quantization, and knowledge distillation, and developed specific techniques like reducing spike firing and pruning time steps. While previous surveys have covered DSNNs algorithms, hardware deployment, and general overviews, focused research on DSNNs compression and efficiency has been lacking. This survey addresses this gap by concentrating on efficient DSNNs and their compression methods. It begins with an exploration of DSNNs' biological background and computational units, highlighting differences from DANNs. It then delves into various compression methods, including pruning, quantization, knowledge distillation, and reducing spike firing, and concludes with suggestions for future research directions.",
      "paper_authors": [
        "Hui Xie",
        "Ge Yang",
        "Wenjuan Gao"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-03",
      "update_time": "2024-06-03",
      "comments": null,
      "repo_url": "#"
    },
    "2406.01167": {
      "paper_id": "2406.01167v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.01167v1",
      "paper_key": "2406.01167",
      "paper_title": "Pattern Formation in a Spiking Neural-Field of Renewal Neurons",
      "paper_url": "http://arxiv.org/abs/2406.01167v1",
      "paper_abstract": "Elucidating the neurophysiological mechanisms underlying neural pattern formation remains an outstanding challenge in Computational Neuroscience. In this paper, we address the issue of understanding the emergence of neural patterns by considering a network of renewal neurons, a well-established class of spiking cells. Taking the thermodynamics limit, the network's dynamics can be accurately represented by a partial differential equation coupled with a nonlocal differential equation. The stationary state of the nonlocal system is determined, and a perturbation analysis is performed to analytically characterize the conditions for the occurrence of Turing instabilities. Considering neural network parameters such as the synaptic coupling and the external drive, we numerically obtain the bifurcation line that separates the asynchronous regime from the emergence of patterns. Our theoretical findings provide a new and insightful perspective on the emergence of Turing patterns in spiking neural networks. In the long term, our formalism will enable the study of neural patterns while maintaining the connections between microscopic cellular properties, network coupling, and the emergence of Turing instabilities.",
      "paper_authors": [
        "Gregory Dumont",
        "Carmen Oana Tarniceriu"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2024-06-03",
      "update_time": "2024-06-03",
      "comments": "23 pages, 10 figures",
      "repo_url": "#"
    },
    "2406.01072": {
      "paper_id": "2406.01072v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.01072v1",
      "paper_key": "2406.01072",
      "paper_title": "Towards Efficient Deep Spiking Neural Networks Construction with Spiking Activity based Pruning",
      "paper_url": "http://arxiv.org/abs/2406.01072v1",
      "paper_abstract": "The emergence of deep and large-scale spiking neural networks (SNNs) exhibiting high performance across diverse complex datasets has led to a need for compressing network models due to the presence of a significant number of redundant structural units, aiming to more effectively leverage their low-power consumption and biological interpretability advantages. Currently, most model compression techniques for SNNs are based on unstructured pruning of individual connections, which requires specific hardware support. Hence, we propose a structured pruning approach based on the activity levels of convolutional kernels named Spiking Channel Activity-based (SCA) network pruning framework. Inspired by synaptic plasticity mechanisms, our method dynamically adjusts the network's structure by pruning and regenerating convolutional kernels during training, enhancing the model's adaptation to the current target task. While maintaining model performance, this approach refines the network architecture, ultimately reducing computational load and accelerating the inference process. This indicates that structured dynamic sparse learning methods can better facilitate the application of deep SNNs in low-power and high-efficiency scenarios.",
      "paper_authors": [
        "Yaxin Li",
        "Qi Xu",
        "Jiangrong Shen",
        "Hongming Xu",
        "Long Chen",
        "Gang Pan"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-03",
      "update_time": "2024-06-03",
      "comments": null,
      "repo_url": "#"
    },
    "2406.00473": {
      "paper_id": "2406.00473v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.00473v1",
      "paper_key": "2406.00473",
      "paper_title": "Pedestrian intention prediction in Adverse Weather Conditions with Spiking Neural Networks and Dynamic Vision Sensors",
      "paper_url": "http://arxiv.org/abs/2406.00473v1",
      "paper_abstract": "This study examines the effectiveness of Spiking Neural Networks (SNNs) paired with Dynamic Vision Sensors (DVS) to improve pedestrian detection in adverse weather, a significant challenge for autonomous vehicles. Utilizing the high temporal resolution and low latency of DVS, which excels in dynamic, low-light, and high-contrast environments, we assess the efficiency of SNNs compared to traditional Convolutional Neural Networks (CNNs).   Our experiments involved testing across diverse weather scenarios using a custom dataset from the CARLA simulator, mirroring real-world variability. SNN models, enhanced with Temporally Effective Batch Normalization, were trained and benchmarked against state-of-the-art CNNs to demonstrate superior accuracy and computational efficiency in complex conditions such as rain and fog.   The results indicate that SNNs, integrated with DVS, significantly reduce computational overhead and improve detection accuracy in challenging conditions compared to CNNs. This highlights the potential of DVS combined with bio-inspired SNN processing to enhance autonomous vehicle perception and decision-making systems, advancing intelligent transportation systems' safety features in varying operational environments.   Additionally, our research indicates that SNNs perform more efficiently in handling long perception windows and prediction tasks, rather than simple pedestrian detection.",
      "paper_authors": [
        "Mustafa Sakhai",
        "Szymon Mazurek",
        "Jakub Caputa",
        "Jan K. Argasi\u0144ski",
        "Maciej Wielgosz"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-01",
      "update_time": "2024-06-01",
      "comments": "Submitted for peer review to IEEE Transactions on Intelligent\n  Transportation Systems",
      "repo_url": "#"
    },
    "2406.00405": {
      "paper_id": "2406.00405v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.00405v2",
      "paper_key": "2406.00405",
      "paper_title": "Autaptic Synaptic Circuit Enhances Spatio-temporal Predictive Learning of Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2406.00405v2",
      "paper_abstract": "Spiking Neural Networks (SNNs) emulate the integrated-fire-leak mechanism found in biological neurons, offering a compelling combination of biological realism and energy efficiency. In recent years, they have gained considerable research interest. However, existing SNNs predominantly rely on the Leaky Integrate-and-Fire (LIF) model and are primarily suited for simple, static tasks. They lack the ability to effectively model long-term temporal dependencies and facilitate spatial information interaction, which is crucial for tackling complex, dynamic spatio-temporal prediction tasks. To tackle these challenges, this paper draws inspiration from the concept of autaptic synapses in biology and proposes a novel Spatio-Temporal Circuit (STC) model. The STC model integrates two learnable adaptive pathways, enhancing the spiking neurons' temporal memory and spatial coordination. We conduct a theoretical analysis of the dynamic parameters in the STC model, highlighting their contribution in establishing long-term memory and mitigating the issue of gradient vanishing. Through extensive experiments on multiple spatio-temporal prediction datasets, we demonstrate that our model outperforms other adaptive models. Furthermore, our model is compatible with existing spiking neuron models, thereby augmenting their dynamic representations. In essence, our work enriches the specificity and topological complexity of SNNs.",
      "paper_authors": [
        "Lihao Wang",
        "Zhaofei Yu"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-01",
      "update_time": "2024-06-05",
      "comments": "Accepted by ICML2024",
      "repo_url": "https://github.com/wangtianyi1874/stclif"
    },
    "2406.00389": {
      "paper_id": "2406.00389v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.00389v1",
      "paper_key": "2406.00389",
      "paper_title": "Understanding the Convergence in Balanced Resonate-and-Fire Neurons",
      "paper_url": "http://arxiv.org/abs/2406.00389v1",
      "paper_abstract": "Resonate-and-Fire (RF) neurons are an interesting complementary model for integrator neurons in spiking neural networks (SNNs). Due to their resonating membrane dynamics they can extract frequency patterns within the time domain. While established RF variants suffer from intrinsic shortcomings, the recently proposed balanced resonate-and-fire (BRF) neuron marked a significant methodological advance in terms of task performance, spiking and parameter efficiency, as well as, general stability and robustness, demonstrated for recurrent SNNs in various sequence learning tasks. One of the most intriguing result, however, was an immense improvement in training convergence speed and smoothness, overcoming the typical convergence dilemma in backprop-based SNN training. This paper aims at providing further intuitions about how and why these convergence advantages emerge. We show that BRF neurons, in contrast to well-established ALIF neurons, span a very clean and smooth - almost convex - error landscape. Furthermore, empirical results reveal that the convergence benefits are predominantly coupled with a divergence boundary-aware optimization, a major component of the BRF formulation that addresses the numerical stability of the time-discrete resonator approximation. These results are supported by a formal investigation of the membrane dynamics indicating that the gradient is transferred back through time without loss of magnitude.",
      "paper_authors": [
        "Saya Higuchi",
        "Sander M. Bohte",
        "Sebastian Otte"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-01",
      "update_time": "2024-06-01",
      "comments": null,
      "repo_url": "#"
    },
    "2406.00239": {
      "paper_id": "2406.00239v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.00239v1",
      "paper_key": "2406.00239",
      "paper_title": "A Review of Pulse-Coupled Neural Network Applications in Computer Vision and Image Processing",
      "paper_url": "http://arxiv.org/abs/2406.00239v1",
      "paper_abstract": "Research in neural models inspired by mammal's visual cortex has led to many spiking neural networks such as pulse-coupled neural networks (PCNNs). These models are oscillating, spatio-temporal models stimulated with images to produce several time-based responses. This paper reviews PCNN's state of the art, covering its mathematical formulation, variants, and other simplifications found in the literature. We present several applications in which PCNN architectures have successfully addressed some fundamental image processing and computer vision challenges, including image segmentation, edge detection, medical imaging, image fusion, image compression, object recognition, and remote sensing. Results achieved in these applications suggest that the PCNN architecture generates useful perceptual information relevant to a wide variety of computer vision tasks.",
      "paper_authors": [
        "Nurul Rafi",
        "Pablo Rivas"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-01",
      "update_time": "2024-06-01",
      "comments": "The 25th International Conference on Image Processing, Computer\n  Vision, and Pattern Recognition (IPCV 2021)",
      "repo_url": "#"
    },
    "2406.00177": {
      "paper_id": "2406.00177v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.00177v1",
      "paper_key": "2406.00177",
      "paper_title": "Flexible and Efficient Surrogate Gradient Modeling with Forward Gradient Injection",
      "paper_url": "http://arxiv.org/abs/2406.00177v1",
      "paper_abstract": "Automatic differentiation is a key feature of present deep learning frameworks. Moreover, they typically provide various ways to specify custom gradients within the computation graph, which is of particular importance for defining surrogate gradients in the realms of non-differentiable operations such as the Heaviside function in spiking neural networks (SNNs). PyTorch, for example, allows the custom specification of the backward pass of an operation by overriding its backward method. Other frameworks provide comparable options. While these methods are common practice and usually work well, they also have several disadvantages such as limited flexibility, additional source code overhead, poor usability, or a potentially strong negative impact on the effectiveness of automatic model optimization procedures. In this paper, an alternative way to formulate surrogate gradients is presented, namely, forward gradient injection (FGI). FGI applies a simple but effective combination of basic standard operations to inject an arbitrary gradient shape into the computational graph directly within the forward pass. It is demonstrated that using FGI is straightforward and convenient. Moreover, it is shown that FGI can significantly increase the model performance in comparison to custom backward methods in SNNs when using TorchScript. These results are complemented with a general performance study on recurrent SNNs with TorchScript and torch.compile, revealing the potential for a training speedup of more than 7x and an inference speedup of more than 16x in comparison with pure PyTorch.",
      "paper_authors": [
        "Sebastian Otte"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-05-31",
      "update_time": "2024-05-31",
      "comments": null,
      "repo_url": "#"
    },
    "2405.20694": {
      "paper_id": "2405.20694v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.20694v1",
      "paper_key": "2405.20694",
      "paper_title": "Robust Stable Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2405.20694v1",
      "paper_abstract": "Spiking neural networks (SNNs) are gaining popularity in deep learning due to their low energy budget on neuromorphic hardware. However, they still face challenges in lacking sufficient robustness to guard safety-critical applications such as autonomous driving. Many studies have been conducted to defend SNNs from the threat of adversarial attacks. This paper aims to uncover the robustness of SNN through the lens of the stability of nonlinear systems. We are inspired by the fact that searching for parameters altering the leaky integrate-and-fire dynamics can enhance their robustness. Thus, we dive into the dynamics of membrane potential perturbation and simplify the formulation of the dynamics. We present that membrane potential perturbation dynamics can reliably convey the intensity of perturbation. Our theoretical analyses imply that the simplified perturbation dynamics satisfy input-output stability. Thus, we propose a training framework with modified SNN neurons and to reduce the mean square of membrane potential perturbation aiming at enhancing the robustness of SNN. Finally, we experimentally verify the effectiveness of the framework in the setting of Gaussian noise training and adversarial training on the image classification task.",
      "paper_authors": [
        "Jianhao Ding",
        "Zhiyu Pan",
        "Yujia Liu",
        "Zhaofei Yu",
        "Tiejun Huang"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-05-31",
      "update_time": "2024-05-31",
      "comments": "Accepted by ICML2024",
      "repo_url": "https://github.com/DingJianhao/stable-snn"
    },
    "2405.20355": {
      "paper_id": "2405.20355v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.20355v1",
      "paper_key": "2405.20355",
      "paper_title": "Enhancing Adversarial Robustness in SNNs with Sparse Gradients",
      "paper_url": "http://arxiv.org/abs/2405.20355v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) have attracted great attention for their energy-efficient operations and biologically inspired structures, offering potential advantages over Artificial Neural Networks (ANNs) in terms of energy efficiency and interpretability. Nonetheless, similar to ANNs, the robustness of SNNs remains a challenge, especially when facing adversarial attacks. Existing techniques, whether adapted from ANNs or specifically designed for SNNs, exhibit limitations in training SNNs or defending against strong attacks. In this paper, we propose a novel approach to enhance the robustness of SNNs through gradient sparsity regularization. We observe that SNNs exhibit greater resilience to random perturbations compared to adversarial perturbations, even at larger scales. Motivated by this, we aim to narrow the gap between SNNs under adversarial and random perturbations, thereby improving their overall robustness. To achieve this, we theoretically prove that this performance gap is upper bounded by the gradient sparsity of the probability associated with the true label concerning the input image, laying the groundwork for a practical strategy to train robust SNNs by regularizing the gradient sparsity. We validate the effectiveness of our approach through extensive experiments on both image-based and event-based datasets. The results demonstrate notable improvements in the robustness of SNNs. Our work highlights the importance of gradient sparsity in SNNs and its role in enhancing robustness.",
      "paper_authors": [
        "Yujia Liu",
        "Tong Bu",
        "Jianhao Ding",
        "Zecheng Hao",
        "Tiejun Huang",
        "Zhaofei Yu"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-05-30",
      "update_time": "2024-05-30",
      "comments": "accepted by ICML 2024",
      "repo_url": "#"
    },
    "2405.19687": {
      "paper_id": "2405.19687v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.19687v2",
      "paper_key": "2405.19687",
      "paper_title": "Autonomous Driving with Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2405.19687v2",
      "paper_abstract": "Autonomous driving demands an integrated approach that encompasses perception, prediction, and planning, all while operating under strict energy constraints to enhance scalability and environmental sustainability. We present Spiking Autonomous Driving (SAD), the first unified Spiking Neural Network (SNN) to address the energy challenges faced by autonomous driving systems through its event-driven and energy-efficient nature. SAD is trained end-to-end and consists of three main modules: perception, which processes inputs from multi-view cameras to construct a spatiotemporal bird's eye view; prediction, which utilizes a novel dual-pathway with spiking neurons to forecast future states; and planning, which generates safe trajectories considering predicted occupancy, traffic rules, and ride comfort. Evaluated on the nuScenes dataset, SAD achieves competitive performance in perception, prediction, and planning tasks, while drawing upon the energy efficiency of SNNs. This work highlights the potential of neuromorphic computing to be applied to energy-efficient autonomous driving, a critical step toward sustainable and safety-critical automotive technology. Our code is available at \\url{https://github.com/ridgerchu/SAD}.",
      "paper_authors": [
        "Rui-Jie Zhu",
        "Ziqing Wang",
        "Leilani Gilpin",
        "Jason K. Eshraghian"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-05-30",
      "update_time": "2024-05-31",
      "comments": null,
      "repo_url": "https://github.com/ridgerchu/sad"
    },
    "2405.18828": {
      "paper_id": "2405.18828v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.18828v1",
      "paper_key": "2405.18828",
      "paper_title": "CHANI: Correlation-based Hawkes Aggregation of Neurons with bio-Inspiration",
      "paper_url": "http://arxiv.org/abs/2405.18828v1",
      "paper_abstract": "The present work aims at proving mathematically that a neural network inspired by biology can learn a classification task thanks to local transformations only. In this purpose, we propose a spiking neural network named CHANI (Correlation-based Hawkes Aggregation of Neurons with bio-Inspiration), whose neurons activity is modeled by Hawkes processes. Synaptic weights are updated thanks to an expert aggregation algorithm, providing a local and simple learning rule. We were able to prove that our network can learn on average and asymptotically. Moreover, we demonstrated that it automatically produces neuronal assemblies in the sense that the network can encode several classes and that a same neuron in the intermediate layers might be activated by more than one class, and we provided numerical simulations on synthetic dataset. This theoretical approach contrasts with the traditional empirical validation of biologically inspired networks and paves the way for understanding how local learning rules enable neurons to form assemblies able to represent complex concepts.",
      "paper_authors": [
        "Sophie Jaffard",
        "Samuel Vaiter",
        "Patricia Reynaud-Bouret"
      ],
      "primary_category": "math.ST",
      "publish_time": "2024-05-29",
      "update_time": "2024-05-29",
      "comments": null,
      "repo_url": "https://github.com/SophieJaffard/CHANI"
    },
    "2405.18587": {
      "paper_id": "2405.18587v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.18587v2",
      "paper_key": "2405.18587",
      "paper_title": "Emergence and long-term maintenance of modularity in plastic networks of spiking neurons",
      "paper_url": "http://arxiv.org/abs/2405.18587v2",
      "paper_abstract": "In the last three decades it has become clear that cortical regions, interconnected via white-matter fibers, form a modular and hierarchical network. This organization, which has also been seen at the microscopic level in the form of interconnected neural assemblies, is believed to support the coexistence of segregation (specialization) and integration (binding) of information. A fundamental open question is to understand how this complex structure can emerge in the brain. Here, we made a first step to address this question and propose that adaptation to various inputs could be the key driving mechanism for the formation of structural assemblies. To test this idea, we develop a model of quadratic integrate-and-fire spiking neurons, trained to stimuli targetting distinct sub-populations. The model is designed to satisfy several biologically plausible constraints: (i) the network contains excitatory and inhibitory neurons with Hebbian and anti-Hebbian STDP; and (ii) neither the neuronal activity nor the synaptic weights are frozen after the learning phase. Instead, the network continues firing spontaneously while synaptic plasticity remains active. We find that only the combination of the two inhibitory STDP sub-populations allows for the formation of stable modular organization in the network, with each sub-population playing a distinct role. The Hebbian sub-population controls for the firing rate and the anti-Hebbian mediates pattern selectivity. After the learning phase, the network activity settles into an asynchronous irregular resting-state, resembling the behaviour typically observed in-vivo in the cortex. This post-learning activity also displays spontaneous memory recalls, which are fundamental for the long-term consolidation of the learned memory items. The model introduced represents a starting point for the joint investigation of neural dynamics, connectivity and plasticity.",
      "paper_authors": [
        "Rapha\u00ebl Bergoin",
        "Alessandro Torcini",
        "Gustavo Deco",
        "Mathias Quoy",
        "Gorka Zamora-L\u00f3pez"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2024-05-28",
      "update_time": "2024-07-13",
      "comments": "28 pages, 11 figures",
      "repo_url": "#"
    },
    "2405.18019": {
      "paper_id": "2405.18019v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.18019v2",
      "paper_key": "2405.18019",
      "paper_title": "Mutual Information Analysis of Neuromorphic Coding for Distributed Wireless Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2405.18019v2",
      "paper_abstract": "Wireless spiking neural networks (WSNNs) allow energy-efficient communications, especially when considering edge intelligence and learning for both terrestrial beyond 5G/6G and space networking systems. Recent research work has revealed that distributed wireless SNNs (DWSNNs) show good performance in terms of inference accuracy and low energy consumption of edge devices, under the constraints of limited bandwidth and spike loss probability. Following this reasoning, this technology can be promising for wireless sensor networks (WSNs) in space applications, where the energy constraint is predominant. In this work, we focus on neuromorphic impulse radio (IR) transmission techniques for DWSNNs, quantitatively evaluating the features of different coding algorithms that can be viewed as IR modulations. Specifically, the main contribution of this work is the evaluation of information-theoretic measures that may help in quantifying performance trade-offs among existing neuromorphic coding techniques.",
      "paper_authors": [
        "Pietro Savazzi",
        "Anna Vizziello",
        "Fabio Dell'Acqua"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2024-05-28",
      "update_time": "2024-09-16",
      "comments": null,
      "repo_url": "#"
    },
    "2405.17903": {
      "paper_id": "2405.17903v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.17903v1",
      "paper_key": "2405.17903",
      "paper_title": "Reliable Object Tracking by Multimodal Hybrid Feature Extraction and Transformer-Based Fusion",
      "paper_url": "http://arxiv.org/abs/2405.17903v1",
      "paper_abstract": "Visual object tracking, which is primarily based on visible light image sequences, encounters numerous challenges in complicated scenarios, such as low light conditions, high dynamic ranges, and background clutter. To address these challenges, incorporating the advantages of multiple visual modalities is a promising solution for achieving reliable object tracking. However, the existing approaches usually integrate multimodal inputs through adaptive local feature interactions, which cannot leverage the full potential of visual cues, thus resulting in insufficient feature modeling. In this study, we propose a novel multimodal hybrid tracker (MMHT) that utilizes frame-event-based data for reliable single object tracking. The MMHT model employs a hybrid backbone consisting of an artificial neural network (ANN) and a spiking neural network (SNN) to extract dominant features from different visual modalities and then uses a unified encoder to align the features across different domains. Moreover, we propose an enhanced transformer-based module to fuse multimodal features using attention mechanisms. With these methods, the MMHT model can effectively construct a multiscale and multidimensional visual feature space and achieve discriminative feature modeling. Extensive experiments demonstrate that the MMHT model exhibits competitive performance in comparison with that of other state-of-the-art methods. Overall, our results highlight the effectiveness of the MMHT model in terms of addressing the challenges faced in visual object tracking tasks.",
      "paper_authors": [
        "Hongze Sun",
        "Rui Liu",
        "Wuque Cai",
        "Jun Wang",
        "Yue Wang",
        "Huajin Tang",
        "Yan Cui",
        "Dezhong Yao",
        "Daqing Guo"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-28",
      "update_time": "2024-05-28",
      "comments": "16 pages, 7 figures, 9 tabes; This work has been submitted for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible",
      "repo_url": "https://github.com/GuoLab-UESTC/MMHT"
    },
    "2405.16851": {
      "paper_id": "2405.16851v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.16851v1",
      "paper_key": "2405.16851",
      "paper_title": "Temporal Spiking Neural Networks with Synaptic Delay for Graph Reasoning",
      "paper_url": "http://arxiv.org/abs/2405.16851v1",
      "paper_abstract": "Spiking neural networks (SNNs) are investigated as biologically inspired models of neural computation, distinguished by their computational capability and energy efficiency due to precise spiking times and sparse spikes with event-driven computation. A significant question is how SNNs can emulate human-like graph-based reasoning of concepts and relations, especially leveraging the temporal domain optimally. This paper reveals that SNNs, when amalgamated with synaptic delay and temporal coding, are proficient in executing (knowledge) graph reasoning. It is elucidated that spiking time can function as an additional dimension to encode relation properties via a neural-generalized path formulation. Empirical results highlight the efficacy of temporal delay in relation processing and showcase exemplary performance in diverse graph reasoning tasks. The spiking model is theoretically estimated to achieve $20\\times$ energy savings compared to non-spiking counterparts, deepening insights into the capabilities and potential of biologically inspired SNNs for efficient reasoning. The code is available at https://github.com/pkuxmq/GRSNN.",
      "paper_authors": [
        "Mingqing Xiao",
        "Yixin Zhu",
        "Di He",
        "Zhouchen Lin"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-05-27",
      "update_time": "2024-05-27",
      "comments": "Accepted by ICML 2024",
      "repo_url": "https://github.com/pkuxmq/grsnn"
    },
    "2405.16466": {
      "paper_id": "2405.16466v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.16466v1",
      "paper_key": "2405.16466",
      "paper_title": "High-Performance Temporal Reversible Spiking Neural Networks with $O(L)$ Training Memory and $O(1)$ Inference Cost",
      "paper_url": "http://arxiv.org/abs/2405.16466v1",
      "paper_abstract": "Multi-timestep simulation of brain-inspired Spiking Neural Networks (SNNs) boost memory requirements during training and increase inference energy cost. Current training methods cannot simultaneously solve both training and inference dilemmas. This work proposes a novel Temporal Reversible architecture for SNNs (T-RevSNN) to jointly address the training and inference challenges by altering the forward propagation of SNNs. We turn off the temporal dynamics of most spiking neurons and design multi-level temporal reversible interactions at temporal turn-on spiking neurons, resulting in a $O(L)$ training memory. Combined with the temporal reversible nature, we redesign the input encoding and network organization of SNNs to achieve $O(1)$ inference energy cost. Then, we finely adjust the internal units and residual connections of the basic SNN block to ensure the effectiveness of sparse temporal information interaction. T-RevSNN achieves excellent accuracy on ImageNet, while the memory efficiency, training time acceleration, and inference energy efficiency can be significantly improved by $8.6 \\times$, $2.0 \\times$, and $1.6 \\times$, respectively. This work is expected to break the technical bottleneck of significantly increasing memory cost and training time for large-scale SNNs while maintaining high performance and low inference energy cost. Source code and models are available at: https://github.com/BICLab/T-RevSNN.",
      "paper_authors": [
        "JiaKui Hu",
        "Man Yao",
        "Xuerui Qiu",
        "Yuhong Chou",
        "Yuxuan Cai",
        "Ning Qiao",
        "Yonghong Tian",
        "Bo XU",
        "Guoqi Li"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-05-26",
      "update_time": "2024-05-26",
      "comments": "Accepted by ICML2024",
      "repo_url": "#"
    },
    "2405.16023": {
      "paper_id": "2405.16023v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.16023v1",
      "paper_key": "2405.16023",
      "paper_title": "Spiking Neural Network Phase Encoding for Cognitive Computing",
      "paper_url": "http://arxiv.org/abs/2405.16023v1",
      "paper_abstract": "This paper presents a novel approach for signal reconstruction using Spiking Neural Networks (SNN) based on the principles of Cognitive Informatics and Cognitive Computing. The proposed SNN leverages the Discrete Fourier Transform (DFT) to represent and reconstruct arbitrary time series signals. By employing N spiking neurons, the SNN captures the frequency components of the input signal, with each neuron assigned a unique frequency. The relationship between the magnitude and phase of the spiking neurons and the DFT coefficients is explored, enabling the reconstruction of the original signal. Additionally, the paper discusses the encoding of impulse delays and the phase differences between adjacent frequency components. This research contributes to the field of signal processing and provides insights into the application of SNN for cognitive signal analysis and reconstruction.",
      "paper_authors": [
        "Lei Zhang"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-05-25",
      "update_time": "2024-05-25",
      "comments": "8 pages, 9 figures, IEEE ICCI*CC 2023: 2023 IEEE 22nd International\n  Conference on Cognitive Informatics and Cognitive Computing Stanford Univ.\n  Palo Alto, CA, United States, August 19-21, 2023",
      "repo_url": "#"
    },
    "2405.15616": {
      "paper_id": "2405.15616v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.15616v1",
      "paper_key": "2405.15616",
      "paper_title": "Neuromorphic dreaming: A pathway to efficient learning in artificial agents",
      "paper_url": "http://arxiv.org/abs/2405.15616v1",
      "paper_abstract": "Achieving energy efficiency in learning is a key challenge for artificial intelligence (AI) computing platforms. Biological systems demonstrate remarkable abilities to learn complex skills quickly and efficiently. Inspired by this, we present a hardware implementation of model-based reinforcement learning (MBRL) using spiking neural networks (SNNs) on mixed-signal analog/digital neuromorphic hardware. This approach leverages the energy efficiency of mixed-signal neuromorphic chips while achieving high sample efficiency through an alternation of online learning, referred to as the \"awake\" phase, and offline learning, known as the \"dreaming\" phase. The model proposed includes two symbiotic networks: an agent network that learns by combining real and simulated experiences, and a learned world model network that generates the simulated experiences. We validate the model by training the hardware implementation to play the Atari game Pong. We start from a baseline consisting of an agent network learning without a world model and dreaming, which successfully learns to play the game. By incorporating dreaming, the number of required real game experiences are reduced significantly compared to the baseline. The networks are implemented using a mixed-signal neuromorphic processor, with the readout layers trained using a computer in-the-loop, while the other layers remain fixed. These results pave the way toward energy-efficient neuromorphic learning systems capable of rapid learning in real world applications and use-cases.",
      "paper_authors": [
        "Ingo Blakowski",
        "Dmitrii Zendrikov",
        "Cristiano Capone",
        "Giacomo Indiveri"
      ],
      "primary_category": "cs.AI",
      "publish_time": "2024-05-24",
      "update_time": "2024-05-24",
      "comments": null,
      "repo_url": "https://github.com/blakeyy/neuromorphic_dreaming"
    },
    "2405.15539": {
      "paper_id": "2405.15539v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.15539v1",
      "paper_key": "2405.15539",
      "paper_title": "A generalized neural tangent kernel for surrogate gradient learning",
      "paper_url": "http://arxiv.org/abs/2405.15539v1",
      "paper_abstract": "State-of-the-art neural network training methods depend on the gradient of the network function. Therefore, they cannot be applied to networks whose activation functions do not have useful derivatives, such as binary and discrete-time spiking neural networks. To overcome this problem, the activation function's derivative is commonly substituted with a surrogate derivative, giving rise to surrogate gradient learning (SGL). This method works well in practice but lacks theoretical foundation. The neural tangent kernel (NTK) has proven successful in the analysis of gradient descent. Here, we provide a generalization of the NTK, which we call the surrogate gradient NTK, that enables the analysis of SGL. First, we study a naive extension of the NTK to activation functions with jumps, demonstrating that gradient descent for such activation functions is also ill-posed in the infinite-width limit. To address this problem, we generalize the NTK to gradient descent with surrogate derivatives, i.e., SGL. We carefully define this generalization and expand the existing key theorems on the NTK with mathematical rigor. Further, we illustrate our findings with numerical experiments. Finally, we numerically compare SGL in networks with sign activation function and finite width to kernel regression with the surrogate gradient NTK; the results confirm that the surrogate gradient NTK provides a good characterization of SGL.",
      "paper_authors": [
        "Luke Eilers",
        "Raoul-Martin Memmesheimer",
        "Sven Goedeke"
      ],
      "primary_category": "stat.ML",
      "publish_time": "2024-05-24",
      "update_time": "2024-05-24",
      "comments": "52 pages, 3 figures + 2 supplementary figures",
      "repo_url": "#"
    },
    "2405.14851": {
      "paper_id": "2405.14851v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.14851v1",
      "paper_key": "2405.14851",
      "paper_title": "Domain Wall Magnetic Tunnel Junction Reliable Integrate and Fire Neuron",
      "paper_url": "http://arxiv.org/abs/2405.14851v1",
      "paper_abstract": "In spiking neural networks, neuron dynamics are described by the biologically realistic integrate-and-fire model that captures membrane potential accumulation and above-threshold firing behaviors. Among the hardware implementations of integrate-and-fire neuron devices, one important feature, reset, has been largely ignored. Here, we present the design and fabrication of a magnetic domain wall and magnetic tunnel junction based artificial integrate-and-fire neuron device that achieves reliable reset at the end of the integrate-fire cycle. We demonstrate the domain propagation in the domain wall racetrack (integration), reading using a magnetic tunnel junction (fire), and reset as the domain is ejected from the racetrack, showing the artificial neuron can be operated continuously over 100 integrate-fire-reset cycles. Both pulse amplitude and pulse number encoding is demonstrated. The device data is applied on an image classification task using a spiking neural network and shown to have comparable performance to an ideal leaky, integrate-and-fire neural network. These results achieve the first demonstration of reliable integrate-fire-reset in domain wall-magnetic tunnel junction-based neuron devices and shows the promise of spintronics for neuromorphic computing.",
      "paper_authors": [
        "Can Cui1",
        "Sam Liu",
        "Jaesuk Kwon",
        "Jean Anne C. Incorvia"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-05-23",
      "update_time": "2024-05-23",
      "comments": "17 pages, 5 figures",
      "repo_url": "#"
    },
    "2405.14474": {
      "paper_id": "2405.14474v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.14474v1",
      "paper_key": "2405.14474",
      "paper_title": "Time Cell Inspired Temporal Codebook in Spiking Neural Networks for Enhanced Image Generation",
      "paper_url": "http://arxiv.org/abs/2405.14474v1",
      "paper_abstract": "This paper presents a novel approach leveraging Spiking Neural Networks (SNNs) to construct a Variational Quantized Autoencoder (VQ-VAE) with a temporal codebook inspired by hippocampal time cells. This design captures and utilizes temporal dependencies, significantly enhancing the generative capabilities of SNNs. Neuroscientific research has identified hippocampal \"time cells\" that fire sequentially during temporally structured experiences. Our temporal codebook emulates this behavior by triggering the activation of time cell populations based on similarity measures as input stimuli pass through it. We conducted extensive experiments on standard benchmark datasets, including MNIST, FashionMNIST, CIFAR10, CelebA, and downsampled LSUN Bedroom, to validate our model's performance. Furthermore, we evaluated the effectiveness of the temporal codebook on neuromorphic datasets NMNIST and DVS-CIFAR10, and demonstrated the model's capability with high-resolution datasets such as CelebA-HQ, LSUN Bedroom, and LSUN Church. The experimental results indicate that our method consistently outperforms existing SNN-based generative models across multiple datasets, achieving state-of-the-art performance. Notably, our approach excels in generating high-resolution and temporally consistent data, underscoring the crucial role of temporal information in SNN-based generative modeling.",
      "paper_authors": [
        "Linghao Feng",
        "Dongcheng Zhao",
        "Sicheng Shen",
        "Yiting Dong",
        "Guobin Shen",
        "Yi Zeng"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-05-23",
      "update_time": "2024-05-23",
      "comments": null,
      "repo_url": "#"
    },
    "2405.14398": {
      "paper_id": "2405.14398v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.14398v1",
      "paper_key": "2405.14398",
      "paper_title": "SpGesture: Source-Free Domain-adaptive sEMG-based Gesture Recognition with Jaccard Attentive Spiking Neural Network",
      "paper_url": "http://arxiv.org/abs/2405.14398v1",
      "paper_abstract": "Surface electromyography (sEMG) based gesture recognition offers a natural and intuitive interaction modality for wearable devices. Despite significant advancements in sEMG-based gesture-recognition models, existing methods often suffer from high computational latency and increased energy consumption. Additionally, the inherent instability of sEMG signals, combined with their sensitivity to distribution shifts in real-world settings, compromises model robustness.   To tackle these challenges, we propose a novel SpGesture framework based on Spiking Neural Networks, which possesses several unique merits compared with existing methods: (1) Robustness: By utilizing membrane potential as a memory list, we pioneer the introduction of Source-Free Domain Adaptation into SNN for the first time. This enables SpGesture to mitigate the accuracy degradation caused by distribution shifts. (2) High Accuracy: With a novel Spiking Jaccard Attention, SpGesture enhances the SNNs' ability to represent sEMG features, leading to a notable rise in system accuracy. To validate SpGesture's performance, we collected a new sEMG gesture dataset which has different forearm postures, where SpGesture achieved the highest accuracy among the baselines ($89.26\\%$). Moreover, the actual deployment on the CPU demonstrated a system latency below 100ms, well within real-time requirements. This impressive performance showcases SpGesture's potential to enhance the applicability of sEMG in real-world scenarios. The code is available at https://anonymous.4open.science/r/SpGesture.",
      "paper_authors": [
        "Weiyu Guo",
        "Ying Sun",
        "Yijie Xu",
        "Ziyue Qiao",
        "Yongkui Yang",
        "Hui Xiong"
      ],
      "primary_category": "cs.HC",
      "publish_time": "2024-05-23",
      "update_time": "2024-05-23",
      "comments": null,
      "repo_url": "#"
    },
    "2405.14362": {
      "paper_id": "2405.14362v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.14362v1",
      "paper_key": "2405.14362",
      "paper_title": "Advancing Spiking Neural Networks for Sequential Modeling with Central Pattern Generators",
      "paper_url": "http://arxiv.org/abs/2405.14362v1",
      "paper_abstract": "Spiking neural networks (SNNs) represent a promising approach to developing artificial neural networks that are both energy-efficient and biologically plausible. However, applying SNNs to sequential tasks, such as text classification and time-series forecasting, has been hindered by the challenge of creating an effective and hardware-friendly spike-form positional encoding (PE) strategy. Drawing inspiration from the central pattern generators (CPGs) in the human brain, which produce rhythmic patterned outputs without requiring rhythmic inputs, we propose a novel PE technique for SNNs, termed CPG-PE. We demonstrate that the commonly used sinusoidal PE is mathematically a specific solution to the membrane potential dynamics of a particular CPG. Moreover, extensive experiments across various domains, including time-series forecasting, natural language processing, and image classification, show that SNNs with CPG-PE outperform their conventional counterparts. Additionally, we perform analysis experiments to elucidate the mechanism through which SNNs encode positional information and to explore the function of CPGs in the human brain. This investigation may offer valuable insights into the fundamental principles of neural computation.",
      "paper_authors": [
        "Changze Lv",
        "Dongqi Han",
        "Yansen Wang",
        "Xiaoqing Zheng",
        "Xuanjing Huang",
        "Dongsheng Li"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-05-23",
      "update_time": "2024-05-23",
      "comments": null,
      "repo_url": "#"
    },
    "2405.13976": {
      "paper_id": "2405.13976v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.13976v2",
      "paper_key": "2405.13976",
      "paper_title": "EchoSpike Predictive Plasticity: An Online Local Learning Rule for Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2405.13976v2",
      "paper_abstract": "The drive to develop artificial neural networks that efficiently utilize resources has generated significant interest in bio-inspired Spiking Neural Networks (SNNs). These networks are particularly attractive due to their potential in applications requiring low power and memory. This potential is further enhanced by the ability to perform online local learning, enabling them to adapt to dynamic environments. This requires the model to be adaptive in a self-supervised manner. While self-supervised learning has seen great success in many deep learning domains, its application for online local learning in multi-layer SNNs remains underexplored. In this paper, we introduce the \"EchoSpike Predictive Plasticity\" (ESPP) learning rule, a pioneering online local learning rule designed to leverage hierarchical temporal dynamics in SNNs through predictive and contrastive coding. We validate the effectiveness of this approach using benchmark datasets, demonstrating that it performs on par with current state-of-the-art supervised learning rules. The temporal and spatial locality of ESPP makes it particularly well-suited for low-cost neuromorphic processors, representing a significant advancement in developing biologically plausible self-supervised learning models for neuromorphic computing at the edge.",
      "paper_authors": [
        "Lars Graf",
        "Zhe Su",
        "Giacomo Indiveri"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-05-22",
      "update_time": "2024-05-26",
      "comments": "11 pages, 6 figures, submitted to IEEE",
      "repo_url": "#"
    },
    "2405.19351": {
      "paper_id": "2405.19351v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.19351v1",
      "paper_key": "2405.19351",
      "paper_title": "Resonate-and-Fire Spiking Neurons for Target Detection and Hand Gesture Recognition: A Hybrid Approach",
      "paper_url": "http://arxiv.org/abs/2405.19351v1",
      "paper_abstract": "Hand gesture recognition using radar often relies on computationally expensive fast Fourier transforms. This paper proposes an alternative approach that bypasses fast Fourier transforms using resonate-and-fire neurons. These neurons directly detect the hand in the time-domain signal, eliminating the need for fast Fourier transforms to retrieve range information. Following detection, a simple Goertzel algorithm is employed to extract five key features, eliminating the need for a second fast Fourier transform. These features are then fed into a recurrent neural network, achieving an accuracy of 98.21% for classifying five gestures. The proposed approach demonstrates competitive performance with reduced complexity compared to traditional methods",
      "paper_authors": [
        "Ahmed Shaaban",
        "Zeineb Chaabouni",
        "Maximilian Strobel",
        "Wolfgang Furtner",
        "Robert Weigel",
        "Fabian Lurz"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2024-05-22",
      "update_time": "2024-05-22",
      "comments": null,
      "repo_url": "#"
    },
    "2405.13672": {
      "paper_id": "2405.13672v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.13672v2",
      "paper_key": "2405.13672",
      "paper_title": "Advancing Spiking Neural Networks towards Multiscale Spatiotemporal Interaction Learning",
      "paper_url": "http://arxiv.org/abs/2405.13672v2",
      "paper_abstract": "Recent advancements in neuroscience research have propelled the development of Spiking Neural Networks (SNNs), which not only have the potential to further advance neuroscience research but also serve as an energy-efficient alternative to Artificial Neural Networks (ANNs) due to their spike-driven characteristics. However, previous studies often neglected the multiscale information and its spatiotemporal correlation between event data, leading SNN models to approximate each frame of input events as static images. We hypothesize that this oversimplification significantly contributes to the performance gap between SNNs and traditional ANNs. To address this issue, we have designed a Spiking Multiscale Attention (SMA) module that captures multiscale spatiotemporal interaction information. Furthermore, we developed a regularization method named Attention ZoneOut (AZO), which utilizes spatiotemporal attention weights to reduce the model's generalization error through pseudo-ensemble training. Our approach has achieved state-of-the-art results on mainstream neural morphology datasets. Additionally, we have reached a performance of 77.1% on the Imagenet-1K dataset using a 104-layer ResNet architecture enhanced with SMA and AZO. This achievement confirms the state-of-the-art performance of SNNs with non-transformer architectures and underscores the effectiveness of our method in bridging the performance gap between SNN models and traditional ANN models.",
      "paper_authors": [
        "Yimeng Shan",
        "Malu Zhang",
        "Rui-jie Zhu",
        "Xuerui Qiu",
        "Jason K. Eshraghian",
        "Haicheng Qu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-22",
      "update_time": "2024-05-27",
      "comments": null,
      "repo_url": "https://github.com/Ym-Shan/Spiking_Multiscale_Attention_Arxiv"
    },
    "2405.13587": {
      "paper_id": "2405.13587v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.13587v1",
      "paper_key": "2405.13587",
      "paper_title": "Exact Gradients for Stochastic Spiking Neural Networks Driven by Rough Signals",
      "paper_url": "http://arxiv.org/abs/2405.13587v1",
      "paper_abstract": "We introduce a mathematically rigorous framework based on rough path theory to model stochastic spiking neural networks (SSNNs) as stochastic differential equations with event discontinuities (Event SDEs) and driven by c\\`adl\\`ag rough paths. Our formalism is general enough to allow for potential jumps to be present both in the solution trajectories as well as in the driving noise. We then identify a set of sufficient conditions ensuring the existence of pathwise gradients of solution trajectories and event times with respect to the network's parameters and show how these gradients satisfy a recursive relation. Furthermore, we introduce a general-purpose loss function defined by means of a new class of signature kernels indexed on c\\`adl\\`ag rough paths and use it to train SSNNs as generative models. We provide an end-to-end autodifferentiable solver for Event SDEs and make its implementation available as part of the $\\texttt{diffrax}$ library. Our framework is, to our knowledge, the first enabling gradient-based training of SSNNs with noise affecting both the spike timing and the network's dynamics.",
      "paper_authors": [
        "Christian Holberg",
        "Cristopher Salvi"
      ],
      "primary_category": "stat.ML",
      "publish_time": "2024-05-22",
      "update_time": "2024-05-22",
      "comments": null,
      "repo_url": "#"
    },
    "2405.12849": {
      "paper_id": "2405.12849v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.12849v2",
      "paper_key": "2405.12849",
      "paper_title": "Adaptive Robotic Arm Control with a Spiking Recurrent Neural Network on a Digital Accelerator",
      "paper_url": "http://arxiv.org/abs/2405.12849v2",
      "paper_abstract": "With the rise of artificial intelligence, neural network simulations of biological neuron models are being explored to reduce the footprint of learning and inference in resource-constrained task scenarios. A mainstream type of such networks are spiking neural networks (SNNs) based on simplified Integrate and Fire models for which several hardware accelerators have emerged. Among them, the ReckOn chip was introduced as a recurrent SNN allowing for both online training and execution of tasks based on arbitrary sensory modalities, demonstrated for vision, audition, and navigation. As a fully digital and open-source chip, we adapted ReckOn to be implemented on a Xilinx Multiprocessor System on Chip system (MPSoC), facilitating its deployment in embedded systems and increasing the setup flexibility. We present an overview of the system, and a Python framework to use it on a Pynq ZU platform. We validate the architecture and implementation in the new scenario of robotic arm control, and show how the simulated accuracy is preserved with a peak performance of 3.8M events processed per second.",
      "paper_authors": [
        "Alejandro Linares-Barranco",
        "Luciano Prono",
        "Robert Lengenstein",
        "Giacomo Indiveri",
        "Charlotte Frenkel"
      ],
      "primary_category": "cs.AR",
      "publish_time": "2024-05-21",
      "update_time": "2024-06-02",
      "comments": "Under review at ICECS'24",
      "repo_url": "#"
    },
    "2407.09488": {
      "paper_id": "2407.09488v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.09488v1",
      "paper_key": "2407.09488",
      "paper_title": "Manifold Learning via Memory and Context",
      "paper_url": "http://arxiv.org/abs/2407.09488v1",
      "paper_abstract": "Given a memory with infinite capacity, can we solve the learning problem? Apparently, nature has solved this problem as evidenced by the evolution of mammalian brains. Inspired by the organizational principles underlying hippocampal-neocortical systems, we present a navigation-based approach to manifold learning using memory and context. The key insight is to navigate on the manifold and memorize the positions of each route as inductive/design bias of direct-fit-to-nature. We name it navigation-based because our approach can be interpreted as navigating in the latent space of sensorimotor learning via memory (local maps) and context (global indexing). The indexing to the library of local maps within global coordinates is collected by an associative memory serving as the librarian, which mimics the coupling between the hippocampus and the neocortex. In addition to breaking from the notorious bias-variance dilemma and the curse of dimensionality, we discuss the biological implementation of our navigation-based learning by episodic and semantic memories in neural systems. The energy efficiency of navigation-based learning makes it suitable for hardware implementation on non-von Neumann architectures, such as the emerging in-memory computing paradigm, including spiking neural networks and memristor neural networks.",
      "paper_authors": [
        "Xin Li"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2024-05-17",
      "update_time": "2024-05-17",
      "comments": null,
      "repo_url": "#"
    },
    "2406.01600": {
      "paper_id": "2406.01600v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.01600v1",
      "paper_key": "2406.01600",
      "paper_title": "NeuroAssist: Enhancing Cognitive-Computer Synergy with Adaptive AI and Advanced Neural Decoding for Efficient EEG Signal Classification",
      "paper_url": "http://arxiv.org/abs/2406.01600v1",
      "paper_abstract": "Traditional methods of controlling prosthetics frequently encounter difficulties regarding flexibility and responsiveness, which can substantially impact people with varying cognitive and physical abilities. Advancements in computational neuroscience and machine learning (ML) have recently led to the development of highly advanced brain-computer interface (BCI) systems that may be customized to meet individual requirements. To address these issues, we propose NeuroAssist, a sophisticated method for analyzing EEG data that merges state-of-the-art BCI technology with adaptable artificial intelligence (AI) algorithms. NeuroAssist's hybrid neural network design efficiently overcomes the constraints of conventional EEG data processing. Our methodology combines a Natural Language Processing (NLP) BERT model to extract complex features from numerical EEG data and utilizes LSTM networks to handle temporal dynamics. In addition, we integrate spiking neural networks (SNNs) and deep Q-networks (DQN) to improve decision-making and flexibility. Our preprocessing method classifies motor imagery (MI) one-versus-the-rest using a common spatial pattern (CSP) while preserving EEG temporal characteristics. The hybrid architecture of NeuroAssist serves as the DQN's Q-network, enabling continuous feedback-based improvement and adaptability. This enables it to acquire optimal actions through trial and error. This experimental analysis has been conducted on the GigaScience and BCI-competition-IV-2a datasets, which have shown exceptional effectiveness in categorizing MI-EEG signals, obtaining an impressive classification accuracy of 99.17%. NeuroAssist offers a crucial approach to current assistive technology by potentially enhancing the speed and versatility of BCI systems.",
      "paper_authors": [
        "Eeshan G. Dandamudi"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2024-05-17",
      "update_time": "2024-05-17",
      "comments": null,
      "repo_url": "#"
    },
    "2405.10582": {
      "paper_id": "2405.10582v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.10582v1",
      "paper_key": "2405.10582",
      "paper_title": "General oracle inequalities for a penalized log-likelihood criterion based on non-stationary data",
      "paper_url": "http://arxiv.org/abs/2405.10582v1",
      "paper_abstract": "We prove oracle inequalities for a penalized log-likelihood criterion that hold even if the data are not independent and not stationary, based on a martingale approach. The assumptions are checked for various contexts: density estimation with independent and identically distributed (i.i.d) data, hidden Markov models, spiking neural networks, adversarial bandits. In each case, we compare our results to the literature, showing that, although we lose some logarithmic factors in the most classical case (i.i.d.), these results are comparable or more general than the existing results in the more dependent cases.",
      "paper_authors": [
        "Julien Aubert",
        "Luc Leh\u00e9ricy",
        "Patricia Reynaud-Bouret"
      ],
      "primary_category": "math.ST",
      "publish_time": "2024-05-17",
      "update_time": "2024-05-17",
      "comments": null,
      "repo_url": "#"
    },
    "2405.08392": {
      "paper_id": "2405.08392v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.08392v1",
      "paper_key": "2405.08392",
      "paper_title": "Neuromorphic Robust Estimation of Nonlinear Dynamical Systems Applied to Satellite Rendezvous",
      "paper_url": "http://arxiv.org/abs/2405.08392v1",
      "paper_abstract": "State estimation of nonlinear dynamical systems has long aimed to balance accuracy, computational efficiency, robustness, and reliability. The rapid evolution of various industries has amplified the demand for estimation frameworks that satisfy all these factors. This study introduces a neuromorphic approach for robust filtering of nonlinear dynamical systems: SNN-EMSIF (spiking neural network-extended modified sliding innovation filter). SNN-EMSIF combines the computational efficiency and scalability of SNNs with the robustness of EMSIF, an estimation framework designed for nonlinear systems with zero-mean Gaussian noise. Notably, the weight matrices are designed according to the system model, eliminating the need for a learning process. The framework's efficacy is evaluated through comprehensive Monte Carlo simulations, comparing SNN-EMSIF with EKF and EMSIF. Additionally, it is compared with SNN-EKF in the presence of modeling uncertainties and neuron loss, using RMSEs as a metric. The results demonstrate the superior accuracy and robustness of SNN-EMSIF. Further analysis of runtimes and spiking patterns reveals an impressive reduction of 85% in emitted spikes compared to possible spikes, highlighting the computational efficiency of SNN-EMSIF. This framework offers a promising solution for robust estimation in nonlinear dynamical systems, opening new avenues for efficient and reliable estimation in various industries that can benefit from neuromorphic computing.",
      "paper_authors": [
        "Reza Ahmadvand",
        "Sarah Safura Sharif",
        "Yaser Mike Banad"
      ],
      "primary_category": "eess.SY",
      "publish_time": "2024-05-14",
      "update_time": "2024-05-14",
      "comments": "11 figures, 7 tables, 37 pages. arXiv admin note: text overlap with\n  arXiv:2307.07963",
      "repo_url": "#"
    },
    "2405.06277": {
      "paper_id": "2405.06277v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.06277v1",
      "paper_key": "2405.06277",
      "paper_title": "Learning A Spiking Neural Network for Efficient Image Deraining",
      "paper_url": "http://arxiv.org/abs/2405.06277v1",
      "paper_abstract": "Recently, spiking neural networks (SNNs) have demonstrated substantial potential in computer vision tasks. In this paper, we present an Efficient Spiking Deraining Network, called ESDNet. Our work is motivated by the observation that rain pixel values will lead to a more pronounced intensity of spike signals in SNNs. However, directly applying deep SNNs to image deraining task still remains a significant challenge. This is attributed to the information loss and training difficulties that arise from discrete binary activation and complex spatio-temporal dynamics. To this end, we develop a spiking residual block to convert the input into spike signals, then adaptively optimize the membrane potential by introducing attention weights to adjust spike responses in a data-driven manner, alleviating information loss caused by discrete binary activation. By this way, our ESDNet can effectively detect and analyze the characteristics of rain streaks by learning their fluctuations. This also enables better guidance for the deraining process and facilitates high-quality image reconstruction. Instead of relying on the ANN-SNN conversion strategy, we introduce a gradient proxy strategy to directly train the model for overcoming the challenge of training. Experimental results show that our approach gains comparable performance against ANN-based methods while reducing energy consumption by 54%. The code source is available at https://github.com/MingTian99/ESDNet.",
      "paper_authors": [
        "Tianyu Song",
        "Guiyue Jin",
        "Pengpeng Li",
        "Kui Jiang",
        "Xiang Chen",
        "Jiyu Jin"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-10",
      "update_time": "2024-05-10",
      "comments": "Accepted by IJCAI2024",
      "repo_url": "https://github.com/mingtian99/esdnet"
    },
    "2407.03111": {
      "paper_id": "2407.03111v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.03111v2",
      "paper_key": "2407.03111",
      "paper_title": "Compressed Latent Replays for Lightweight Continual Learning on Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2407.03111v2",
      "paper_abstract": "Rehearsal-based Continual Learning (CL) has been intensely investigated in Deep Neural Networks (DNNs). However, its application in Spiking Neural Networks (SNNs) has not been explored in depth. In this paper we introduce the first memory-efficient implementation of Latent Replay (LR)-based CL for SNNs, designed to seamlessly integrate with resource-constrained devices. LRs combine new samples with latent representations of previously learned data, to mitigate forgetting. Experiments on the Heidelberg SHD dataset with Sample and Class-Incremental tasks reach a Top-1 accuracy of 92.5% and 92%, respectively, without forgetting the previously learned information. Furthermore, we minimize the LRs' requirements by applying a time-domain compression, reducing by two orders of magnitude their memory requirement, with respect to a naive rehearsal setup, with a maximum accuracy drop of 4%. On a Multi-Class-Incremental task, our SNN learns 10 new classes from an initial set of 10, reaching a Top-1 accuracy of 78.4% on the full test set.",
      "paper_authors": [
        "Alberto Dequino",
        "Alessio Carpegna",
        "Davide Nadalini",
        "Alessandro Savino",
        "Luca Benini",
        "Stefano Di Carlo",
        "Francesco Conti"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-05-08",
      "update_time": "2024-07-04",
      "comments": null,
      "repo_url": "https://github.com/dequino/spiking-compressed-continual-learning"
    },
    "2405.04049": {
      "paper_id": "2405.04049v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.04049v1",
      "paper_key": "2405.04049",
      "paper_title": "Watermarking Neuromorphic Brains: Intellectual Property Protection in Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2405.04049v1",
      "paper_abstract": "As spiking neural networks (SNNs) gain traction in deploying neuromorphic computing solutions, protecting their intellectual property (IP) has become crucial. Without adequate safeguards, proprietary SNN architectures are at risk of theft, replication, or misuse, which could lead to significant financial losses for the owners. While IP protection techniques have been extensively explored for artificial neural networks (ANNs), their applicability and effectiveness for the unique characteristics of SNNs remain largely unexplored. In this work, we pioneer an investigation into adapting two prominent watermarking approaches, namely, fingerprint-based and backdoor-based mechanisms to secure proprietary SNN architectures. We conduct thorough experiments to evaluate the impact on fidelity, resilience against overwrite threats, and resistance to compression attacks when applying these watermarking techniques to SNNs, drawing comparisons with their ANN counterparts. This study lays the groundwork for developing neuromorphic-aware IP protection strategies tailored to the distinctive dynamics of SNNs.",
      "paper_authors": [
        "Hamed Poursiami",
        "Ihsen Alouani",
        "Maryam Parsa"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-05-07",
      "update_time": "2024-05-07",
      "comments": "7 pages, 7 figures",
      "repo_url": "#"
    },
    "2406.06543": {
      "paper_id": "2406.06543v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.06543v1",
      "paper_key": "2406.06543",
      "paper_title": "SparrowSNN: A Hardware/software Co-design for Energy Efficient ECG Classification",
      "paper_url": "http://arxiv.org/abs/2406.06543v1",
      "paper_abstract": "Heart disease is one of the leading causes of death worldwide. Given its high risk and often asymptomatic nature, real-time continuous monitoring is essential. Unlike traditional artificial neural networks (ANNs), spiking neural networks (SNNs) are well-known for their energy efficiency, making them ideal for wearable devices and energy-constrained edge computing platforms. However, current energy measurement of SNN implementations for detecting heart diseases typically rely on empirical values, often overlooking hardware overhead. Additionally, the integer and fire activations in SNNs require multiple memory accesses and repeated computations, which can further compromise energy efficiency. In this paper, we propose sparrowSNN, a redesign of the standard SNN workflow from a hardware perspective, and present a dedicated ASIC design for SNNs, optimized for ultra-low power wearable devices used in heartbeat classification. Using the MIT-BIH dataset, our SNN achieves a state-of-the-art accuracy of 98.29% for SNNs, with energy consumption of 31.39nJ per inference and power usage of 6.1uW, making sparrowSNN the highest accuracy with the lowest energy use among comparable systems. We also compare the energy-to-accuracy trade-offs between SNNs and quantized ANNs, offering recommendations on insights on how best to use SNNs.",
      "paper_authors": [
        "Zhanglu Yan",
        "Zhenyu Bai",
        "Tulika Mitra",
        "Weng-Fai Wong"
      ],
      "primary_category": "cs.AR",
      "publish_time": "2024-05-06",
      "update_time": "2024-05-06",
      "comments": null,
      "repo_url": "#"
    },
    "2405.04289": {
      "paper_id": "2405.04289v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.04289v2",
      "paper_key": "2405.04289",
      "paper_title": "Direct Training High-Performance Deep Spiking Neural Networks: A Review of Theories and Methods",
      "paper_url": "http://arxiv.org/abs/2405.04289v2",
      "paper_abstract": "Spiking neural networks (SNNs) offer a promising energy-efficient alternative to artificial neural networks (ANNs), in virtue of their high biological plausibility, rich spatial-temporal dynamics, and event-driven computation. The direct training algorithms based on the surrogate gradient method provide sufficient flexibility to design novel SNN architectures and explore the spatial-temporal dynamics of SNNs. According to previous studies, the performance of models is highly dependent on their sizes. Recently, direct training deep SNNs have achieved great progress on both neuromorphic datasets and large-scale static datasets. Notably, transformer-based SNNs show comparable performance with their ANN counterparts. In this paper, we provide a new perspective to summarize the theories and methods for training deep SNNs with high performance in a systematic and comprehensive way, including theory fundamentals, spiking neuron models, advanced SNN models and residual architectures, software frameworks and neuromorphic hardware, applications, and future trends. The reviewed papers are collected at https://github.com/zhouchenlin2096/Awesome-Spiking-Neural-Networks",
      "paper_authors": [
        "Chenlin Zhou",
        "Han Zhang",
        "Liutao Yu",
        "Yumin Ye",
        "Zhaokun Zhou",
        "Liwei Huang",
        "Zhengyu Ma",
        "Xiaopeng Fan",
        "Huihui Zhou",
        "Yonghong Tian"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-05-06",
      "update_time": "2024-07-10",
      "comments": "Accepted by Frontiers in Neuroscience",
      "repo_url": "https://github.com/zhouchenlin2096/awesome-spiking-neural-networks"
    },
    "2405.04546": {
      "paper_id": "2405.04546v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.04546v1",
      "paper_key": "2405.04546",
      "paper_title": "Consciousness Driven Spike Timing Dependent Plasticity",
      "paper_url": "http://arxiv.org/abs/2405.04546v1",
      "paper_abstract": "Spiking Neural Networks (SNNs), recognized for their biological plausibility and energy efficiency, employ sparse and asynchronous spikes for communication. However, the training of SNNs encounters difficulties coming from non-differentiable activation functions and the movement of spike-based inter-layer data. Spike-Timing Dependent Plasticity (STDP), inspired by neurobiology, plays a crucial role in SNN's learning, but its still lacks the conscious part of the brain used for learning. Considering the issue, this research work proposes a Consciousness Driven STDP (CD-STDP), an improved solution addressing inherent limitations observed in conventional STDP models. CD-STDP, designed to infuse the conscious part as coefficients of long-term potentiation (LTP) and long-term depression (LTD), exhibit a dynamic nature. The model connects LTP and LTD coefficients to current and past state of synaptic activities, respectively, enhancing consciousness and adaptability. This consciousness empowers the model to effectively learn while understanding the input patterns. The conscious coefficient adjustment in response to current and past synaptic activity extends the model's conscious and other cognitive capabilities, offering a refined and efficient approach for real-world applications. Evaluations on MNIST, FashionMNIST and CALTECH datasets showcase $CD$-STDP's remarkable accuracy of 98.6%, 85.61% and 99.0%, respectively, in a single hidden layer SNN. In addition, analysis of conscious elements and consciousness of the proposed model on SNN is performed.",
      "paper_authors": [
        "Sushant Yadav",
        "Santosh Chaudhary",
        "Rajesh Kumar"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2024-05-04",
      "update_time": "2024-05-04",
      "comments": "10 pages, 5 figures, Journal",
      "repo_url": "#"
    },
    "2405.02546": {
      "paper_id": "2405.02546v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.02546v3",
      "paper_key": "2405.02546",
      "paper_title": "Scaling SNNs Trained Using Equilibrium Propagation to Convolutional Architectures",
      "paper_url": "http://arxiv.org/abs/2405.02546v3",
      "paper_abstract": "Equilibrium Propagation (EP) is a biologically plausible local learning algorithm initially developed for convergent recurrent neural networks (RNNs), where weight updates rely solely on the connecting neuron states across two phases. The gradient calculations in EP have been shown to approximate the gradients computed by Backpropagation Through Time (BPTT) when an infinitesimally small nudge factor is used. This property makes EP a powerful candidate for training Spiking Neural Networks (SNNs), which are commonly trained by BPTT. However, in the spiking domain, previous studies on EP have been limited to architectures involving few linear layers. In this work, for the first time we provide a formulation for training convolutional spiking convergent RNNs using EP, bridging the gap between spiking and non-spiking convergent RNNs. We demonstrate that for spiking convergent RNNs, there is a mismatch in the maximum pooling and its inverse operation, leading to inaccurate gradient estimation in EP. Substituting this with average pooling resolves this issue and enables accurate gradient estimation for spiking convergent RNNs. We also highlight the memory efficiency of EP compared to BPTT. In the regime of SNNs trained by EP, our experimental results indicate state-of-the-art performance on the MNIST and FashionMNIST datasets, with test errors of 0.97% and 8.89%, respectively. These results are comparable to those of convergent RNNs and SNNs trained by BPTT. These findings underscore EP as an optimal choice for on-chip training and a biologically-plausible method for computing error gradients.",
      "paper_authors": [
        "Jiaqi Lin",
        "Malyaban Bal",
        "Abhronil Sengupta"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-05-04",
      "update_time": "2024-07-02",
      "comments": null,
      "repo_url": "#"
    },
    "2405.02146": {
      "paper_id": "2405.02146v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.02146v1",
      "paper_key": "2405.02146",
      "paper_title": "A Spiking Neural Network Decoder for Implantable Brain Machine Interfaces and its Sparsity-aware Deployment on RISC-V Microcontrollers",
      "paper_url": "http://arxiv.org/abs/2405.02146v1",
      "paper_abstract": "Implantable Brain-machine interfaces (BMIs) are promising for motor rehabilitation and mobility augmentation, and they demand accurate and energy-efficient algorithms. In this paper, we propose a novel spiking neural network (SNN) decoder for regression tasks for implantable BMIs. The SNN is trained with enhanced spatio-temporal backpropagation to fully leverage its capability to handle temporal problems. The proposed SNN decoder outperforms the state-of-the-art Kalman filter and artificial neural network (ANN) decoders in offline finger velocity decoding tasks. The decoder is deployed on a RISC-V-based hardware platform and optimized to exploit sparsity. The proposed implementation has an average power consumption of 0.50 mW in a duty-cycled mode. When conducting continuous inference without duty-cycling, it achieves an energy efficiency of 1.88 uJ per inference, which is 5.5X less than the baseline ANN. Additionally, the average decoding latency is 0.12 ms for each inference, which is 5.7X faster than the ANN implementation.",
      "paper_authors": [
        "Jiawei Liao",
        "Oscar Toomey",
        "Xiaying Wang",
        "Lars Widmer",
        "Cynthia A. Chestek",
        "Luca Benini",
        "Taekwang Jang"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2024-05-03",
      "update_time": "2024-05-03",
      "comments": null,
      "repo_url": "#"
    },
    "2405.02019": {
      "paper_id": "2405.02019v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.02019v1",
      "paper_key": "2405.02019",
      "paper_title": "Fast Algorithms for Spiking Neural Network Simulation with FPGAs",
      "paper_url": "http://arxiv.org/abs/2405.02019v1",
      "paper_abstract": "Using OpenCL-based high-level synthesis, we create a number of spiking neural network (SNN) simulators for the Potjans-Diesmann cortical microcircuit for a high-end Field-Programmable Gate Array (FPGA). Our best simulators simulate the circuit 25\\% faster than real-time, require less than 21 nJ per synaptic event, and are bottle-necked by the device's on-chip memory. Speed-wise they compare favorably to the state-of-the-art GPU-based simulators and their energy usage is lower than any other published result. This result is the first for simulating the circuit on a single hardware accelerator. We also extensively analyze the techniques and algorithms we implement our simulators with, many of which can be realized on other types of hardware. Thus, this article is of interest to any researcher or practitioner interested in efficient SNN simulation, whether they target FPGAs or not.",
      "paper_authors": [
        "Bj\u00f6rn A. Lindqvist",
        "Artur Podobas"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-05-03",
      "update_time": "2024-05-03",
      "comments": "34 pages",
      "repo_url": "#"
    },
    "2405.01419": {
      "paper_id": "2405.01419v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.01419v2",
      "paper_key": "2405.01419",
      "paper_title": "Natural Language to Verilog: Design of a Recurrent Spiking Neural Network using Large Language Models and ChatGPT",
      "paper_url": "http://arxiv.org/abs/2405.01419v2",
      "paper_abstract": "This paper investigates the use of Large Language Models (LLMs) for automating the generation of hardware description code, aiming to explore their potential in supporting and enhancing the development of efficient neuromorphic computing architectures. Building on our prior work, we employ OpenAI's ChatGPT4 and natural language prompts to synthesize a RTL Verilog module of a programmable recurrent spiking neural network, while also generating test benches to assess the system's correctness. The resultant design was validated in three case studies, the exclusive OR,the IRIS flower classification and the MNIST hand-written digit classification, achieving accuracies of up to 96.6%. To verify its synthesizability and implementability, the design was prototyped on a field-programmable gate array and implemented on SkyWater 130 nm technology by using an open-source electronic design automation flow. Additionally, we have submitted it to Tiny Tapeout 6 chip fabrication program to further evaluate the system on-chip performance in the future.",
      "paper_authors": [
        "Paola Vitolo",
        "George Psaltakis",
        "Michael Tomlinson",
        "Gian Domenico Licciardo",
        "Andreas G. Andreou"
      ],
      "primary_category": "cs.AR",
      "publish_time": "2024-05-02",
      "update_time": "2024-08-14",
      "comments": "This paper was presented at the IEEE/ACM International Conference on\n  Neuromorphic Systems (ICONS), July 30-Aug 2, 2024, Arlington, VA, USA",
      "repo_url": "#"
    },
    "2405.01305": {
      "paper_id": "2405.01305v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.01305v2",
      "paper_key": "2405.01305",
      "paper_title": "Distributed Representations Enable Robust Multi-Timescale Symbolic Computation in Neuromorphic Hardware",
      "paper_url": "http://arxiv.org/abs/2405.01305v2",
      "paper_abstract": "Programming recurrent spiking neural networks (RSNNs) to robustly perform multi-timescale computation remains a difficult challenge. To address this, we describe a single-shot weight learning scheme to embed robust multi-timescale dynamics into attractor-based RSNNs, by exploiting the properties of high-dimensional distributed representations. We embed finite state machines into the RSNN dynamics by superimposing a symmetric autoassociative weight matrix and asymmetric transition terms, which are each formed by the vector binding of an input and heteroassociative outer-products between states. Our approach is validated through simulations with highly non-ideal weights; an experimental closed-loop memristive hardware setup; and on Loihi 2, where it scales seamlessly to large state machines. This work introduces a scalable approach to embed robust symbolic computation through recurrent dynamics into neuromorphic hardware, without requiring parameter fine-tuning or significant platform-specific optimisation. Moreover, it demonstrates that distributed symbolic representations serve as a highly capable representation-invariant language for cognitive algorithms in neuromorphic hardware.",
      "paper_authors": [
        "Madison Cotteret",
        "Hugh Greatorex",
        "Alpha Renner",
        "Junren Chen",
        "Emre Neftci",
        "Huaqiang Wu",
        "Giacomo Indiveri",
        "Martin Ziegler",
        "Elisabetta Chicca"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-05-02",
      "update_time": "2024-07-16",
      "comments": "15 pages, 6 figures. Supplementary material: 7 pages, 6 figures",
      "repo_url": "#"
    },
    "2405.00627": {
      "paper_id": "2405.00627v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.00627v1",
      "paper_key": "2405.00627",
      "paper_title": "Koopman-based Deep Learning for Nonlinear System Estimation",
      "paper_url": "http://arxiv.org/abs/2405.00627v1",
      "paper_abstract": "Nonlinear differential equations are encountered as models of fluid flow, spiking neurons, and many other systems of interest in the real world. Common features of these systems are that their behaviors are difficult to describe exactly and invariably unmodeled dynamics present challenges in making precise predictions. In many cases the models exhibit extremely complicated behavior due to bifurcations and chaotic regimes. In this paper, we present a novel data-driven linear estimator that uses Koopman operator theory to extract finite-dimensional representations of complex nonlinear systems. The extracted model is used together with a deep reinforcement learning network that learns the optimal stepwise actions to predict future states of the original nonlinear system. Our estimator is also adaptive to a diffeomorphic transformation of the nonlinear system which enables transfer learning to compute state estimates of the transformed system without relearning from scratch.",
      "paper_authors": [
        "Zexin Sun",
        "Mingyu Chen",
        "John Baillieul"
      ],
      "primary_category": "eess.SY",
      "publish_time": "2024-05-01",
      "update_time": "2024-05-01",
      "comments": "11 pages",
      "repo_url": "#"
    },
    "2405.00433": {
      "paper_id": "2405.00433v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.00433v1",
      "paper_key": "2405.00433",
      "paper_title": "Weight Sparsity Complements Activity Sparsity in Neuromorphic Language Models",
      "paper_url": "http://arxiv.org/abs/2405.00433v1",
      "paper_abstract": "Activity and parameter sparsity are two standard methods of making neural networks computationally more efficient. Event-based architectures such as spiking neural networks (SNNs) naturally exhibit activity sparsity, and many methods exist to sparsify their connectivity by pruning weights. While the effect of weight pruning on feed-forward SNNs has been previously studied for computer vision tasks, the effects of pruning for complex sequence tasks like language modeling are less well studied since SNNs have traditionally struggled to achieve meaningful performance on these tasks. Using a recently published SNN-like architecture that works well on small-scale language modeling, we study the effects of weight pruning when combined with activity sparsity. Specifically, we study the trade-off between the multiplicative efficiency gains the combination affords and its effect on task performance for language modeling. To dissect the effects of the two sparsities, we conduct a comparative analysis between densely activated models and sparsely activated event-based models across varying degrees of connectivity sparsity. We demonstrate that sparse activity and sparse connectivity complement each other without a proportional drop in task performance for an event-based neural network trained on the Penn Treebank and WikiText-2 language modeling datasets. Our results suggest sparsely connected event-based neural networks are promising candidates for effective and efficient sequence modeling.",
      "paper_authors": [
        "Rishav Mukherji",
        "Mark Sch\u00f6ne",
        "Khaleelulla Khan Nazeer",
        "Christian Mayr",
        "David Kappel",
        "Anand Subramoney"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-05-01",
      "update_time": "2024-05-01",
      "comments": "arXiv admin note: text overlap with arXiv:2311.07625",
      "repo_url": "#"
    },
    "2404.19419": {
      "paper_id": "2404.19419v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.19419v2",
      "paper_key": "2404.19419",
      "paper_title": "Active Dendrites Enable Efficient Continual Learning in Time-To-First-Spike Neural Networks",
      "paper_url": "http://arxiv.org/abs/2404.19419v2",
      "paper_abstract": "While the human brain efficiently adapts to new tasks from a continuous stream of information, neural network models struggle to learn from sequential information without catastrophically forgetting previously learned tasks. This limitation presents a significant hurdle in deploying edge devices in real-world scenarios where information is presented in an inherently sequential manner. Active dendrites of pyramidal neurons play an important role in the brain ability to learn new tasks incrementally. By exploiting key properties of time-to-first-spike encoding and leveraging its high sparsity, we present a novel spiking neural network model enhanced with active dendrites. Our model can efficiently mitigate catastrophic forgetting in temporally-encoded SNNs, which we demonstrate with an end-of-training accuracy across tasks of 88.3% on the test set using the Split MNIST dataset. Furthermore, we provide a novel digital hardware architecture that paves the way for real-world deployment in edge devices. Using a Xilinx Zynq-7020 SoC FPGA, we demonstrate a 100-% match with our quantized software model, achieving an average inference time of 37.3 ms and an 80.0% accuracy.",
      "paper_authors": [
        "Lorenzo Pes",
        "Rick Luiken",
        "Federico Corradi",
        "Charlotte Frenkel"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-04-30",
      "update_time": "2024-06-11",
      "comments": "This work was accepted and presented at AICAS 2024",
      "repo_url": "#"
    },
    "2404.19165": {
      "paper_id": "2404.19165v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.19165v1",
      "paper_key": "2404.19165",
      "paper_title": "DelGrad: Exact gradients in spiking networks for learning transmission delays and weights",
      "paper_url": "http://arxiv.org/abs/2404.19165v1",
      "paper_abstract": "Spiking neural networks (SNNs) inherently rely on the timing of signals for representing and processing information. Transmission delays play an important role in shaping these temporal characteristics. Recent work has demonstrated the substantial advantages of learning these delays along with synaptic weights, both in terms of accuracy and memory efficiency. However, these approaches suffer from drawbacks in terms of precision and efficiency, as they operate in discrete time and with approximate gradients, while also requiring membrane potential recordings for calculating parameter updates. To alleviate these issues, we propose an analytical approach for calculating exact loss gradients with respect to both synaptic weights and delays in an event-based fashion. The inclusion of delays emerges naturally within our proposed formalism, enriching the model's search space with a temporal dimension. Our algorithm is purely based on the timing of individual spikes and does not require access to other variables such as membrane potentials. We explicitly compare the impact on accuracy and parameter efficiency of different types of delays - axonal, dendritic and synaptic. Furthermore, while previous work on learnable delays in SNNs has been mostly confined to software simulations, we demonstrate the functionality and benefits of our approach on the BrainScaleS-2 neuromorphic platform.",
      "paper_authors": [
        "Julian G\u00f6ltz",
        "Jimmy Weber",
        "Laura Kriener",
        "Peter Lake",
        "Melika Payvand",
        "Mihai A. Petrovici"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-04-30",
      "update_time": "2024-04-30",
      "comments": "15 pages, 7 figures",
      "repo_url": "#"
    },
    "2404.18066": {
      "paper_id": "2404.18066v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.18066v1",
      "paper_key": "2404.18066",
      "paper_title": "Quantized Context Based LIF Neurons for Recurrent Spiking Neural Networks in 45nm",
      "paper_url": "http://arxiv.org/abs/2404.18066v1",
      "paper_abstract": "In this study, we propose the first hardware implementation of a context-based recurrent spiking neural network (RSNN) emphasizing on integrating dual information streams within the neocortical pyramidal neurons specifically Context- Dependent Leaky Integrate and Fire (CLIF) neuron models, essential element in RSNN. We present a quantized version of the CLIF neuron (qCLIF), developed through a hardware-software codesign approach utilizing the sparse activity of RSNN. Implemented in a 45nm technology node, the qCLIF is compact (900um^2) and achieves a high accuracy of 90% despite 8 bit quantization on DVS gesture classification dataset. Our analysis spans a network configuration from 10 to 200 qCLIF neurons, supporting up to 82k synapses within a 1.86 mm^2 footprint, demonstrating scalability and efficiency",
      "paper_authors": [
        "Sai Sukruth Bezugam",
        "Yihao Wu",
        "JaeBum Yoo",
        "Dmitri Strukov",
        "Bongjin Kim"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-04-28",
      "update_time": "2024-04-28",
      "comments": "7 Pages, 7 Figures, 2 Tables",
      "repo_url": "#"
    },
    "2404.17719": {
      "paper_id": "2404.17719v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.17719v3",
      "paper_key": "2404.17719",
      "paper_title": "Stochastic Spiking Neural Networks with First-to-Spike Coding",
      "paper_url": "http://arxiv.org/abs/2404.17719v3",
      "paper_abstract": "Spiking Neural Networks (SNNs), recognized as the third generation of neural networks, are known for their bio-plausibility and energy efficiency, especially when implemented on neuromorphic hardware. However, the majority of existing studies on SNNs have concentrated on deterministic neurons with rate coding, a method that incurs substantial computational overhead due to lengthy information integration times and fails to fully harness the brain's probabilistic inference capabilities and temporal dynamics. In this work, we explore the merger of novel computing and information encoding schemes in SNN architectures where we integrate stochastic spiking neuron models with temporal coding techniques. Through extensive benchmarking with other deterministic SNNs and rate-based coding, we investigate the tradeoffs of our proposal in terms of accuracy, inference latency, spiking sparsity, energy consumption, and robustness. Our work is the first to extend the scalability of direct training approaches of stochastic SNNs with temporal encoding to VGG architectures and beyond-MNIST datasets.",
      "paper_authors": [
        "Yi Jiang",
        "Sen Lu",
        "Abhronil Sengupta"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-04-26",
      "update_time": "2024-07-01",
      "comments": null,
      "repo_url": "#"
    },
    "2404.17456": {
      "paper_id": "2404.17456v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.17456v1",
      "paper_key": "2404.17456",
      "paper_title": "Converting High-Performance and Low-Latency SNNs through Explicit Modelling of Residual Error in ANNs",
      "paper_url": "http://arxiv.org/abs/2404.17456v1",
      "paper_abstract": "Spiking neural networks (SNNs) have garnered interest due to their energy efficiency and superior effectiveness on neuromorphic chips compared with traditional artificial neural networks (ANNs). One of the mainstream approaches to implementing deep SNNs is the ANN-SNN conversion, which integrates the efficient training strategy of ANNs with the energy-saving potential and fast inference capability of SNNs. However, under extreme low-latency conditions, the existing conversion theory suggests that the problem of misrepresentation of residual membrane potentials in SNNs, i.e., the inability of IF neurons with a reset-by-subtraction mechanism to respond to residual membrane potentials beyond the range from resting potential to threshold, leads to a performance gap in the converted SNNs compared to the original ANNs. This severely limits the possibility of practical application of SNNs on delay-sensitive edge devices. Existing conversion methods addressing this problem usually involve modifying the state of the conversion spiking neurons. However, these methods do not consider their adaptability and compatibility with neuromorphic chips. We propose a new approach based on explicit modeling of residual errors as additive noise. The noise is incorporated into the activation function of the source ANN, which effectively reduces the residual error. Our experiments on the CIFAR10/100 dataset verify that our approach exceeds the prevailing ANN-SNN conversion methods and directly trained SNNs concerning accuracy and the required time steps. Overall, our method provides new ideas for improving SNN performance under ultra-low-latency conditions and is expected to promote practical neuromorphic hardware applications for further development.",
      "paper_authors": [
        "Zhipeng Huang",
        "Jianhao Ding",
        "Zhiyu Pan",
        "Haoran Li",
        "Ying Fang",
        "Zhaofei Yu",
        "Jian K. Liu"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-04-26",
      "update_time": "2024-04-26",
      "comments": null,
      "repo_url": "#"
    },
    "2404.17335": {
      "paper_id": "2404.17335v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.17335v2",
      "paper_key": "2404.17335",
      "paper_title": "A Novel Spike Transformer Network for Depth Estimation from Event Cameras via Cross-modality Knowledge Distillation",
      "paper_url": "http://arxiv.org/abs/2404.17335v2",
      "paper_abstract": "Depth estimation is crucial for interpreting complex environments, especially in areas such as autonomous vehicle navigation and robotics. Nonetheless, obtaining accurate depth readings from event camera data remains a formidable challenge. Event cameras operate differently from traditional digital cameras, continuously capturing data and generating asynchronous binary spikes that encode time, location, and light intensity. Yet, the unique sampling mechanisms of event cameras render standard image based algorithms inadequate for processing spike data. This necessitates the development of innovative, spike-aware algorithms tailored for event cameras, a task compounded by the irregularity, continuity, noise, and spatial and temporal characteristics inherent in spiking data.Harnessing the strong generalization capabilities of transformer neural networks for spatiotemporal data, we propose a purely spike-driven spike transformer network for depth estimation from spiking camera data. To address performance limitations with Spiking Neural Networks (SNN), we introduce a novel single-stage cross-modality knowledge transfer framework leveraging knowledge from a large vision foundational model of artificial neural networks (ANN) (DINOv2) to enhance the performance of SNNs with limited data. Our experimental results on both synthetic and real datasets show substantial improvements over existing models, with notable gains in Absolute Relative and Square Relative errors (49% and 39.77% improvements over the benchmark model Spike-T, respectively). Besides accuracy, the proposed model also demonstrates reduced power consumptions, a critical factor for practical applications.",
      "paper_authors": [
        "Xin Zhang",
        "Liangxiu Han",
        "Tam Sobeih",
        "Lianghao Han",
        "Darren Dancey"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-04-26",
      "update_time": "2024-05-01",
      "comments": "16 pages",
      "repo_url": "#"
    },
    "2404.17241": {
      "paper_id": "2404.17241v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.17241v1",
      "paper_key": "2404.17241",
      "paper_title": "Synchronized Stepwise Control of Firing and Learning Thresholds in a Spiking Randomly Connected Neural Network toward Hardware Implementation",
      "paper_url": "http://arxiv.org/abs/2404.17241v1",
      "paper_abstract": "We propose hardware-oriented models of intrinsic plasticity (IP) and synaptic plasticity (SP) for spiking randomly connected recursive neural network (RNN). Although the potential of RNNs for temporal data processing has been demonstrated, randomness of the network architecture often causes performance degradation. Self-organization mechanism using IP and SP can mitigate the degradation, therefore, we compile these functions in a spiking neuronal model. To implement the function of IP, a variable firing threshold is introduced to each excitatory neuron in the RNN that changes stepwise in accordance with its activity. We also define other thresholds for SP that synchronize with the firing threshold, which determine the direction of stepwise synaptic update that is executed on receiving a pre-synaptic spike. We demonstrate the effectiveness of our model through simulations of temporal data learning and anomaly detection with a spiking RNN using publicly available electrocardiograms. Considering hardware implementation, we employ discretized thresholds and synaptic weights and show that these parameters can be reduced to binary if the RNN architecture is appropriately designed. This contributes to minimization of the circuit of the neuronal system having IP and SP.",
      "paper_authors": [
        "Kumiko Nomura",
        "Yoshifumi Nishi"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-04-26",
      "update_time": "2024-04-26",
      "comments": "18 pages, 9 figures, 1 table",
      "repo_url": "#"
    },
    "2404.17092": {
      "paper_id": "2404.17092v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.17092v1",
      "paper_key": "2404.17092",
      "paper_title": "Defending Spiking Neural Networks against Adversarial Attacks through Image Purification",
      "paper_url": "http://arxiv.org/abs/2404.17092v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) aim to bridge the gap between neuroscience and machine learning by emulating the structure of the human nervous system. However, like convolutional neural networks, SNNs are vulnerable to adversarial attacks. To tackle the challenge, we propose a biologically inspired methodology to enhance the robustness of SNNs, drawing insights from the visual masking effect and filtering theory. First, an end-to-end SNN-based image purification model is proposed to defend against adversarial attacks, including a noise extraction network and a non-blind denoising network. The former network extracts noise features from noisy images, while the latter component employs a residual U-Net structure to reconstruct high-quality noisy images and generate clean images. Simultaneously, a multi-level firing SNN based on Squeeze-and-Excitation Network is introduced to improve the robustness of the classifier. Crucially, the proposed image purification network serves as a pre-processing module, avoiding modifications to classifiers. Unlike adversarial training, our method is highly flexible and can be seamlessly integrated with other defense strategies. Experimental results on various datasets demonstrate that the proposed methodology outperforms state-of-the-art baselines in terms of defense effectiveness, training time, and resource consumption.",
      "paper_authors": [
        "Weiran Chen",
        "Qi Sun",
        "Qi Xu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-04-26",
      "update_time": "2024-04-26",
      "comments": "8 pages, 5 figures, ECAI2024 under review",
      "repo_url": "#"
    },
    "2404.17048": {
      "paper_id": "2404.17048v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.17048v1",
      "paper_key": "2404.17048",
      "paper_title": "Transductive Spiking Graph Neural Networks for Loihi",
      "paper_url": "http://arxiv.org/abs/2404.17048v1",
      "paper_abstract": "Graph neural networks have emerged as a specialized branch of deep learning, designed to address problems where pairwise relations between objects are crucial. Recent advancements utilize graph convolutional neural networks to extract features within graph structures. Despite promising results, these methods face challenges in real-world applications due to sparse features, resulting in inefficient resource utilization. Recent studies draw inspiration from the mammalian brain and employ spiking neural networks to model and learn graph structures. However, these approaches are limited to traditional Von Neumann-based computing systems, which still face hardware inefficiencies. In this study, we present a fully neuromorphic implementation of spiking graph neural networks designed for Loihi 2. We optimize network parameters using Lava Bayesian Optimization, a novel hyperparameter optimization system compatible with neuromorphic computing architectures. We showcase the performance benefits of combining neuromorphic Bayesian optimization with our approach for citation graph classification using fixed-precision spiking neurons. Our results demonstrate the capability of integer-precision, Loihi 2 compatible spiking neural networks in performing citation graph classification with comparable accuracy to existing floating point implementations.",
      "paper_authors": [
        "Shay Snyder",
        "Victoria Clerico",
        "Guojing Cong",
        "Shruti Kulkarni",
        "Catherine Schuman",
        "Sumedh R. Risbud",
        "Maryam Parsa"
      ],
      "primary_category": "cs.ET",
      "publish_time": "2024-04-25",
      "update_time": "2024-04-25",
      "comments": "6 pages, 4 figures, 3 tables",
      "repo_url": "#"
    },
    "2404.16664": {
      "paper_id": "2404.16664v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.16664v1",
      "paper_key": "2404.16664",
      "paper_title": "Lu.i -- A low-cost electronic neuron for education and outreach",
      "paper_url": "http://arxiv.org/abs/2404.16664v1",
      "paper_abstract": "With an increasing presence of science throughout all parts of society, there is a rising expectation for researchers to effectively communicate their work and, equally, for teachers to discuss contemporary findings in their classrooms. While the community can resort to an established set of teaching aids for the fundamental concepts of most natural sciences, there is a need for similarly illustrative experiments and demonstrators in neuroscience. We therefore introduce Lu.i: a parametrizable electronic implementation of the leaky-integrate-and-fire neuron model in an engaging form factor. These palm-sized neurons can be used to visualize and experience the dynamics of individual cells and small spiking neural networks. When stimulated with real or simulated sensory input, Lu.i demonstrates brain-inspired information processing in the hands of a student. As such, it is actively used at workshops, in classrooms, and for science communication. As a versatile tool for teaching and outreach, Lu.i nurtures the comprehension of neuroscience research and neuromorphic engineering among future generations of scientists and in the general public.",
      "paper_authors": [
        "Yannik Stradmann",
        "Julian G\u00f6ltz",
        "Mihai A. Petrovici",
        "Johannes Schemmel",
        "Sebastian Billaudelle"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2024-04-25",
      "update_time": "2024-04-25",
      "comments": null,
      "repo_url": "https://github.com/giant-axon/lu.i-neuron-pcb"
    },
    "2404.16582": {
      "paper_id": "2404.16582v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.16582v1",
      "paper_key": "2404.16582",
      "paper_title": "Directional intermodular coupling enriches functional complexity in biological neuronal networks",
      "paper_url": "http://arxiv.org/abs/2404.16582v1",
      "paper_abstract": "Hierarchically modular organization is a canonical network topology that is evolutionarily conserved in the nervous systems of animals. Within the network, neurons form directional connections defined by the growth of their axonal terminals. However, this topology is dissimilar to the network formed by dissociated neurons in culture because they form randomly connected networks on homogeneous substrates. In this study, we fabricated microfluidic devices to reconstitute hierarchically modular neuronal networks in culture (in vitro) and investigated how non-random structures, such as directional connectivity between modules, affect global network dynamics. Embedding directional connections in a pseudo-feedforward manner suppressed excessive synchrony in cultured neuronal networks and enhanced the integration-segregation balance. Modeling the behavior of biological neuronal networks using spiking neural networks (SNNs) further revealed that modularity and directionality cooperate to shape such network dynamics. Finally, we demonstrate that for a given network topology, the statistics of network dynamics, such as global network activation, correlation coefficient, and functional complexity, can be analytically predicted based on eigendecomposition of the transition matrix in the state-transition model. Hence, the integration of bioengineering and cell culture technologies enables us not only to reconstitute complex network circuitry in the nervous system but also to understand the structure-function relationships in biological neuronal networks by bridging theoretical modeling with in vitro experiments.",
      "paper_authors": [
        "Nobuaki Monma",
        "Hideaki Yamamoto",
        "Naoya Fujiwara",
        "Hakuba Murota",
        "Satoshi Moriya",
        "Ayumi Hirano-Iwata",
        "Shigeo Sato"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2024-04-25",
      "update_time": "2024-04-25",
      "comments": "42 pages, 5 figures, 8 supplementary figures, 2 supplementary tables",
      "repo_url": "#"
    },
    "2404.16208": {
      "paper_id": "2404.16208v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.16208v1",
      "paper_key": "2404.16208",
      "paper_title": "GPU-RANC: A CUDA Accelerated Simulation Framework for Neuromorphic Architectures",
      "paper_url": "http://arxiv.org/abs/2404.16208v1",
      "paper_abstract": "Open-source simulation tools play a crucial role for neuromorphic application engineers and hardware architects to investigate performance bottlenecks and explore design optimizations before committing to silicon. Reconfigurable Architecture for Neuromorphic Computing (RANC) is one such tool that offers ability to execute pre-trained Spiking Neural Network (SNN) models within a unified ecosystem through both software-based simulation and FPGA-based emulation. RANC has been utilized by the community with its flexible and highly parameterized design to study implementation bottlenecks, tune architectural parameters or modify neuron behavior based on application insights and study the trade space on hardware performance and network accuracy. In designing architectures for use in neuromorphic computing, there are an incredibly large number of configuration parameters such as number and precision of weights per neuron, neuron and axon counts per core, network topology, and neuron behavior. To accelerate such studies and provide users with a streamlined productive design space exploration, in this paper we introduce the GPU-based implementation of RANC. We summarize our parallelization approach and quantify the speedup gains achieved with GPU-based tick-accurate simulations across various use cases. We demonstrate up to 780 times speedup compared to serial version of the RANC simulator based on a 512 neuromorphic core MNIST inference application. We believe that the RANC ecosystem now provides a much more feasible avenue in the research of exploring different optimizations for accelerating SNNs and performing richer studies by enabling rapid convergence to optimized neuromorphic architectures.",
      "paper_authors": [
        "Sahil Hassan",
        "Michael Inouye",
        "Miguel C. Gonzalez",
        "Ilkin Aliyev",
        "Joshua Mack",
        "Maisha Hafiz",
        "Ali Akoglu"
      ],
      "primary_category": "cs.ET",
      "publish_time": "2024-04-24",
      "update_time": "2024-04-24",
      "comments": "Accepted for publication in Neuro-Inspired Computational Elements\n  (NICE) Workshop 2024",
      "repo_url": "#"
    },
    "2404.15627": {
      "paper_id": "2404.15627v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.15627v1",
      "paper_key": "2404.15627",
      "paper_title": "Biologically-Informed Excitatory and Inhibitory Balance for Robust Spiking Neural Network Training",
      "paper_url": "http://arxiv.org/abs/2404.15627v1",
      "paper_abstract": "Spiking neural networks drawing inspiration from biological constraints of the brain promise an energy-efficient paradigm for artificial intelligence. However, challenges exist in identifying guiding principles to train these networks in a robust fashion. In addition, training becomes an even more difficult problem when incorporating biological constraints of excitatory and inhibitory connections. In this work, we identify several key factors, such as low initial firing rates and diverse inhibitory spiking patterns, that determine the overall ability to train spiking networks with various ratios of excitatory to inhibitory neurons on AI-relevant datasets. The results indicate networks with the biologically realistic 80:20 excitatory:inhibitory balance can reliably train at low activity levels and in noisy environments. Additionally, the Van Rossum distance, a measure of spike train synchrony, provides insight into the importance of inhibitory neurons to increase network robustness to noise. This work supports further biologically-informed large-scale networks and energy efficient hardware implementations.",
      "paper_authors": [
        "Joseph A. Kilgore",
        "Jeffrey D. Kopsick",
        "Giorgio A. Ascoli",
        "Gina C. Adam"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-04-24",
      "update_time": "2024-04-24",
      "comments": null,
      "repo_url": "#"
    },
    "2404.15597": {
      "paper_id": "2404.15597v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.15597v1",
      "paper_key": "2404.15597",
      "paper_title": "GRSN: Gated Recurrent Spiking Neurons for POMDPs and MARL",
      "paper_url": "http://arxiv.org/abs/2404.15597v1",
      "paper_abstract": "Spiking neural networks (SNNs) are widely applied in various fields due to their energy-efficient and fast-inference capabilities. Applying SNNs to reinforcement learning (RL) can significantly reduce the computational resource requirements for agents and improve the algorithm's performance under resource-constrained conditions. However, in current spiking reinforcement learning (SRL) algorithms, the simulation results of multiple time steps can only correspond to a single-step decision in RL. This is quite different from the real temporal dynamics in the brain and also fails to fully exploit the capacity of SNNs to process temporal data. In order to address this temporal mismatch issue and further take advantage of the inherent temporal dynamics of spiking neurons, we propose a novel temporal alignment paradigm (TAP) that leverages the single-step update of spiking neurons to accumulate historical state information in RL and introduces gated units to enhance the memory capacity of spiking neurons. Experimental results show that our method can solve partially observable Markov decision processes (POMDPs) and multi-agent cooperation problems with similar performance as recurrent neural networks (RNNs) but with about 50% power consumption.",
      "paper_authors": [
        "Lang Qin",
        "Ziming Wang",
        "Runhao Jiang",
        "Rui Yan",
        "Huajin Tang"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-04-24",
      "update_time": "2024-04-24",
      "comments": null,
      "repo_url": "#"
    },
    "2404.15524": {
      "paper_id": "2404.15524v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.15524v1",
      "paper_key": "2404.15524",
      "paper_title": "A Rapid Adapting and Continual Learning Spiking Neural Network Path Planning Algorithm for Mobile Robots",
      "paper_url": "http://arxiv.org/abs/2404.15524v1",
      "paper_abstract": "Mapping traversal costs in an environment and planning paths based on this map are important for autonomous navigation. We present a neurobotic navigation system that utilizes a Spiking Neural Network Wavefront Planner and E-prop learning to concurrently map and plan paths in a large and complex environment. We incorporate a novel method for mapping which, when combined with the Spiking Wavefront Planner, allows for adaptive planning by selectively considering any combination of costs. The system is tested on a mobile robot platform in an outdoor environment with obstacles and varying terrain. Results indicate that the system is capable of discerning features in the environment using three measures of cost, (1) energy expenditure by the wheels, (2) time spent in the presence of obstacles, and (3) terrain slope. In just twelve hours of online training, E-prop learns and incorporates traversal costs into the path planning maps by updating the delays in the Spiking Wavefront Planner. On simulated paths, the Spiking Wavefront Planner plans significantly shorter and lower cost paths than A* and RRT*. The spiking wavefront planner is compatible with neuromorphic hardware and could be used for applications requiring low size, weight, and power.",
      "paper_authors": [
        "Harrison Espino",
        "Robert Bain",
        "Jeffrey L. Krichmar"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-04-23",
      "update_time": "2024-04-23",
      "comments": "8 pages, 7 figures, 2 tables",
      "repo_url": "#"
    },
    "2404.14964": {
      "paper_id": "2404.14964v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.14964v2",
      "paper_key": "2404.14964",
      "paper_title": "Elucidating the theoretical underpinnings of surrogate gradient learning in spiking neural networks",
      "paper_url": "http://arxiv.org/abs/2404.14964v2",
      "paper_abstract": "Training spiking neural networks to approximate complex functions is essential for studying information processing in the brain and neuromorphic computing. Yet, the binary nature of spikes constitutes a challenge for direct gradient-based training. To sidestep this problem, surrogate gradients have proven empirically successful, but their theoretical foundation remains elusive. Here, we investigate the relation of surrogate gradients to two theoretically well-founded approaches. On the one hand, we consider smoothed probabilistic models, which, due to lack of support for automatic differentiation, are impractical for training deep spiking neural networks, yet provide gradients equivalent to surrogate gradients in single neurons. On the other hand, we examine stochastic automatic differentiation, which is compatible with discrete randomness but has never been applied to spiking neural network training. We find that the latter provides the missing theoretical basis for surrogate gradients in stochastic spiking neural networks. We further show that surrogate gradients in deterministic networks correspond to a particular asymptotic case and numerically confirm the effectiveness of surrogate gradients in stochastic multi-layer spiking neural networks. Finally, we illustrate that surrogate gradients are not conservative fields and, thus, not gradients of a surrogate loss. Our work provides the missing theoretical foundation for surrogate gradients and an analytically well-founded solution for end-to-end training of stochastic spiking neural networks.",
      "paper_authors": [
        "Julia Gygax",
        "Friedemann Zenke"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-04-23",
      "update_time": "2024-06-06",
      "comments": "25 pages, 7 figures + 3 supplementary figures",
      "repo_url": "https://github.com/fmi-basel/surrogate-gradient-theory"
    },
    "2404.14325": {
      "paper_id": "2404.14325v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.14325v2",
      "paper_key": "2404.14325",
      "paper_title": "Adapting to time: why nature evolved a diverse set of neurons",
      "paper_url": "http://arxiv.org/abs/2404.14325v2",
      "paper_abstract": "Brains have evolved a diverse set of neurons with varying morphologies, physiological properties and rich dynamics that impact their processing of temporal information. By contrast, most neural network models include a homogeneous set of units that only vary in terms of their spatial parameters (weights and biases). To investigate the importance of temporal parameters to neural function, we trained spiking neural networks on tasks of varying temporal complexity, with different subsets of parameters held constant. We find that in a tightly resource constrained setting, adapting conduction delays is essential to solve all test conditions, and indeed that it is possible to solve these tasks using only temporal parameters (delays and time constants) with weights held constant. In the most complex spatio-temporal task we studied, we found that an adaptable bursting parameter was essential. More generally, allowing for adaptation of both temporal and spatial parameters increases network robustness to noise, an important feature for both biological brains and neuromorphic computing systems. In summary, our findings highlight how rich and adaptable dynamics are key to solving temporally structured tasks at a low neural resource cost, which may be part of the reason why biological neurons vary so dramatically in their physiological properties.",
      "paper_authors": [
        "Karim G. Habashy",
        "Benjamin D. Evans",
        "Dan F. M. Goodman",
        "Jeffrey S. Bowers"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-04-22",
      "update_time": "2024-05-21",
      "comments": "14 pages, 6 figures",
      "repo_url": "#"
    },
    "2405.05141": {
      "paper_id": "2405.05141v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.05141v1",
      "paper_key": "2405.05141",
      "paper_title": "Learning-to-learn enables rapid learning with phase-change memory-based in-memory computing",
      "paper_url": "http://arxiv.org/abs/2405.05141v1",
      "paper_abstract": "There is a growing demand for low-power, autonomously learning artificial intelligence (AI) systems that can be applied at the edge and rapidly adapt to the specific situation at deployment site. However, current AI models struggle in such scenarios, often requiring extensive fine-tuning, computational resources, and data. In contrast, humans can effortlessly adjust to new tasks by transferring knowledge from related ones. The concept of learning-to-learn (L2L) mimics this process and enables AI models to rapidly adapt with only little computational effort and data. In-memory computing neuromorphic hardware (NMHW) is inspired by the brain's operating principles and mimics its physical co-location of memory and compute. In this work, we pair L2L with in-memory computing NMHW based on phase-change memory devices to build efficient AI models that can rapidly adapt to new tasks. We demonstrate the versatility of our approach in two scenarios: a convolutional neural network performing image classification and a biologically-inspired spiking neural network generating motor commands for a real robotic arm. Both models rapidly learn with few parameter updates. Deployed on the NMHW, they perform on-par with their software equivalents. Moreover, meta-training of these models can be performed in software with high-precision, alleviating the need for accurate hardware models.",
      "paper_authors": [
        "Thomas Ortner",
        "Horst Petschenig",
        "Athanasios Vasilopoulos",
        "Roland Renner",
        "\u0160pela Brglez",
        "Thomas Limbacher",
        "Enrique Pi\u00f1ero",
        "Alejandro Linares Barranco",
        "Angeliki Pantazi",
        "Robert Legenstein"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-04-22",
      "update_time": "2024-04-22",
      "comments": "16 pages and 3 appendix pages; Preprint currently under review",
      "repo_url": "#"
    },
    "2404.14024": {
      "paper_id": "2404.14024v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.14024v1",
      "paper_key": "2404.14024",
      "paper_title": "Exploring neural oscillations during speech perception via surrogate gradient spiking neural networks",
      "paper_url": "http://arxiv.org/abs/2404.14024v1",
      "paper_abstract": "Understanding cognitive processes in the brain demands sophisticated models capable of replicating neural dynamics at large scales. We present a physiologically inspired speech recognition architecture, compatible and scalable with deep learning frameworks, and demonstrate that end-to-end gradient descent training leads to the emergence of neural oscillations in the central spiking neural network. Significant cross-frequency couplings, indicative of these oscillations, are measured within and across network layers during speech processing, whereas no such interactions are observed when handling background noise inputs. Furthermore, our findings highlight the crucial inhibitory role of feedback mechanisms, such as spike frequency adaptation and recurrent connections, in regulating and synchronising neural activity to improve recognition performance. Overall, on top of developing our understanding of synchronisation phenomena notably observed in the human auditory pathway, our architecture exhibits dynamic and efficient information processing, with relevance to neuromorphic technology.",
      "paper_authors": [
        "Alexandre Bittar",
        "Philip N. Garner"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-04-22",
      "update_time": "2024-04-22",
      "comments": null,
      "repo_url": "#"
    },
    "2404.13888": {
      "paper_id": "2404.13888v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.13888v2",
      "paper_key": "2404.13888",
      "paper_title": "Functions of Direct and Indirect Pathways for Action Selection Are Quantitatively Analyzed in A Spiking Neural Network of The Basal Ganglia",
      "paper_url": "http://arxiv.org/abs/2404.13888v2",
      "paper_abstract": "We are concerned about action selection in the basal ganglia (BG). We quantitatively analyze functions of direct pathway (DP) and indirect pathway (IP) for action selection in a spiking neural network with 3 competing channels. For such quantitative analysis, in each channel, we obtain the competition degree ${\\cal C}_d$, given by the ratio of strength of DP (${\\cal S}_{DP}$) to strength of IP (${\\cal S}_{IP}$) (i.e., ${\\cal C}_d = {\\cal S}_{DP} / {\\cal S}_{IP}$). Then, a desired action is selected in the channel with the largest ${\\cal C}_d$. Desired action selection is made mainly due to strong focused inhibitory projection to the output nucleus, SNr (substantia nigra pars reticulata) via the DP in the corresponding channel. Unlike the case of DP, there are two types of IPs; intra-channel IP and inter-channel IP, due to widespread diffusive excitation from the STN (subthalamic nucleus). The intra-channel IP serves a function of brake to suppress the desired action selection. In contrast, the inter-channel IP to the SNr in the neighboring channels suppresses competing actions, leading to highlight the desired action selection. In this way, function of the inter-channel IP is opposite to that of the intra-channel IP. However, to the best of our knowledge, no quantitative analysis for such functions of the DP and the two IPs was made. Here, through direct calculations of the DP and the intra- and the inter-channel IP presynaptic currents into the SNr in each channel, we obtain the competition degree of each channel to determine a desired action, and then functions of the DP and the intra- and inter-channel IPs are quantitatively made clear.",
      "paper_authors": [
        "Sang-Yoon Kim",
        "Woochang Lim"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2024-04-22",
      "update_time": "2024-07-03",
      "comments": null,
      "repo_url": "#"
    },
    "2404.12771": {
      "paper_id": "2404.12771v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.12771v1",
      "paper_key": "2404.12771",
      "paper_title": "Phase-space analysis of a two-section InP laser as an all-optical spiking neuron: dependency on control and design parameters",
      "paper_url": "http://arxiv.org/abs/2404.12771v1",
      "paper_abstract": "Using a rate-equation model we numerically evaluate the carrier concentration and photon number in an integrated two-section semiconductor laser, and analyse its dynamics in three-dimensional phase space. The simulation comprises compact model descriptions extracted from a commercially-available generic InP technology platform, allowing us to model an applied reverse-bias voltage to the saturable absorber. We use the model to study the influence of the injected gain current, reverse-bias voltage, and cavity mirror reflectivity on the excitable operation state, which is the operation mode desired for the laser to act as an all-optical integrated neuron. We show in phase-space that our model is capable of demonstrating four different operation modes, i.e. cw, self-pulsating and an on-set and excitable mode under optical pulse injection. In addition, we show that lowering the reflectivity of one of the cavity mirrors greatly enhances the control parameter space for excitable operation, enabling more relaxed operation parameter control and lower power consumption of an integrated two-section laser neuron.",
      "paper_authors": [
        "Lukas Puts",
        "Daan Lenstra",
        "Kevin Williams",
        "Weiming Yao"
      ],
      "primary_category": "physics.optics",
      "publish_time": "2024-04-19",
      "update_time": "2024-04-19",
      "comments": "11 pages, 10 figures",
      "repo_url": "#"
    },
    "2404.10599": {
      "paper_id": "2404.10599v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.10599v1",
      "paper_key": "2404.10599",
      "paper_title": "Towards free-response paradigm: a theory on decision-making in spiking neural networks",
      "paper_url": "http://arxiv.org/abs/2404.10599v1",
      "paper_abstract": "The energy-efficient and brain-like information processing abilities of Spiking Neural Networks (SNNs) have attracted considerable attention, establishing them as a crucial element of brain-inspired computing. One prevalent challenge encountered by SNNs is the trade-off between inference speed and accuracy, which requires sufficient time to achieve the desired level of performance. Drawing inspiration from animal behavior experiments that demonstrate a connection between decision-making reaction times, task complexity, and confidence levels, this study seeks to apply these insights to SNNs. The focus is on understanding how SNNs make inferences, with a particular emphasis on untangling the interplay between signal and noise in decision-making processes. The proposed theoretical framework introduces a new optimization objective for SNN training, highlighting the importance of not only the accuracy of decisions but also the development of predictive confidence through learning from past experiences. Experimental results demonstrate that SNNs trained according to this framework exhibit improved confidence expression, leading to better decision-making outcomes. In addition, a strategy is introduced for efficient decision-making during inference, which allows SNNs to complete tasks more quickly and can use stopping times as indicators of decision confidence. By integrating neuroscience insights with neuromorphic computing, this study opens up new possibilities to explore the capabilities of SNNs and advance their application in complex decision-making scenarios.",
      "paper_authors": [
        "Zhichao Zhu",
        "Yang Qi",
        "Wenlian Lu",
        "Zhigang Wang",
        "Lu Cao",
        "Jianfeng Feng"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-04-16",
      "update_time": "2024-04-16",
      "comments": "27 pages, 6 figures, 3 tables",
      "repo_url": "#"
    },
    "2404.10597": {
      "paper_id": "2404.10597v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.10597v1",
      "paper_key": "2404.10597",
      "paper_title": "Hardware-aware training of models with synaptic delays for digital event-driven neuromorphic processors",
      "paper_url": "http://arxiv.org/abs/2404.10597v1",
      "paper_abstract": "Configurable synaptic delays are a basic feature in many neuromorphic neural network hardware accelerators. However, they have been rarely used in model implementations, despite their promising impact on performance and efficiency in tasks that exhibit complex (temporal) dynamics, as it has been unclear how to optimize them. In this work, we propose a framework to train and deploy, in digital neuromorphic hardware, highly performing spiking neural network models (SNNs) where apart from the synaptic weights, the per-synapse delays are also co-optimized. Leveraging spike-based back-propagation-through-time, the training accounts for both platform constraints, such as synaptic weight precision and the total number of parameters per core, as a function of the network size. In addition, a delay pruning technique is used to reduce memory footprint with a low cost in performance. We evaluate trained models in two neuromorphic digital hardware platforms: Intel Loihi and Imec Seneca. Loihi offers synaptic delay support using the so-called Ring-Buffer hardware structure. Seneca does not provide native hardware support for synaptic delays. A second contribution of this paper is therefore a novel area- and memory-efficient hardware structure for acceleration of synaptic delays, which we have integrated in Seneca. The evaluated benchmark involves several models for solving the SHD (Spiking Heidelberg Digits) classification task, where minimal accuracy degradation during the transition from software to hardware is demonstrated. To our knowledge, this is the first work showcasing how to train and deploy hardware-aware models parameterized with synaptic delays, on multicore neuromorphic hardware accelerators.",
      "paper_authors": [
        "Alberto Patino-Saucedo",
        "Roy Meijer",
        "Amirreza Yousefzadeh",
        "Manil-Dev Gomony",
        "Federico Corradi",
        "Paul Detteter",
        "Laura Garrido-Regife",
        "Bernabe Linares-Barranco",
        "Manolis Sifalakis"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-04-16",
      "update_time": "2024-04-16",
      "comments": null,
      "repo_url": "#"
    },
    "2405.00700": {
      "paper_id": "2405.00700v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.00700v1",
      "paper_key": "2405.00700",
      "paper_title": "Oxygen vacancies modulated VO2 for neurons and Spiking Neural Network construction",
      "paper_url": "http://arxiv.org/abs/2405.00700v1",
      "paper_abstract": "Artificial neuronal devices are the basic building blocks for neuromorphic computing systems, which have been motivated by realistic brain emulation. Aiming for these applications, various device concepts have been proposed to mimic the neuronal dynamics and functions. While till now, the artificial neuron devices with high efficiency, high stability and low power consumption are still far from practical application. Due to the special insulator-metal phase transition, Vanadium Dioxide (VO2) has been considered as an idea candidate for neuronal device fabrication. However, its intrinsic insulating state requires the VO2 neuronal device to be driven under large bias voltage, resulting in high power consumption and low frequency. Thus in the current study, we have addressed this challenge by preparing oxygen vacancies modulated VO2 film(VO2-x) and fabricating the VO2-x neuronal devices for Spiking Neural Networks (SNNs) construction. Results indicate the neuron devices can be operated under lower voltage with improved processing speed. The proposed VO2-x based back-propagation SNNs (BP-SNNs) system, trained with the MNIST dataset, demonstrates excellent accuracy in image recognition. Our study not only demonstrates the VO2-x based neurons and SNN system for practical application, but also offers an effective way to optimize the future neuromorphic computing systems by defect engineering strategy.",
      "paper_authors": [
        "Liang Li",
        "Ting Zhou",
        "Tong Liu",
        "Zhiwei Liu",
        "Yaping Li",
        "Shuo Wu",
        "Shanguang Zhao",
        "Jinglin Zhu",
        "Meiling Liu",
        "Zhihan Lin",
        "Bowen Sun",
        "Jianjun Li",
        "Fangwen Sun",
        "Chongwen Zou"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-04-16",
      "update_time": "2024-04-16",
      "comments": "18 pages,4 figures",
      "repo_url": "#"
    },
    "2404.10210": {
      "paper_id": "2404.10210v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.10210v2",
      "paper_key": "2404.10210",
      "paper_title": "MK-SGN: A Spiking Graph Convolutional Network with Multimodal Fusion and Knowledge Distillation for Skeleton-based Action Recognition",
      "paper_url": "http://arxiv.org/abs/2404.10210v2",
      "paper_abstract": "In recent years, skeleton-based action recognition, leveraging multimodal Graph Convolutional Networks (GCN), has achieved remarkable results. However, due to their deep structure and reliance on continuous floating-point operations, GCN-based methods are energy-intensive. We propose an innovative Spiking Graph Convolutional Network with Multimodal Fusion and Knowledge Distillation (MK-SGN) to address this issue. By merging the energy efficiency of Spiking Neural Network (SNN) with the graph representation capability of GCN, the proposed MK-SGN reduces energy consumption while maintaining recognition accuracy. Firstly, we convert Graph Convolutional Networks (GCN) into Spiking Graph Convolutional Networks (SGN) establishing a new benchmark and paving the way for future research exploration. During this process, we introduce a spiking attention mechanism and design a Spiking-Spatio Graph Convolution module with a Spatial Global Spiking Attention mechanism (SA-SGC), enhancing feature learning capability. Secondly, we propose a Spiking Multimodal Fusion module (SMF), leveraging mutual information to process multimodal data more efficiently. Lastly, we delve into knowledge distillation methods from multimodal GCN to SGN and propose a novel, integrated method that simultaneously focuses on both intermediate layer distillation and soft label distillation to improve the performance of SGN. MK-SGN outperforms the state-of-the-art GCN-like frameworks on three challenging datasets for skeleton-based action recognition in reducing energy consumption. It also outperforms the state-of-the-art SNN frameworks in accuracy. Specifically, our method reduces energy consumption by more than 98% compared to typical GCN-based methods, while maintaining competitive accuracy on the NTU-RGB+D 60 cross-subject split using 4-time steps.",
      "paper_authors": [
        "Naichuan Zheng",
        "Hailun Xia",
        "Zeyu Liang",
        "Yuanyuan Chai"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-04-16",
      "update_time": "2024-08-03",
      "comments": null,
      "repo_url": "#"
    },
    "2405.00699": {
      "paper_id": "2405.00699v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.00699v1",
      "paper_key": "2405.00699",
      "paper_title": "Direct Training Needs Regularisation: Anytime Optimal Inference Spiking Neural Network",
      "paper_url": "http://arxiv.org/abs/2405.00699v1",
      "paper_abstract": "Spiking Neural Network (SNN) is acknowledged as the next generation of Artificial Neural Network (ANN) and hold great promise in effectively processing spatial-temporal information. However, the choice of timestep becomes crucial as it significantly impacts the accuracy of the neural network training. Specifically, a smaller timestep indicates better performance in efficient computing, resulting in reduced latency and operations. While, using a small timestep may lead to low accuracy due to insufficient information presentation with few spikes. This observation motivates us to develop an SNN that is more reliable for adaptive timestep by introducing a novel regularisation technique, namely Spatial-Temporal Regulariser (STR). Our approach regulates the ratio between the strength of spikes and membrane potential at each timestep. This effectively balances spatial and temporal performance during training, ultimately resulting in an Anytime Optimal Inference (AOI) SNN. Through extensive experiments on frame-based and event-based datasets, our method, in combination with cutoff based on softmax output, achieves state-of-the-art performance in terms of both latency and accuracy. Notably, with STR and cutoff, SNN achieves 2.14 to 2.89 faster in inference compared to the pre-configured timestep with near-zero accuracy drop of 0.50% to 0.64% over the event-based datasets. Code available: https://github.com/Dengyu-Wu/AOI-SNN-Regularisation",
      "paper_authors": [
        "Dengyu Wu",
        "Yi Qi",
        "Kaiwen Cai",
        "Gaojie Jin",
        "Xinping Yi",
        "Xiaowei Huang"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-04-15",
      "update_time": "2024-04-15",
      "comments": null,
      "repo_url": "https://github.com/dengyu-wu/aoi-snn-regularisation"
    },
    "2404.19668": {
      "paper_id": "2404.19668v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.19668v1",
      "paper_key": "2404.19668",
      "paper_title": "SQUAT: Stateful Quantization-Aware Training in Recurrent Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2404.19668v1",
      "paper_abstract": "Weight quantization is used to deploy high-performance deep learning models on resource-limited hardware, enabling the use of low-precision integers for storage and computation. Spiking neural networks (SNNs) share the goal of enhancing efficiency, but adopt an 'event-driven' approach to reduce the power consumption of neural network inference. While extensive research has focused on weight quantization, quantization-aware training (QAT), and their application to SNNs, the precision reduction of state variables during training has been largely overlooked, potentially diminishing inference performance. This paper introduces two QAT schemes for stateful neurons: (i) a uniform quantization strategy, an established method for weight quantization, and (ii) threshold-centered quantization, which allocates exponentially more quantization levels near the firing threshold. Our results show that increasing the density of quantization levels around the firing threshold improves accuracy across several benchmark datasets. We provide an ablation analysis of the effects of weight and state quantization, both individually and combined, and how they impact models. Our comprehensive empirical evaluation includes full precision, 8-bit, 4-bit, and 2-bit quantized SNNs, using QAT, stateful QAT (SQUAT), and post-training quantization methods. The findings indicate that the combination of QAT and SQUAT enhance performance the most, but given the choice of one or the other, QAT improves performance by the larger degree. These trends are consistent all datasets. Our methods have been made available in our Python library snnTorch: https://github.com/jeshraghian/snntorch.",
      "paper_authors": [
        "Sreyes Venkatesh",
        "Razvan Marinescu",
        "Jason K. Eshraghian"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-04-15",
      "update_time": "2024-04-15",
      "comments": "10 pages, 4 figures, accepted at NICE 2024",
      "repo_url": "https://github.com/jeshraghian/snntorch"
    },
    "2404.09331": {
      "paper_id": "2404.09331v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.09331v2",
      "paper_key": "2404.09331",
      "paper_title": "SNN4Agents: A Framework for Developing Energy-Efficient Embodied Spiking Neural Networks for Autonomous Agents",
      "paper_url": "http://arxiv.org/abs/2404.09331v2",
      "paper_abstract": "Recent trends have shown that autonomous agents, such as Autonomous Ground Vehicles (AGVs), Unmanned Aerial Vehicles (UAVs), and mobile robots, effectively improve human productivity in solving diverse tasks. However, since these agents are typically powered by portable batteries, they require extremely low power/energy consumption to operate in a long lifespan. To solve this challenge, neuromorphic computing has emerged as a promising solution, where bio-inspired Spiking Neural Networks (SNNs) use spikes from event-based cameras or data conversion pre-processing to perform sparse computations efficiently. However, the studies of SNN deployments for autonomous agents are still at an early stage. Hence, the optimization stages for enabling efficient embodied SNN deployments for autonomous agents have not been defined systematically. Toward this, we propose a novel framework called SNN4Agents that consists of a set of optimization techniques for designing energy-efficient embodied SNNs targeting autonomous agent applications. Our SNN4Agents employs weight quantization, timestep reduction, and attention window reduction to jointly improve the energy efficiency, reduce the memory footprint, optimize the processing latency, while maintaining high accuracy. In the evaluation, we investigate use cases of event-based car recognition, and explore the trade-offs among accuracy, latency, memory, and energy consumption. The experimental results show that our proposed framework can maintain high accuracy (i.e., 84.12% accuracy) with 68.75% memory saving, 3.58x speed-up, and 4.03x energy efficiency improvement as compared to the state-of-the-art work for NCARS dataset. In this manner, our SNN4Agents framework paves the way toward enabling energy-efficient embodied SNN deployments for autonomous agents.",
      "paper_authors": [
        "Rachmad Vidya Wicaksana Putra",
        "Alberto Marchisio",
        "Muhammad Shafique"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-04-14",
      "update_time": "2024-06-18",
      "comments": "Accepted for publication at Frontiers in Robotics and AI (FROBT) -\n  Section Robot Vision and Artificial Perception",
      "repo_url": "https://github.com/rachmadvwp/snn4agents"
    },
    "2404.09046": {
      "paper_id": "2404.09046v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.09046v1",
      "paper_key": "2404.09046",
      "paper_title": "Phase-Amplitude Description of Stochastic Oscillators: A Parameterization Method Approach",
      "paper_url": "http://arxiv.org/abs/2404.09046v1",
      "paper_abstract": "The parameterization method (PM) provides a broad theoretical and numerical foundation for computing invariant manifolds of dynamical systems. PM implements a change of variables in order to represent trajectories of a system of ordinary differential equations ``as simply as possible.\" In this paper we pursue a similar goal for stochastic oscillator systems. For planar nonlinear stochastic systems that are ``robustly oscillatory\", we find a change of variables through which the dynamics are as simple as possible $\\textit{in the mean}$. We prove existence and uniqueness of a deterministic vector field, the trajectories of which capture the local mean behavior of the stochastic oscillator. We illustrate the construction of such an ``effective vector field\" for several examples, including a limit cycle oscillator perturbed by noise, an excitable system derived from a spiking neuron model, and a spiral sink with noise forcing (2D Ornstein-Uhlenbeck process). The latter examples comprise contingent oscillators that would not sustain rhythmic activity without noise forcing. Finally, we exploit the simplicity of the dynamics after the change of variables to obtain the effective diffusion constant of the resulting phase variable, and the stationary variance of the resulting amplitude (isostable) variable.",
      "paper_authors": [
        "Alberto P\u00e9rez-Cervera",
        "Benjamin Lindner",
        "Peter J. Thomas"
      ],
      "primary_category": "math.DS",
      "publish_time": "2024-04-13",
      "update_time": "2024-04-13",
      "comments": null,
      "repo_url": "#"
    },
    "2405.02316": {
      "paper_id": "2405.02316v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.02316v1",
      "paper_key": "2405.02316",
      "paper_title": "A Cloud-Edge Framework for Energy-Efficient Event-Driven Control: An Integration of Online Supervised Learning, Spiking Neural Networks and Local Plasticity Rules",
      "paper_url": "http://arxiv.org/abs/2405.02316v1",
      "paper_abstract": "This paper presents a novel cloud-edge framework for addressing computational and energy constraints in complex control systems. Our approach centers around a learning-based controller using Spiking Neural Networks (SNN) on physical plants. By integrating a biologically plausible learning method with local plasticity rules, we harness the efficiency, scalability, and low latency of SNNs. This design replicates control signals from a cloud-based controller directly on the plant, reducing the need for constant plant-cloud communication. The plant updates weights only when errors surpass predefined thresholds, ensuring efficiency and robustness in various conditions. Applied to linear workbench systems and satellite rendezvous scenarios, including obstacle avoidance, our architecture dramatically lowers normalized tracking error by 96% with increased network size. The event-driven nature of SNNs minimizes energy consumption, utilizing only about 111 nJ (0.3% of conventional computing requirements). The results demonstrate the system's adjustment to changing work environments and its efficient use of computational and energy resources, with a moderate increase in energy consumption of 27.2% and 37% for static and dynamic obstacles, respectively, compared to non-obstacle scenarios.",
      "paper_authors": [
        "Reza Ahmadvand",
        "Sarah Safura Sharif",
        "Yaser Mike Banad"
      ],
      "primary_category": "eess.SY",
      "publish_time": "2024-04-12",
      "update_time": "2024-04-12",
      "comments": "13 pages, 19 figures",
      "repo_url": "#"
    },
    "2404.08726": {
      "paper_id": "2404.08726v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.08726v1",
      "paper_key": "2404.08726",
      "paper_title": "An Integrated Toolbox for Creating Neuromorphic Edge Applications",
      "paper_url": "http://arxiv.org/abs/2404.08726v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) and neuromorphic models are more efficient and have more biological realism than the activation functions typically used in deep neural networks, transformer models and generative AI. SNNs have local learning rules, are able to learn on small data sets, and can adapt through neuromodulation. Although research has shown their advantages, there are still few compelling practical applications, especially at the edge where sensors and actuators need to be processed in a timely fashion. One reason for this might be that SNNs are much more challenging to understand, build, and operate due to their intrinsic properties. For instance, the mathematical foundation involves differential equations rather than basic activation functions. To address these challenges, we have developed CARLsim++. It is an integrated toolbox that enables fast and easy creation of neuromorphic applications. It encapsulates the mathematical intrinsics and low-level C++ programming by providing a graphical user interface for users who do not have a background in software engineering but still want to create neuromorphic models. Developers can easily configure inputs and outputs to devices and robots. These can be accurately simulated before deploying on physical devices. CARLsim++ can lead to rapid development of neuromorphic applications for simulation or edge processing.",
      "paper_authors": [
        "Lars Niedermeier",
        "Jeffrey L. Krichmar"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-04-12",
      "update_time": "2024-04-12",
      "comments": "8 pages, 5 figures, NICE 2024",
      "repo_url": "#"
    },
    "2404.06469": {
      "paper_id": "2404.06469v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.06469v1",
      "paper_key": "2404.06469",
      "paper_title": "Neuromorphic In-Context Learning for Energy-Efficient MIMO Symbol Detection",
      "paper_url": "http://arxiv.org/abs/2404.06469v1",
      "paper_abstract": "In-context learning (ICL), a property demonstrated by transformer-based sequence models, refers to the automatic inference of an input-output mapping based on examples of the mapping provided as context. ICL requires no explicit learning, i.e., no explicit updates of model weights, directly mapping context and new input to the new output. Prior work has proved the usefulness of ICL for detection in MIMO channels. In this setting, the context is given by pilot symbols, and ICL automatically adapts a detector, or equalizer, to apply to newly received signals. However, the implementation tested in prior art was based on conventional artificial neural networks (ANNs), which may prove too energy-demanding to be run on mobile devices. This paper evaluates a neuromorphic implementation of the transformer for ICL-based MIMO detection. This approach replaces ANNs with spiking neural networks (SNNs), and implements the attention mechanism via stochastic computing, requiring no multiplications, but only logical AND operations and counting. When using conventional digital CMOS hardware, the proposed implementation is shown to preserve accuracy, with a reduction in power consumption ranging from $5.4\\times$ to $26.8\\times$, depending on the model sizes, as compared to ANN-based implementations.",
      "paper_authors": [
        "Zihang Song",
        "Osvaldo Simeone",
        "Bipin Rajendran"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2024-04-09",
      "update_time": "2024-04-09",
      "comments": null,
      "repo_url": "#"
    },
    "2404.05858": {
      "paper_id": "2404.05858v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.05858v1",
      "paper_key": "2404.05858",
      "paper_title": "A Neuromorphic Approach to Obstacle Avoidance in Robot Manipulation",
      "paper_url": "http://arxiv.org/abs/2404.05858v1",
      "paper_abstract": "Neuromorphic computing mimics computational principles of the brain in $\\textit{silico}$ and motivates research into event-based vision and spiking neural networks (SNNs). Event cameras (ECs) exclusively capture local intensity changes and offer superior power consumption, response latencies, and dynamic ranges. SNNs replicate biological neuronal dynamics and have demonstrated potential as alternatives to conventional artificial neural networks (ANNs), such as in reducing energy expenditure and inference time in visual classification. Nevertheless, these novel paradigms remain scarcely explored outside the domain of aerial robots.   To investigate the utility of brain-inspired sensing and data processing, we developed a neuromorphic approach to obstacle avoidance on a camera-equipped manipulator. Our approach adapts high-level trajectory plans with reactive maneuvers by processing emulated event data in a convolutional SNN, decoding neural activations into avoidance motions, and adjusting plans using a dynamic motion primitive. We conducted experiments with a Kinova Gen3 arm performing simple reaching tasks that involve obstacles in sets of distinct task scenarios and in comparison to a non-adaptive baseline.   Our neuromorphic approach facilitated reliable avoidance of imminent collisions in simulated and real-world experiments, where the baseline consistently failed. Trajectory adaptations had low impacts on safety and predictability criteria. Among the notable SNN properties were the correlation of computations with the magnitude of perceived motions and a robustness to different event emulation methods. Tests with a DAVIS346 EC showed similar performance, validating our experimental event emulation. Our results motivate incorporating SNN learning, utilizing neuromorphic processors, and further exploring the potential of neuromorphic methods.",
      "paper_authors": [
        "Ahmed Faisal Abdelrahman",
        "Matias Valdenegro-Toro",
        "Maren Bennewitz",
        "Paul G. Pl\u00f6ger"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-04-08",
      "update_time": "2024-04-08",
      "comments": "35 pages, accepted at IJRR, authors' version",
      "repo_url": "#"
    },
    "2404.05807": {
      "paper_id": "2404.05807v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.05807v1",
      "paper_key": "2404.05807",
      "paper_title": "Slax: A Composable JAX Library for Rapid and Flexible Prototyping of Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2404.05807v1",
      "paper_abstract": "Recent advances to algorithms for training spiking neural networks (SNNs) often leverage their unique dynamics. While backpropagation through time (BPTT) with surrogate gradients dominate the field, a rich landscape of alternatives can situate algorithms across various points in the performance, bio-plausibility, and complexity landscape. Evaluating and comparing algorithms is currently a cumbersome and error-prone process, requiring them to be repeatedly re-implemented. We introduce Slax, a JAX-based library designed to accelerate SNN algorithm design, compatible with the broader JAX and Flax ecosystem. Slax provides optimized implementations of diverse training algorithms, allowing direct performance comparison. Its toolkit includes methods to visualize and debug algorithms through loss landscapes, gradient similarities, and other metrics of model behavior during training.",
      "paper_authors": [
        "Thomas M. Summe",
        "Siddharth Joshi"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-04-08",
      "update_time": "2024-04-08",
      "comments": "13 pages, 11 figures, early draft",
      "repo_url": "#"
    },
    "2404.04549": {
      "paper_id": "2404.04549v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.04549v1",
      "paper_key": "2404.04549",
      "paper_title": "Efficient Learning Using Spiking Neural Networks Equipped With Affine Encoders and Decoders",
      "paper_url": "http://arxiv.org/abs/2404.04549v1",
      "paper_abstract": "We study the learning problem associated with spiking neural networks. Specifically, we consider hypothesis sets of spiking neural networks with affine temporal encoders and decoders and simple spiking neurons having only positive synaptic weights. We demonstrate that the positivity of the weights continues to enable a wide range of expressivity results, including rate-optimal approximation of smooth functions or approximation without the curse of dimensionality. Moreover, positive-weight spiking neural networks are shown to depend continuously on their parameters which facilitates classical covering number-based generalization statements. Finally, we observe that from a generalization perspective, contrary to feedforward neural networks or previous results for general spiking neural networks, the depth has little to no adverse effect on the generalization capabilities.",
      "paper_authors": [
        "A. Martina Neuman",
        "Philipp Christian Petersen"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-04-06",
      "update_time": "2024-04-06",
      "comments": null,
      "repo_url": "#"
    },
    "2404.03714": {
      "paper_id": "2404.03714v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.03714v1",
      "paper_key": "2404.03714",
      "paper_title": "SpikeExplorer: hardware-oriented Design Space Exploration for Spiking Neural Networks on FPGA",
      "paper_url": "http://arxiv.org/abs/2404.03714v1",
      "paper_abstract": "One of today's main concerns is to bring Artificial Intelligence power to embedded systems for edge applications. The hardware resources and power consumption required by state-of-the-art models are incompatible with the constrained environments observed in edge systems, such as IoT nodes and wearable devices. Spiking Neural Networks (SNNs) can represent a solution in this sense: inspired by neuroscience, they reach unparalleled power and resource efficiency when run on dedicated hardware accelerators. However, when designing such accelerators, the amount of choices that can be taken is huge. This paper presents SpikExplorer, a modular and flexible Python tool for hardware-oriented Automatic Design Space Exploration to automate the configuration of FPGA accelerators for SNNs. Using Bayesian optimizations, SpikerExplorer enables hardware-centric multi-objective optimization, supporting factors such as accuracy, area, latency, power, and various combinations during the exploration process. The tool searches the optimal network architecture, neuron model, and internal and training parameters, trying to reach the desired constraints imposed by the user. It allows for a straightforward network configuration, providing the full set of explored points for the user to pick the trade-off that best fits the needs. The potential of SpikExplorer is showcased using three benchmark datasets. It reaches 95.8% accuracy on the MNIST dataset, with a power consumption of 180mW/image and a latency of 0.12 ms/image, making it a powerful tool for automatically optimizing SNNs.",
      "paper_authors": [
        "Dario Padovano",
        "Alessio Carpegna",
        "Alessandro Savino",
        "Stefano Di Carlo"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-04-04",
      "update_time": "2024-04-04",
      "comments": null,
      "repo_url": "#"
    },
    "2404.03493": {
      "paper_id": "2404.03493v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.03493v2",
      "paper_key": "2404.03493",
      "paper_title": "A Methodology to Study the Impact of Spiking Neural Network Parameters considering Event-Based Automotive Data",
      "paper_url": "http://arxiv.org/abs/2404.03493v2",
      "paper_abstract": "Autonomous Driving (AD) systems are considered as the future of human mobility and transportation. Solving computer vision tasks such as image classification and object detection/segmentation, with high accuracy and low power/energy consumption, is highly needed to realize AD systems in real life. These requirements can potentially be satisfied by Spiking Neural Networks (SNNs). However, the state-of-the-art works in SNN-based AD systems still focus on proposing network models that can achieve high accuracy, and they have not systematically studied the roles of SNN parameters when used for learning event-based automotive data. Therefore, we still lack understanding of how to effectively develop SNN models for AD systems. Toward this, we propose a novel methodology to systematically study and analyze the impact of SNN parameters considering event-based automotive data, then leverage this analysis for enhancing SNN developments. To do this, we first explore different settings of SNN parameters that directly affect the learning mechanism (i.e., batch size, learning rate, neuron threshold potential, and weight decay), then analyze the accuracy results. Afterward, we propose techniques that jointly improve SNN accuracy and reduce training time. Experimental results show that our methodology can improve the SNN models for AD systems than the state-of-the-art, as it achieves higher accuracy (i.e., 86%) for the NCARS dataset, and it can also achieve iso-accuracy (i.e., ~85% with standard deviation less than 0.5%) while speeding up the training time by 1.9x. In this manner, our research work provides a set of guidelines for SNN parameter enhancements, thereby enabling the practical developments of SNN-based AD systems.",
      "paper_authors": [
        "Iqra Bano",
        "Rachmad Vidya Wicaksana Putra",
        "Alberto Marchisio",
        "Muhammad Shafique"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-04-04",
      "update_time": "2024-04-05",
      "comments": "7 pages, 13 figures, 1 table",
      "repo_url": "#"
    },
    "2404.03325": {
      "paper_id": "2404.03325v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.03325v1",
      "paper_key": "2404.03325",
      "paper_title": "Embodied Neuromorphic Artificial Intelligence for Robotics: Perspectives, Challenges, and Research Development Stack",
      "paper_url": "http://arxiv.org/abs/2404.03325v1",
      "paper_abstract": "Robotic technologies have been an indispensable part for improving human productivity since they have been helping humans in completing diverse, complex, and intensive tasks in a fast yet accurate and efficient way. Therefore, robotic technologies have been deployed in a wide range of applications, ranging from personal to industrial use-cases. However, current robotic technologies and their computing paradigm still lack embodied intelligence to efficiently interact with operational environments, respond with correct/expected actions, and adapt to changes in the environments. Toward this, recent advances in neuromorphic computing with Spiking Neural Networks (SNN) have demonstrated the potential to enable the embodied intelligence for robotics through bio-plausible computing paradigm that mimics how the biological brain works, known as \"neuromorphic artificial intelligence (AI)\". However, the field of neuromorphic AI-based robotics is still at an early stage, therefore its development and deployment for solving real-world problems expose new challenges in different design aspects, such as accuracy, adaptability, efficiency, reliability, and security. To address these challenges, this paper will discuss how we can enable embodied neuromorphic AI for robotic systems through our perspectives: (P1) Embodied intelligence based on effective learning rule, training mechanism, and adaptability; (P2) Cross-layer optimizations for energy-efficient neuromorphic computing; (P3) Representative and fair benchmarks; (P4) Low-cost reliability and safety enhancements; (P5) Security and privacy for neuromorphic computing; and (P6) A synergistic development for energy-efficient and robust neuromorphic-based robotics. Furthermore, this paper identifies research challenges and opportunities, as well as elaborates our vision for future research development toward embodied neuromorphic AI for robotics.",
      "paper_authors": [
        "Rachmad Vidya Wicaksana Putra",
        "Alberto Marchisio",
        "Fakhreddine Zayer",
        "Jorge Dias",
        "Muhammad Shafique"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-04-04",
      "update_time": "2024-04-04",
      "comments": "8 pages, 9 figures, 1 table",
      "repo_url": "#"
    },
    "2404.02248": {
      "paper_id": "2404.02248v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.02248v1",
      "paper_key": "2404.02248",
      "paper_title": "A Fully-Configurable Open-Source Software-Defined Digital Quantized Spiking Neural Core Architecture",
      "paper_url": "http://arxiv.org/abs/2404.02248v1",
      "paper_abstract": "We introduce QUANTISENC, a fully configurable open-source software-defined digital quantized spiking neural core architecture to advance research in neuromorphic computing. QUANTISENC is designed hierarchically using a bottom-up methodology with multiple neurons in each layer and multiple layers in each core. The number of layers and neurons per layer can be configured via software in a top-down methodology to generate the hardware for a target spiking neural network (SNN) model. QUANTISENC uses leaky integrate and fire neurons (LIF) and current-based excitatory and inhibitory synapses (CUBA). The nonlinear dynamics of a neuron can be configured at run-time via programming its internal control registers. Each neuron performs signed fixed-point arithmetic with user-defined quantization and decimal precision. QUANTISENC supports all-to-all, one-to-one, and Gaussian connections between layers. Its hardware-software interface is integrated with a PyTorch-based SNN simulator. This integration allows to define and train an SNN model in PyTorch and evaluate the hardware performance (e.g., area, power, latency, and throughput) through FPGA prototyping and ASIC design. The hardware-software interface also takes advantage of the layer-based architecture and distributed memory organization of QUANTISENC to enable pipelining by overlapping computations on streaming data. Overall, the proposed software-defined hardware design methodology offers flexibility similar to that of high-level synthesis (HLS), but provides better hardware performance with zero hardware development effort. We evaluate QUANTISENC using three spiking datasets and show its superior performance against state-of the-art designs.",
      "paper_authors": [
        "Shadi Matinizadeh",
        "Noah Pacik-Nelson",
        "Ioannis Polykretis",
        "Krupa Tishbi",
        "Suman Kumar",
        "M. L. Varshika",
        "Arghavan Mohammadhassani",
        "Abhishek Mishra",
        "Nagarajan Kandasamy",
        "James Shackleford",
        "Eric Gallo",
        "Anup Das"
      ],
      "primary_category": "cs.AR",
      "publish_time": "2024-04-02",
      "update_time": "2024-04-02",
      "comments": null,
      "repo_url": "#"
    },
    "2404.01897": {
      "paper_id": "2404.01897v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.01897v1",
      "paper_key": "2404.01897",
      "paper_title": "Continuous Spiking Graph Neural Networks",
      "paper_url": "http://arxiv.org/abs/2404.01897v1",
      "paper_abstract": "Continuous graph neural networks (CGNNs) have garnered significant attention due to their ability to generalize existing discrete graph neural networks (GNNs) by introducing continuous dynamics. They typically draw inspiration from diffusion-based methods to introduce a novel propagation scheme, which is analyzed using ordinary differential equations (ODE). However, the implementation of CGNNs requires significant computational power, making them challenging to deploy on battery-powered devices. Inspired by recent spiking neural networks (SNNs), which emulate a biological inference process and provide an energy-efficient neural architecture, we incorporate the SNNs with CGNNs in a unified framework, named Continuous Spiking Graph Neural Networks (COS-GNN). We employ SNNs for graph node representation at each time step, which are further integrated into the ODE process along with time. To enhance information preservation and mitigate information loss in SNNs, we introduce the high-order structure of COS-GNN, which utilizes the second-order ODE for spiking representation and continuous propagation. Moreover, we provide the theoretical proof that COS-GNN effectively mitigates the issues of exploding and vanishing gradients, enabling us to capture long-range dependencies between nodes. Experimental results on graph-based learning tasks demonstrate the effectiveness of the proposed COS-GNN over competitive baselines.",
      "paper_authors": [
        "Nan Yin",
        "Mengzhu Wan",
        "Li Shen",
        "Hitesh Laxmichand Patel",
        "Baopu Li",
        "Bin Gu",
        "Huan Xiong"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-04-02",
      "update_time": "2024-04-02",
      "comments": null,
      "repo_url": "#"
    },
    "2404.01685": {
      "paper_id": "2404.01685v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.01685v2",
      "paper_key": "2404.01685",
      "paper_title": "A Methodology for Improving Accuracy of Embedded Spiking Neural Networks through Kernel Size Scaling",
      "paper_url": "http://arxiv.org/abs/2404.01685v2",
      "paper_abstract": "Spiking Neural Networks (SNNs) can offer ultra low power/ energy consumption for machine learning-based applications due to their sparse spike-based operations. Currently, most of the SNN architectures need a significantly larger model size to achieve higher accuracy, which is not suitable for resource-constrained embedded applications. Therefore, developing SNNs that can achieve high accuracy with acceptable memory footprint is highly needed. Toward this, we propose a novel methodology that improves the accuracy of SNNs through kernel size scaling. Its key steps include investigating the impact of different kernel sizes on the accuracy, devising new sets of kernel sizes, generating SNN architectures based on the selected kernel sizes, and analyzing the accuracy-memory trade-offs for SNN model selection. The experimental results show that our methodology achieves higher accuracy than state-of-the-art (93.24% accuracy for CIFAR10 and 70.84% accuracy for CIFAR100) with less than 10M parameters and up to 3.45x speed-up of searching time, thereby making it suitable for embedded applications.",
      "paper_authors": [
        "Rachmad Vidya Wicaksana Putra",
        "Muhammad Shafique"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-04-02",
      "update_time": "2024-04-04",
      "comments": "3 pages, 3 figures",
      "repo_url": "#"
    },
    "2404.01174": {
      "paper_id": "2404.01174v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.01174v2",
      "paper_key": "2404.01174",
      "paper_title": "SpikeMba: Multi-Modal Spiking Saliency Mamba for Temporal Video Grounding",
      "paper_url": "http://arxiv.org/abs/2404.01174v2",
      "paper_abstract": "Temporal video grounding (TVG) is a critical task in video content understanding, requiring precise alignment between video content and natural language instructions. Despite significant advancements, existing methods face challenges in managing confidence bias towards salient objects and capturing long-term dependencies in video sequences. To address these issues, we introduce SpikeMba: a multi-modal spiking saliency mamba for temporal video grounding. Our approach integrates Spiking Neural Networks (SNNs) with state space models (SSMs) to leverage their unique advantages in handling different aspects of the task. Specifically, we use SNNs to develop a spiking saliency detector that generates the proposal set. The detector emits spike signals when the input signal exceeds a predefined threshold, resulting in a dynamic and binary saliency proposal set. To enhance the model's capability to retain and infer contextual information, we introduce relevant slots which learnable tensors that encode prior knowledge. These slots work with the contextual moment reasoner to maintain a balance between preserving contextual information and exploring semantic relevance dynamically. The SSMs facilitate selective information propagation, addressing the challenge of long-term dependency in video content. By combining SNNs for proposal generation and SSMs for effective contextual reasoning, SpikeMba addresses confidence bias and long-term dependencies, thereby significantly enhancing fine-grained multimodal relationship capture. Our experiments demonstrate the effectiveness of SpikeMba, which consistently outperforms state-of-the-art methods across mainstream benchmarks.",
      "paper_authors": [
        "Wenrui Li",
        "Xiaopeng Hong",
        "Ruiqin Xiong",
        "Xiaopeng Fan"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-04-01",
      "update_time": "2024-05-23",
      "comments": null,
      "repo_url": "#"
    },
    "2404.01359": {
      "paper_id": "2404.01359v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.01359v1",
      "paper_key": "2404.01359",
      "paper_title": "Parallel Proportional Fusion of Spiking Quantum Neural Network for Optimizing Image Classification",
      "paper_url": "http://arxiv.org/abs/2404.01359v1",
      "paper_abstract": "The recent emergence of the hybrid quantum-classical neural network (HQCNN) architecture has garnered considerable attention due to the potential advantages associated with integrating quantum principles to enhance various facets of machine learning algorithms and computations. However, the current investigated serial structure of HQCNN, wherein information sequentially passes from one network to another, often imposes limitations on the trainability and expressivity of the network. In this study, we introduce a novel architecture termed Parallel Proportional Fusion of Quantum and Spiking Neural Networks (PPF-QSNN). The dataset information is simultaneously fed into both the spiking neural network and the variational quantum circuits, with the outputs amalgamated in proportion to their individual contributions. We systematically assess the impact of diverse PPF-QSNN parameters on network performance for image classification, aiming to identify the optimal configuration. Numerical results on the MNIST dataset unequivocally illustrate that our proposed PPF-QSNN outperforms both the existing spiking neural network and the serial quantum neural network across metrics such as accuracy, loss, and robustness. This study introduces a novel and effective amalgamation approach for HQCNN, thereby laying the groundwork for the advancement and application of quantum advantage in artificial intelligent computations.",
      "paper_authors": [
        "Zuyu Xu",
        "Kang Shen",
        "Pengnian Cai",
        "Tao Yang",
        "Yuanming Hu",
        "Shixian Chen",
        "Yunlai Zhu",
        "Zuheng Wu",
        "Yuehua Dai",
        "Jun Wang",
        "Fei Yang"
      ],
      "primary_category": "quant-ph",
      "publish_time": "2024-04-01",
      "update_time": "2024-04-01",
      "comments": null,
      "repo_url": "#"
    },
    "2404.00383": {
      "paper_id": "2404.00383v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.00383v1",
      "paper_key": "2404.00383",
      "paper_title": "SpikingJET: Enhancing Fault Injection for Fully and Convolutional Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2404.00383v1",
      "paper_abstract": "As artificial neural networks become increasingly integrated into safety-critical systems such as autonomous vehicles, devices for medical diagnosis, and industrial automation, ensuring their reliability in the face of random hardware faults becomes paramount. This paper introduces SpikingJET, a novel fault injector designed specifically for fully connected and convolutional Spiking Neural Networks (SNNs). Our work underscores the critical need to evaluate the resilience of SNNs to hardware faults, considering their growing prominence in real-world applications. SpikingJET provides a comprehensive platform for assessing the resilience of SNNs by inducing errors and injecting faults into critical components such as synaptic weights, neuron model parameters, internal states, and activation functions. This paper demonstrates the effectiveness of Spiking-JET through extensive software-level experiments on various SNN architectures, revealing insights into their vulnerability and resilience to hardware faults. Moreover, highlighting the importance of fault resilience in SNNs contributes to the ongoing effort to enhance the reliability and safety of Neural Network (NN)-powered systems in diverse domains.",
      "paper_authors": [
        "Anil Bayram Gogebakan",
        "Enrico Magliano",
        "Alessio Carpegna",
        "Annachiara Ruospo",
        "Alessandro Savino",
        "Stefano Di Carlo"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-03-30",
      "update_time": "2024-03-30",
      "comments": null,
      "repo_url": "#"
    },
    "2403.20163": {
      "paper_id": "2403.20163v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.20163v1",
      "paper_key": "2403.20163",
      "paper_title": "Biologically-Plausible Topology Improved Spiking Actor Network for Efficient Deep Reinforcement Learning",
      "paper_url": "http://arxiv.org/abs/2403.20163v1",
      "paper_abstract": "The success of Deep Reinforcement Learning (DRL) is largely attributed to utilizing Artificial Neural Networks (ANNs) as function approximators. Recent advances in neuroscience have unveiled that the human brain achieves efficient reward-based learning, at least by integrating spiking neurons with spatial-temporal dynamics and network topologies with biologically-plausible connectivity patterns. This integration process allows spiking neurons to efficiently combine information across and within layers via nonlinear dendritic trees and lateral interactions. The fusion of these two topologies enhances the network's information-processing ability, crucial for grasping intricate perceptions and guiding decision-making procedures. However, ANNs and brain networks differ significantly. ANNs lack intricate dynamical neurons and only feature inter-layer connections, typically achieved by direct linear summation, without intra-layer connections. This limitation leads to constrained network expressivity. To address this, we propose a novel alternative for function approximator, the Biologically-Plausible Topology improved Spiking Actor Network (BPT-SAN), tailored for efficient decision-making in DRL. The BPT-SAN incorporates spiking neurons with intricate spatial-temporal dynamics and introduces intra-layer connections, enhancing spatial-temporal state representation and facilitating more precise biological simulations. Diverging from the conventional direct linear weighted sum, the BPT-SAN models the local nonlinearities of dendritic trees within the inter-layer connections. For the intra-layer connections, the BPT-SAN introduces lateral interactions between adjacent neurons, integrating them into the membrane potential formula to ensure accurate spike firing.",
      "paper_authors": [
        "Duzhen Zhang",
        "Qingyu Wang",
        "Tielin Zhang",
        "Bo Xu"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-03-29",
      "update_time": "2024-03-29",
      "comments": "Work in Progress",
      "repo_url": "#"
    },
    "2408.13018": {
      "paper_id": "2408.13018v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.13018v1",
      "paper_key": "2408.13018",
      "paper_title": "Robust Iterative Value Conversion: Deep Reinforcement Learning for Neurochip-driven Edge Robots",
      "paper_url": "http://arxiv.org/abs/2408.13018v1",
      "paper_abstract": "A neurochip is a device that reproduces the signal processing mechanisms of brain neurons and calculates Spiking Neural Networks (SNNs) with low power consumption and at high speed. Thus, neurochips are attracting attention from edge robot applications, which suffer from limited battery capacity. This paper aims to achieve deep reinforcement learning (DRL) that acquires SNN policies suitable for neurochip implementation. Since DRL requires a complex function approximation, we focus on conversion techniques from Floating Point NN (FPNN) because it is one of the most feasible SNN techniques. However, DRL requires conversions to SNNs for every policy update to collect the learning samples for a DRL-learning cycle, which updates the FPNN policy and collects the SNN policy samples. Accumulative conversion errors can significantly degrade the performance of the SNN policies. We propose Robust Iterative Value Conversion (RIVC) as a DRL that incorporates conversion error reduction and robustness to conversion errors. To reduce them, FPNN is optimized with the same number of quantization bits as an SNN. The FPNN output is not significantly changed by quantization. To robustify the conversion error, an FPNN policy that is applied with quantization is updated to increase the gap between the probability of selecting the optimal action and other actions. This step prevents unexpected replacements of the policy's optimal actions. We verified RIVC's effectiveness on a neurochip-driven robot. The results showed that RIVC consumed 1/15 times less power and increased the calculation speed by five times more than an edge CPU (quad-core ARM Cortex-A72). The previous framework with no countermeasures against conversion errors failed to train the policies. Videos from our experiments are available: https://youtu.be/Q5Z0-BvK1Tc.",
      "paper_authors": [
        "Yuki Kadokawa",
        "Tomohito Kodera",
        "Yoshihisa Tsurumine",
        "Shinya Nishimura",
        "Takamitsu Matsubara"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-08-23",
      "update_time": "2024-08-23",
      "comments": "Accepted by Robotics and Autonomous Systems",
      "repo_url": "#"
    },
    "2408.12767": {
      "paper_id": "2408.12767v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.12767v1",
      "paper_key": "2408.12767",
      "paper_title": "When In-memory Computing Meets Spiking Neural Networks -- A Perspective on Device-Circuit-System-and-Algorithm Co-design",
      "paper_url": "http://arxiv.org/abs/2408.12767v1",
      "paper_abstract": "This review explores the intersection of bio-plausible artificial intelligence in the form of Spiking Neural Networks (SNNs) with the analog In-Memory Computing (IMC) domain, highlighting their collective potential for low-power edge computing environments. Through detailed investigation at the device, circuit, and system levels, we highlight the pivotal synergies between SNNs and IMC architectures. Additionally, we emphasize the critical need for comprehensive system-level analyses, considering the inter-dependencies between algorithms, devices, circuit & system parameters, crucial for optimal performance. An in-depth analysis leads to identification of key system-level bottlenecks arising from device limitations which can be addressed using SNN-specific algorithm-hardware co-design techniques. This review underscores the imperative for holistic device to system design space co-exploration, highlighting the critical aspects of hardware and algorithm research endeavors for low-power neuromorphic solutions.",
      "paper_authors": [
        "Abhishek Moitra",
        "Abhiroop Bhattacharjee",
        "Yuhang Li",
        "Youngeun Kim",
        "Priyadarshini Panda"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-08-22",
      "update_time": "2024-08-22",
      "comments": "19 Pages, 13 Figures",
      "repo_url": "#"
    },
    "2408.12608": {
      "paper_id": "2408.12608v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.12608v1",
      "paper_key": "2408.12608",
      "paper_title": "A frugal Spiking Neural Network for unsupervised classification of continuous multivariate temporal data",
      "paper_url": "http://arxiv.org/abs/2408.12608v1",
      "paper_abstract": "As neural interfaces become more advanced, there has been an increase in the volume and complexity of neural data recordings. These interfaces capture rich information about neural dynamics that call for efficient, real-time processing algorithms to spontaneously extract and interpret patterns of neural dynamics. Moreover, being able to do so in a fully unsupervised manner is critical as patterns in vast streams of neural data might not be easily identifiable by the human eye. Formal Deep Neural Networks (DNNs) have come a long way in performing pattern recognition tasks for various static and sequential pattern recognition applications. However, these networks usually require large labeled datasets for training and have high power consumption preventing their future embedding in active brain implants. An alternative aimed at addressing these issues are Spiking Neural Networks (SNNs) which are neuromorphic and use more biologically plausible neurons with evolving membrane potentials. In this context, we introduce here a frugal single-layer SNN designed for fully unsupervised identification and classification of multivariate temporal patterns in continuous data with a sequential approach. We show that, with only a handful number of neurons, this strategy is efficient to recognize highly overlapping multivariate temporal patterns, first on simulated data, and then on Mel Cepstral representations of speech sounds and finally on multichannel neural data. This approach relies on several biologically inspired plasticity rules, including Spike-timing-dependent plasticity (STDP), Short-term plasticity (STP) and intrinsic plasticity (IP). These results pave the way towards highly frugal SNNs for fully unsupervised and online-compatible learning of complex multivariate temporal patterns for future embedding in dedicated very-low power hardware.",
      "paper_authors": [
        "Sai Deepesh Pokala",
        "Marie Bernert",
        "Takuya Nanami",
        "Takashi Kohno",
        "Timoth\u00e9e L\u00e9vi",
        "Blaise Yvert"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-08-08",
      "update_time": "2024-08-08",
      "comments": null,
      "repo_url": "#"
    },
    "2403.18609": {
      "paper_id": "2403.18609v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.18609v1",
      "paper_key": "2403.18609",
      "paper_title": "A survey on learning models of spiking neural membrane systems and spiking neural networks",
      "paper_url": "http://arxiv.org/abs/2403.18609v1",
      "paper_abstract": "Spiking neural networks (SNN) are a biologically inspired model of neural networks with certain brain-like properties. In the past few decades, this model has received increasing attention in computer science community, owing also to the successful phenomenon of deep learning. In SNN, communication between neurons takes place through the spikes and spike trains. This differentiates these models from the ``standard'' artificial neural networks (ANN) where the frequency of spikes is replaced by real-valued signals. Spiking neural P systems (SNPS) can be considered a branch of SNN based more on the principles of formal automata, with many variants developed within the framework of the membrane computing theory. In this paper, we first briefly compare structure and function, advantages and drawbacks of SNN and SNPS. A key part of the article is a survey of recent results and applications of machine learning and deep learning models of both SNN and SNPS formalisms.",
      "paper_authors": [
        "Prithwineel Paul",
        "Petr Sosik",
        "Lucie Ciencialova"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-03-27",
      "update_time": "2024-03-27",
      "comments": null,
      "repo_url": "#"
    },
    "2403.18607": {
      "paper_id": "2403.18607v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.18607v1",
      "paper_key": "2403.18607",
      "paper_title": "Spikewhisper: Temporal Spike Backdoor Attacks on Federated Neuromorphic Learning over Low-power Devices",
      "paper_url": "http://arxiv.org/abs/2403.18607v1",
      "paper_abstract": "Federated neuromorphic learning (FedNL) leverages event-driven spiking neural networks and federated learning frameworks to effectively execute intelligent analysis tasks over amounts of distributed low-power devices but also perform vulnerability to poisoning attacks. The threat of backdoor attacks on traditional deep neural networks typically comes from time-invariant data. However, in FedNL, unknown threats may be hidden in time-varying spike signals. In this paper, we start to explore a novel vulnerability of FedNL-based systems with the concept of time division multiplexing, termed Spikewhisper, which allows attackers to evade detection as much as possible, as multiple malicious clients can imperceptibly poison with different triggers at different timeslices. In particular, the stealthiness of Spikewhisper is derived from the time-domain divisibility of global triggers, in which each malicious client pastes only one local trigger to a certain timeslice in the neuromorphic sample, and also the polarity and motion of each local trigger can be configured by attackers. Extensive experiments based on two different neuromorphic datasets demonstrate that the attack success rate of Spikewispher is higher than the temporally centralized attacks. Besides, it is validated that the effect of Spikewispher is sensitive to the trigger duration.",
      "paper_authors": [
        "Hanqing Fu",
        "Gaolei Li",
        "Jun Wu",
        "Jianhua Li",
        "Xi Lin",
        "Kai Zhou",
        "Yuchen Liu"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-03-27",
      "update_time": "2024-03-27",
      "comments": null,
      "repo_url": "#"
    },
    "2403.18388": {
      "paper_id": "2403.18388v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.18388v1",
      "paper_key": "2403.18388",
      "paper_title": "FTBC: Forward Temporal Bias Correction for Optimizing ANN-SNN Conversion",
      "paper_url": "http://arxiv.org/abs/2403.18388v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) offer a promising avenue for energy-efficient computing compared with Artificial Neural Networks (ANNs), closely mirroring biological neural processes. However, this potential comes with inherent challenges in directly training SNNs through spatio-temporal backpropagation -- stemming from the temporal dynamics of spiking neurons and their discrete signal processing -- which necessitates alternative ways of training, most notably through ANN-SNN conversion. In this work, we introduce a lightweight Forward Temporal Bias Correction (FTBC) technique, aimed at enhancing conversion accuracy without the computational overhead. We ground our method on provided theoretical findings that through proper temporal bias calibration the expected error of ANN-SNN conversion can be reduced to be zero after each time step. We further propose a heuristic algorithm for finding the temporal bias only in the forward pass, thus eliminating the computational burden of backpropagation and we evaluate our method on CIFAR-10/100 and ImageNet datasets, achieving a notable increase in accuracy on all datasets. Codes are released at a GitHub repository.",
      "paper_authors": [
        "Xiaofeng Wu",
        "Velibor Bojkovic",
        "Bin Gu",
        "Kun Suo",
        "Kai Zou"
      ],
      "primary_category": "cs.AI",
      "publish_time": "2024-03-27",
      "update_time": "2024-03-27",
      "comments": null,
      "repo_url": "#"
    },
    "2403.18228": {
      "paper_id": "2403.18228v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.18228v1",
      "paper_key": "2403.18228",
      "paper_title": "Fourier or Wavelet bases as counterpart self-attention in spikformer for efficient visual classification",
      "paper_url": "http://arxiv.org/abs/2403.18228v1",
      "paper_abstract": "Energy-efficient spikformer has been proposed by integrating the biologically plausible spiking neural network (SNN) and artificial Transformer, whereby the Spiking Self-Attention (SSA) is used to achieve both higher accuracy and lower computational cost. However, it seems that self-attention is not always necessary, especially in sparse spike-form calculation manners. In this paper, we innovatively replace vanilla SSA (using dynamic bases calculating from Query and Key) with spike-form Fourier Transform, Wavelet Transform, and their combinations (using fixed triangular or wavelets bases), based on a key hypothesis that both of them use a set of basis functions for information transformation. Hence, the Fourier-or-Wavelet-based spikformer (FWformer) is proposed and verified in visual classification tasks, including both static image and event-based video datasets. The FWformer can achieve comparable or even higher accuracies ($0.4\\%$-$1.5\\%$), higher running speed ($9\\%$-$51\\%$ for training and $19\\%$-$70\\%$ for inference), reduced theoretical energy consumption ($20\\%$-$25\\%$), and reduced GPU memory usage ($4\\%$-$26\\%$), compared to the standard spikformer. Our result indicates the continuous refinement of new Transformers, that are inspired either by biological discovery (spike-form), or information theory (Fourier or Wavelet Transform), is promising.",
      "paper_authors": [
        "Qingyu Wang",
        "Duzhen Zhang",
        "Tilelin Zhang",
        "Bo Xu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-03-27",
      "update_time": "2024-03-27",
      "comments": "18 pages, 2 figures. arXiv admin note: substantial text overlap with\n  arXiv:2308.02557",
      "repo_url": "#"
    },
    "2403.17040": {
      "paper_id": "2403.17040v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.17040v1",
      "paper_key": "2403.17040",
      "paper_title": "Enhancing Graph Representation Learning with Attention-Driven Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2403.17040v1",
      "paper_abstract": "Graph representation learning has become a crucial task in machine learning and data mining due to its potential for modeling complex structures such as social networks, chemical compounds, and biological systems. Spiking neural networks (SNNs) have recently emerged as a promising alternative to traditional neural networks for graph learning tasks, benefiting from their ability to efficiently encode and process temporal and spatial information. In this paper, we propose a novel approach that integrates attention mechanisms with SNNs to improve graph representation learning. Specifically, we introduce an attention mechanism for SNN that can selectively focus on important nodes and corresponding features in a graph during the learning process. We evaluate our proposed method on several benchmark datasets and show that it achieves comparable performance compared to existing graph learning techniques.",
      "paper_authors": [
        "Huifeng Yin",
        "Mingkun Xu",
        "Jing Pei",
        "Lei Deng"
      ],
      "primary_category": "cs.AI",
      "publish_time": "2024-03-25",
      "update_time": "2024-03-25",
      "comments": null,
      "repo_url": "#"
    },
    "2403.16674": {
      "paper_id": "2403.16674v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.16674v1",
      "paper_key": "2403.16674",
      "paper_title": "Understanding the Functional Roles of Modelling Components in Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2403.16674v1",
      "paper_abstract": "Spiking neural networks (SNNs), inspired by the neural circuits of the brain, are promising in achieving high computational efficiency with biological fidelity. Nevertheless, it is quite difficult to optimize SNNs because the functional roles of their modelling components remain unclear. By designing and evaluating several variants of the classic model, we systematically investigate the functional roles of key modelling components, leakage, reset, and recurrence, in leaky integrate-and-fire (LIF) based SNNs. Through extensive experiments, we demonstrate how these components influence the accuracy, generalization, and robustness of SNNs. Specifically, we find that the leakage plays a crucial role in balancing memory retention and robustness, the reset mechanism is essential for uninterrupted temporal processing and computational efficiency, and the recurrence enriches the capability to model complex dynamics at a cost of robustness degradation. With these interesting observations, we provide optimization suggestions for enhancing the performance of SNNs in different scenarios. This work deepens the understanding of how SNNs work, which offers valuable guidance for the development of more effective and robust neuromorphic models.",
      "paper_authors": [
        "Huifeng Yin",
        "Hanle Zheng",
        "Jiayi Mao",
        "Siyuan Ding",
        "Xing Liu",
        "Mingkun Xu",
        "Yifan Hu",
        "Jing Pei",
        "Lei Deng"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-03-25",
      "update_time": "2024-03-25",
      "comments": null,
      "repo_url": "#"
    },
    "2403.16552": {
      "paper_id": "2403.16552v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.16552v1",
      "paper_key": "2403.16552",
      "paper_title": "QKFormer: Hierarchical Spiking Transformer using Q-K Attention",
      "paper_url": "http://arxiv.org/abs/2403.16552v1",
      "paper_abstract": "Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with Transformer architectures, have attracted significant attention due to their potential for energy efficiency and high performance. However, existing models in this domain still suffer from suboptimal performance. We introduce several innovations to improve the performance: i) We propose a novel spike-form Q-K attention mechanism, tailored for SNNs, which efficiently models the importance of token or channel dimensions through binary vectors with linear complexity. ii) We incorporate the hierarchical structure, which significantly benefits the performance of both the brain and artificial neural networks, into spiking transformers to obtain multi-scale spiking representation. iii) We design a versatile and powerful patch embedding module with a deformed shortcut specifically for spiking transformers. Together, we develop QKFormer, a hierarchical spiking transformer based on Q-K attention with direct training. QKFormer shows significantly superior performance over existing state-of-the-art SNN models on various mainstream datasets. Notably, with comparable size to Spikformer (66.34 M, 74.81%), QKFormer (64.96 M) achieves a groundbreaking top-1 accuracy of 85.65% on ImageNet-1k, substantially outperforming Spikformer by 10.84%. To our best knowledge, this is the first time that directly training SNNs have exceeded 85% accuracy on ImageNet-1K. The code and models are publicly available at https://github.com/zhouchenlin2096/QKFormer",
      "paper_authors": [
        "Chenlin Zhou",
        "Han Zhang",
        "Zhaokun Zhou",
        "Liutao Yu",
        "Liwei Huang",
        "Xiaopeng Fan",
        "Li Yuan",
        "Zhengyu Ma",
        "Huihui Zhou",
        "Yonghong Tian"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-03-25",
      "update_time": "2024-03-25",
      "comments": "10 pages, code: https://github.com/zhouchenlin2096/QKFormer",
      "repo_url": "https://github.com/zhouchenlin2096/qkformer"
    },
    "2403.16438": {
      "paper_id": "2403.16438v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.16438v1",
      "paper_key": "2403.16438",
      "paper_title": "Real-time Neuron Segmentation for Voltage Imaging",
      "paper_url": "http://arxiv.org/abs/2403.16438v1",
      "paper_abstract": "In voltage imaging, where the membrane potentials of individual neurons are recorded at from hundreds to thousand frames per second using fluorescence microscopy, data processing presents a challenge. Even a fraction of a minute of recording with a limited image size yields gigabytes of video data consisting of tens of thousands of frames, which can be time-consuming to process. Moreover, millisecond-level short exposures lead to noisy video frames, obscuring neuron footprints especially in deep-brain samples where noisy signals are buried in background fluorescence. To address this challenge, we propose a fast neuron segmentation method able to detect multiple, potentially overlapping, spiking neurons from noisy video frames, and implement a data processing pipeline incorporating the proposed segmentation method along with GPU-accelerated motion correction. By testing on existing datasets as well as on new datasets we introduce, we show that our pipeline extracts neuron footprints that agree well with human annotation even from cluttered datasets, and demonstrate real-time processing of voltage imaging data on a single desktop computer for the first time.",
      "paper_authors": [
        "Yosuke Bando",
        "Ramdas Pillai",
        "Atsushi Kajita",
        "Farhan Abdul Hakeem",
        "Yves Quemener",
        "Hua-an Tseng",
        "Kiryl D. Piatkevich",
        "Changyang Linghu",
        "Xue Han",
        "Edward S. Boyden"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2024-03-25",
      "update_time": "2024-03-25",
      "comments": null,
      "repo_url": "#"
    },
    "2403.16327": {
      "paper_id": "2403.16327v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.16327v1",
      "paper_key": "2403.16327",
      "paper_title": "Artificial Neural Microcircuits as Building Blocks: Concept and Challenges",
      "paper_url": "http://arxiv.org/abs/2403.16327v1",
      "paper_abstract": "Artificial Neural Networks (ANNs) are one of the most widely employed forms of bio-inspired computation. However the current trend is for ANNs to be structurally homogeneous. Furthermore, this structural homogeneity requires the application of complex training and learning tools that produce application specific ANNs, susceptible to pitfalls such as overfitting. In this paper, an new approach is explored, inspired by the role played in biology by Neural Microcircuits, the so called ``fundamental processing elements'' of organic nervous systems. How large neural networks, particularly Spiking Neural Networks (SNNs) can be assembled using Artificial Neural Microcircuits (ANMs), intended as off-the-shelf components, is articulated; the results of initial work to produce a catalogue of such Microcircuits though the use of Novelty Search is shown; followed by efforts to expand upon this initial work, including a discussion of challenges uncovered during these efforts and explorations of methods by which they might be overcome.",
      "paper_authors": [
        "Andrew Walter",
        "Shimeng Wu",
        "Andy M. Tyrrell",
        "Liam McDaid",
        "Malachy McElholm",
        "Nidhin Thandassery Sumithran",
        "Jim Harkin",
        "Martin A. Trefzer"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-03-24",
      "update_time": "2024-03-24",
      "comments": "12 pages, 31 figures, 3 tables, submitted to A-Life Journal for\n  review",
      "repo_url": "#"
    },
    "2403.15717": {
      "paper_id": "2403.15717v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.15717v1",
      "paper_key": "2403.15717",
      "paper_title": "Ev-Edge: Efficient Execution of Event-based Vision Algorithms on Commodity Edge Platforms",
      "paper_url": "http://arxiv.org/abs/2403.15717v1",
      "paper_abstract": "Event cameras have emerged as a promising sensing modality for autonomous navigation systems, owing to their high temporal resolution, high dynamic range and negligible motion blur. To process the asynchronous temporal event streams from such sensors, recent research has shown that a mix of Artificial Neural Networks (ANNs), Spiking Neural Networks (SNNs) as well as hybrid SNN-ANN algorithms are necessary to achieve high accuracies across a range of perception tasks. However, we observe that executing such workloads on commodity edge platforms which feature heterogeneous processing elements such as CPUs, GPUs and neural accelerators results in inferior performance. This is due to the mismatch between the irregular nature of event streams and diverse characteristics of algorithms on the one hand and the underlying hardware platform on the other. We propose Ev-Edge, a framework that contains three key optimizations to boost the performance of event-based vision systems on edge platforms: (1) An Event2Sparse Frame converter directly transforms raw event streams into sparse frames, enabling the use of sparse libraries with minimal encoding overheads (2) A Dynamic Sparse Frame Aggregator merges sparse frames at runtime by trading off the temporal granularity of events and computational demand thereby improving hardware utilization (3) A Network Mapper maps concurrently executing tasks to different processing elements while also selecting layer precision by considering both compute and communication overheads. On several state-of-art networks for a range of autonomous navigation tasks, Ev-Edge achieves 1.28x-2.05x improvements in latency and 1.23x-2.15x in energy over an all-GPU implementation on the NVIDIA Jetson Xavier AGX platform for single-task execution scenarios. Ev-Edge also achieves 1.43x-1.81x latency improvements over round-robin scheduling methods in multi-task execution scenarios.",
      "paper_authors": [
        "Shrihari Sridharan",
        "Surya Selvam",
        "Kaushik Roy",
        "Anand Raghunathan"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-03-23",
      "update_time": "2024-03-23",
      "comments": null,
      "repo_url": "#"
    },
    "2403.15192": {
      "paper_id": "2403.15192v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.15192v1",
      "paper_key": "2403.15192",
      "paper_title": "SFOD: Spiking Fusion Object Detector",
      "paper_url": "http://arxiv.org/abs/2403.15192v1",
      "paper_abstract": "Event cameras, characterized by high temporal resolution, high dynamic range, low power consumption, and high pixel bandwidth, offer unique capabilities for object detection in specialized contexts. Despite these advantages, the inherent sparsity and asynchrony of event data pose challenges to existing object detection algorithms. Spiking Neural Networks (SNNs), inspired by the way the human brain codes and processes information, offer a potential solution to these difficulties. However, their performance in object detection using event cameras is limited in current implementations. In this paper, we propose the Spiking Fusion Object Detector (SFOD), a simple and efficient approach to SNN-based object detection. Specifically, we design a Spiking Fusion Module, achieving the first-time fusion of feature maps from different scales in SNNs applied to event cameras. Additionally, through integrating our analysis and experiments conducted during the pretraining of the backbone network on the NCAR dataset, we delve deeply into the impact of spiking decoding strategies and loss functions on model performance. Thereby, we establish state-of-the-art classification results based on SNNs, achieving 93.7\\% accuracy on the NCAR dataset. Experimental results on the GEN1 detection dataset demonstrate that the SFOD achieves a state-of-the-art mAP of 32.1\\%, outperforming existing SNN-based approaches. Our research not only underscores the potential of SNNs in object detection with event cameras but also propels the advancement of SNNs. Code is available at https://github.com/yimeng-fan/SFOD.",
      "paper_authors": [
        "Yimeng Fan",
        "Wei Zhang",
        "Changsong Liu",
        "Mingyang Li",
        "Wenrui Lu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-03-22",
      "update_time": "2024-03-22",
      "comments": "Accepted by CVPR2024",
      "repo_url": "https://github.com/yimeng-fan/SFOD"
    },
    "2403.14302": {
      "paper_id": "2403.14302v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.14302v2",
      "paper_key": "2403.14302",
      "paper_title": "SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2403.14302v2",
      "paper_abstract": "The remarkable success of Vision Transformers in Artificial Neural Networks (ANNs) has led to a growing interest in incorporating the self-attention mechanism and transformer-based architecture into Spiking Neural Networks (SNNs). While existing methods propose spiking self-attention mechanisms that are compatible with SNNs, they lack reasonable scaling methods, and the overall architectures proposed by these methods suffer from a bottleneck in effectively extracting local features. To address these challenges, we propose a novel spiking self-attention mechanism named Dual Spike Self-Attention (DSSA) with a reasonable scaling method. Based on DSSA, we propose a novel spiking Vision Transformer architecture called SpikingResformer, which combines the ResNet-based multi-stage architecture with our proposed DSSA to improve both performance and energy efficiency while reducing parameters. Experimental results show that SpikingResformer achieves higher accuracy with fewer parameters and lower energy consumption than other spiking Vision Transformer counterparts. Notably, our SpikingResformer-L achieves 79.40% top-1 accuracy on ImageNet with 4 time-steps, which is the state-of-the-art result in the SNN field.",
      "paper_authors": [
        "Xinyu Shi",
        "Zecheng Hao",
        "Zhaofei Yu"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-03-21",
      "update_time": "2024-03-28",
      "comments": "To be published in the 2024 IEEE/CVF Conference on Computer Vision\n  and Pattern Recognition (CVPR)",
      "repo_url": "https://github.com/xyshi2000/spikingresformer"
    },
    "2403.15480": {
      "paper_id": "2403.15480v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.15480v1",
      "paper_key": "2403.15480",
      "paper_title": "SpikeGraphormer: A High-Performance Graph Transformer with Spiking Graph Attention",
      "paper_url": "http://arxiv.org/abs/2403.15480v1",
      "paper_abstract": "Recently, Graph Transformers have emerged as a promising solution to alleviate the inherent limitations of Graph Neural Networks (GNNs) and enhance graph representation performance. Unfortunately, Graph Transformers are computationally expensive due to the quadratic complexity inherent in self-attention when applied over large-scale graphs, especially for node tasks. In contrast, spiking neural networks (SNNs), with event-driven and binary spikes properties, can perform energy-efficient computation. In this work, we propose a novel insight into integrating SNNs with Graph Transformers and design a Spiking Graph Attention (SGA) module. The matrix multiplication is replaced by sparse addition and mask operations. The linear complexity enables all-pair node interactions on large-scale graphs with limited GPU memory. To our knowledge, our work is the first attempt to introduce SNNs into Graph Transformers. Furthermore, we design SpikeGraphormer, a Dual-branch architecture, combining a sparse GNN branch with our SGA-driven Graph Transformer branch, which can simultaneously perform all-pair node interactions and capture local neighborhoods. SpikeGraphormer consistently outperforms existing state-of-the-art approaches across various datasets and makes substantial improvements in training time, inference time, and GPU memory cost (10 ~ 20x lower than vanilla self-attention). It also performs well in cross-domain applications (image and text classification). We release our code at https://github.com/PHD-lanyu/SpikeGraphormer.",
      "paper_authors": [
        "Yundong Sun",
        "Dongjie Zhu",
        "Yansong Wang",
        "Zhaoshuo Tian",
        "Ning Cao",
        "Gregory O'Hared"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-03-21",
      "update_time": "2024-03-21",
      "comments": null,
      "repo_url": "https://github.com/phd-lanyu/spikegraphormer"
    },
    "2403.12574": {
      "paper_id": "2403.12574v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.12574v1",
      "paper_key": "2403.12574",
      "paper_title": "EAS-SNN: End-to-End Adaptive Sampling and Representation for Event-based Detection with Recurrent Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2403.12574v1",
      "paper_abstract": "Event cameras, with their high dynamic range and temporal resolution, are ideally suited for object detection, especially under scenarios with motion blur and challenging lighting conditions. However, while most existing approaches prioritize optimizing spatiotemporal representations with advanced detection backbones and early aggregation functions, the crucial issue of adaptive event sampling remains largely unaddressed. Spiking Neural Networks (SNNs), which operate on an event-driven paradigm through sparse spike communication, emerge as a natural fit for addressing this challenge. In this study, we discover that the neural dynamics of spiking neurons align closely with the behavior of an ideal temporal event sampler. Motivated by this insight, we propose a novel adaptive sampling module that leverages recurrent convolutional SNNs enhanced with temporal memory, facilitating a fully end-to-end learnable framework for event-based detection. Additionally, we introduce Residual Potential Dropout (RPD) and Spike-Aware Training (SAT) to regulate potential distribution and address performance degradation encountered in spike-based sampling modules. Through rigorous testing on neuromorphic datasets for event-based detection, our approach demonstrably surpasses existing state-of-the-art spike-based methods, achieving superior performance with significantly fewer parameters and time steps. For instance, our method achieves a 4.4\\% mAP improvement on the Gen1 dataset, while requiring 38\\% fewer parameters and three time steps. Moreover, the applicability and effectiveness of our adaptive sampling methodology extend beyond SNNs, as demonstrated through further validation on conventional non-spiking detection models.",
      "paper_authors": [
        "Ziming Wang",
        "Ziling Wang",
        "Huaning Li",
        "Lang Qin",
        "Runhao Jiang",
        "De Ma",
        "Huajin Tang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-03-19",
      "update_time": "2024-03-19",
      "comments": null,
      "repo_url": "#"
    },
    "2403.12462": {
      "paper_id": "2403.12462v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.12462v1",
      "paper_key": "2403.12462",
      "paper_title": "Topological Representations of Heterogeneous Learning Dynamics of Recurrent Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2403.12462v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) have become an essential paradigm in neuroscience and artificial intelligence, providing brain-inspired computation. Recent advances in literature have studied the network representations of deep neural networks. However, there has been little work that studies representations learned by SNNs, especially using unsupervised local learning methods like spike-timing dependent plasticity (STDP). Recent work by \\cite{barannikov2021representation} has introduced a novel method to compare topological mappings of learned representations called Representation Topology Divergence (RTD). Though useful, this method is engineered particularly for feedforward deep neural networks and cannot be used for recurrent networks like Recurrent SNNs (RSNNs). This paper introduces a novel methodology to use RTD to measure the difference between distributed representations of RSNN models with different learning methods. We propose a novel reformulation of RSNNs using feedforward autoencoder networks with skip connections to help us compute the RTD for recurrent networks. Thus, we investigate the learning capabilities of RSNN trained using STDP and the role of heterogeneity in the synaptic dynamics in learning such representations. We demonstrate that heterogeneous STDP in RSNNs yield distinct representations than their homogeneous and surrogate gradient-based supervised learning counterparts. Our results provide insights into the potential of heterogeneous SNN models, aiding the development of more efficient and biologically plausible hybrid artificial intelligence systems.",
      "paper_authors": [
        "Biswadeep Chakraborty",
        "Saibal Mukhopadhyay"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-03-19",
      "update_time": "2024-03-19",
      "comments": "Accepted in IEEE World Congress on Computational Intelligence (IEEE\n  WCCI) 2024",
      "repo_url": "#"
    },
    "2403.11138": {
      "paper_id": "2403.11138v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.11138v3",
      "paper_key": "2403.11138",
      "paper_title": "Spiking Wavelet Transformer",
      "paper_url": "http://arxiv.org/abs/2403.11138v3",
      "paper_abstract": "Spiking neural networks (SNNs) offer an energy-efficient alternative to conventional deep learning by mimicking the event-driven processing of the brain. Incorporating the Transformers with SNNs has shown promise for accuracy, yet it is incompetent to capture high-frequency patterns like moving edge and pixel-level brightness changes due to their reliance on global self-attention operations. Porting frequency representations in SNN is challenging yet crucial for event-driven vision. To address this issue, we propose the Spiking Wavelet Transformer (SWformer), an attention-free architecture that effectively learns comprehensive spatial-frequency features in a spike-driven manner by leveraging the sparse wavelet transform. The critical component is a Frequency-Aware Token Mixer (FATM) with three branches: 1) spiking wavelet learner for spatial-frequency domain learning, 2) convolution-based learner for spatial feature extraction, and 3) spiking pointwise convolution for cross-channel information aggregation. We also adopt negative spike dynamics to strengthen the frequency representation further. This enables the SWformer to outperform vanilla Spiking Transformers in capturing high-frequency visual components, as evidenced by our empirical results. Experiments on both static and neuromorphic datasets demonstrate SWformer's effectiveness in capturing spatial-frequency patterns in a multiplication-free, event-driven fashion, outperforming state-of-the-art SNNs. SWformer achieves an over 50% reduction in energy consumption, a 21.1% reduction in parameter count, and a 2.40% performance improvement on the ImageNet dataset compared to vanilla Spiking Transformers.",
      "paper_authors": [
        "Yuetong Fang",
        "Ziqing Wang",
        "Lingfeng Zhang",
        "Jiahang Cao",
        "Honglei Chen",
        "Renjing Xu"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-03-17",
      "update_time": "2024-03-26",
      "comments": null,
      "repo_url": "#"
    },
    "2403.10677": {
      "paper_id": "2403.10677v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.10677v1",
      "paper_key": "2403.10677",
      "paper_title": "Spiking Neural Networks for Fast-Moving Object Detection on Neuromorphic Hardware Devices Using an Event-Based Camera",
      "paper_url": "http://arxiv.org/abs/2403.10677v1",
      "paper_abstract": "Table tennis is a fast-paced and exhilarating sport that demands agility, precision, and fast reflexes. In recent years, robotic table tennis has become a popular research challenge for robot perception algorithms. Fast and accurate ball detection is crucial for enabling a robotic arm to rally the ball back successfully. Previous approaches have employed conventional frame-based cameras with Convolutional Neural Networks (CNNs) or traditional computer vision methods. In this paper, we propose a novel solution that combines an event-based camera with Spiking Neural Networks (SNNs) for ball detection. We use multiple state-of-the-art SNN frameworks and develop a SNN architecture for each of them, complying with their corresponding constraints. Additionally, we implement the SNN solution across multiple neuromorphic edge devices, conducting comparisons of their accuracies and run-times. This furnishes robotics researchers with a benchmark illustrating the capabilities achievable with each SNN framework and a corresponding neuromorphic edge device. Next to this comparison of SNN solutions for robots, we also show that an SNN on a neuromorphic edge device is able to run in real-time in a closed loop robotic system, a table tennis robot in our use case.",
      "paper_authors": [
        "Andreas Ziegler",
        "Karl Vetter",
        "Thomas Gossard",
        "Jonas Tebbe",
        "Andreas Zell"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-03-15",
      "update_time": "2024-03-15",
      "comments": null,
      "repo_url": "#"
    },
    "2403.10173": {
      "paper_id": "2403.10173v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.10173v1",
      "paper_key": "2403.10173",
      "paper_title": "A Hybrid SNN-ANN Network for Event-based Object Detection with Spatial and Temporal Attention",
      "paper_url": "http://arxiv.org/abs/2403.10173v1",
      "paper_abstract": "Event cameras offer high temporal resolution and dynamic range with minimal motion blur, making them promising for object detection tasks. While Spiking Neural Networks (SNNs) are a natural match for event-based sensory data and enable ultra-energy efficient and low latency inference on neuromorphic hardware, Artificial Neural Networks (ANNs) tend to display more stable training dynamics and faster convergence resulting in greater task performance. Hybrid SNN-ANN approaches are a promising alternative, enabling to leverage the strengths of both SNN and ANN architectures. In this work, we introduce the first Hybrid Attention-based SNN-ANN backbone for object detection using event cameras. We propose a novel Attention-based SNN-ANN bridge module to capture sparse spatial and temporal relations from the SNN layer and convert them into dense feature maps for the ANN part of the backbone. Experimental results demonstrate that our proposed method surpasses baseline hybrid and SNN-based approaches by significant margins, with results comparable to existing ANN-based methods. Extensive ablation studies confirm the effectiveness of our proposed modules and architectural choices. These results pave the way toward a hybrid SNN-ANN architecture that achieves ANN like performance at a drastically reduced parameter budget. We implemented the SNN blocks on digital neuromorphic hardware to investigate latency and power consumption and demonstrate the feasibility of our approach.",
      "paper_authors": [
        "Soikat Hasan Ahmed",
        "Jan Finkbeiner",
        "Emre Neftci"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-03-15",
      "update_time": "2024-03-15",
      "comments": null,
      "repo_url": "#"
    },
    "2403.09274": {
      "paper_id": "2403.09274v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.09274v1",
      "paper_key": "2403.09274",
      "paper_title": "EventRPG: Event Data Augmentation with Relevance Propagation Guidance",
      "paper_url": "http://arxiv.org/abs/2403.09274v1",
      "paper_abstract": "Event camera, a novel bio-inspired vision sensor, has drawn a lot of attention for its low latency, low power consumption, and high dynamic range. Currently, overfitting remains a critical problem in event-based classification tasks for Spiking Neural Network (SNN) due to its relatively weak spatial representation capability. Data augmentation is a simple but efficient method to alleviate overfitting and improve the generalization ability of neural networks, and saliency-based augmentation methods are proven to be effective in the image processing field. However, there is no approach available for extracting saliency maps from SNNs. Therefore, for the first time, we present Spiking Layer-Time-wise Relevance Propagation rule (SLTRP) and Spiking Layer-wise Relevance Propagation rule (SLRP) in order for SNN to generate stable and accurate CAMs and saliency maps. Based on this, we propose EventRPG, which leverages relevance propagation on the spiking neural network for more efficient augmentation. Our proposed method has been evaluated on several SNN structures, achieving state-of-the-art performance in object recognition tasks including N-Caltech101, CIFAR10-DVS, with accuracies of 85.62% and 85.55%, as well as action recognition task SL-Animals with an accuracy of 91.59%. Our code is available at https://github.com/myuansun/EventRPG.",
      "paper_authors": [
        "Mingyuan Sun",
        "Donghao Zhang",
        "Zongyuan Ge",
        "Jiaxu Wang",
        "Jia Li",
        "Zheng Fang",
        "Renjing Xu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-03-14",
      "update_time": "2024-03-14",
      "comments": "Accepted by ICLR 2024",
      "repo_url": "https://github.com/myuansun/eventrpg"
    },
    "2403.08928": {
      "paper_id": "2403.08928v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.08928v1",
      "paper_key": "2403.08928",
      "paper_title": "Neuromorphic force-control in an industrial task: validating energy and latency benefits",
      "paper_url": "http://arxiv.org/abs/2403.08928v1",
      "paper_abstract": "As robots become smarter and more ubiquitous, optimizing the power consumption of intelligent compute becomes imperative towards ensuring the sustainability of technological advancements. Neuromorphic computing hardware makes use of biologically inspired neural architectures to achieve energy and latency improvements compared to conventional von Neumann computing architecture. Applying these benefits to robots has been demonstrated in several works in the field of neurorobotics, typically on relatively simple control tasks. Here, we introduce an example of neuromorphic computing applied to the real-world industrial task of object insertion. We trained a spiking neural network (SNN) to perform force-torque feedback control using a reinforcement learning approach in simulation. We then ported the SNN to the Intel neuromorphic research chip Loihi interfaced with a KUKA robotic arm. At inference time we show latency competitive with current CPU/GPU architectures, two orders of magnitude less energy usage in comparison to traditional low-energy edge-hardware. We offer this example as a proof of concept implementation of a neuromoprhic controller in real-world robotic setting, highlighting the benefits of neuromorphic hardware for the development of intelligent controllers for robots.",
      "paper_authors": [
        "Camilo Amaya",
        "Evan Eames",
        "Gintautas Palinauskas",
        "Alexander Perzylo",
        "Yulia Sandamirskaya",
        "Axel von Arnim"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-03-13",
      "update_time": "2024-03-13",
      "comments": "Submitted to IROS 2024",
      "repo_url": "#"
    },
    "2403.09723": {
      "paper_id": "2403.09723v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.09723v1",
      "paper_key": "2403.09723",
      "paper_title": "Texture Recognition Using a Biologically Plausible Spiking Phase-Locked Loop Model for Spike Train Frequency Decomposition",
      "paper_url": "http://arxiv.org/abs/2403.09723v1",
      "paper_abstract": "In this paper, we present a novel spiking neural network model designed to perform frequency decomposition of spike trains. Our model emulates neural microcircuits theorized in the somatosensory cortex, rendering it a biologically plausible candidate for decoding the spike trains observed in tactile peripheral nerves. We demonstrate the capacity of simple neurons and synapses to replicate the phase-locked loop (PLL) and explore the emergent properties when considering multiple spiking phase-locked loops (sPLLs) with diverse oscillations. We illustrate how these sPLLs can decode textures using the spectral features elicited in peripheral nerves. Leveraging our model's frequency decomposition abilities, we improve state-of-the-art performances on a Multifrequency Spike Train (MST) dataset. This work offers valuable insights into neural processing and presents a practical framework for enhancing artificial neural network capabilities in complex pattern recognition tasks.",
      "paper_authors": [
        "Michele Mastella",
        "Tesse Tiemens",
        "Elisabetta Chicca"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2024-03-12",
      "update_time": "2024-03-12",
      "comments": "16 pages, 4 figures, journal",
      "repo_url": "https://github.com/bics-rug/spll"
    },
    "2404.07941": {
      "paper_id": "2404.07941v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.07941v1",
      "paper_key": "2404.07941",
      "paper_title": "SiGNN: A Spike-induced Graph Neural Network for Dynamic Graph Representation Learning",
      "paper_url": "http://arxiv.org/abs/2404.07941v1",
      "paper_abstract": "In the domain of dynamic graph representation learning (DGRL), the efficient and comprehensive capture of temporal evolution within real-world networks is crucial. Spiking Neural Networks (SNNs), known as their temporal dynamics and low-power characteristic, offer an efficient solution for temporal processing in DGRL task. However, owing to the spike-based information encoding mechanism of SNNs, existing DGRL methods employed SNNs face limitations in their representational capacity. Given this issue, we propose a novel framework named Spike-induced Graph Neural Network (SiGNN) for learning enhanced spatialtemporal representations on dynamic graphs. In detail, a harmonious integration of SNNs and GNNs is achieved through an innovative Temporal Activation (TA) mechanism. Benefiting from the TA mechanism, SiGNN not only effectively exploits the temporal dynamics of SNNs but also adeptly circumvents the representational constraints imposed by the binary nature of spikes. Furthermore, leveraging the inherent adaptability of SNNs, we explore an in-depth analysis of the evolutionary patterns within dynamic graphs across multiple time granularities. This approach facilitates the acquisition of a multiscale temporal node representation.Extensive experiments on various real-world dynamic graph datasets demonstrate the superior performance of SiGNN in the node classification task.",
      "paper_authors": [
        "Dong Chen",
        "Shuai Zheng",
        "Muhao Xu",
        "Zhenfeng Zhu",
        "Yao Zhao"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-03-11",
      "update_time": "2024-03-11",
      "comments": null,
      "repo_url": "#"
    },
    "2403.06233": {
      "paper_id": "2403.06233v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.06233v1",
      "paper_key": "2403.06233",
      "paper_title": "Finding Visual Saliency in Continuous Spike Stream",
      "paper_url": "http://arxiv.org/abs/2403.06233v1",
      "paper_abstract": "As a bio-inspired vision sensor, the spike camera emulates the operational principles of the fovea, a compact retinal region, by employing spike discharges to encode the accumulation of per-pixel luminance intensity. Leveraging its high temporal resolution and bio-inspired neuromorphic design, the spike camera holds significant promise for advancing computer vision applications. Saliency detection mimics the behavior of human beings and captures the most salient region from the scenes. In this paper, we investigate the visual saliency in the continuous spike stream for the first time. To effectively process the binary spike stream, we propose a Recurrent Spiking Transformer (RST) framework, which is based on a full spiking neural network. Our framework enables the extraction of spatio-temporal features from the continuous spatio-temporal spike stream while maintaining low power consumption. To facilitate the training and validation of our proposed model, we build a comprehensive real-world spike-based visual saliency dataset, enriched with numerous light conditions. Extensive experiments demonstrate the superior performance of our Recurrent Spiking Transformer framework in comparison to other spike neural network-based methods. Our framework exhibits a substantial margin of improvement in capturing and highlighting visual saliency in the spike stream, which not only provides a new perspective for spike-based saliency segmentation but also shows a new paradigm for full SNN-based transformer models. The code and dataset are available at \\url{https://github.com/BIT-Vision/SVS}.",
      "paper_authors": [
        "Lin Zhu",
        "Xianzhang Chen",
        "Xiao Wang",
        "Hua Huang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-03-10",
      "update_time": "2024-03-10",
      "comments": "Accepted by AAAI 2024",
      "repo_url": "https://github.com/bit-vision/svs"
    },
    "2403.05772": {
      "paper_id": "2403.05772v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.05772v1",
      "paper_key": "2403.05772",
      "paper_title": "sVAD: A Robust, Low-Power, and Light-Weight Voice Activity Detection with Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2403.05772v1",
      "paper_abstract": "Speech applications are expected to be low-power and robust under noisy conditions. An effective Voice Activity Detection (VAD) front-end lowers the computational need. Spiking Neural Networks (SNNs) are known to be biologically plausible and power-efficient. However, SNN-based VADs have yet to achieve noise robustness and often require large models for high performance. This paper introduces a novel SNN-based VAD model, referred to as sVAD, which features an auditory encoder with an SNN-based attention mechanism. Particularly, it provides effective auditory feature representation through SincNet and 1D convolution, and improves noise robustness with attention mechanisms. The classifier utilizes Spiking Recurrent Neural Networks (sRNN) to exploit temporal speech information. Experimental results demonstrate that our sVAD achieves remarkable noise robustness and meanwhile maintains low power consumption and a small footprint, making it a promising solution for real-world VAD applications.",
      "paper_authors": [
        "Qu Yang",
        "Qianhui Liu",
        "Nan Li",
        "Meng Ge",
        "Zeyang Song",
        "Haizhou Li"
      ],
      "primary_category": "cs.SD",
      "publish_time": "2024-03-09",
      "update_time": "2024-03-09",
      "comments": "Accepted by ICASSP 2024",
      "repo_url": "#"
    },
    "2403.04162": {
      "paper_id": "2403.04162v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.04162v1",
      "paper_key": "2403.04162",
      "paper_title": "Noisy Spiking Actor Network for Exploration",
      "paper_url": "http://arxiv.org/abs/2403.04162v1",
      "paper_abstract": "As a general method for exploration in deep reinforcement learning (RL), NoisyNet can produce problem-specific exploration strategies. Spiking neural networks (SNNs), due to their binary firing mechanism, have strong robustness to noise, making it difficult to realize efficient exploration with local disturbances. To solve this exploration problem, we propose a noisy spiking actor network (NoisySAN) that introduces time-correlated noise during charging and transmission. Moreover, a noise reduction method is proposed to find a stable policy for the agent. Extensive experimental results demonstrate that our method outperforms the state-of-the-art performance on a wide range of continuous control tasks from OpenAI gym.",
      "paper_authors": [
        "Ding Chen",
        "Peixi Peng",
        "Tiejun Huang",
        "Yonghong Tian"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-03-07",
      "update_time": "2024-03-07",
      "comments": "13 pages, 6 figures",
      "repo_url": "#"
    },
    "2403.03775": {
      "paper_id": "2403.03775v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.03775v1",
      "paper_key": "2403.03775",
      "paper_title": "Photonic-electronic spiking neuron with multi-modal and multi-wavelength excitatory and inhibitory operation for high-speed neuromorphic sensing and computing",
      "paper_url": "http://arxiv.org/abs/2403.03775v1",
      "paper_abstract": "We report a multi-modal spiking neuron that allows optical and electronic input and control, and wavelength-multiplexing operation, for use in novel high-speed neuromorphic sensing and computing functionalities. The photonic-electronic neuron is built with a micro-scale, nanostructure resonant tunnelling diode (RTD) with photodetection (PD) capability. Leveraging the advantageous intrinsic properties of this RTD-PD system, namely highly nonlinear characteristics, photo-sensitivity, light-induced I-V curve shift, and the ability to deliver excitable responses under electrical and optical inputs, we successfully achieve flexible neuromorphic spike activation and inhibition regimes through photonic-electrical control. We also demonstrate the ability of this RTD-PD spiking sensing-processing neuron to operate under the simultaneous arrival of multiple wavelength-multiplexed optical signals, due to its large photodetection spectral window (covering the 1310 and 1550 nm telecom wavelength bands). Our results highlight the potential of RTD photonic-electronic neurons to reproduce multiple key excitatory and inhibitory spiking regimes, at high speed (ns-rate spiking responses, with faster sub-ns regimes theoretically predicted) and low energy (requiring only ~10 mV and ~150 microW, electrical and optical input amplitudes, respectively), similar in nature to those commonly found in the biological neurons of the visual system and the brain. This work offers a highly promising approach for the realisation of high-speed, energy-efficient photonic-electronic spiking neurons and spiking neural networks, enabling multi-modal and multi-wavelength operation for sensing and information processing tasks. This work therefore paves the way for innovative high-speed, photonic-electronic, and spike-based neuromorphic sensing and computing systems and artificial intelligence hardware.",
      "paper_authors": [
        "Weikang Zhang",
        "Mat\u011bj Hejda",
        "Qusay Raghib Ali Al-Taai",
        "Dafydd Owen-Newns",
        "Bruno Romeira",
        "Jos\u00e9 M. L. Figueiredo",
        "Joshua Robertson",
        "Edward Wasige",
        "Antonio Hurtado"
      ],
      "primary_category": "physics.optics",
      "publish_time": "2024-03-06",
      "update_time": "2024-03-06",
      "comments": "12 pages, 9 figures",
      "repo_url": "#"
    },
    "2403.03409": {
      "paper_id": "2403.03409v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.03409v1",
      "paper_key": "2403.03409",
      "paper_title": "Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales for Pruning Recurrent SNN",
      "paper_url": "http://arxiv.org/abs/2403.03409v1",
      "paper_abstract": "Recurrent Spiking Neural Networks (RSNNs) have emerged as a computationally efficient and brain-inspired learning model. The design of sparse RSNNs with fewer neurons and synapses helps reduce the computational complexity of RSNNs. Traditionally, sparse SNNs are obtained by first training a dense and complex SNN for a target task, and, then, pruning neurons with low activity (activity-based pruning) while maintaining task performance. In contrast, this paper presents a task-agnostic methodology for designing sparse RSNNs by pruning a large randomly initialized model. We introduce a novel Lyapunov Noise Pruning (LNP) algorithm that uses graph sparsification methods and utilizes Lyapunov exponents to design a stable sparse RSNN from a randomly initialized RSNN. We show that the LNP can leverage diversity in neuronal timescales to design a sparse Heterogeneous RSNN (HRSNN). Further, we show that the same sparse HRSNN model can be trained for different tasks, such as image classification and temporal prediction. We experimentally show that, in spite of being task-agnostic, LNP increases computational efficiency (fewer neurons and synapses) and prediction performance of RSNNs compared to traditional activity-based pruning of trained dense models.",
      "paper_authors": [
        "Biswadeep Chakraborty",
        "Beomseok Kang",
        "Harshit Kumar",
        "Saibal Mukhopadhyay"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-03-06",
      "update_time": "2024-03-06",
      "comments": "Published as a conference paper at ICLR 2024",
      "repo_url": "#"
    },
    "2403.06998": {
      "paper_id": "2403.06998v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.06998v2",
      "paper_key": "2403.06998",
      "paper_title": "High-speed Low-consumption sEMG-based Transient-state micro-Gesture Recognition",
      "paper_url": "http://arxiv.org/abs/2403.06998v2",
      "paper_abstract": "Gesture recognition on wearable devices is extensively applied in human-computer interaction. Electromyography (EMG) has been used in many gesture recognition systems for its rapid perception of muscle signals. However, analyzing EMG signals on devices, like smart wristbands, usually needs inference models to have high performances, such as low inference latency, low power consumption, and low memory occupation. Therefore, this paper proposes an improved spiking neural network (SNN) to achieve these goals. We propose an adaptive multi-delta coding as a spiking coding method to improve recognition accuracy. We propose two additive solvers for SNN, which can reduce inference energy consumption and amount of parameters significantly, and improve the robustness of temporal differences. In addition, we propose a linear action detection method TAD-LIF, which is suitable for SNNs. TAD-LIF is an improved LIF neuron that can detect transient-state gestures quickly and accurately. We collected two datasets from 20 subjects including 6 micro gestures. The collection devices are two designed lightweight consumer-level sEMG wristbands (3 and 8 electrode channels respectively). Compared to CNN, FCN, and normal SNN-based methods, the proposed SNN has higher recognition accuracy. The accuracy of the proposed SNN is 83.85% and 93.52% on the two datasets respectively. In addition, the inference latency of the proposed SNN is about 1% of CNN, the power consumption is about 0.1% of CNN, and the memory occupation is about 20% of CNN. The proposed methods can be used for precise, high-speed, and low-power micro-gesture recognition tasks, and are suitable for consumer-level intelligent wearable devices, which is a general way to achieve ubiquitous computing.",
      "paper_authors": [
        "Youfang Han",
        "Wei Zhao",
        "Xiangjin Chen",
        "Xin Meng"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2024-03-04",
      "update_time": "2024-03-13",
      "comments": null,
      "repo_url": "#"
    },
    "2403.00450": {
      "paper_id": "2403.00450v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.00450v1",
      "paper_key": "2403.00450",
      "paper_title": "Parallel Hyperparameter Optimization Of Spiking Neural Network",
      "paper_url": "http://arxiv.org/abs/2403.00450v1",
      "paper_abstract": "Spiking Neural Networks (SNN). SNNs are based on a more biologically inspired approach than usual artificial neural networks. Such models are characterized by complex dynamics between neurons and spikes. These are very sensitive to the hyperparameters, making their optimization challenging. To tackle hyperparameter optimization of SNNs, we initially extended the signal loss issue of SNNs to what we call silent networks. These networks fail to emit enough spikes at their outputs due to mistuned hyperparameters or architecture. Generally, search spaces are heavily restrained, sometimes even discretized, to prevent the sampling of such networks. By defining an early stopping criterion detecting silent networks and by designing specific constraints, we were able to instantiate larger and more flexible search spaces. We applied a constrained Bayesian optimization technique, which was asynchronously parallelized, as the evaluation time of a SNN is highly stochastic. Large-scale experiments were carried-out on a multi-GPU Petascale architecture. By leveraging silent networks, results show an acceleration of the search, while maintaining good performances of both the optimization algorithm and the best solution obtained. We were able to apply our methodology to two popular training algorithms, known as spike timing dependent plasticity and surrogate gradient. Early detection allowed us to prevent worthless and costly computation, directing the search toward promising hyperparameter combinations. Our methodology could be applied to multi-objective problems, where the spiking activity is often minimized to reduce the energy consumption. In this scenario, it becomes essential to find the delicate frontier between low-spiking and silent networks. Finally, our approach may have implications for neural architecture search, particularly in defining suitable spiking architectures.",
      "paper_authors": [
        "Thomas Firmin",
        "Pierre Boulet",
        "El-Ghazali Talbi"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-03-01",
      "update_time": "2024-03-01",
      "comments": null,
      "repo_url": "https://github.com/thomasfirmin/hpo_snn"
    },
    "2403.00270": {
      "paper_id": "2403.00270v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.00270v1",
      "paper_key": "2403.00270",
      "paper_title": "Event-Driven Learning for Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2403.00270v1",
      "paper_abstract": "Brain-inspired spiking neural networks (SNNs) have gained prominence in the field of neuromorphic computing owing to their low energy consumption during feedforward inference on neuromorphic hardware. However, it remains an open challenge how to effectively benefit from the sparse event-driven property of SNNs to minimize backpropagation learning costs. In this paper, we conduct a comprehensive examination of the existing event-driven learning algorithms, reveal their limitations, and propose novel solutions to overcome them. Specifically, we introduce two novel event-driven learning methods: the spike-timing-dependent event-driven (STD-ED) and membrane-potential-dependent event-driven (MPD-ED) algorithms. These proposed algorithms leverage precise neuronal spike timing and membrane potential, respectively, for effective learning. The two methods are extensively evaluated on static and neuromorphic datasets to confirm their superior performance. They outperform existing event-driven counterparts by up to 2.51% for STD-ED and 6.79% for MPD-ED on the CIFAR-100 dataset. In addition, we theoretically and experimentally validate the energy efficiency of our methods on neuromorphic hardware. On-chip learning experiments achieved a remarkable 30-fold reduction in energy consumption over time-step-based surrogate gradient methods. The demonstrated efficiency and efficacy of the proposed event-driven learning methods emphasize their potential to significantly advance the fields of neuromorphic computing, offering promising avenues for energy-efficiency applications.",
      "paper_authors": [
        "Wenjie Wei",
        "Malu Zhang",
        "Jilin Zhang",
        "Ammar Belatreche",
        "Jibin Wu",
        "Zijing Xu",
        "Xuerui Qiu",
        "Hong Chen",
        "Yang Yang",
        "Haizhou Li"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-03-01",
      "update_time": "2024-03-01",
      "comments": null,
      "repo_url": "#"
    },
    "2402.19139": {
      "paper_id": "2402.19139v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.19139v1",
      "paper_key": "2402.19139",
      "paper_title": "A Unified Evaluation Framework for Spiking Neural Network Hardware Accelerators Based on Emerging Non-Volatile Memory Devices",
      "paper_url": "http://arxiv.org/abs/2402.19139v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) have emerged as a promising paradigm, offering event-driven and energy-efficient computation. In recent studies, various devices tailored for SNN synapses and neurons have been proposed, leveraging the unique characteristics of emerging non-volatile memory (eNVM) technologies. While substantial progress has been made in exploring the capabilities of SNNs and designing dedicated hardware components, there exists a critical gap in establishing a unified approach for evaluating hardware-level metrics. Specifically, metrics such as latency, and energy consumption, are pivotal in assessing the practical viability and efficiency of the constructed neural network. In this article, we address this gap by presenting a comprehensive framework for evaluating hardware-level metrics in SNNs based on non-volatile memory devices. We systematically analyze the impact of synaptic and neuronal components on energy consumption providing a unified perspective for assessing the overall efficiency of the network. In this study, our emphasis lies on the neuron and synaptic device based on magnetic skyrmions. Nevertheless, our framework is versatile enough to encompass other emerging devices as well. Utilizing our proposed skyrmionic devices, the constructed SNN demonstrates an inference accuracy of approximately 98% and achieves energy consumption on the order of pJ when processing the Modified National Institute of Standards and Technology (MNIST) handwritten digit dataset.",
      "paper_authors": [
        "Debasis Das",
        "Xuanyao Fong"
      ],
      "primary_category": "cond-mat.other",
      "publish_time": "2024-02-29",
      "update_time": "2024-02-29",
      "comments": null,
      "repo_url": "#"
    },
    "2402.19061": {
      "paper_id": "2402.19061v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.19061v1",
      "paper_key": "2402.19061",
      "paper_title": "Optimal ANN-SNN Conversion with Group Neurons",
      "paper_url": "http://arxiv.org/abs/2402.19061v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) have emerged as a promising third generation of neural networks, offering unique characteristics such as binary outputs, high sparsity, and biological plausibility. However, the lack of effective learning algorithms remains a challenge for SNNs. For instance, while converting artificial neural networks (ANNs) to SNNs circumvents the need for direct training of SNNs, it encounters issues related to conversion errors and high inference time delays. In order to reduce or even eliminate conversion errors while decreasing inference time-steps, we have introduced a novel type of neuron called Group Neurons (GNs). One GN is composed of multiple Integrate-and-Fire (IF) neurons as members, and its neural dynamics are meticulously designed. Based on GNs, we have optimized the traditional ANN-SNN conversion framework. Specifically, we replace the IF neurons in the SNNs obtained by the traditional conversion framework with GNs. The resulting SNNs, which utilize GNs, are capable of achieving accuracy levels comparable to ANNs even within extremely short inference time-steps. The experiments on CIFAR10, CIFAR100, and ImageNet datasets demonstrate the superiority of the proposed methods in terms of both inference accuracy and latency. Code is available at https://github.com/Lyu6PosHao/ANN2SNN_GN.",
      "paper_authors": [
        "Liuzhenghao Lv",
        "Wei Fang",
        "Li Yuan",
        "Yonghong Tian"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-02-29",
      "update_time": "2024-02-29",
      "comments": "Accepted by International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP) 2024",
      "repo_url": "https://github.com/lyu6poshao/ann2snn_gn"
    },
    "2402.18994": {
      "paper_id": "2402.18994v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.18994v1",
      "paper_key": "2402.18994",
      "paper_title": "Spyx: A Library for Just-In-Time Compiled Optimization of Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2402.18994v1",
      "paper_abstract": "As the role of artificial intelligence becomes increasingly pivotal in modern society, the efficient training and deployment of deep neural networks have emerged as critical areas of focus. Recent advancements in attention-based large neural architectures have spurred the development of AI accelerators, facilitating the training of extensive, multi-billion parameter models. Despite their effectiveness, these powerful networks often incur high execution costs in production environments. Neuromorphic computing, inspired by biological neural processes, offers a promising alternative. By utilizing temporally-sparse computations, Spiking Neural Networks (SNNs) offer to enhance energy efficiency through a reduced and low-power hardware footprint. However, the training of SNNs can be challenging due to their recurrent nature which cannot as easily leverage the massive parallelism of modern AI accelerators. To facilitate the investigation of SNN architectures and dynamics researchers have sought to bridge Python-based deep learning frameworks such as PyTorch or TensorFlow with custom-implemented compute kernels. This paper introduces Spyx, a new and lightweight SNN simulation and optimization library designed in JAX. By pre-staging data in the expansive vRAM of contemporary accelerators and employing extensive JIT compilation, Spyx allows for SNN optimization to be executed as a unified, low-level program on NVIDIA GPUs or Google TPUs. This approach achieves optimal hardware utilization, surpassing the performance of many existing SNN training frameworks while maintaining considerable flexibility.",
      "paper_authors": [
        "Kade M. Heckel",
        "Thomas Nowotny"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-02-29",
      "update_time": "2024-02-29",
      "comments": null,
      "repo_url": "https://github.com/kmheckel/spyx"
    },
    "2402.18390": {
      "paper_id": "2402.18390v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.18390v1",
      "paper_key": "2402.18390",
      "paper_title": "Neuromorphic Event-Driven Semantic Communication in Microgrids",
      "paper_url": "http://arxiv.org/abs/2402.18390v1",
      "paper_abstract": "Synergies between advanced communications, computing and artificial intelligence are unraveling new directions of coordinated operation and resiliency in microgrids. On one hand, coordination among sources is facilitated by distributed, privacy-minded processing at multiple locations, whereas on the other hand, it also creates exogenous data arrival paths for adversaries that can lead to cyber-physical attacks amongst other reliability issues in the communication layer. This long-standing problem necessitates new intrinsic ways of exchanging information between converters through power lines to optimize the system's control performance. Going beyond the existing power and data co-transfer technologies that are limited by efficiency and scalability concerns, this paper proposes neuromorphic learning to implant communicative features using spiking neural networks (SNNs) at each node, which is trained collaboratively in an online manner simply using the power exchanges between the nodes. As opposed to the conventional neuromorphic sensors that operate with spiking signals, we employ an event-driven selective process to collect sparse data for training of SNNs. Finally, its multi-fold effectiveness and reliable performance is validated under simulation conditions with different microgrid topologies and components to establish a new direction in the sense-actuate-compute cycle for power electronic dominated grids and microgrids.",
      "paper_authors": [
        "Xiaoguang Diao",
        "Yubo Song",
        "Subham Sahoo",
        "Yuan Li"
      ],
      "primary_category": "cs.ET",
      "publish_time": "2024-02-28",
      "update_time": "2024-02-28",
      "comments": "The manuscript has been accepted for publication in IEEE Transactions\n  on Smart Grid",
      "repo_url": "#"
    },
    "2402.16384": {
      "paper_id": "2402.16384v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.16384v2",
      "paper_key": "2402.16384",
      "paper_title": "Scalable Superconductor Neuron with Ternary Synaptic Connections for Ultra-Fast SNN Hardware",
      "paper_url": "http://arxiv.org/abs/2402.16384v2",
      "paper_abstract": "A novel high-fan-in differential superconductor neuron structure designed for ultra-high-performance Spiking Neural Network (SNN) accelerators is presented. Utilizing a high-fan-in neuron structure allows us to design SNN accelerators with more synaptic connections, enhancing the overall network capabilities. The proposed neuron design is based on superconductor electronics fabric, incorporating multiple superconducting loops, each with two Josephson Junctions. This arrangement enables each input data branch to have positive and negative inductive coupling, supporting excitatory and inhibitory synaptic data. Compatibility with synaptic devices and thresholding operation is achieved using a single flux quantum (SFQ) pulse-based logic style. The neuron design, along with ternary synaptic connections, forms the foundation for a superconductor-based SNN inference. To demonstrate the capabilities of our design, we train the SNN using snnTorch, augmenting the PyTorch framework. After pruning, the demonstrated SNN inference achieves an impressive 96.1% accuracy on MNIST images. Notably, the network exhibits a remarkable throughput of 8.92 GHz while consuming only 1.5 nJ per inference, including the energy consumption associated with cooling to 4K. These results underscore the potential of superconductor electronics in developing high-performance and ultra-energy-efficient neural network accelerator architectures.",
      "paper_authors": [
        "Mustafa Altay Karamuftuoglu",
        "Beyza Zeynep Ucpinar",
        "Arash Fayyazi",
        "Sasan Razmkhah",
        "Mehdi Kamal",
        "Massoud Pedram"
      ],
      "primary_category": "cond-mat.supr-con",
      "publish_time": "2024-02-26",
      "update_time": "2024-02-27",
      "comments": "9 pages, 5 figures, 2 tables",
      "repo_url": "#"
    },
    "2402.15969": {
      "paper_id": "2402.15969v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.15969v1",
      "paper_key": "2402.15969",
      "paper_title": "Efficient Online Learning for Networks of Two-Compartment Spiking Neurons",
      "paper_url": "http://arxiv.org/abs/2402.15969v1",
      "paper_abstract": "The brain-inspired Spiking Neural Networks (SNNs) have garnered considerable research interest due to their superior performance and energy efficiency in processing temporal signals. Recently, a novel multi-compartment spiking neuron model, namely the Two-Compartment LIF (TC-LIF) model, has been proposed and exhibited a remarkable capacity for sequential modelling. However, training the TC-LIF model presents challenges stemming from the large memory consumption and the issue of gradient vanishing associated with the Backpropagation Through Time (BPTT) algorithm. To address these challenges, online learning methodologies emerge as a promising solution. Yet, to date, the application of online learning methods in SNNs has been predominantly confined to simplified Leaky Integrate-and-Fire (LIF) neuron models. In this paper, we present a novel online learning method specifically tailored for networks of TC-LIF neurons. Additionally, we propose a refined TC-LIF neuron model called Adaptive TC-LIF, which is carefully designed to enhance temporal information integration in online learning scenarios. Extensive experiments, conducted on various sequential benchmarks, demonstrate that our approach successfully preserves the superior sequential modeling capabilities of the TC-LIF neuron while incorporating the training efficiency and hardware friendliness of online learning. As a result, it offers a multitude of opportunities to leverage neuromorphic solutions for processing temporal signals.",
      "paper_authors": [
        "Yujia Yin",
        "Xinyi Chen",
        "Chenxiang Ma",
        "Jibin Wu",
        "Kay Chen Tan"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-02-25",
      "update_time": "2024-02-25",
      "comments": null,
      "repo_url": "#"
    },
    "2403.00790": {
      "paper_id": "2403.00790v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.00790v1",
      "paper_key": "2403.00790",
      "paper_title": "Structuring Concept Space with the Musical Circle of Fifths by Utilizing Music Grammar Based Activations",
      "paper_url": "http://arxiv.org/abs/2403.00790v1",
      "paper_abstract": "In this paper, we explore the intriguing similarities between the structure of a discrete neural network, such as a spiking network, and the composition of a piano piece. While both involve nodes or notes that are activated sequentially or in parallel, the latter benefits from the rich body of music theory to guide meaningful combinations. We propose a novel approach that leverages musical grammar to regulate activations in a spiking neural network, allowing for the representation of symbols as attractors. By applying rules for chord progressions from music theory, we demonstrate how certain activations naturally follow others, akin to the concept of attraction. Furthermore, we introduce the concept of modulating keys to navigate different basins of attraction within the network. Ultimately, we show that the map of concepts in our model is structured by the musical circle of fifths, highlighting the potential for leveraging music theory principles in deep learning algorithms.",
      "paper_authors": [
        "Tofara Moyo"
      ],
      "primary_category": "cs.SD",
      "publish_time": "2024-02-22",
      "update_time": "2024-02-22",
      "comments": "3 pages",
      "repo_url": "#"
    },
    "2402.11984": {
      "paper_id": "2402.11984v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.11984v1",
      "paper_key": "2402.11984",
      "paper_title": "Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2402.11984v1",
      "paper_abstract": "Neuromorphic computing with spiking neural networks is promising for energy-efficient artificial intelligence (AI) applications. However, different from humans who continually learn different tasks in a lifetime, neural network models suffer from catastrophic forgetting. How could neuronal operations solve this problem is an important question for AI and neuroscience. Many previous studies draw inspiration from observed neuroscience phenomena and propose episodic replay or synaptic metaplasticity, but they are not guaranteed to explicitly preserve knowledge for neuron populations. Other works focus on machine learning methods with more mathematical grounding, e.g., orthogonal projection on high dimensional spaces, but there is no neural correspondence for neuromorphic computing. In this work, we develop a new method with neuronal operations based on lateral connections and Hebbian learning, which can protect knowledge by projecting activity traces of neurons into an orthogonal subspace so that synaptic weight update will not interfere with old tasks. We show that Hebbian and anti-Hebbian learning on recurrent lateral connections can effectively extract the principal subspace of neural activities and enable orthogonal projection. This provides new insights into how neural circuits and Hebbian learning can help continual learning, and also how the concept of orthogonal projection can be realized in neuronal systems. Our method is also flexible to utilize arbitrary training methods based on presynaptic activities/traces. Experiments show that our method consistently solves forgetting for spiking neural networks with nearly zero forgetting under various supervised training methods with different error propagation approaches, and outperforms previous approaches under various settings. Our method can pave a solid path for building continual neuromorphic computing systems.",
      "paper_authors": [
        "Mingqing Xiao",
        "Qingyan Meng",
        "Zongpeng Zhang",
        "Di He",
        "Zhouchen Lin"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-02-19",
      "update_time": "2024-02-19",
      "comments": "Accepted by ICLR 2024",
      "repo_url": "https://github.com/pkuxmq/hlop-snn"
    },
    "2402.11748": {
      "paper_id": "2402.11748v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.11748v2",
      "paper_key": "2402.11748",
      "paper_title": "Low-power SNN-based audio source localisation using a Hilbert Transform spike encoding scheme",
      "paper_url": "http://arxiv.org/abs/2402.11748v2",
      "paper_abstract": "Sound source localisation is used in many consumer electronics devices, to help isolate audio from individual speakers and to reject noise. Localization is frequently accomplished by \"beamforming\" algorithms, which combine microphone audio streams to improve received signal power from particular incident source directions. Beamforming algorithms generally use knowledge of the frequency components of the audio source, along with the known microphone array geometry, to analytically phase-shift microphone streams before combining them. A dense set of band-pass filters is often used to obtain known-frequency \"narrowband\" components from wide-band audio streams. These approaches achieve high accuracy, but state of the art narrowband beamforming algorithms are computationally demanding, and are therefore difficult to integrate into low-power IoT devices. We demonstrate a novel method for sound source localisation in arbitrary microphone arrays, designed for efficient implementation in ultra-low-power spiking neural networks (SNNs). We use a novel short-time Hilbert transform (STHT) to remove the need for demanding band-pass filtering of audio, and introduce a new accompanying method for audio encoding with spiking events. Our beamforming and localisation approach achieves state-of-the-art accuracy for SNN methods, and comparable with traditional non-SNN super-resolution approaches. We deploy our method to low-power SNN audio inference hardware, and achieve much lower power consumption compared with super-resolution methods. We demonstrate that signal processing approaches can be co-designed with spiking neural network implementations to achieve high levels of power efficiency. Our new Hilbert-transform-based method for beamforming promises to also improve the efficiency of traditional DSP-based signal processing.",
      "paper_authors": [
        "Saeid Haghighatshoar",
        "Dylan R Muir"
      ],
      "primary_category": "cs.SD",
      "publish_time": "2024-02-19",
      "update_time": "2024-02-25",
      "comments": null,
      "repo_url": "https://github.com/synsense/haghighatshoarmuir2024"
    },
    "2402.11662": {
      "paper_id": "2402.11662v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.11662v1",
      "paper_key": "2402.11662",
      "paper_title": "TDE-3: An improved prior for optical flow computation in spiking neural networks",
      "paper_url": "http://arxiv.org/abs/2402.11662v1",
      "paper_abstract": "Motion detection is a primary task required for robotic systems to perceive and navigate in their environment. Proposed in the literature bioinspired neuromorphic Time-Difference Encoder (TDE-2) combines event-based sensors and processors with spiking neural networks to provide real-time and energy-efficient motion detection through extracting temporal correlations between two points in space. However, on the algorithmic level, this design leads to loss of direction-selectivity of individual TDEs in textured environments. Here we propose an augmented 3-point TDE (TDE-3) with additional inhibitory input that makes TDE-3 direction-selectivity robust in textured environments. We developed a procedure to train the new TDE-3 using backpropagation through time and surrogate gradients to linearly map input velocities into an output spike count or an Inter-Spike Interval (ISI). Our work is the first instance of training a spiking neuron to have a specific ISI. Using synthetic data we compared training and inference with spike count and ISI with respect to changes in stimuli dynamic range, spatial frequency, and level of noise. ISI turns out to be more robust towards variation in spatial frequency, whereas the spike count is a more reliable training signal in the presence of noise. We performed the first in-depth quantitative investigation of optical flow coding with TDE and compared TDE-2 vs TDE-3 in terms of energy-efficiency and coding precision. Results show that on the network level both detectors show similar precision (20 degree angular error, 88% correlation with ground truth). Yet, due to the more robust direction-selectivity of individual TDEs, TDE-3 based network spike less and hence is more energy-efficient. Reported precision is on par with model-based methods but the spike-based processing of the TDEs provides allows more energy-efficient inference with neuromorphic hardware.",
      "paper_authors": [
        "Matthew Yedutenko",
        "Federico Paredes-Valles",
        "Lyes Khacef",
        "Guido C. H. E. De Croon"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-02-18",
      "update_time": "2024-02-18",
      "comments": null,
      "repo_url": "#"
    },
    "2402.11612": {
      "paper_id": "2402.11612v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.11612v2",
      "paper_key": "2402.11612",
      "paper_title": "Impact of the La2NiO4+\u03b4 oxygen content on the synaptic properties of the TiN/La2NiO4+\u03b4/Pt memristive devices",
      "paper_url": "http://arxiv.org/abs/2402.11612v2",
      "paper_abstract": "The rapid development of brain-inspired computing requires new artificial components and architectures for its hardware implementation. In this regard, memristive devices emerged as potential candidates for artificial synapses because of their ability to emulate the plasticity of the biological synapses. In this work, the synaptic behavior of the TiN/La2NiO4+{\\delta}/Pt memristive devices based on thermally annealed La2NiO4+{\\delta} films is thoroughly investigated. Using electron energy loss spectroscopy, we show that annealing using reducing (Ar) or oxidizing (O2) atmospheres affects the interstitial oxygen content ({\\delta}) in the La2NiO4+{\\delta} films. Electrical characterization shows that both devices exhibit long-term potentiation/depression and spike-timing-dependent plasticity, which makes them suitable for neuromorphic applications. At the same time, the Ar annealed TiN/La2NiO4+{\\delta}/Pt device demonstrates non-volatile properties with low energy consumption during the learning process. On the other hand, in the O2 annealed TiN/La2NiO4+{\\delta}/Pt device the resistive switching behavior is more volatile and requires more energy for synaptic learning. Finally, the simulation tools show that spiking neural network architectures with unsupervised learning rules based on the experimental data achieve high inference accuracy in the digit recognition task, which proves the potential of TiN/La2NiO4+{\\delta}/Pt devices for artificial synapse applications.",
      "paper_authors": [
        "Aleksandra Koroleva",
        "Thoai-Khanh Khuu",
        "C\u00e9sar Mag\u00e9n",
        "Herv\u00e9 Roussel",
        "Carmen Jim\u00e9nez",
        "C\u00e9line Ternon",
        "Elena-Ioana Vatajelu",
        "M\u00f3nica Burriel"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "publish_time": "2024-02-18",
      "update_time": "2024-07-30",
      "comments": null,
      "repo_url": "#"
    },
    "2402.11588": {
      "paper_id": "2402.11588v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.11588v2",
      "paper_key": "2402.11588",
      "paper_title": "SDiT: Spiking Diffusion Model with Transformer",
      "paper_url": "http://arxiv.org/abs/2402.11588v2",
      "paper_abstract": "Spiking neural networks (SNNs) have low power consumption and bio-interpretable characteristics, and are considered to have tremendous potential for energy-efficient computing. However, the exploration of SNNs on image generation tasks remains very limited, and a unified and effective structure for SNN-based generative models has yet to be proposed. In this paper, we explore a novel diffusion model architecture within spiking neural networks. We utilize transformer to replace the commonly used U-net structure in mainstream diffusion models. It can generate higher quality images with relatively lower computational cost and shorter sampling time. It aims to provide an empirical baseline for research of generative models based on SNNs. Experiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets demonstrate that our work is highly competitive compared to existing SNN generative models.",
      "paper_authors": [
        "Shu Yang",
        "Hanzhi Ma",
        "Chengting Yu",
        "Aili Wang",
        "Er-Ping Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-02-18",
      "update_time": "2024-02-24",
      "comments": null,
      "repo_url": "#"
    },
    "2402.11322": {
      "paper_id": "2402.11322v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.11322v3",
      "paper_key": "2402.11322",
      "paper_title": "SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for Spiking Neural Network-based Autonomous Agents",
      "paper_url": "http://arxiv.org/abs/2402.11322v3",
      "paper_abstract": "Autonomous mobile agents (e.g., UAVs and UGVs) are typically expected to incur low power/energy consumption for solving machine learning tasks (such as object recognition), as these mobile agents are usually powered by portable batteries. These requirements can be fulfilled by Spiking Neural Networks (SNNs), since their bio-inspired spike-based operations offer high accuracy and ultra low-power/energy computation. Currently, most of the SNN architectures are derived from Artificial Neural Networks whose neurons' architectures and operations are different from SNNs, or developed without considering memory budgets from the underlying processing hardware of autonomous mobile agents. These limitations hinder SNNs from reaching their full potential in accuracy and efficiency. Toward this, we propose SpikeNAS, a novel fast memory-aware neural architecture search (NAS) framework for SNNs that quickly finds an appropriate SNN architecture with high accuracy under the given memory budgets from autonomous mobile agents. To do this, our SpikeNAS employs several key steps: analyzing the impacts of network operations on the accuracy, enhancing the network architecture to improve the learning quality, and developing a fast memory-aware search algorithm. The experimental results show that our SpikeNAS improves the searching time and maintains high accuracy as compared to state-of-the-art while meeting the given memory budgets (e.g., 4.4x faster search with 1.3% accuracy improvement for CIFAR100, using an Nvidia RTX 6000 Ada GPU machine), thereby quickly providing the appropriate SNN architecture for the memory-constrained autonomous mobile agents.",
      "paper_authors": [
        "Rachmad Vidya Wicaksana Putra",
        "Muhammad Shafique"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-02-17",
      "update_time": "2024-04-05",
      "comments": "8 pages, 13 figures, 2 tables",
      "repo_url": "#"
    },
    "2404.03663": {
      "paper_id": "2404.03663v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.03663v1",
      "paper_key": "2404.03663",
      "paper_title": "Spike-driven Transformer V2: Meta Spiking Neural Network Architecture Inspiring the Design of Next-generation Neuromorphic Chips",
      "paper_url": "http://arxiv.org/abs/2404.03663v1",
      "paper_abstract": "Neuromorphic computing, which exploits Spiking Neural Networks (SNNs) on neuromorphic chips, is a promising energy-efficient alternative to traditional AI. CNN-based SNNs are the current mainstream of neuromorphic computing. By contrast, no neuromorphic chips are designed especially for Transformer-based SNNs, which have just emerged, and their performance is only on par with CNN-based SNNs, offering no distinct advantage. In this work, we propose a general Transformer-based SNN architecture, termed as ``Meta-SpikeFormer\", whose goals are: 1) Lower-power, supports the spike-driven paradigm that there is only sparse addition in the network; 2) Versatility, handles various vision tasks; 3) High-performance, shows overwhelming performance advantages over CNN-based SNNs; 4) Meta-architecture, provides inspiration for future next-generation Transformer-based neuromorphic chip designs. Specifically, we extend the Spike-driven Transformer in \\citet{yao2023spike} into a meta architecture, and explore the impact of structure, spike-driven self-attention, and skip connection on its performance. On ImageNet-1K, Meta-SpikeFormer achieves 80.0\\% top-1 accuracy (55M), surpassing the current state-of-the-art (SOTA) SNN baselines (66M) by 3.7\\%. This is the first direct training SNN backbone that can simultaneously supports classification, detection, and segmentation, obtaining SOTA results in SNNs. Finally, we discuss the inspiration of the meta SNN architecture for neuromorphic chip design. Source code and models are available at \\url{https://github.com/BICLab/Spike-Driven-Transformer-V2}.",
      "paper_authors": [
        "Man Yao",
        "Jiakui Hu",
        "Tianxiang Hu",
        "Yifan Xu",
        "Zhaokun Zhou",
        "Yonghong Tian",
        "Bo Xu",
        "Guoqi Li"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-02-15",
      "update_time": "2024-02-15",
      "comments": "Accepted by ICLR2024. Code and Model:\n  https://github.com/BICLab/Spike-Driven-Transformer-V2",
      "repo_url": "https://github.com/biclab/spike-driven-transformer-v2"
    },
    "2402.09109": {
      "paper_id": "2402.09109v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.09109v1",
      "paper_key": "2402.09109",
      "paper_title": "Stochastic Spiking Attention: Accelerating Attention with Stochastic Computing in Spiking Networks",
      "paper_url": "http://arxiv.org/abs/2402.09109v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) have been recently integrated into Transformer architectures due to their potential to reduce computational demands and to improve power efficiency. Yet, the implementation of the attention mechanism using spiking signals on general-purpose computing platforms remains inefficient. In this paper, we propose a novel framework leveraging stochastic computing (SC) to effectively execute the dot-product attention for SNN-based Transformers. We demonstrate that our approach can achieve high classification accuracy ($83.53\\%$) on CIFAR-10 within 10 time steps, which is comparable to the performance of a baseline artificial neural network implementation ($83.66\\%$). We estimate that the proposed SC approach can lead to over $6.3\\times$ reduction in computing energy and $1.7\\times$ reduction in memory access costs for a digital CMOS-based ASIC design. We experimentally validate our stochastic attention block design through an FPGA implementation, which is shown to achieve $48\\times$ lower latency as compared to a GPU implementation, while consuming $15\\times$ less power.",
      "paper_authors": [
        "Zihang Song",
        "Prabodh Katti",
        "Osvaldo Simeone",
        "Bipin Rajendran"
      ],
      "primary_category": "cs.AR",
      "publish_time": "2024-02-14",
      "update_time": "2024-02-14",
      "comments": null,
      "repo_url": "#"
    },
    "2402.06284": {
      "paper_id": "2402.06284v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.06284v1",
      "paper_key": "2402.06284",
      "paper_title": "Towards Chip-in-the-loop Spiking Neural Network Training via Metropolis-Hastings Sampling",
      "paper_url": "http://arxiv.org/abs/2402.06284v1",
      "paper_abstract": "This paper studies the use of Metropolis-Hastings sampling for training Spiking Neural Network (SNN) hardware subject to strong unknown non-idealities, and compares the proposed approach to the common use of the backpropagation of error (backprop) algorithm and surrogate gradients, widely used to train SNNs in literature. Simulations are conducted within a chip-in-the-loop training context, where an SNN subject to unknown distortion must be trained to detect cancer from measurements, within a biomedical application context. Our results show that the proposed approach strongly outperforms the use of backprop by up to $27\\%$ higher accuracy when subject to strong hardware non-idealities. Furthermore, our results also show that the proposed approach outperforms backprop in terms of SNN generalization, needing $>10 \\times$ less training data for achieving effective accuracy. These findings make the proposed training approach well-suited for SNN implementations in analog subthreshold circuits and other emerging technologies where unknown hardware non-idealities can jeopardize backprop.",
      "paper_authors": [
        "Ali Safa",
        "Vikrant Jaltare",
        "Samira Sebt",
        "Kameron Gano",
        "Johannes Leugering",
        "Georges Gielen",
        "Gert Cauwenberghs"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-02-09",
      "update_time": "2024-02-09",
      "comments": null,
      "repo_url": "#"
    },
    "2402.06211": {
      "paper_id": "2402.06211v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.06211v1",
      "paper_key": "2402.06211",
      "paper_title": "Fine-Tuning Surrogate Gradient Learning for Optimal Hardware Performance in Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2402.06211v1",
      "paper_abstract": "The highly sparse activations in Spiking Neural Networks (SNNs) can provide tremendous energy efficiency benefits when carefully exploited in hardware. The behavior of sparsity in SNNs is uniquely shaped by the dataset and training hyperparameters. This work reveals novel insights into the impacts of training on hardware performance. Specifically, we explore the trade-offs between model accuracy and hardware efficiency. We focus on three key hyperparameters: surrogate gradient functions, beta, and membrane threshold. Results on an FPGA-based hardware platform show that the fast sigmoid surrogate function yields a lower firing rate with similar accuracy compared to the arctangent surrogate on the SVHN dataset. Furthermore, by cross-sweeping the beta and membrane threshold hyperparameters, we can achieve a 48% reduction in hardware-based inference latency with only 2.88% trade-off in inference accuracy compared to the default setting. Overall, this study highlights the importance of fine-tuning model hyperparameters as crucial for designing efficient SNN hardware accelerators, evidenced by the fine-tuned model achieving a 1.72x improvement in accelerator efficiency (FPS/W) compared to the most recent work.",
      "paper_authors": [
        "Ilkin Aliyev",
        "Tosiron Adegbija"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-02-09",
      "update_time": "2024-02-09",
      "comments": null,
      "repo_url": "#"
    },
    "2402.06210": {
      "paper_id": "2402.06210v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.06210v1",
      "paper_key": "2402.06210",
      "paper_title": "PULSE: Parametric Hardware Units for Low-power Sparsity-Aware Convolution Engine",
      "paper_url": "http://arxiv.org/abs/2402.06210v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) have become popular for their more bio-realistic behavior than Artificial Neural Networks (ANNs). However, effectively leveraging the intrinsic, unstructured sparsity of SNNs in hardware is challenging, especially due to the variability in sparsity across network layers. This variability depends on several factors, including the input dataset, encoding scheme, and neuron model. Most existing SNN accelerators fail to account for the layer-specific workloads of an application (model + dataset), leading to high energy consumption. To address this, we propose a design-time parametric hardware generator that takes layer-wise sparsity and the number of processing elements as inputs and synthesizes the corresponding hardware. The proposed design compresses sparse spike trains using a priority encoder and efficiently shifts the activations across the network's layers. We demonstrate the robustness of our proposed approach by first profiling a given application's characteristics followed by performing efficient resource allocation. Results on a Xilinx Kintex FPGA (Field Programmable Gate Arrays) using MNIST, FashionMNIST, and SVHN datasets show a 3.14x improvement in accelerator efficiency (FPS/W) compared to a sparsity-oblivious systolic array-based accelerator. Compared to the most recent sparsity-aware work, our solution improves efficiency by 1.72x.",
      "paper_authors": [
        "Ilkin Aliyev",
        "Tosiron Adegbija"
      ],
      "primary_category": "cs.AR",
      "publish_time": "2024-02-09",
      "update_time": "2024-02-09",
      "comments": "IEEE International Symposium on Circuits and Systems (ISCAS) 2024",
      "repo_url": "#"
    },
    "2402.05441": {
      "paper_id": "2402.05441v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.05441v1",
      "paper_key": "2402.05441",
      "paper_title": "Spiking Neural Network Enhanced Hand Gesture Recognition Using Low-Cost Single-photon Avalanche Diode Array",
      "paper_url": "http://arxiv.org/abs/2402.05441v1",
      "paper_abstract": "We present a compact spiking convolutional neural network (SCNN) and spiking multilayer perceptron (SMLP) to recognize ten different gestures in dark and bright light environments, using a $9.6 single-photon avalanche diode (SPAD) array. In our hand gesture recognition (HGR) system, photon intensity data was leveraged to train and test the network. A vanilla convolutional neural network (CNN) was also implemented to compare the performance of SCNN with the same network topologies and training strategies. Our SCNN was trained from scratch instead of being converted from the CNN. We tested the three models in dark and ambient light (AL)-corrupted environments. The results indicate that SCNN achieves comparable accuracy (90.8%) to CNN (92.9%) and exhibits lower floating operations with only 8 timesteps. SMLP also presents a trade-off between computational workload and accuracy. The code and collected datasets of this work are available at https://github.com/zzy666666zzy/TinyLiDAR_NET_SNN.",
      "paper_authors": [
        "Zhenya Zang",
        "Xingda Li",
        "David Day Uei Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-02-08",
      "update_time": "2024-02-08",
      "comments": "9 pages, 5 figures",
      "repo_url": "#"
    },
    "2402.05423": {
      "paper_id": "2402.05423v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.05423v2",
      "paper_key": "2402.05423",
      "paper_title": "MTSA-SNN: A Multi-modal Time Series Analysis Model Based on Spiking Neural Network",
      "paper_url": "http://arxiv.org/abs/2402.05423v2",
      "paper_abstract": "Time series analysis and modelling constitute a crucial research area. Traditional artificial neural networks struggle with complex, non-stationary time series data due to high computational complexity, limited ability to capture temporal information, and difficulty in handling event-driven data. To address these challenges, we propose a Multi-modal Time Series Analysis Model Based on Spiking Neural Network (MTSA-SNN). The Pulse Encoder unifies the encoding of temporal images and sequential information in a common pulse-based representation. The Joint Learning Module employs a joint learning function and weight allocation mechanism to fuse information from multi-modal pulse signals complementary. Additionally, we incorporate wavelet transform operations to enhance the model's ability to analyze and evaluate temporal information. Experimental results demonstrate that our method achieved superior performance on three complex time-series tasks. This work provides an effective event-driven approach to overcome the challenges associated with analyzing intricate temporal information. Access to the source code is available at https://github.com/Chenngzz/MTSA-SNN}{https://github.com/Chenngzz/MTSA-SNN",
      "paper_authors": [
        "Chengzhi Liu",
        "Zheng Tao",
        "Zihong Luo",
        "Chenghao Liu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-02-08",
      "update_time": "2024-03-04",
      "comments": "6 pages, 6 figures, published to International Conference on Computer\n  Supported Cooperative Work in Design",
      "repo_url": "https://github.com/chenngzz/mtsa-snn"
    },
    "2402.04798": {
      "paper_id": "2402.04798v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.04798v2",
      "paper_key": "2402.04798",
      "paper_title": "Spiking-PhysFormer: Camera-Based Remote Photoplethysmography with Parallel Spike-driven Transformer",
      "paper_url": "http://arxiv.org/abs/2402.04798v2",
      "paper_abstract": "Artificial neural networks (ANNs) can help camera-based remote photoplethysmography (rPPG) in measuring cardiac activity and physiological signals from facial videos, such as pulse wave, heart rate and respiration rate with better accuracy. However, most existing ANN-based methods require substantial computing resources, which poses challenges for effective deployment on mobile devices. Spiking neural networks (SNNs), on the other hand, hold immense potential for energy-efficient deep learning owing to their binary and event-driven architecture. To the best of our knowledge, we are the first to introduce SNNs into the realm of rPPG, proposing a hybrid neural network (HNN) model, the Spiking-PhysFormer, aimed at reducing power consumption. Specifically, the proposed Spiking-PhyFormer consists of an ANN-based patch embedding block, SNN-based transformer blocks, and an ANN-based predictor head. First, to simplify the transformer block while preserving its capacity to aggregate local and global spatio-temporal features, we design a parallel spike transformer block to replace sequential sub-blocks. Additionally, we propose a simplified spiking self-attention mechanism that omits the value parameter without compromising the model's performance. Experiments conducted on four datasets-PURE, UBFC-rPPG, UBFC-Phys, and MMPD demonstrate that the proposed model achieves a 12.4\\% reduction in power consumption compared to PhysFormer. Additionally, the power consumption of the transformer block is reduced by a factor of 12.2, while maintaining decent performance as PhysFormer and other ANN-based models.",
      "paper_authors": [
        "Mingxuan Liu",
        "Jiankai Tang",
        "Haoxiang Li",
        "Jiahao Qi",
        "Siwei Li",
        "Kegang Wang",
        "Yuntao Wang",
        "Hong Chen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-02-07",
      "update_time": "2024-02-09",
      "comments": "Mingxuan Liu and Jiankai Tang are co-first authors of the article",
      "repo_url": "#"
    },
    "2402.04663": {
      "paper_id": "2402.04663v4",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.04663v4",
      "paper_key": "2402.04663",
      "paper_title": "CLIF: Complementary Leaky Integrate-and-Fire Neuron for Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2402.04663v4",
      "paper_abstract": "Spiking neural networks (SNNs) are promising brain-inspired energy-efficient models. Compared to conventional deep Artificial Neural Networks (ANNs), SNNs exhibit superior efficiency and capability to process temporal information. However, it remains a challenge to train SNNs due to their undifferentiable spiking mechanism. The surrogate gradients method is commonly used to train SNNs, but often comes with an accuracy disadvantage over ANNs counterpart. We link the degraded accuracy to the vanishing of gradient on the temporal dimension through the analytical and experimental study of the training process of Leaky Integrate-and-Fire (LIF) Neuron-based SNNs. Moreover, we propose the Complementary Leaky Integrate-and-Fire (CLIF) Neuron. CLIF creates extra paths to facilitate the backpropagation in computing temporal gradient while keeping binary output. CLIF is hyperparameter-free and features broad applicability. Extensive experiments on a variety of datasets demonstrate CLIF's clear performance advantage over other neuron models. Furthermore, the CLIF's performance even slightly surpasses superior ANNs with identical network structure and training conditions. The code is available at https://github.com/HuuYuLong/Complementary-LIF.",
      "paper_authors": [
        "Yulong Huang",
        "Xiaopeng Lin",
        "Hongwei Ren",
        "Haotian Fu",
        "Yue Zhou",
        "Zunchang Liu",
        "Biao Pan",
        "Bojun Cheng"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-02-07",
      "update_time": "2024-07-15",
      "comments": null,
      "repo_url": "https://github.com/huuyulong/complementary-lif"
    },
    "2403.09678": {
      "paper_id": "2403.09678v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.09678v1",
      "paper_key": "2403.09678",
      "paper_title": "A Quasi-Stationary Approach to Metastability in a System of Spiking Neurons with Synaptic Plasticity",
      "paper_url": "http://arxiv.org/abs/2403.09678v1",
      "paper_abstract": "After reviewing the behavioral studies of working memory and of the cellular substrate of the latter, we argue that metastable states constitute candidates for the type of transient information storage required by working memory. We then present a simple neural network model made of stochastic units whose synapses exhibit short-term facilitation. The Markov process dynamics of this model was specifically designed to be analytically tractable, simple to simulate numerically and to exhibit a quasi-stationary distribution (QSD). Since the state space is finite this QSD is also a Yaglom limit, which allows us to bridge the gap between quasi-stationarity and metastability by considering the relative orders of magnitude of the relaxation and absorption times. We present first analytical results: characterization of the absorbing region of the Markov process, irreducibility outside this absorbing region and consequently existence and uniqueness of a QSD. We then apply Perron-Frobenius spectral analysis to obtain any specific QSD, and design an approximate method for the first moments of this QSD when the exact method is intractable. Finally we use these methods to study the relaxation time toward the QSD and establish numerically the memorylessness of the time of extinction.",
      "paper_authors": [
        "Christophe Pouzat",
        "Morgan Andr\u00e9"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2024-02-07",
      "update_time": "2024-02-07",
      "comments": null,
      "repo_url": "https://gitlab.com/c_pouzat/metastability-in-a-system-of-spiking-neurons-with-synaptic-plasticity"
    },
    "2402.04596": {
      "paper_id": "2402.04596v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.04596v1",
      "paper_key": "2402.04596",
      "paper_title": "Towards Improved Imbalance Robustness in Continual Multi-Label Learning with Dual Output Spiking Architecture (DOSA)",
      "paper_url": "http://arxiv.org/abs/2402.04596v1",
      "paper_abstract": "Algorithms designed for addressing typical supervised classification problems can only learn from a fixed set of samples and labels, making them unsuitable for the real world, where data arrives as a stream of samples often associated with multiple labels over time. This motivates the study of task-agnostic continual multi-label learning problems. While algorithms using deep learning approaches for continual multi-label learning have been proposed in the recent literature, they tend to be computationally heavy. Although spiking neural networks (SNNs) offer a computationally efficient alternative to artificial neural networks, existing literature has not used SNNs for continual multi-label learning. Also, accurately determining multiple labels with SNNs is still an open research problem. This work proposes a dual output spiking architecture (DOSA) to bridge these research gaps. A novel imbalance-aware loss function is also proposed, improving the multi-label classification performance of the model by making it more robust to data imbalance. A modified F1 score is presented to evaluate the effectiveness of the proposed loss function in handling imbalance. Experiments on several benchmark multi-label datasets show that DOSA trained with the proposed loss function shows improved robustness to data imbalance and obtains better continual multi-label learning performance than CIFDM, a previous state-of-the-art algorithm.",
      "paper_authors": [
        "Sourav Mishra",
        "Shirin Dora",
        "Suresh Sundaram"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-02-07",
      "update_time": "2024-02-07",
      "comments": "8 pages, 4 figures, 4 tables, 45 references. Submitted to IJCNN 2024",
      "repo_url": "#"
    },
    "2403.08804": {
      "paper_id": "2403.08804v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.08804v1",
      "paper_key": "2403.08804",
      "paper_title": "Forward Direct Feedback Alignment for Online Gradient Estimates of Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2403.08804v1",
      "paper_abstract": "There is an interest in finding energy efficient alternatives to current state of the art neural network training algorithms. Spiking neural network are a promising approach, because they can be simulated energy efficiently on neuromorphic hardware platforms. However, these platforms come with limitations on the design of the training algorithm. Most importantly, backpropagation cannot be implemented on those. We propose a novel neuromorphic algorithm, the \\textit{Spiking Forward Direct Feedback Alignment} (SFDFA) algorithm, an adaption of \\textit{Forward Direct Feedback Alignment} to train SNNs. SFDFA estimates the weights between output and hidden neurons as feedback connections. The main contribution of this paper is to describe how exact local gradients of spikes can be computed in an online manner while taking into account the intra-neuron dependencies between post-synaptic spikes and derive a dynamical system for neuromorphic hardware compatibility. We compare the SFDFA algorithm with a number of competitor algorithms and show that the proposed algorithm achieves higher performance and convergence rates.",
      "paper_authors": [
        "Florian Bacho",
        "Dminique Chu"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-02-06",
      "update_time": "2024-02-06",
      "comments": null,
      "repo_url": "#"
    },
    "2402.03767": {
      "paper_id": "2402.03767v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.03767v1",
      "paper_key": "2402.03767",
      "paper_title": "Magnetic Field Gated and Current Controlled Spintronic Mem-transistor Neuron -based Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2402.03767v1",
      "paper_abstract": "Spintronic devices, such as the domain walls and skyrmions, have shown significant potential for applications in energy-efficient data storage and beyond CMOS computing architectures. In recent years, spiking neural networks have shown more bio-plausibility. Based on the magnetic multilayer spintronic devices, we demonstrate the magnetic field-gated Leaky integrate and fire neuron characteristics for the spiking neural network applications. The LIF characteristics are controlled by the current pulses, which drive the domain wall, and an external magnetic field is used as the bias to tune the firing properties of the neuron. Thus, the device works like a gate-controlled LIF neuron, acting like a spintronic Mem-Transistor device. We develop a LIF neuron model based on the measured characteristics to show the device integration in the system-level SNNs. We extend the study and propose a scaled version of the demonstrated device with a multilayer spintronic domain wall magnetic tunnel junction as a LIF neuron. using the combination of SOT and the variation of the demagnetization energy across the thin film, the modified leaky integrate and fire LIF neuron characteristics are realized in the proposed devices. The neuron device characteristics are modeled as the modified LIF neuron model. Finally, we integrate the measured and simulated neuron models in the 3-layer spiking neural network and convolutional spiking neural network CSNN framework to test these spiking neuron models for classification of the MNIST and FMNIST datasets. In both architectures, the network achieves classification accuracy above 96%. Considering the good system-level performance, mem-transistor properties, and promise for scalability. The presented devices show an excellent properties for neuromorphic computing applications.",
      "paper_authors": [
        "Aijaz H. Lone",
        "Meng Tang",
        "Daniel N. Rahimi",
        "Xuecui Zou",
        "Dongxing Zheng",
        "Hossein Fariborzi",
        "Xixiang Zhang",
        "Gianluca Setti"
      ],
      "primary_category": "physics.app-ph",
      "publish_time": "2024-02-06",
      "update_time": "2024-02-06",
      "comments": "33 pages, 10 figures",
      "repo_url": "#"
    },
    "2402.02886": {
      "paper_id": "2402.02886v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.02886v1",
      "paper_key": "2402.02886",
      "paper_title": "Time-Distributed Backdoor Attacks on Federated Spiking Learning",
      "paper_url": "http://arxiv.org/abs/2402.02886v1",
      "paper_abstract": "This paper investigates the vulnerability of spiking neural networks (SNNs) and federated learning (FL) to backdoor attacks using neuromorphic data. Despite the efficiency of SNNs and the privacy advantages of FL, particularly in low-powered devices, we demonstrate that these systems are susceptible to such attacks. We first assess the viability of using FL with SNNs using neuromorphic data, showing its potential usage. Then, we evaluate the transferability of known FL attack methods to SNNs, finding that these lead to suboptimal attack performance. Therefore, we explore backdoor attacks involving single and multiple attackers to improve the attack performance. Our primary contribution is developing a novel attack strategy tailored to SNNs and FL, which distributes the backdoor trigger temporally and across malicious devices, enhancing the attack's effectiveness and stealthiness. In the best case, we achieve a 100 attack success rate, 0.13 MSE, and 98.9 SSIM. Moreover, we adapt and evaluate an existing defense against backdoor attacks, revealing its inadequacy in protecting SNNs. This study underscores the need for robust security measures in deploying SNNs and FL, particularly in the context of backdoor attacks.",
      "paper_authors": [
        "Gorka Abad",
        "Stjepan Picek",
        "Aitor Urbieta"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-02-05",
      "update_time": "2024-02-05",
      "comments": null,
      "repo_url": "#"
    },
    "2402.01533": {
      "paper_id": "2402.01533v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.01533v2",
      "paper_key": "2402.01533",
      "paper_title": "Efficient and Effective Time-Series Forecasting with Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2402.01533v2",
      "paper_abstract": "Spiking neural networks (SNNs), inspired by the spiking behavior of biological neurons, provide a unique pathway for capturing the intricacies of temporal data. However, applying SNNs to time-series forecasting is challenging due to difficulties in effective temporal alignment, complexities in encoding processes, and the absence of standardized guidelines for model selection. In this paper, we propose a framework for SNNs in time-series forecasting tasks, leveraging the efficiency of spiking neurons in processing temporal information. Through a series of experiments, we demonstrate that our proposed SNN-based approaches achieve comparable or superior results to traditional time-series forecasting methods on diverse benchmarks with much less energy consumption. Furthermore, we conduct detailed analysis experiments to assess the SNN's capacity to capture temporal dependencies within time-series data, offering valuable insights into its nuanced strengths and effectiveness in modeling the intricate dynamics of temporal data. Our study contributes to the expanding field of SNNs and offers a promising alternative for time-series forecasting tasks, presenting a pathway for the development of more biologically inspired and temporally aware forecasting models. Our code is available at https://github.com/microsoft/SeqSNN.",
      "paper_authors": [
        "Changze Lv",
        "Yansen Wang",
        "Dongqi Han",
        "Xiaoqing Zheng",
        "Xuanjing Huang",
        "Dongsheng Li"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-02-02",
      "update_time": "2024-05-29",
      "comments": null,
      "repo_url": "https://github.com/microsoft/seqsnn"
    },
    "2402.14603": {
      "paper_id": "2402.14603v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.14603v1",
      "paper_key": "2402.14603",
      "paper_title": "Balanced Resonate-and-Fire Neurons",
      "paper_url": "http://arxiv.org/abs/2402.14603v1",
      "paper_abstract": "The resonate-and-fire (RF) neuron, introduced over two decades ago, is a simple, efficient, yet biologically plausible spiking neuron model, which can extract frequency patterns within the time domain due to its resonating membrane dynamics. However, previous RF formulations suffer from intrinsic shortcomings that limit effective learning and prevent exploiting the principled advantage of RF neurons. Here, we introduce the balanced RF (BRF) neuron, which alleviates some of the intrinsic limitations of vanilla RF neurons and demonstrates its effectiveness within recurrent spiking neural networks (RSNNs) on various sequence learning tasks. We show that networks of BRF neurons achieve overall higher task performance, produce only a fraction of the spikes, and require significantly fewer parameters as compared to modern RSNNs. Moreover, BRF-RSNN consistently provide much faster and more stable training convergence, even when bridging many hundreds of time steps during backpropagation through time (BPTT). These results underscore that our BRF-RSNN is a strong candidate for future large-scale RSNN architectures, further lines of research in SNN methodology, and more efficient hardware implementations.",
      "paper_authors": [
        "Saya Higuchi",
        "Sebastian Kairat",
        "Sander M. Bohte",
        "Sebastian Otte"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-02-02",
      "update_time": "2024-02-02",
      "comments": null,
      "repo_url": "#"
    },
    "2402.01287": {
      "paper_id": "2402.01287v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.01287v2",
      "paper_key": "2402.01287",
      "paper_title": "Spiking CenterNet: A Distillation-boosted Spiking Neural Network for Object Detection",
      "paper_url": "http://arxiv.org/abs/2402.01287v2",
      "paper_abstract": "In the era of AI at the edge, self-driving cars, and climate change, the need for energy-efficient, small, embedded AI is growing. Spiking Neural Networks (SNNs) are a promising approach to address this challenge, with their event-driven information flow and sparse activations. We propose Spiking CenterNet for object detection on event data. It combines an SNN CenterNet adaptation with an efficient M2U-Net-based decoder. Our model significantly outperforms comparable previous work on Prophesee's challenging GEN1 Automotive Detection Dataset while using less than half the energy. Distilling the knowledge of a non-spiking teacher into our SNN further increases performance. To the best of our knowledge, our work is the first approach that takes advantage of knowledge distillation in the field of spiking object detection.",
      "paper_authors": [
        "Lennard Bodden",
        "Franziska Schwaiger",
        "Duc Bach Ha",
        "Lars Kreuzberg",
        "Sven Behnke"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-02-02",
      "update_time": "2024-06-06",
      "comments": "8 pages, 5 figures. Accepted at IJCNN 2024",
      "repo_url": "#"
    },
    "2402.01782": {
      "paper_id": "2402.01782v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.01782v1",
      "paper_key": "2402.01782",
      "paper_title": "Benchmarking Spiking Neural Network Learning Methods with Varying Locality",
      "paper_url": "http://arxiv.org/abs/2402.01782v1",
      "paper_abstract": "Spiking Neural Networks (SNNs), providing more realistic neuronal dynamics, have shown to achieve performance comparable to Artificial Neural Networks (ANNs) in several machine learning tasks. Information is processed as spikes within SNNs in an event-based mechanism that significantly reduces energy consumption. However, training SNNs is challenging due to the non-differentiable nature of the spiking mechanism. Traditional approaches, such as Backpropagation Through Time (BPTT), have shown effectiveness but comes with additional computational and memory costs and are biologically implausible. In contrast, recent works propose alternative learning methods with varying degrees of locality, demonstrating success in classification tasks. In this work, we show that these methods share similarities during the training process, while they present a trade-off between biological plausibility and performance. Further, this research examines the implicitly recurrent nature of SNNs and investigates the influence of addition of explicit recurrence to SNNs. We experimentally prove that the addition of explicit recurrent weights enhances the robustness of SNNs. We also investigate the performance of local learning methods under gradient and non-gradient based adversarial attacks.",
      "paper_authors": [
        "Jiaqi Lin",
        "Sen Lu",
        "Malyaban Bal",
        "Abhronil Sengupta"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-02-01",
      "update_time": "2024-02-01",
      "comments": null,
      "repo_url": "#"
    },
    "2402.00449": {
      "paper_id": "2402.00449v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.00449v3",
      "paper_key": "2402.00449",
      "paper_title": "Parallel Spiking Unit for Efficient Training of Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2402.00449v3",
      "paper_abstract": "Efficient parallel computing has become a pivotal element in advancing artificial intelligence. Yet, the deployment of Spiking Neural Networks (SNNs) in this domain is hampered by their inherent sequential computational dependency. This constraint arises from the need for each time step's processing to rely on the preceding step's outcomes, significantly impeding the adaptability of SNN models to massively parallel computing environments. Addressing this challenge, our paper introduces the innovative Parallel Spiking Unit (PSU) and its two derivatives, the Input-aware PSU (IPSU) and Reset-aware PSU (RPSU). These variants skillfully decouple the leaky integration and firing mechanisms in spiking neurons while probabilistically managing the reset process. By preserving the fundamental computational attributes of the spiking neuron model, our approach enables the concurrent computation of all membrane potential instances within the SNN, facilitating parallel spike output generation and substantially enhancing computational efficiency. Comprehensive testing across various datasets, including static and sequential images, Dynamic Vision Sensor (DVS) data, and speech datasets, demonstrates that the PSU and its variants not only significantly boost performance and simulation speed but also augment the energy efficiency of SNNs through enhanced sparsity in neural activity. These advancements underscore the potential of our method in revolutionizing SNN deployment for high-performance parallel computing applications.",
      "paper_authors": [
        "Yang Li",
        "Yinqian Sun",
        "Xiang He",
        "Yiting Dong",
        "Dongcheng Zhao",
        "Yi Zeng"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-02-01",
      "update_time": "2024-06-08",
      "comments": null,
      "repo_url": "#"
    },
    "2402.00411": {
      "paper_id": "2402.00411v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.00411v1",
      "paper_key": "2402.00411",
      "paper_title": "LM-HT SNN: Enhancing the Performance of SNN to ANN Counterpart through Learnable Multi-hierarchical Threshold Model",
      "paper_url": "http://arxiv.org/abs/2402.00411v1",
      "paper_abstract": "Compared to traditional Artificial Neural Network (ANN), Spiking Neural Network (SNN) has garnered widespread academic interest for its intrinsic ability to transmit information in a more biological-inspired and energy-efficient manner. However, despite previous efforts to optimize the learning gradients and model structure of SNNs through various methods, SNNs still lag behind ANNs in terms of performance to some extent. The recently proposed multi-threshold model provides more possibilities for further enhancing the learning capability of SNNs. In this paper, we rigorously analyze the relationship among the multi-threshold model, vanilla spiking model and quantized ANNs from a mathematical perspective, then propose a novel LM-HT model, which is an equidistant multi-hierarchical model that can dynamically regulate the global input current and membrane potential leakage on the time dimension. In addition, we note that the direct training algorithm based on the LM-HT model can seamlessly integrate with the traditional ANN-SNN Conversion framework. This novel hybrid learning framework can effectively improve the relatively poor performance of converted SNNs under low time latency. Extensive experimental results have demonstrated that our LM-HT model can significantly outperform previous state-of-the-art works on various types of datasets, which promote SNNs to achieve a brand-new level of performance comparable to quantized ANNs.",
      "paper_authors": [
        "Zecheng Hao",
        "Xinyu Shi",
        "Zhiyu Pan",
        "Yujia Liu",
        "Zhaofei Yu",
        "Tiejun Huang"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-02-01",
      "update_time": "2024-02-01",
      "comments": "15 pages, 2 figures",
      "repo_url": "#"
    },
    "2402.00906": {
      "paper_id": "2402.00906v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.00906v2",
      "paper_key": "2402.00906",
      "paper_title": "BrainLeaks: On the Privacy-Preserving Properties of Neuromorphic Architectures against Model Inversion Attacks",
      "paper_url": "http://arxiv.org/abs/2402.00906v2",
      "paper_abstract": "With the mainstream integration of machine learning into security-sensitive domains such as healthcare and finance, concerns about data privacy have intensified. Conventional artificial neural networks (ANNs) have been found vulnerable to several attacks that can leak sensitive data. Particularly, model inversion (MI) attacks enable the reconstruction of data samples that have been used to train the model. Neuromorphic architectures have emerged as a paradigm shift in neural computing, enabling asynchronous and energy-efficient computation. However, little to no existing work has investigated the privacy of neuromorphic architectures against model inversion. Our study is motivated by the intuition that the non-differentiable aspect of spiking neural networks (SNNs) might result in inherent privacy-preserving properties, especially against gradient-based attacks. To investigate this hypothesis, we propose a thorough exploration of SNNs' privacy-preserving capabilities. Specifically, we develop novel inversion attack strategies that are comprehensively designed to target SNNs, offering a comparative analysis with their conventional ANN counterparts. Our experiments, conducted on diverse event-based and static datasets, demonstrate the effectiveness of the proposed attack strategies and therefore questions the assumption of inherent privacy-preserving in neuromorphic architectures.",
      "paper_authors": [
        "Hamed Poursiami",
        "Ihsen Alouani",
        "Maryam Parsa"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-02-01",
      "update_time": "2024-05-07",
      "comments": "7 pages, 4 figures, 4 tables",
      "repo_url": "#"
    },
    "2401.17911": {
      "paper_id": "2401.17911v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.17911v1",
      "paper_key": "2401.17911",
      "paper_title": "SNNLP: Energy-Efficient Natural Language Processing Using Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2401.17911v1",
      "paper_abstract": "As spiking neural networks receive more attention, we look toward applications of this computing paradigm in fields other than computer vision and signal processing. One major field, underexplored in the neuromorphic setting, is Natural Language Processing (NLP), where most state-of-the-art solutions still heavily rely on resource-consuming and power-hungry traditional deep learning architectures. Therefore, it is compelling to design NLP models for neuromorphic architectures due to their low energy requirements, with the additional benefit of a more human-brain-like operating model for processing information. However, one of the biggest issues with bringing NLP to the neuromorphic setting is in properly encoding text into a spike train so that it can be seamlessly handled by both current and future SNN architectures. In this paper, we compare various methods of encoding text as spikes and assess each method's performance in an associated SNN on a downstream NLP task, namely, sentiment analysis. Furthermore, we go on to propose a new method of encoding text as spikes that outperforms a widely-used rate-coding technique, Poisson rate-coding, by around 13\\% on our benchmark NLP tasks. Subsequently, we demonstrate the energy efficiency of SNNs implemented in hardware for the sentiment analysis task compared to traditional deep neural networks, observing an energy efficiency increase of more than 32x during inference and 60x during training while incurring the expected energy-performance tradeoff.",
      "paper_authors": [
        "R. Alexander Knipper",
        "Kaniz Mishty",
        "Mehdi Sadi",
        "Shubhra Kanti Karmaker Santu"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-01-31",
      "update_time": "2024-01-31",
      "comments": null,
      "repo_url": "https://github.com/alexknipper/snnlp"
    },
    "2401.17370": {
      "paper_id": "2401.17370v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.17370v1",
      "paper_key": "2401.17370",
      "paper_title": "Picosecond transfer from short-term to long-term memory in analog antiferromagnetic memory device",
      "paper_url": "http://arxiv.org/abs/2401.17370v1",
      "paper_abstract": "Experiments in materials with a compensated ordering of magnetic moments have demonstrated a potential for approaching the thermodynamic limit of the fastest and least-dissipative operation of a digital memory bit. In addition, these materials are very promising for a construction of energy-efficient analog devices with neuromorphic functionalities, which are inspired by computing-in-memory capabilities of the human brain. In this paper, we report on experimental separation of switching-related and heat-related resistance signal dynamics in memory devices microfabricated from CuMnAs antiferromagnetic metal. We show that the memory variable multilevel resistance can be used as a long-term memory (LTM), lasting up to minutes at room temperature. In addition, ultrafast reflectivity change and heat dissipation from nanoscale-thickness CuMnAs films, taking place on picosecond to hundreds of nanoseconds time scales, can be used as a short-term memory (STM). Information about input stimuli, represented by femtosecond laser pulses, can be transferred from STM to LTM after rehearsals at picosecond to nanosecond times in these memory devices, where information can be retrieved at times up to 10^15 longer than the input pulse duration. Our results open a route towards ultra-fast low-power implementations of spiking neuron and synapse functionalities using a resistive analog antiferromagnetic memory.",
      "paper_authors": [
        "M. Surynek",
        "J. Zubac",
        "K. Olejnik",
        "A. Farkas",
        "F. Krizek",
        "L. Nadvornik",
        "P. Kubascik",
        "F. Trojanek",
        "R. P. Campion",
        "V. Novak",
        "T. Jungwirth",
        "P. Nemec"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "publish_time": "2024-01-30",
      "update_time": "2024-01-30",
      "comments": "21 pages, 4 figures",
      "repo_url": "#"
    },
    "2401.16841": {
      "paper_id": "2401.16841v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.16841v1",
      "paper_key": "2401.16841",
      "paper_title": "jaxsnn: Event-driven Gradient Estimation for Analog Neuromorphic Hardware",
      "paper_url": "http://arxiv.org/abs/2401.16841v1",
      "paper_abstract": "Traditional neuromorphic hardware architectures rely on event-driven computation, where the asynchronous transmission of events, such as spikes, triggers local computations within synapses and neurons. While machine learning frameworks are commonly used for gradient-based training, their emphasis on dense data structures poses challenges for processing asynchronous data such as spike trains. This problem is particularly pronounced for typical tensor data structures. In this context, we present a novel library (jaxsnn) built on top of JAX, that departs from conventional machine learning frameworks by providing flexibility in the data structures used and the handling of time, while maintaining Autograd functionality and composability. Our library facilitates the simulation of spiking neural networks and gradient estimation, with a focus on compatibility with time-continuous neuromorphic backends, such as the BrainScaleS-2 system, during the forward pass. This approach opens avenues for more efficient and flexible training of spiking neural networks, bridging the gap between traditional neuromorphic architectures and contemporary machine learning frameworks.",
      "paper_authors": [
        "Eric M\u00fcller",
        "Moritz Althaus",
        "Elias Arnold",
        "Philipp Spilger",
        "Christian Pehle",
        "Johannes Schemmel"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-01-30",
      "update_time": "2024-01-30",
      "comments": null,
      "repo_url": "#"
    },
    "2401.16840": {
      "paper_id": "2401.16840v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.16840v1",
      "paper_key": "2401.16840",
      "paper_title": "Towards Large-scale Network Emulation on Analog Neuromorphic Hardware",
      "paper_url": "http://arxiv.org/abs/2401.16840v1",
      "paper_abstract": "We present a novel software feature for the BrainScaleS-2 accelerated neuromorphic platform that facilitates the emulation of partitioned large-scale spiking neural networks. This approach is well suited for many deep spiking neural networks, where the constraint of the largest recurrent subnetwork fitting on the substrate or the limited fan-in of neurons is often not a limitation in practice. We demonstrate the training of two deep spiking neural network models, using the MNIST and EuroSAT datasets, that exceed the physical size constraints of a single-chip BrainScaleS-2 system. The ability to emulate and train networks larger than the substrate provides a pathway for accurate performance evaluation in planned or scaled systems, ultimately advancing the development and understanding of large-scale models and neuromorphic computing architectures.",
      "paper_authors": [
        "Elias Arnold",
        "Philipp Spilger",
        "Jan V. Straub",
        "Eric M\u00fcller",
        "Dominik Dold",
        "Gabriele Meoni",
        "Johannes Schemmel"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-01-30",
      "update_time": "2024-01-30",
      "comments": null,
      "repo_url": "#"
    },
    "2403.08786": {
      "paper_id": "2403.08786v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.08786v1",
      "paper_key": "2403.08786",
      "paper_title": "One-Spike SNN: Single-Spike Phase Coding with Base Manipulation for ANN-to-SNN Conversion Loss Minimization",
      "paper_url": "http://arxiv.org/abs/2403.08786v1",
      "paper_abstract": "As spiking neural networks (SNNs) are event-driven, energy efficiency is higher than conventional artificial neural networks (ANNs). Since SNN delivers data through discrete spikes, it is difficult to use gradient methods for training, limiting its accuracy. To keep the accuracy of SNNs similar to ANN counterparts, pre-trained ANNs are converted to SNNs (ANN-to-SNN conversion). During the conversion, encoding activations of ANNs to a set of spikes in SNNs is crucial for minimizing the conversion loss. In this work, we propose a single-spike phase coding as an encoding scheme that minimizes the number of spikes to transfer data between SNN layers. To minimize the encoding error due to single-spike approximation in phase coding, threshold shift and base manipulation are proposed. Without any additional retraining or architectural constraints on ANNs, the proposed conversion method does not lose inference accuracy (0.58% on average) verified on three convolutional neural networks (CNNs) with CIFAR and ImageNet datasets.In addition, graph convolutional networks (GCNs) are converted to SNNs successfully with an average accuracy loss of 0.90%.Most importantly, the energy efficiency of our SNN improves by 4.6~17.3 X compared to the ANN baseline.",
      "paper_authors": [
        "Sangwoo Hwang",
        "Jaeha Kung"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-01-30",
      "update_time": "2024-01-30",
      "comments": "11 pages, 10 figures",
      "repo_url": "#"
    },
    "2401.16159": {
      "paper_id": "2401.16159v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.16159v1",
      "paper_key": "2401.16159",
      "paper_title": "Learned Spike Encoding of the Channel Response for Low-Power Environment Sensing",
      "paper_url": "http://arxiv.org/abs/2401.16159v1",
      "paper_abstract": "Radio Frequency (RF) sensing holds the potential for enabling pervasive monitoring applications. However, modern sensing algorithms imply complex operations, which clash with the energy-constrained nature of edge sensing devices. This calls for the development of new processing and learning techniques that can strike a suitable balance between performance and energy efficiency. Spiking Neural Networks (SNNs) have recently emerged as an energy-efficient alternative to conventional neural networks for edge computing applications. They process information in the form of sparse binary spike trains, thus potentially reducing energy consumption by several orders of magnitude. Their fruitful use for RF signal processing critically depends on the representation of RF signals in the form of spike signals. We underline that existing spike encoding algorithms to do so generally produce inaccurate signal representations and dense (i.e., inefficient) spike trains. In this work, we propose a lightweight neural architecture that learns a tailored spike encoding representations of RF channel responses by jointly reconstructing the input and its spectral content. By leveraging a tunable regularization term, our approach enables fine-grained control over the performance-energy trade-off of the system. Our numerical results show that the proposed method outperforms existing encoding algorithms in terms of reconstruction error and sparsity of the obtained spike encodings.",
      "paper_authors": [
        "Eleonora Cicciarella",
        "Riccardo Mazzieri",
        "Jacopo Pegoraro",
        "Michele Rossi"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2024-01-29",
      "update_time": "2024-01-29",
      "comments": "6 pages, 5figures, 2 tables",
      "repo_url": "#"
    },
    "2402.10078": {
      "paper_id": "2402.10078v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.10078v1",
      "paper_key": "2402.10078",
      "paper_title": "EventF2S: Asynchronous and Sparse Spiking AER Framework using Neuromorphic-Friendly Algorithm",
      "paper_url": "http://arxiv.org/abs/2402.10078v1",
      "paper_abstract": "Bio-inspired Address Event Representation (AER) sensors have attracted significant popularity owing to their low power consumption, high sparsity, and high temporal resolution. Spiking Neural Network (SNN) has become the inherent choice for AER data processing. However, the integration of the AER-SNN paradigm has not adequately explored asynchronous processing, neuromorphic compatibility, and sparse spiking, which are the key requirements of resource-constrained applications. To address this gap, we introduce a brain-inspired AER-SNN object recognition solution, which includes a data encoder integrated with a First-To-Spike recognition network. Being fascinated by the functionality of neurons in the visual cortex, we designed the solution to be asynchronous and compatible with neuromorphic hardware. Furthermore, we have adapted the principle of denoising and First-To-Spike coding to achieve optimal spike signaling, significantly reducing computation costs. Experimental evaluation has demonstrated that the proposed method incurs significantly less computation cost to achieve state-of-the-art competitive accuracy. Overall, the proposed solution offers an asynchronous and cost-effective AER recognition system that harnesses the full potential of AER sensors.",
      "paper_authors": [
        "Lakshmi Annamalai",
        "Chetan Singh Thakur"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-01-28",
      "update_time": "2024-01-28",
      "comments": null,
      "repo_url": "#"
    },
    "2401.15453": {
      "paper_id": "2401.15453v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.15453v1",
      "paper_key": "2401.15453",
      "paper_title": "Bayesian Inference Accelerator for Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2401.15453v1",
      "paper_abstract": "Bayesian neural networks offer better estimates of model uncertainty compared to frequentist networks. However, inference involving Bayesian models requires multiple instantiations or sampling of the network parameters, requiring significant computational resources. Compared to traditional deep learning networks, spiking neural networks (SNNs) have the potential to reduce computational area and power, thanks to their event-driven and spike-based computational framework. Most works in literature either address frequentist SNN models or non-spiking Bayesian neural networks. In this work, we demonstrate an optimization framework for developing and implementing efficient Bayesian SNNs in hardware by additionally restricting network weights to be binary-valued to further decrease power and area consumption. We demonstrate accuracies comparable to Bayesian binary networks with full-precision Bernoulli parameters, while requiring up to $25\\times$ less spikes than equivalent binary SNN implementations. We show the feasibility of the design by mapping it onto Zynq-7000, a lightweight SoC, and achieve a $6.5 \\times$ improvement in GOPS/DSP while utilizing up to 30 times less power compared to the state-of-the-art.",
      "paper_authors": [
        "Prabodh Katti",
        "Anagha Nimbekar",
        "Chen Li",
        "Amit Acharyya",
        "Bashir M. Al-Hashimi",
        "Bipin Rajendran"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-01-27",
      "update_time": "2024-01-27",
      "comments": "Submitted and Accepted in ISCAS 2024",
      "repo_url": "#"
    },
    "2401.15212": {
      "paper_id": "2401.15212v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.15212v1",
      "paper_key": "2401.15212",
      "paper_title": "Speed-based Filtration and DBSCAN of Event-based Camera Data with Neuromorphic Computing",
      "paper_url": "http://arxiv.org/abs/2401.15212v1",
      "paper_abstract": "Spiking neural networks are powerful computational elements that pair well with event-based cameras (EBCs). In this work, we present two spiking neural network architectures that process events from EBCs: one that isolates and filters out events based on their speeds, and another that clusters events based on the DBSCAN algorithm.",
      "paper_authors": [
        "Charles P. Rizzo",
        "Catherine D. Schuman",
        "James S. Plank"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-01-26",
      "update_time": "2024-01-26",
      "comments": "8 pages, 5 figures, Submitted to Neuro Inspired Computational\n  Elements Conference 2024",
      "repo_url": "#"
    },
    "2401.14652": {
      "paper_id": "2401.14652v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.14652v2",
      "paper_key": "2401.14652",
      "paper_title": "LitE-SNN: Designing Lightweight and Efficient Spiking Neural Network through Spatial-Temporal Compressive Network Search and Joint Optimization",
      "paper_url": "http://arxiv.org/abs/2401.14652v2",
      "paper_abstract": "Spiking Neural Networks (SNNs) mimic the information-processing mechanisms of the human brain and are highly energy-efficient, making them well-suited for low-power edge devices. However, the pursuit of accuracy in current studies leads to large, long-timestep SNNs, conflicting with the resource constraints of these devices. In order to design lightweight and efficient SNNs, we propose a new approach named LitE-SNN that incorporates both spatial and temporal compression into the automated network design process. Spatially, we present a novel Compressive Convolution block (CompConv) to expand the search space to support pruning and mixed-precision quantization. Temporally, we are the first to propose a compressive timestep search to identify the optimal number of timesteps under specific computation cost constraints. Finally, we formulate a joint optimization to simultaneously learn the architecture parameters and spatial-temporal compression strategies to achieve high performance while minimizing memory and computation costs. Experimental results on CIFAR-10, CIFAR-100, and Google Speech Command datasets demonstrate our proposed LitE-SNNs can achieve competitive or even higher accuracy with remarkably smaller model sizes and fewer computation costs.",
      "paper_authors": [
        "Qianhui Liu",
        "Jiaqi Yan",
        "Malu Zhang",
        "Gang Pan",
        "Haizhou Li"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-01-26",
      "update_time": "2024-05-13",
      "comments": null,
      "repo_url": "#"
    },
    "2402.10920": {
      "paper_id": "2402.10920v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.10920v1",
      "paper_key": "2402.10920",
      "paper_title": "Designing Silicon Brains using LLM: Leveraging ChatGPT for Automated Description of a Spiking Neuron Array",
      "paper_url": "http://arxiv.org/abs/2402.10920v1",
      "paper_abstract": "Large language models (LLMs) have made headlines for synthesizing correct-sounding responses to a variety of prompts, including code generation. In this paper, we present the prompts used to guide ChatGPT4 to produce a synthesizable and functional verilog description for the entirety of a programmable Spiking Neuron Array ASIC. This design flow showcases the current state of using ChatGPT4 for natural language driven hardware design. The AI-generated design was verified in simulation using handcrafted testbenches and has been submitted for fabrication in Skywater 130nm through Tiny Tapeout 5 using an open-source EDA flow.",
      "paper_authors": [
        "Michael Tomlinson",
        "Joe Li",
        "Andreas Andreou"
      ],
      "primary_category": "cs.AR",
      "publish_time": "2024-01-25",
      "update_time": "2024-01-25",
      "comments": null,
      "repo_url": "https://github.com/Andreou-JHULabOrg/tinytapeout_05_chatgpt_snn"
    },
    "2402.10069": {
      "paper_id": "2402.10069v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.10069v2",
      "paper_key": "2402.10069",
      "paper_title": "Learning fast changing slow in spiking neural networks",
      "paper_url": "http://arxiv.org/abs/2402.10069v2",
      "paper_abstract": "Reinforcement learning (RL) faces substantial challenges when applied to real-life problems, primarily stemming from the scarcity of available data due to limited interactions with the environment. This limitation is exacerbated by the fact that RL often demands a considerable volume of data for effective learning. The complexity escalates further when implementing RL in recurrent spiking networks, where inherent noise introduced by spikes adds a layer of difficulty. Life-long learning machines must inherently resolve the plasticity-stability paradox. Striking a balance between acquiring new knowledge and maintaining stability is crucial for artificial agents. To address this challenge, we draw inspiration from machine learning technology and introduce a biologically plausible implementation of proximal policy optimization, referred to as lf-cs (learning fast changing slow). Our approach results in two notable advancements: firstly, the capacity to assimilate new information into a new policy without requiring alterations to the current policy; and secondly, the capability to replay experiences without experiencing policy divergence. Furthermore, when contrasted with other experience replay (ER) techniques, our method demonstrates the added advantage of being computationally efficient in an online setting. We demonstrate that the proposed methodology enhances the efficiency of learning, showcasing its potential impact on neuromorphic and real-world applications.",
      "paper_authors": [
        "Cristiano Capone",
        "Paolo Muratore"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-01-25",
      "update_time": "2024-04-09",
      "comments": "14 pages, 4 figures",
      "repo_url": "#"
    },
    "2401.12559": {
      "paper_id": "2401.12559v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.12559v1",
      "paper_key": "2401.12559",
      "paper_title": "A robust balancing mechanism for spiking neural networks",
      "paper_url": "http://arxiv.org/abs/2401.12559v1",
      "paper_abstract": "Dynamical balance of excitation and inhibition is usually invoked to explain the irregular low firing activity observed in the cortex. We propose a robust nonlinear balancing mechanism for a random network of spiking neurons, which works also in absence of strong external currents. Biologically, the mechanism exploits the plasticity of excitatory-excitatory synapses induced by short-term depression. Mathematically, the nonlinear response of the synaptic activity is the key ingredient responsible for the emergence of a stable balanced regime. Our claim is supported by a simple self-consistent analysis accompanied by extensive simulations performed for increasing network sizes. The observed regime is essentially fluctuation driven and characterized by highly irregular spiking dynamics of all neurons.",
      "paper_authors": [
        "Antonio Politi",
        "Alessandro Torcini"
      ],
      "primary_category": "cond-mat.dis-nn",
      "publish_time": "2024-01-23",
      "update_time": "2024-01-23",
      "comments": "9 pages, 4 figures",
      "repo_url": "#"
    },
    "2401.11687": {
      "paper_id": "2401.11687v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.11687v3",
      "paper_key": "2401.11687",
      "paper_title": "TIM: An Efficient Temporal Interaction Module for Spiking Transformer",
      "paper_url": "http://arxiv.org/abs/2401.11687v3",
      "paper_abstract": "Spiking Neural Networks (SNNs), as the third generation of neural networks, have gained prominence for their biological plausibility and computational efficiency, especially in processing diverse datasets. The integration of attention mechanisms, inspired by advancements in neural network architectures, has led to the development of Spiking Transformers. These have shown promise in enhancing SNNs' capabilities, particularly in the realms of both static and neuromorphic datasets. Despite their progress, a discernible gap exists in these systems, specifically in the Spiking Self Attention (SSA) mechanism's effectiveness in leveraging the temporal processing potential of SNNs. To address this, we introduce the Temporal Interaction Module (TIM), a novel, convolution-based enhancement designed to augment the temporal data processing abilities within SNN architectures. TIM's integration into existing SNN frameworks is seamless and efficient, requiring minimal additional parameters while significantly boosting their temporal information handling capabilities. Through rigorous experimentation, TIM has demonstrated its effectiveness in exploiting temporal information, leading to state-of-the-art performance across various neuromorphic datasets. The code is available at https://github.com/BrainCog-X/Brain-Cog/tree/main/examples/TIM.",
      "paper_authors": [
        "Sicheng Shen",
        "Dongcheng Zhao",
        "Guobin Shen",
        "Yi Zeng"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-01-22",
      "update_time": "2024-05-09",
      "comments": "Accepted by the 33rd International Joint Conference on Artificial\n  Intelligence(IJCAI 2024)",
      "repo_url": "https://github.com/braincog-x/brain-cog"
    },
    "2402.09424": {
      "paper_id": "2402.09424v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.09424v1",
      "paper_key": "2402.09424",
      "paper_title": "Epilepsy Seizure Detection and Prediction using an Approximate Spiking Convolutional Transformer",
      "paper_url": "http://arxiv.org/abs/2402.09424v1",
      "paper_abstract": "Epilepsy is a common disease of the nervous system. Timely prediction of seizures and intervention treatment can significantly reduce the accidental injury of patients and protect the life and health of patients. This paper presents a neuromorphic Spiking Convolutional Transformer, named Spiking Conformer, to detect and predict epileptic seizure segments from scalped long-term electroencephalogram (EEG) recordings. We report evaluation results from the Spiking Conformer model using the Boston Children's Hospital-MIT (CHB-MIT) EEG dataset. By leveraging spike-based addition operations, the Spiking Conformer significantly reduces the classification computational cost compared to the non-spiking model. Additionally, we introduce an approximate spiking neuron layer to further reduce spike-triggered neuron updates by nearly 38% without sacrificing accuracy. Using raw EEG data as input, the proposed Spiking Conformer achieved an average sensitivity rate of 94.9% and a specificity rate of 99.3% for the seizure detection task, and 96.8%, 89.5% for the seizure prediction task, and needs >10x fewer operations compared to the non-spiking equivalent model.",
      "paper_authors": [
        "Qinyu Chen",
        "Congyi Sun",
        "Chang Gao",
        "Shih-Chii Liu"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2024-01-21",
      "update_time": "2024-01-21",
      "comments": "To be published at the 2024 IEEE International Symposium on Circuits\n  and Systems (ISCAS), Singapore",
      "repo_url": "#"
    },
    "2401.10793": {
      "paper_id": "2401.10793v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.10793v2",
      "paper_key": "2401.10793",
      "paper_title": "TDC-less Direct Time-of-Flight Imaging Using Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2401.10793v2",
      "paper_abstract": "3D depth sensors using single-photon avalanche diodes (SPADs) are becoming increasingly common in applications such as autonomous navigation and object detection. Recent designs implement on-chip histogramming time-to-digital converters (TDCs) to compress the photon timestamps and reduce the bottleneck in the read-out and processing of large volumes of photon data. However, the use of full histogramming with large SPAD arrays poses significant challenges due to the associated demands in silicon area and power consumption. We propose a TDC-less dToF sensor which uses Spiking Neural Networks (SNN) to process the SPAD events directly. The proposed SNN is trained and tested on synthetic SPAD events, and while it offers five times lower precision in depth prediction than a classic centre-of-mass (CoM) algorithm (applied to histograms of the events), it achieves similar Mean Absolute Error (MAE) with faster processing speeds and significantly lower power consumption is anticipated.",
      "paper_authors": [
        "Jack MacLean",
        "Brian Stewart",
        "Istvan Gyongy"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2024-01-19",
      "update_time": "2024-02-12",
      "comments": "7 Pages, 9 Figures",
      "repo_url": "#"
    },
    "2401.08001": {
      "paper_id": "2401.08001v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.08001v1",
      "paper_key": "2401.08001",
      "paper_title": "TT-SNN: Tensor Train Decomposition for Efficient Spiking Neural Network Training",
      "paper_url": "http://arxiv.org/abs/2401.08001v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) have gained significant attention as a potentially energy-efficient alternative for standard neural networks with their sparse binary activation. However, SNNs suffer from memory and computation overhead due to spatio-temporal dynamics and multiple backpropagation computations across timesteps during training. To address this issue, we introduce Tensor Train Decomposition for Spiking Neural Networks (TT-SNN), a method that reduces model size through trainable weight decomposition, resulting in reduced storage, FLOPs, and latency. In addition, we propose a parallel computation pipeline as an alternative to the typical sequential tensor computation, which can be flexibly integrated into various existing SNN architectures. To the best of our knowledge, this is the first of its kind application of tensor decomposition in SNNs. We validate our method using both static and dynamic datasets, CIFAR10/100 and N-Caltech101, respectively. We also propose a TT-SNN-tailored training accelerator to fully harness the parallelism in TT-SNN. Our results demonstrate substantial reductions in parameter size (7.98X), FLOPs (9.25X), training time (17.7%), and training energy (28.3%) during training for the N-Caltech101 dataset, with negligible accuracy degradation.",
      "paper_authors": [
        "Donghyun Lee",
        "Ruokai Yin",
        "Youngeun Kim",
        "Abhishek Moitra",
        "Yuhang Li",
        "Priyadarshini Panda"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-01-15",
      "update_time": "2024-01-15",
      "comments": null,
      "repo_url": "#"
    },
    "2401.06911": {
      "paper_id": "2401.06911v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.06911v1",
      "paper_key": "2401.06911",
      "paper_title": "Performance Evaluation of Neuromorphic Hardware for Onboard Satellite Communication Applications",
      "paper_url": "http://arxiv.org/abs/2401.06911v1",
      "paper_abstract": "Spiking neural networks (SNNs) implemented on neuromorphic processors (NPs) can enhance the energy efficiency of deployments of artificial intelligence (AI) for specific workloads. As such, NP represents an interesting opportunity for implementing AI tasks on board power-limited satellite communication spacecraft. In this article, we disseminate the findings of a recently completed study which targeted the comparison in terms of performance and power-consumption of different satellite communication use cases implemented on standard AI accelerators and on NPs. In particular, the article describes three prominent use cases, namely payload resource optimization, onboard interference detection and classification, and dynamic receive beamforming; and compare the performance of conventional convolutional neural networks (CNNs) implemented on Xilinx's VCK5000 Versal development card and SNNs on Intel's neuromorphic chip Loihi 2.",
      "paper_authors": [
        "Eva Lagunas",
        "Flor Ortiz",
        "Geoffrey Eappen",
        "Saed Daoud",
        "Wallace Alves Martins",
        "Jorge Querol",
        "Symeon Chatzinotas",
        "Nicolas Skatchkovsky",
        "Bipin Rajendran",
        "Osvaldo Simeone"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2024-01-12",
      "update_time": "2024-01-12",
      "comments": "submitted to IEEE Commun. Magazine",
      "repo_url": "#"
    },
    "2401.06563": {
      "paper_id": "2401.06563v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.06563v1",
      "paper_key": "2401.06563",
      "paper_title": "Resource-Efficient Gesture Recognition using Low-Resolution Thermal Camera via Spiking Neural Networks and Sparse Segmentation",
      "paper_url": "http://arxiv.org/abs/2401.06563v1",
      "paper_abstract": "This work proposes a novel approach for hand gesture recognition using an inexpensive, low-resolution (24 x 32) thermal sensor processed by a Spiking Neural Network (SNN) followed by Sparse Segmentation and feature-based gesture classification via Robust Principal Component Analysis (R-PCA). Compared to the use of standard RGB cameras, the proposed system is insensitive to lighting variations while being significantly less expensive compared to high-frequency radars, time-of-flight cameras and high-resolution thermal sensors previously used in literature. Crucially, this paper shows that the innovative use of the recently proposed Monostable Multivibrator (MMV) neural networks as a new class of SNN achieves more than one order of magnitude smaller memory and compute complexity compared to deep learning approaches, while reaching a top gesture recognition accuracy of 93.9% using a 5-class thermal camera dataset acquired in a car cabin, within an automotive context. Our dataset is released for helping future research.",
      "paper_authors": [
        "Ali Safa",
        "Wout Mommen",
        "Lars Keuninckx"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-01-12",
      "update_time": "2024-01-12",
      "comments": null,
      "repo_url": "#"
    },
    "2401.06471": {
      "paper_id": "2401.06471v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.06471v1",
      "paper_key": "2401.06471",
      "paper_title": "A Brain-inspired Computational Model for Human-like Concept Learning",
      "paper_url": "http://arxiv.org/abs/2401.06471v1",
      "paper_abstract": "Concept learning is a fundamental aspect of human cognition and plays a critical role in mental processes such as categorization, reasoning, memory, and decision-making. Researchers across various disciplines have shown consistent interest in the process of concept acquisition in individuals. To elucidate the mechanisms involved in human concept learning, this study examines the findings from computational neuroscience and cognitive psychology. These findings indicate that the brain's representation of concepts relies on two essential components: multisensory representation and text-derived representation. These two types of representations are coordinated by a semantic control system, ultimately leading to the acquisition of concepts. Drawing inspiration from this mechanism, the study develops a human-like computational model for concept learning based on spiking neural networks. By effectively addressing the challenges posed by diverse sources and imbalanced dimensionality of the two forms of concept representations, the study successfully attains human-like concept representations. Tests involving similar concepts demonstrate that our model, which mimics the way humans learn concepts, yields representations that closely align with human cognition.",
      "paper_authors": [
        "Yuwei Wang",
        "Yi Zeng"
      ],
      "primary_category": "cs.AI",
      "publish_time": "2024-01-12",
      "update_time": "2024-01-12",
      "comments": null,
      "repo_url": "#"
    },
    "2401.06808": {
      "paper_id": "2401.06808v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.06808v1",
      "paper_key": "2401.06808",
      "paper_title": "Grounded learning for compositional vector semantics",
      "paper_url": "http://arxiv.org/abs/2401.06808v1",
      "paper_abstract": "Categorical compositional distributional semantics is an approach to modelling language that combines the success of vector-based models of meaning with the compositional power of formal semantics. However, this approach was developed without an eye to cognitive plausibility. Vector representations of concepts and concept binding are also of interest in cognitive science, and have been proposed as a way of representing concepts within a biologically plausible spiking neural network. This work proposes a way for compositional distributional semantics to be implemented within a spiking neural network architecture, with the potential to address problems in concept binding, and give a small implementation. We also describe a means of training word representations using labelled images.",
      "paper_authors": [
        "Martha Lewis"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-01-10",
      "update_time": "2024-01-10",
      "comments": null,
      "repo_url": "#"
    },
    "2401.04628": {
      "paper_id": "2401.04628v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.04628v2",
      "paper_key": "2401.04628",
      "paper_title": "Multi-Neuron Representations of Hierarchical Concepts in Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2401.04628v2",
      "paper_abstract": "We describe how hierarchical concepts can be represented in three types of layered neural networks. The aim is to support recognition of the concepts when partial information about the concepts is presented, and also when some of the neurons in the network might fail. Our failure model involves initial random failures. The three types of networks are: feed-forward networks with high connectivity, feed-forward networks with low connectivity, and layered networks with low connectivity and with both forward edges and \"lateral\" edges within layers. In order to achieve fault-tolerance, the representations all use multiple representative neurons for each concept. We show how recognition can work in all three of these settings, and quantify how the probability of correct recognition depends on several parameters, including the number of representatives and the neuron failure probability. We also discuss how these representations might be learned, in all three types of networks. For the feed-forward networks, the learning algorithms are similar to ones used in [4], whereas for networks with lateral edges, the algorithms are generally inspired by work on the assembly calculus [3, 6, 7].",
      "paper_authors": [
        "Nancy A. Lynch"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-01-09",
      "update_time": "2024-04-11",
      "comments": "In the previous version, we had an unduly restrictive statement about\n  the order of choices of included edges, input set, and failing neurons. We\n  have loosened this restriction. We generalized the model slightly to allow\n  more freedom in choice of maximum layer number. We fixed a few typos and\n  polished wording in a few places",
      "repo_url": "#"
    },
    "2401.04491": {
      "paper_id": "2401.04491v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.04491v1",
      "paper_key": "2401.04491",
      "paper_title": "SpiNNaker2: A Large-Scale Neuromorphic System for Event-Based and Asynchronous Machine Learning",
      "paper_url": "http://arxiv.org/abs/2401.04491v1",
      "paper_abstract": "The joint progress of artificial neural networks (ANNs) and domain specific hardware accelerators such as GPUs and TPUs took over many domains of machine learning research. This development is accompanied by a rapid growth of the required computational demands for larger models and more data. Concurrently, emerging properties of foundation models such as in-context learning drive new opportunities for machine learning applications. However, the computational cost of such applications is a limiting factor of the technology in data centers, and more importantly in mobile devices and edge systems. To mediate the energy footprint and non-trivial latency of contemporary systems, neuromorphic computing systems deeply integrate computational principles of neurobiological systems by leveraging low-power analog and digital technologies. SpiNNaker2 is a digital neuromorphic chip developed for scalable machine learning. The event-based and asynchronous design of SpiNNaker2 allows the composition of large-scale systems involving thousands of chips. This work features the operating principles of SpiNNaker2 systems, outlining the prototype of novel machine learning applications. These applications range from ANNs over bio-inspired spiking neural networks to generalized event-based neural networks. With the successful development and deployment of SpiNNaker2, we aim to facilitate the advancement of event-based and asynchronous algorithms for future generations of machine learning systems.",
      "paper_authors": [
        "Hector A. Gonzalez",
        "Jiaxin Huang",
        "Florian Kelber",
        "Khaleelulla Khan Nazeer",
        "Tim Langer",
        "Chen Liu",
        "Matthias Lohrmann",
        "Amirhossein Rostami",
        "Mark Sch\u00f6ne",
        "Bernhard Vogginger",
        "Timo C. Wunderlich",
        "Yexin Yan",
        "Mahmoud Akl",
        "Christian Mayr"
      ],
      "primary_category": "cs.ET",
      "publish_time": "2024-01-09",
      "update_time": "2024-01-09",
      "comments": "Submitted at the Workshop on Machine Learning with New Compute\n  Paradigms at NeurIPS 2023 (MLNPCP 2023)",
      "repo_url": "#"
    },
    "2401.04486": {
      "paper_id": "2401.04486v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.04486v1",
      "paper_key": "2401.04486",
      "paper_title": "Take A Shortcut Back: Mitigating the Gradient Vanishing for Training Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2401.04486v1",
      "paper_abstract": "The Spiking Neural Network (SNN) is a biologically inspired neural network infrastructure that has recently garnered significant attention. It utilizes binary spike activations to transmit information, thereby replacing multiplications with additions and resulting in high energy efficiency. However, training an SNN directly poses a challenge due to the undefined gradient of the firing spike process. Although prior works have employed various surrogate gradient training methods that use an alternative function to replace the firing process during back-propagation, these approaches ignore an intrinsic problem: gradient vanishing. To address this issue, we propose a shortcut back-propagation method in our paper, which advocates for transmitting the gradient directly from the loss to the shallow layers. This enables us to present the gradient to the shallow layers directly, thereby significantly mitigating the gradient vanishing problem. Additionally, this method does not introduce any burden during the inference phase. To strike a balance between final accuracy and ease of training, we also propose an evolutionary training framework and implement it by inducing a balance coefficient that dynamically changes with the training epoch, which further improves the network's performance. Extensive experiments conducted over static and dynamic datasets using several popular network structures reveal that our method consistently outperforms state-of-the-art methods.",
      "paper_authors": [
        "Yufei Guo",
        "Yuanpei Chen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-01-09",
      "update_time": "2024-01-09",
      "comments": null,
      "repo_url": "#"
    },
    "2401.05444": {
      "paper_id": "2401.05444v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.05444v1",
      "paper_key": "2401.05444",
      "paper_title": "Fully Spiking Actor Network with Intra-layer Connections for Reinforcement Learning",
      "paper_url": "http://arxiv.org/abs/2401.05444v1",
      "paper_abstract": "With the help of special neuromorphic hardware, spiking neural networks (SNNs) are expected to realize artificial intelligence (AI) with less energy consumption. It provides a promising energy-efficient way for realistic control tasks by combining SNNs with deep reinforcement learning (DRL). In this paper, we focus on the task where the agent needs to learn multi-dimensional deterministic policies to control, which is very common in real scenarios. Recently, the surrogate gradient method has been utilized for training multi-layer SNNs, which allows SNNs to achieve comparable performance with the corresponding deep networks in this task. Most existing spike-based RL methods take the firing rate as the output of SNNs, and convert it to represent continuous action space (i.e., the deterministic policy) through a fully-connected (FC) layer. However, the decimal characteristic of the firing rate brings the floating-point matrix operations to the FC layer, making the whole SNN unable to deploy on the neuromorphic hardware directly. To develop a fully spiking actor network without any floating-point matrix operations, we draw inspiration from the non-spiking interneurons found in insects and employ the membrane voltage of the non-spiking neurons to represent the action. Before the non-spiking neurons, multiple population neurons are introduced to decode different dimensions of actions. Since each population is used to decode a dimension of action, we argue that the neurons in each population should be connected in time domain and space domain. Hence, the intra-layer connections are used in output populations to enhance the representation capacity. Finally, we propose a fully spiking actor network with intra-layer connections (ILC-SAN).",
      "paper_authors": [
        "Ding Chen",
        "Peixi Peng",
        "Tiejun Huang",
        "Yonghong Tian"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-01-09",
      "update_time": "2024-01-09",
      "comments": "13 pages, 6 figures",
      "repo_url": "#"
    },
    "2401.03719": {
      "paper_id": "2401.03719v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.03719v1",
      "paper_key": "2401.03719",
      "paper_title": "Enhancing Adaptive History Reserving by Spiking Convolutional Block Attention Module in Recurrent Neural Networks",
      "paper_url": "http://arxiv.org/abs/2401.03719v1",
      "paper_abstract": "Spiking neural networks (SNNs) serve as one type of efficient model to process spatio-temporal patterns in time series, such as the Address-Event Representation data collected from Dynamic Vision Sensor (DVS). Although convolutional SNNs have achieved remarkable performance on these AER datasets, benefiting from the predominant spatial feature extraction ability of convolutional structure, they ignore temporal features related to sequential time points. In this paper, we develop a recurrent spiking neural network (RSNN) model embedded with an advanced spiking convolutional block attention module (SCBAM) component to combine both spatial and temporal features of spatio-temporal patterns. It invokes the history information in spatial and temporal channels adaptively through SCBAM, which brings the advantages of efficient memory calling and history redundancy elimination. The performance of our model was evaluated in DVS128-Gesture dataset and other time-series datasets. The experimental results show that the proposed SRNN-SCBAM model makes better use of the history information in spatial and temporal dimensions with less memory space, and achieves higher accuracy compared to other models.",
      "paper_authors": [
        "Qi Xu",
        "Yuyuan Gao",
        "Jiangrong Shen",
        "Yaxin Li",
        "Xuming Ran",
        "Huajin Tang",
        "Gang Pan"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-01-08",
      "update_time": "2024-01-08",
      "comments": null,
      "repo_url": "#"
    },
    "2401.04134": {
      "paper_id": "2401.04134v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.04134v1",
      "paper_key": "2401.04134",
      "paper_title": "Web Neural Network with Complete DiGraphs",
      "paper_url": "http://arxiv.org/abs/2401.04134v1",
      "paper_abstract": "This paper introduces a new neural network model that aims to mimic the biological brain more closely by structuring the network as a complete directed graph that processes continuous data for each timestep. Current neural networks have structures that vaguely mimic the brain structure, such as neurons, convolutions, and recurrence. The model proposed in this paper adds additional structural properties by introducing cycles into the neuron connections and removing the sequential nature commonly seen in other network layers. Furthermore, the model has continuous input and output, inspired by spiking neural networks, which allows the network to learn a process of classification, rather than simply returning the final result.",
      "paper_authors": [
        "Frank Li"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-01-07",
      "update_time": "2024-01-07",
      "comments": null,
      "repo_url": "#"
    },
    "2401.10844": {
      "paper_id": "2401.10844v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.10844v1",
      "paper_key": "2401.10844",
      "paper_title": "Neural Population Decoding and Imbalanced Multi-Omic Datasets For Cancer Subtype Diagnosis",
      "paper_url": "http://arxiv.org/abs/2401.10844v1",
      "paper_abstract": "Recent strides in the field of neural computation has seen the adoption of Winner Take All (WTA) circuits to facilitate the unification of hierarchical Bayesian inference and spiking neural networks as a neurobiologically plausible model of information processing. Current research commonly validates the performance of these networks via classification tasks, particularly of the MNIST dataset. However, researchers have not yet reached consensus about how best to translate the stochastic responses from these networks into discrete decisions, a process known as population decoding. Despite being an often underexamined part of SNNs, in this work we show that population decoding has a significanct impact on the classification performance of WTA networks. For this purpose, we apply a WTA network to the problem of cancer subtype diagnosis from multi omic data, using datasets from The Cancer Genome Atlas (TCGA). In doing so we utilise a novel implementation of gene similarity networks, a feature encoding technique based on Kohoens self organising map algorithm. We further show that the impact of selecting certain population decoding methods is amplified when facing imbalanced datasets.",
      "paper_authors": [
        "Charles Theodore Kent",
        "Leila Bagheriye",
        "Johan Kwisthout"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-01-06",
      "update_time": "2024-01-06",
      "comments": "This paper has been accepted in BIOINFORMATICS 2024 (BIOSTEC 2024)",
      "repo_url": "#"
    },
    "2401.10843": {
      "paper_id": "2401.10843v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.10843v1",
      "paper_key": "2401.10843",
      "paper_title": "Training a General Spiking Neural Network with Improved Efficiency and Minimum Latency",
      "paper_url": "http://arxiv.org/abs/2401.10843v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) that operate in an event-driven manner and employ binary spike representation have recently emerged as promising candidates for energy-efficient computing. However, a cost bottleneck arises in obtaining high-performance SNNs: training a SNN model requires a large number of time steps in addition to the usual learning iterations, hence this limits their energy efficiency. This paper proposes a general training framework that enhances feature learning and activation efficiency within a limited time step, providing a new solution for more energy-efficient SNNs. Our framework allows SNN neurons to learn robust spike feature from different receptive fields and update neuron states by utilizing both current stimuli and recurrence information transmitted from other neurons. This setting continuously complements information within a single time step. Additionally, we propose a projection function to merge these two stimuli to smoothly optimize neuron weights (spike firing threshold and activation). We evaluate the proposal for both convolution and recurrent models. Our experimental results indicate state-of-the-art visual classification tasks, including CIFAR10, CIFAR100, and TinyImageNet.Our framework achieves 72.41% and 72.31% top-1 accuracy with only 1 time step on CIFAR100 for CNNs and RNNs, respectively. Our method reduces 10x and 3x joule energy than a standard ANN and SNN, respectively, on CIFAR10, without additional time steps.",
      "paper_authors": [
        "Yunpeng Yao",
        "Man Wu",
        "Zheng Chen",
        "Renyuan Zhang"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-01-05",
      "update_time": "2024-01-05",
      "comments": "Accepted by ACML 2023",
      "repo_url": "https://github.com/iverss1/ecml_snn"
    },
    "2401.02020": {
      "paper_id": "2401.02020v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.02020v1",
      "paper_key": "2401.02020",
      "paper_title": "Spikformer V2: Join the High Accuracy Club on ImageNet with an SNN Ticket",
      "paper_url": "http://arxiv.org/abs/2401.02020v1",
      "paper_abstract": "Spiking Neural Networks (SNNs), known for their biologically plausible architecture, face the challenge of limited performance. The self-attention mechanism, which is the cornerstone of the high-performance Transformer and also a biologically inspired structure, is absent in existing SNNs. To this end, we explore the potential of leveraging both self-attention capability and biological properties of SNNs, and propose a novel Spiking Self-Attention (SSA) and Spiking Transformer (Spikformer). The SSA mechanism eliminates the need for softmax and captures the sparse visual feature employing spike-based Query, Key, and Value. This sparse computation without multiplication makes SSA efficient and energy-saving. Further, we develop a Spiking Convolutional Stem (SCS) with supplementary convolutional layers to enhance the architecture of Spikformer. The Spikformer enhanced with the SCS is referred to as Spikformer V2. To train larger and deeper Spikformer V2, we introduce a pioneering exploration of Self-Supervised Learning (SSL) within the SNN. Specifically, we pre-train Spikformer V2 with masking and reconstruction style inspired by the mainstream self-supervised Transformer, and then finetune the Spikformer V2 on the image classification on ImageNet. Extensive experiments show that Spikformer V2 outperforms other previous surrogate training and ANN2SNN methods. An 8-layer Spikformer V2 achieves an accuracy of 80.38% using 4 time steps, and after SSL, a 172M 16-layer Spikformer V2 reaches an accuracy of 81.10% with just 1 time step. To the best of our knowledge, this is the first time that the SNN achieves 80+% accuracy on ImageNet. The code will be available at Spikformer V2.",
      "paper_authors": [
        "Zhaokun Zhou",
        "Kaiwei Che",
        "Wei Fang",
        "Keyu Tian",
        "Yuesheng Zhu",
        "Shuicheng Yan",
        "Yonghong Tian",
        "Li Yuan"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-01-04",
      "update_time": "2024-01-04",
      "comments": null,
      "repo_url": "https://github.com/zk-zhou/spikformer"
    },
    "2401.10904": {
      "paper_id": "2401.10904v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.10904v1",
      "paper_key": "2401.10904",
      "paper_title": "A Review of Findings from Neuroscience and Cognitive Psychology as Possible Inspiration for the Path to Artificial General Intelligence",
      "paper_url": "http://arxiv.org/abs/2401.10904v1",
      "paper_abstract": "This review aims to contribute to the quest for artificial general intelligence by examining neuroscience and cognitive psychology methods for potential inspiration. Despite the impressive advancements achieved by deep learning models in various domains, they still have shortcomings in abstract reasoning and causal understanding. Such capabilities should be ultimately integrated into artificial intelligence systems in order to surpass data-driven limitations and support decision making in a way more similar to human intelligence. This work is a vertical review that attempts a wide-ranging exploration of brain function, spanning from lower-level biological neurons, spiking neural networks, and neuronal ensembles to higher-level concepts such as brain anatomy, vector symbolic architectures, cognitive and categorization models, and cognitive architectures. The hope is that these concepts may offer insights for solutions in artificial general intelligence.",
      "paper_authors": [
        "Florin Leon"
      ],
      "primary_category": "cs.AI",
      "publish_time": "2024-01-03",
      "update_time": "2024-01-03",
      "comments": "143 pages, 49 figures, 244 references",
      "repo_url": "#"
    },
    "2401.01141": {
      "paper_id": "2401.01141v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.01141v1",
      "paper_key": "2401.01141",
      "paper_title": "Spiker+: a framework for the generation of efficient Spiking Neural Networks FPGA accelerators for inference at the edge",
      "paper_url": "http://arxiv.org/abs/2401.01141v1",
      "paper_abstract": "Including Artificial Neural Networks in embedded systems at the edge allows applications to exploit Artificial Intelligence capabilities directly within devices operating at the network periphery. This paper introduces Spiker+, a comprehensive framework for generating efficient, low-power, and low-area customized Spiking Neural Networks (SNN) accelerators on FPGA for inference at the edge. Spiker+ presents a configurable multi-layer hardware SNN, a library of highly efficient neuron architectures, and a design framework, enabling the development of complex neural network accelerators with few lines of Python code. Spiker+ is tested on two benchmark datasets, the MNIST and the Spiking Heidelberg Digits (SHD). On the MNIST, it demonstrates competitive performance compared to state-of-the-art SNN accelerators. It outperforms them in terms of resource allocation, with a requirement of 7,612 logic cells and 18 Block RAMs (BRAMs), which makes it fit in very small FPGA, and power consumption, draining only 180mW for a complete inference on an input image. The latency is comparable to the ones observed in the state-of-the-art, with 780us/img. To the authors' knowledge, Spiker+ is the first SNN accelerator tested on the SHD. In this case, the accelerator requires 18,268 logic cells and 51 BRAM, with an overall power consumption of 430mW and a latency of 54 us for a complete inference on input data. This underscores the significance of Spiker+ in the hardware-accelerated SNN landscape, making it an excellent solution to deploy configurable and tunable SNN architectures in resource and power-constrained edge applications.",
      "paper_authors": [
        "Alessio Carpegna",
        "Alessandro Savino",
        "Stefano Di Carlo"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-01-02",
      "update_time": "2024-01-02",
      "comments": null,
      "repo_url": "#"
    },
    "2401.01912": {
      "paper_id": "2401.01912v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.01912v1",
      "paper_key": "2401.01912",
      "paper_title": "Shrinking Your TimeStep: Towards Low-Latency Neuromorphic Object Recognition with Spiking Neural Network",
      "paper_url": "http://arxiv.org/abs/2401.01912v1",
      "paper_abstract": "Neuromorphic object recognition with spiking neural networks (SNNs) is the cornerstone of low-power neuromorphic computing. However, existing SNNs suffer from significant latency, utilizing 10 to 40 timesteps or more, to recognize neuromorphic objects. At low latencies, the performance of existing SNNs is drastically degraded. In this work, we propose the Shrinking SNN (SSNN) to achieve low-latency neuromorphic object recognition without reducing performance. Concretely, we alleviate the temporal redundancy in SNNs by dividing SNNs into multiple stages with progressively shrinking timesteps, which significantly reduces the inference latency. During timestep shrinkage, the temporal transformer smoothly transforms the temporal scale and preserves the information maximally. Moreover, we add multiple early classifiers to the SNN during training to mitigate the mismatch between the surrogate gradient and the true gradient, as well as the gradient vanishing/exploding, thus eliminating the performance degradation at low latency. Extensive experiments on neuromorphic datasets, CIFAR10-DVS, N-Caltech101, and DVS-Gesture have revealed that SSNN is able to improve the baseline accuracy by 6.55% ~ 21.41%. With only 5 average timesteps and without any data augmentation, SSNN is able to achieve an accuracy of 73.63% on CIFAR10-DVS. This work presents a heterogeneous temporal scale SNN and provides valuable insights into the development of high-performance, low-latency SNNs.",
      "paper_authors": [
        "Yongqi Ding",
        "Lin Zuo",
        "Mengmeng Jing",
        "Pei He",
        "Yongjun Xiao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-01-02",
      "update_time": "2024-01-02",
      "comments": "Accepted by AAAI 2024",
      "repo_url": "#"
    },
    "2401.00746": {
      "paper_id": "2401.00746v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.00746v1",
      "paper_key": "2401.00746",
      "paper_title": "Learn to integrate parts for whole through correlated neural variability",
      "paper_url": "http://arxiv.org/abs/2401.00746v1",
      "paper_abstract": "Sensory perception originates from the responses of sensory neurons, which react to a collection of sensory signals linked to various physical attributes of a singular perceptual object. Unraveling how the brain extracts perceptual information from these neuronal responses is a pivotal challenge in both computational neuroscience and machine learning. Here we introduce a statistical mechanical theory, where perceptual information is first encoded in the correlated variability of sensory neurons and then reformatted into the firing rates of downstream neurons. Applying this theory, we illustrate the encoding of motion direction using neural covariance and demonstrate high-fidelity direction recovery by spiking neural networks. Networks trained under this theory also show enhanced performance in classifying natural images, achieving higher accuracy and faster inference speed. Our results challenge the traditional view of neural covariance as a secondary factor in neural coding, highlighting its potential influence on brain function.",
      "paper_authors": [
        "Zhichao Zhu",
        "Yang Qi",
        "Wenlian Lu",
        "Jianfeng Feng"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2024-01-01",
      "update_time": "2024-01-01",
      "comments": "18 pages, 5 figures",
      "repo_url": "https://github.com/brainsoupfactory/moment-neural-network"
    },
    "2401.00369": {
      "paper_id": "2401.00369v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.00369v1",
      "paper_key": "2401.00369",
      "paper_title": "Analysis of biologically plausible neuron models for regression with spiking neural networks",
      "paper_url": "http://arxiv.org/abs/2401.00369v1",
      "paper_abstract": "This paper explores the impact of biologically plausible neuron models on the performance of Spiking Neural Networks (SNNs) for regression tasks. While SNNs are widely recognized for classification tasks, their application to Scientific Machine Learning and regression remains underexplored. We focus on the membrane component of SNNs, comparing four neuron models: Leaky Integrate-and-Fire, FitzHugh-Nagumo, Izhikevich, and Hodgkin-Huxley. We investigate their effect on SNN accuracy and efficiency for function regression tasks, by using Euler and Runge-Kutta 4th-order approximation schemes. We show how more biologically plausible neuron models improve the accuracy of SNNs while reducing the number of spikes in the system. The latter represents an energetic gain on actual neuromorphic chips since it directly reflects the amount of energy required for the computations.",
      "paper_authors": [
        "Mario De Florio",
        "Adar Kahana",
        "George Em Karniadakis"
      ],
      "primary_category": "math.NA",
      "publish_time": "2023-12-31",
      "update_time": "2023-12-31",
      "comments": null,
      "repo_url": "#"
    },
    "2312.17582": {
      "paper_id": "2312.17582v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.17582v1",
      "paper_key": "2312.17582",
      "paper_title": "Darwin3: A large-scale neuromorphic chip with a Novel ISA and On-Chip Learning",
      "paper_url": "http://arxiv.org/abs/2312.17582v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) are gaining increasing attention for their biological plausibility and potential for improved computational efficiency. To match the high spatial-temporal dynamics in SNNs, neuromorphic chips are highly desired to execute SNNs in hardware-based neuron and synapse circuits directly. This paper presents a large-scale neuromorphic chip named Darwin3 with a novel instruction set architecture(ISA), which comprises 10 primary instructions and a few extended instructions. It supports flexible neuron model programming and local learning rule designs. The Darwin3 chip architecture is designed in a mesh of computing nodes with an innovative routing algorithm. We used a compression mechanism to represent synaptic connections, significantly reducing memory usage. The Darwin3 chip supports up to 2.35 million neurons, making it the largest of its kind in neuron scale. The experimental results showed that code density was improved up to 28.3x in Darwin3, and neuron core fan-in and fan-out were improved up to 4096x and 3072x by connection compression compared to the physical memory depth. Our Darwin3 chip also provided memory saving between 6.8X and 200.8X when mapping convolutional spiking neural networks (CSNN) onto the chip, demonstrating state-of-the-art performance in accuracy and latency compared to other neuromorphic chips.",
      "paper_authors": [
        "De Ma",
        "Xiaofei Jin",
        "Shichun Sun",
        "Yitao Li",
        "Xundong Wu",
        "Youneng Hu",
        "Fangchao Yang",
        "Huajin Tang",
        "Xiaolei Zhu",
        "Peng Lin",
        "Gang Pan"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-12-29",
      "update_time": "2023-12-29",
      "comments": null,
      "repo_url": "#"
    },
    "2312.17216": {
      "paper_id": "2312.17216v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.17216v1",
      "paper_key": "2312.17216",
      "paper_title": "SparseProp: Efficient Event-Based Simulation and Training of Sparse Recurrent Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2312.17216v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) are biologically-inspired models that are capable of processing information in streams of action potentials. However, simulating and training SNNs is computationally expensive due to the need to solve large systems of coupled differential equations. In this paper, we introduce SparseProp, a novel event-based algorithm for simulating and training sparse SNNs. Our algorithm reduces the computational cost of both the forward and backward pass operations from O(N) to O(log(N)) per network spike, thereby enabling numerically exact simulations of large spiking networks and their efficient training using backpropagation through time. By leveraging the sparsity of the network, SparseProp eliminates the need to iterate through all neurons at each spike, employing efficient state updates instead. We demonstrate the efficacy of SparseProp across several classical integrate-and-fire neuron models, including a simulation of a sparse SNN with one million LIF neurons. This results in a speed-up exceeding four orders of magnitude relative to previous event-based implementations. Our work provides an efficient and exact solution for training large-scale spiking neural networks and opens up new possibilities for building more sophisticated brain-inspired models.",
      "paper_authors": [
        "Rainer Engelken"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2023-12-28",
      "update_time": "2023-12-28",
      "comments": "10 pages, 4 figures, accepted at NeurIPS",
      "repo_url": "https://github.com/rainerengelken/sparseprop"
    },
    "2401.10748": {
      "paper_id": "2401.10748v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.10748v2",
      "paper_key": "2401.10748",
      "paper_title": "Fast gradient-free activation maximization for neurons in spiking neural networks",
      "paper_url": "http://arxiv.org/abs/2401.10748v2",
      "paper_abstract": "Elements of neural networks, both biological and artificial, can be described by their selectivity for specific cognitive features. Understanding these features is important for understanding the inner workings of neural networks. For a living system, such as a neuron, whose response to a stimulus is unknown and not differentiable, the only way to reveal these features is through a feedback loop that exposes it to a large set of different stimuli. The properties of these stimuli should be varied iteratively in order to maximize the neuronal response. To utilize this feedback loop for a biological neural network, it is important to run it quickly and efficiently in order to reach the stimuli that maximizes certain neurons' activation with the least number of iterations possible. Here we present a framework with an efficient design for such a loop. We successfully tested it on an artificial spiking neural network (SNN), which is a model that simulates the asynchronous spiking activity of neurons in living brains. Our optimization method for activation maximization is based on the low-rank Tensor Train decomposition of the discrete activation function. The optimization space is the latent parameter space of images generated by SN-GAN or VQ-VAE generative models. To our knowledge, this is the first time that effective AM has been applied to SNNs. We track changes in the optimal stimuli for artificial neurons during training and show that highly selective neurons can form already in the early epochs of training and in the early layers of a convolutional spiking network. This formation of refined optimal stimuli is associated with an increase in classification accuracy. Some neurons, especially in the deeper layers, may gradually change the concepts they are selective for during learning, potentially explaining their importance for model performance.",
      "paper_authors": [
        "Nikita Pospelov",
        "Andrei Chertkov",
        "Maxim Beketov",
        "Ivan Oseledets",
        "Konstantin Anokhin"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-12-28",
      "update_time": "2024-06-25",
      "comments": null,
      "repo_url": "https://github.com/iabs-neuro/mango"
    },
    "2312.16071": {
      "paper_id": "2312.16071v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.16071v1",
      "paper_key": "2312.16071",
      "paper_title": "Event-based Shape from Polarization with Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2312.16071v1",
      "paper_abstract": "Recent advances in event-based shape determination from polarization offer a transformative approach that tackles the trade-off between speed and accuracy in capturing surface geometries. In this paper, we investigate event-based shape from polarization using Spiking Neural Networks (SNNs), introducing the Single-Timestep and Multi-Timestep Spiking UNets for effective and efficient surface normal estimation. Specificially, the Single-Timestep model processes event-based shape as a non-temporal task, updating the membrane potential of each spiking neuron only once, thereby reducing computational and energy demands. In contrast, the Multi-Timestep model exploits temporal dynamics for enhanced data extraction. Extensive evaluations on synthetic and real-world datasets demonstrate that our models match the performance of state-of-the-art Artifical Neural Networks (ANNs) in estimating surface normals, with the added advantage of superior energy efficiency. Our work not only contributes to the advancement of SNNs in event-based sensing but also sets the stage for future explorations in optimizing SNN architectures, integrating multi-modal data, and scaling for applications on neuromorphic hardware.",
      "paper_authors": [
        "Peng Kang",
        "Srutarshi Banerjee",
        "Henry Chopp",
        "Aggelos Katsaggelos",
        "Oliver Cossairt"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-12-26",
      "update_time": "2023-12-26",
      "comments": "25 pages",
      "repo_url": "#"
    },
    "2401.10257": {
      "paper_id": "2401.10257v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.10257v1",
      "paper_key": "2401.10257",
      "paper_title": "Curriculum Design Helps Spiking Neural Networks to Classify Time Series",
      "paper_url": "http://arxiv.org/abs/2401.10257v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) have a greater potential for modeling time series data than Artificial Neural Networks (ANNs), due to their inherent neuron dynamics and low energy consumption. However, it is difficult to demonstrate their superiority in classification accuracy, because current efforts mainly focus on designing better network structures. In this work, enlighten by brain-inspired science, we find that, not only the structure but also the learning process should be human-like. To achieve this, we investigate the power of Curriculum Learning (CL) on SNNs by designing a novel method named CSNN with two theoretically guaranteed mechanisms: The active-to-dormant training order makes the curriculum similar to that of human learning and suitable for spiking neurons; The value-based regional encoding makes the neuron activity to mimic the brain memory when learning sequential data. Experiments on multiple time series sources including simulated, sensor, motion, and healthcare demonstrate that CL has a more positive effect on SNNs than ANNs with about twice the accuracy change, and CSNN can increase about 3% SNNs' accuracy by improving network sparsity, neuron firing status, anti-noise ability, and convergence speed.",
      "paper_authors": [
        "Chenxi Sun",
        "Hongyan Li",
        "Moxian Song",
        "Derun Can",
        "Shenda Hong"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-12-26",
      "update_time": "2023-12-26",
      "comments": "11 pages, 3 figures",
      "repo_url": "#"
    },
    "2312.15805": {
      "paper_id": "2312.15805v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.15805v2",
      "paper_key": "2312.15805",
      "paper_title": "Astrocyte Regulated Neuromorphic Central Pattern Generator Control of Legged Robotic Locomotion",
      "paper_url": "http://arxiv.org/abs/2312.15805v2",
      "paper_abstract": "Neuromorphic computing systems, where information is transmitted through action potentials in a bio-plausible fashion, is gaining increasing interest due to its promise of low-power event-driven computing. Application of neuromorphic computing in robotic locomotion research have largely focused on Central Pattern Generators (CPGs) for bionics robotic control algorithms - inspired from neural circuits governing the collaboration of the limb muscles in animal movement. Implementation of artificial CPGs on neuromorphic hardware platforms can potentially enable adaptive and energy-efficient edge robotics applications in resource constrained environments. However, underlying rewiring mechanisms in CPG for gait emergence process is not well understood. This work addresses the missing gap in literature pertaining to CPG plasticity and underscores the critical homeostatic functionality of astrocytes - a cellular component in the brain that is believed to play a major role in multiple brain functions. This paper introduces an astrocyte regulated Spiking Neural Network (SNN)-based CPG for learning locomotion gait through Reward-Modulated STDP for quadruped robots, where the astrocytes help build inhibitory connections among the artificial motor neurons in different limbs. The SNN-based CPG is simulated on a multi-object physics simulation platform resulting in the emergence of a trotting gait while running the robot on flat ground. $23.3\\times$ computational power savings is observed in comparison to a state-of-the-art reinforcement learning based robot control algorithm. Such a neuroscience-algorithm co-design approach can potentially enable a quantum leap in the functionality of neuromorphic systems incorporating glial cell functionality.",
      "paper_authors": [
        "Zhuangyu Han",
        "Abhronil Sengupta"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-12-25",
      "update_time": "2024-01-05",
      "comments": null,
      "repo_url": "#"
    },
    "2401.08649": {
      "paper_id": "2401.08649v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.08649v1",
      "paper_key": "2401.08649",
      "paper_title": "Deep Pulse-Coupled Neural Networks",
      "paper_url": "http://arxiv.org/abs/2401.08649v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) capture the information processing mechanism of the brain by taking advantage of spiking neurons, such as the Leaky Integrate-and-Fire (LIF) model neuron, which incorporates temporal dynamics and transmits information via discrete and asynchronous spikes. However, the simplified biological properties of LIF ignore the neuronal coupling and dendritic structure of real neurons, which limits the spatio-temporal dynamics of neurons and thus reduce the expressive power of the resulting SNNs. In this work, we leverage a more biologically plausible neural model with complex dynamics, i.e., a pulse-coupled neural network (PCNN), to improve the expressiveness and recognition performance of SNNs for vision tasks. The PCNN is a type of cortical model capable of emulating the complex neuronal activities in the primary visual cortex. We construct deep pulse-coupled neural networks (DPCNNs) by replacing commonly used LIF neurons in SNNs with PCNN neurons. The intra-coupling in existing PCNN models limits the coupling between neurons only within channels. To address this limitation, we propose inter-channel coupling, which allows neurons in different feature maps to interact with each other. Experimental results show that inter-channel coupling can efficiently boost performance with fewer neurons, synapses, and less training time compared to widening the networks. For instance, compared to the LIF-based SNN with wide VGG9, DPCNN with VGG9 uses only 50%, 53%, and 73% of neurons, synapses, and training time, respectively. Furthermore, we propose receptive field and time dependent batch normalization (RFTD-BN) to speed up the convergence and performance of DPCNNs.",
      "paper_authors": [
        "Zexiang Yi",
        "Jing Lian",
        "Yunliang Qi",
        "Zhaofei Yu",
        "Huajin Tang",
        "Yide Ma",
        "Jizhao Liu"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-12-24",
      "update_time": "2023-12-24",
      "comments": null,
      "repo_url": "#"
    },
    "2312.14548": {
      "paper_id": "2312.14548v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.14548v1",
      "paper_key": "2312.14548",
      "paper_title": "NeuroRIS: Neuromorphic-Inspired Metasurfaces",
      "paper_url": "http://arxiv.org/abs/2312.14548v1",
      "paper_abstract": "Reconfigurable intelligent surfaces (RISs) operate similarly to electromagnetic (EM) mirrors and remarkably go beyond Snell law to generate an applicable EM environment allowing for flexible adaptation and fostering sustainability in terms of economic deployment and energy efficiency. However, the conventional RIS is controlled through high-latency field programmable gate array or micro-controller circuits usually implementing artificial neural networks (ANNs) for tuning the RIS phase array that have also very high energy requirements. Most importantly, conventional RIS are unable to function under realistic scenarios i.e, high-mobility/low-end user equipment (UE). In this paper, we benefit from the advanced computing power of neuromorphic processors and design a new type of RIS named \\emph{NeuroRIS}, to supporting high mobility UEs through real time adaptation to the ever-changing wireless channel conditions. To this end, the neuromorphic processing unit tunes all the RIS meta-elements in the orders of $\\rm{ns}$ for particular switching circuits e.g., varactors while exhibiting significantly low energy requirements since it is based on event-driven processing through spiking neural networks for accurate and efficient phase-shift vector design. Numerical results show that the NeuroRIS achieves very close rate performance to a conventional RIS-based on ANNs, while requiring significantly reduced energy consumption with the latter.",
      "paper_authors": [
        "Christos G. Tsinos",
        "Alexandros-Apostolos A. Boulogeorgos",
        "Theodoros A. Tsiftsis"
      ],
      "primary_category": "eess.SY",
      "publish_time": "2023-12-22",
      "update_time": "2023-12-22",
      "comments": "4 pages, 3 figures",
      "repo_url": "#"
    },
    "2312.14261": {
      "paper_id": "2312.14261v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.14261v1",
      "paper_key": "2312.14261",
      "paper_title": "Low-power event-based face detection with asynchronous neuromorphic hardware",
      "paper_url": "http://arxiv.org/abs/2312.14261v1",
      "paper_abstract": "The rise of mobility, IoT and wearables has shifted processing to the edge of the sensors, driven by the need to reduce latency, communication costs and overall energy consumption. While deep learning models have achieved remarkable results in various domains, their deployment at the edge for real-time applications remains computationally expensive. Neuromorphic computing emerges as a promising paradigm shift, characterized by co-localized memory and computing as well as event-driven asynchronous sensing and processing. In this work, we demonstrate the possibility of solving the ubiquitous computer vision task of object detection at the edge with low-power requirements, using the event-based N-Caltech101 dataset. We present the first instance of an on-chip spiking neural network for event-based face detection deployed on the SynSense Speck neuromorphic chip, which comprises both an event-based sensor and a spike-based asynchronous processor implementing Integrate-and-Fire neurons. We show how to reduce precision discrepancies between off-chip clock-driven simulation used for training and on-chip event-driven inference. This involves using a multi-spike version of the Integrate-and-Fire neuron on simulation, where spikes carry values that are proportional to the extent the membrane potential exceeds the firing threshold. We propose a robust strategy to train spiking neural networks with back-propagation through time using multi-spike activation and firing rate regularization and demonstrate how to decode output spikes into bounding boxes. We show that the power consumption of the chip is directly proportional to the number of synaptic operations in the spiking neural network, and we explore the trade-off between power consumption and detection precision with different firing rate regularization, achieving an on-chip face detection mAP[0.5] of ~0.6 while consuming only ~20 mW.",
      "paper_authors": [
        "Caterina Caccavella",
        "Federico Paredes-Vall\u00e9s",
        "Marco Cannici",
        "Lyes Khacef"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-12-21",
      "update_time": "2023-12-21",
      "comments": null,
      "repo_url": "#"
    },
    "2312.12909": {
      "paper_id": "2312.12909v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.12909v1",
      "paper_key": "2312.12909",
      "paper_title": "Energy-efficient Spiking Neural Network Equalization for IM/DD Systems with Optimized Neural Encoding",
      "paper_url": "http://arxiv.org/abs/2312.12909v1",
      "paper_abstract": "We propose an energy-efficient equalizer for IM/DD systems based on spiking neural networks. We optimize a neural spike encoding that boosts the equalizer's performance while decreasing energy consumption.",
      "paper_authors": [
        "Alexander von Bank",
        "Eike-Manuel Edelmann",
        "Laurent Schmalen"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2023-12-20",
      "update_time": "2023-12-20",
      "comments": "Accepted for publication at OFC 2024",
      "repo_url": "https://github.com/kit-cel/optispike"
    },
    "2401.05373": {
      "paper_id": "2401.05373v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.05373v3",
      "paper_key": "2401.05373",
      "paper_title": "Dynamic Spiking Framework for Graph Neural Networks",
      "paper_url": "http://arxiv.org/abs/2401.05373v3",
      "paper_abstract": "The integration of Spiking Neural Networks (SNNs) and Graph Neural Networks (GNNs) is gradually attracting attention due to the low power consumption and high efficiency in processing the non-Euclidean data represented by graphs. However, as a common problem, dynamic graph representation learning faces challenges such as high complexity and large memory overheads. Current work often uses SNNs instead of Recurrent Neural Networks (RNNs) by using binary features instead of continuous ones for efficient training, which would overlooks graph structure information and leads to the loss of details during propagation. Additionally, optimizing dynamic spiking models typically requires propagation of information across time steps, which increases memory requirements. To address these challenges, we present a framework named \\underline{Dy}namic \\underline{S}p\\underline{i}king \\underline{G}raph \\underline{N}eural Networks (\\method{}). To mitigate the information loss problem, \\method{} propagates early-layer information directly to the last layer for information compensation. To accommodate the memory requirements, we apply the implicit differentiation on the equilibrium state, which does not rely on the exact reverse of the forward computation. While traditional implicit differentiation methods are usually used for static situations, \\method{} extends it to the dynamic graph setting. Extensive experiments on three large-scale real-world dynamic graph datasets validate the effectiveness of \\method{} on dynamic node classification tasks with lower computational costs.",
      "paper_authors": [
        "Nan Yin",
        "Mengzhu Wang",
        "Zhenghan Chen",
        "Giulia De Masi",
        "Bin Gu",
        "Huan Xiong"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-12-15",
      "update_time": "2024-07-30",
      "comments": null,
      "repo_url": "#"
    },
    "2312.09391": {
      "paper_id": "2312.09391v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.09391v1",
      "paper_key": "2312.09391",
      "paper_title": "Exploiting Symmetric Temporally Sparse BPTT for Efficient RNN Training",
      "paper_url": "http://arxiv.org/abs/2312.09391v1",
      "paper_abstract": "Recurrent Neural Networks (RNNs) are useful in temporal sequence tasks. However, training RNNs involves dense matrix multiplications which require hardware that can support a large number of arithmetic operations and memory accesses. Implementing online training of RNNs on the edge calls for optimized algorithms for an efficient deployment on hardware. Inspired by the spiking neuron model, the Delta RNN exploits temporal sparsity during inference by skipping over the update of hidden states from those inactivated neurons whose change of activation across two timesteps is below a defined threshold. This work describes a training algorithm for Delta RNNs that exploits temporal sparsity in the backward propagation phase to reduce computational requirements for training on the edge. Due to the symmetric computation graphs of forward and backward propagation during training, the gradient computation of inactivated neurons can be skipped. Results show a reduction of $\\sim$80% in matrix operations for training a 56k parameter Delta LSTM on the Fluent Speech Commands dataset with negligible accuracy loss. Logic simulations of a hardware accelerator designed for the training algorithm show 2-10X speedup in matrix computations for an activation sparsity range of 50%-90%. Additionally, we show that the proposed Delta RNN training will be useful for online incremental learning on edge devices with limited computing resources.",
      "paper_authors": [
        "Xi Chen",
        "Chang Gao",
        "Zuowen Wang",
        "Longbiao Cheng",
        "Sheng Zhou",
        "Shih-Chii Liu",
        "Tobi Delbruck"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2023-12-14",
      "update_time": "2023-12-14",
      "comments": "Accepted by the 38th Annual AAAI Conference on Artificial\n  Intelligence (AAAI-24)",
      "repo_url": "#"
    },
    "2312.09084": {
      "paper_id": "2312.09084v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.09084v3",
      "paper_key": "2312.09084",
      "paper_title": "Language Modeling on a SpiNNaker 2 Neuromorphic Chip",
      "paper_url": "http://arxiv.org/abs/2312.09084v3",
      "paper_abstract": "As large language models continue to scale in size rapidly, so too does the computational power required to run them. Event-based networks on neuromorphic devices offer a potential way to reduce energy consumption for inference significantly. However, to date, most event-based networks that can run on neuromorphic hardware, including spiking neural networks (SNNs), have not achieved task performance even on par with LSTM models for language modeling. As a result, language modeling on neuromorphic devices has seemed a distant prospect. In this work, we demonstrate the first-ever implementation of a language model on a neuromorphic device - specifically the SpiNNaker 2 chip - based on a recently published event-based architecture called the EGRU. SpiNNaker 2 is a many-core neuromorphic chip designed for large-scale asynchronous processing, while the EGRU is architected to leverage such hardware efficiently while maintaining competitive task performance. This implementation marks the first time a neuromorphic language model matches LSTMs, setting the stage for taking task performance to the level of large language models. We also demonstrate results on a gesture recognition task based on inputs from a DVS camera. Overall, our results showcase the feasibility of this neuro-inspired neural network in hardware, highlighting significant gains versus conventional hardware in energy efficiency for the common use case of single batch inference.",
      "paper_authors": [
        "Khaleelulla Khan Nazeer",
        "Mark Sch\u00f6ne",
        "Rishav Mukherji",
        "Bernhard Vogginger",
        "Christian Mayr",
        "David Kappel",
        "Anand Subramoney"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-12-14",
      "update_time": "2024-01-24",
      "comments": null,
      "repo_url": "#"
    },
    "2312.08960": {
      "paper_id": "2312.08960v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.08960v1",
      "paper_key": "2312.08960",
      "paper_title": "DenRAM: Neuromorphic Dendritic Architecture with RRAM for Efficient Temporal Processing with Delays",
      "paper_url": "http://arxiv.org/abs/2312.08960v1",
      "paper_abstract": "An increasing number of neuroscience studies are highlighting the importance of spatial dendritic branching in pyramidal neurons in the brain for supporting non-linear computation through localized synaptic integration. In particular, dendritic branches play a key role in temporal signal processing and feature detection, using coincidence detection (CD) mechanisms, made possible by the presence of synaptic delays that align temporally disparate inputs for effective integration. Computational studies on spiking neural networks further highlight the significance of delays for CD operations, enabling spatio-temporal pattern recognition within feed-forward neural networks without the need for recurrent architectures. In this work, we present DenRAM, the first realization of a spiking neural network with analog dendritic circuits, integrated into a 130nm technology node coupled with resistive memory (RRAM) technology. DenRAM's dendritic circuits use the RRAM devices to implement both delays and synaptic weights in the network. By configuring the RRAM devices to reproduce bio-realistic timescales, and through exploiting their heterogeneity, we experimentally demonstrate DenRAM's capability to replicate synaptic delay profiles, and efficiently implement CD for spatio-temporal pattern recognition. To validate the architecture, we conduct comprehensive system-level simulations on two representative temporal benchmarks, highlighting DenRAM's resilience to analog hardware noise, and its superior accuracy compared to recurrent architectures with an equivalent number of parameters. DenRAM not only brings rich temporal processing capabilities to neuromorphic architectures, but also reduces the memory footprint of edge devices, provides high accuracy on temporal benchmarks, and represents a significant step-forward in low-power real-time signal processing technologies.",
      "paper_authors": [
        "Simone DAgostino",
        "Filippo Moro",
        "Tristan Torchet",
        "Yigit Demirag",
        "Laurent Grenouillet",
        "Giacomo Indiveri",
        "Elisa Vianello",
        "Melika Payvand"
      ],
      "primary_category": "cs.ET",
      "publish_time": "2023-12-14",
      "update_time": "2023-12-14",
      "comments": null,
      "repo_url": "#"
    },
    "2401.00955": {
      "paper_id": "2401.00955v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.00955v1",
      "paper_key": "2401.00955",
      "paper_title": "Learning Long Sequences in Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2401.00955v1",
      "paper_abstract": "Spiking neural networks (SNNs) take inspiration from the brain to enable energy-efficient computations. Since the advent of Transformers, SNNs have struggled to compete with artificial networks on modern sequential tasks, as they inherit limitations from recurrent neural networks (RNNs), with the added challenge of training with non-differentiable binary spiking activations. However, a recent renewed interest in efficient alternatives to Transformers has given rise to state-of-the-art recurrent architectures named state space models (SSMs). This work systematically investigates, for the first time, the intersection of state-of-the-art SSMs with SNNs for long-range sequence modelling. Results suggest that SSM-based SNNs can outperform the Transformer on all tasks of a well-established long-range sequence modelling benchmark. It is also shown that SSM-based SNNs can outperform current state-of-the-art SNNs with fewer parameters on sequential image classification. Finally, a novel feature mixing layer is introduced, improving SNN accuracy while challenging assumptions about the role of binary activations in SNNs. This work paves the way for deploying powerful SSM-based architectures, such as large language models, to neuromorphic hardware for energy-efficient long-range sequence modelling.",
      "paper_authors": [
        "Matei Ioan Stan",
        "Oliver Rhodes"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-12-14",
      "update_time": "2023-12-14",
      "comments": "18 pages, 10 Figures/Tables",
      "repo_url": "#"
    },
    "2312.08213": {
      "paper_id": "2312.08213v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.08213v2",
      "paper_key": "2312.08213",
      "paper_title": "Accelerated Event-Based Feature Detection and Compression for Surveillance Video Systems",
      "paper_url": "http://arxiv.org/abs/2312.08213v2",
      "paper_abstract": "The strong temporal consistency of surveillance video enables compelling compression performance with traditional methods, but downstream vision applications operate on decoded image frames with a high data rate. Since it is not straightforward for applications to extract information on temporal redundancy from the compressed video representations, we propose a novel system which conveys temporal redundancy within a sparse decompressed representation. We leverage a video representation framework called ADDER to transcode framed videos to sparse, asynchronous intensity samples. We introduce mechanisms for content adaptation, lossy compression, and asynchronous forms of classical vision algorithms. We evaluate our system on the VIRAT surveillance video dataset, and we show a median 43.7% speed improvement in FAST feature detection compared to OpenCV. We run the same algorithm as OpenCV, but only process pixels that receive new asynchronous events, rather than process every pixel in an image frame. Our work paves the way for upcoming neuromorphic sensors and is amenable to future applications with spiking neural networks.",
      "paper_authors": [
        "Andrew C. Freeman",
        "Ketan Mayer-Patel",
        "Montek Singh"
      ],
      "primary_category": "cs.MM",
      "publish_time": "2023-12-13",
      "update_time": "2024-02-08",
      "comments": "Accepted for publication in the proceedings of ACM Multimedia Systems\n  '24",
      "repo_url": "#"
    },
    "2312.07922": {
      "paper_id": "2312.07922v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.07922v1",
      "paper_key": "2312.07922",
      "paper_title": "Memory-Efficient Reversible Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2312.07922v1",
      "paper_abstract": "Spiking neural networks (SNNs) are potential competitors to artificial neural networks (ANNs) due to their high energy-efficiency on neuromorphic hardware. However, SNNs are unfolded over simulation time steps during the training process. Thus, SNNs require much more memory than ANNs, which impedes the training of deeper SNN models. In this paper, we propose the reversible spiking neural network to reduce the memory cost of intermediate activations and membrane potentials during training. Firstly, we extend the reversible architecture along temporal dimension and propose the reversible spiking block, which can reconstruct the computational graph and recompute all intermediate variables in forward pass with a reverse process. On this basis, we adopt the state-of-the-art SNN models to the reversible variants, namely reversible spiking ResNet (RevSResNet) and reversible spiking transformer (RevSFormer). Through experiments on static and neuromorphic datasets, we demonstrate that the memory cost per image of our reversible SNNs does not increase with the network depth. On CIFAR10 and CIFAR100 datasets, our RevSResNet37 and RevSFormer-4-384 achieve comparable accuracies and consume 3.79x and 3.00x lower GPU memory per image than their counterparts with roughly identical model complexity and parameters. We believe that this work can unleash the memory constraints in SNN training and pave the way for training extremely large and deep SNNs. The code is available at https://github.com/mi804/RevSNN.git.",
      "paper_authors": [
        "Hong Zhang",
        "Yu Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-12-13",
      "update_time": "2023-12-13",
      "comments": "accepted by AAAI2024",
      "repo_url": "https://github.com/mi804/revsnn"
    },
    "2312.07466": {
      "paper_id": "2312.07466v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.07466v1",
      "paper_key": "2312.07466",
      "paper_title": "Efficient Object Detection in Autonomous Driving using Spiking Neural Networks: Performance, Energy Consumption Analysis, and Insights into Open-set Object Discovery",
      "paper_url": "http://arxiv.org/abs/2312.07466v1",
      "paper_abstract": "Besides performance, efficiency is a key design driver of technologies supporting vehicular perception. Indeed, a well-balanced trade-off between performance and energy consumption is crucial for the sustainability of autonomous vehicles. In this context, the diversity of real-world contexts in which autonomous vehicles can operate motivates the need for empowering perception models with the capability to detect, characterize and identify newly appearing objects by themselves. In this manuscript we elaborate on this threefold conundrum (performance, efficiency and open-world learning) for object detection modeling tasks over image data collected from vehicular scenarios. Specifically, we show that well-performing and efficient models can be realized by virtue of Spiking Neural Networks (SNNs), reaching competitive levels of detection performance when compared to their non-spiking counterparts at dramatic energy consumption savings (up to 85%) and a slightly improved robustness against image noise. Our experiments herein offered also expose qualitatively the complexity of detecting new objects based on the preliminary results of a simple approach to discriminate potential object proposals in the captured image.",
      "paper_authors": [
        "Aitor Martinez Seras",
        "Javier Del Ser",
        "Pablo Garcia-Bringas"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-12-12",
      "update_time": "2023-12-12",
      "comments": "8 pages, 5 figures, presented at ITSC2023",
      "repo_url": "https://github.com/aitor-martinez-seras/snn-automotive-object-detection"
    },
    "2312.07625": {
      "paper_id": "2312.07625v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.07625v2",
      "paper_key": "2312.07625",
      "paper_title": "Astrocyte-Enabled Advancements in Spiking Neural Networks for Large Language Modeling",
      "paper_url": "http://arxiv.org/abs/2312.07625v2",
      "paper_abstract": "Within the complex neuroarchitecture of the brain, astrocytes play crucial roles in development, structure, and metabolism. These cells regulate neural activity through tripartite synapses, directly impacting cognitive processes such as learning and memory. Despite the growing recognition of astrocytes' significance, traditional Spiking Neural Network (SNN) models remain predominantly neuron-centric, overlooking the profound influence of astrocytes on neural dynamics. Inspired by these biological insights, we have developed an Astrocyte-Modulated Spiking Unit (AM-SU), an innovative framework that integrates neuron-astrocyte interactions into the computational paradigm, demonstrating wide applicability across various hardware platforms. Our Astrocyte-Modulated Spiking Neural Network (AstroSNN) exhibits exceptional performance in tasks involving memory retention and natural language generation, particularly in handling long-term dependencies and complex linguistic structures. The design of AstroSNN not only enhances its biological authenticity but also introduces novel computational dynamics, enabling more effective processing of complex temporal dependencies. Furthermore, AstroSNN shows low latency, high throughput, and reduced memory usage in practical applications, making it highly suitable for resource-constrained environments. By successfully integrating astrocytic dynamics into intelligent neural networks, our work narrows the gap between biological plausibility and neural modeling, laying the groundwork for future biologically-inspired neural computing research that includes both neurons and astrocytes.",
      "paper_authors": [
        "Guobin Shen",
        "Dongcheng Zhao",
        "Yiting Dong",
        "Yang Li",
        "Jindong Li",
        "Kang Sun",
        "Yi Zeng"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-12-12",
      "update_time": "2023-12-26",
      "comments": null,
      "repo_url": "#"
    },
    "2312.06900": {
      "paper_id": "2312.06900v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.06900v1",
      "paper_key": "2312.06900",
      "paper_title": "When Bio-Inspired Computing meets Deep Learning: Low-Latency, Accurate, & Energy-Efficient Spiking Neural Networks from Artificial Neural Networks",
      "paper_url": "http://arxiv.org/abs/2312.06900v1",
      "paper_abstract": "Bio-inspired Spiking Neural Networks (SNN) are now demonstrating comparable accuracy to intricate convolutional neural networks (CNN), all while delivering remarkable energy and latency efficiency when deployed on neuromorphic hardware. In particular, ANN-to-SNN conversion has recently gained significant traction in developing deep SNNs with close to state-of-the-art (SOTA) test accuracy on complex image recognition tasks. However, advanced ANN-to-SNN conversion approaches demonstrate that for lossless conversion, the number of SNN time steps must equal the number of quantization steps in the ANN activation function. Reducing the number of time steps significantly increases the conversion error. Moreover, the spiking activity of the SNN, which dominates the compute energy in neuromorphic chips, does not reduce proportionally with the number of time steps. To mitigate the accuracy concern, we propose a novel ANN-to-SNN conversion framework, that incurs an exponentially lower number of time steps compared to that required in the SOTA conversion approaches. Our framework modifies the SNN integrate-and-fire (IF) neuron model with identical complexity and shifts the bias term of each batch normalization (BN) layer in the trained ANN. To mitigate the spiking activity concern, we propose training the source ANN with a fine-grained L1 regularizer with surrogate gradients that encourages high spike sparsity in the converted SNN. Our proposed framework thus yields lossless SNNs with ultra-low latency, ultra-low compute energy, thanks to the ultra-low timesteps and high spike sparsity, and ultra-high test accuracy, for example, 73.30% with only 4 time steps on the ImageNet dataset.",
      "paper_authors": [
        "Gourav Datta",
        "Zeyu Liu",
        "James Diffenderfer",
        "Bhavya Kailkhura",
        "Peter A. Beerel"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-12-12",
      "update_time": "2023-12-12",
      "comments": "Under review",
      "repo_url": "#"
    },
    "2312.14954": {
      "paper_id": "2312.14954v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.14954v1",
      "paper_key": "2312.14954",
      "paper_title": "Neuromorphic Co-Design as a Game",
      "paper_url": "http://arxiv.org/abs/2312.14954v1",
      "paper_abstract": "Co-design is a prominent topic presently in computing, speaking to the mutual benefit of coordinating design choices of several layers in the technology stack. For example, this may be designing algorithms which can most efficiently take advantage of the acceleration properties of a given architecture, while simultaneously designing the hardware to support the structural needs of a class of computation. The implications of these design decisions are influential enough to be deemed a lottery, enabling an idea to win out over others irrespective of the individual merits. Coordination is a well studied topic in the mathematics of game theory, where in many cases without a coordination mechanism the outcome is sub-optimal. Here we consider what insights game theoretic analysis can offer for computer architecture co-design. In particular, we consider the interplay between algorithm and architecture advances in the field of neuromorphic computing. Analyzing developments of spiking neural network algorithms and neuromorphic hardware as a co-design game we use the Stag Hunt model to illustrate challenges for spiking algorithms or architectures to advance the field independently and advocate for a strategic pursuit to advance neuromorphic computing.",
      "paper_authors": [
        "Craig M. Vineyard",
        "William M. Severa",
        "James B. Aimone"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-12-11",
      "update_time": "2023-12-11",
      "comments": "8 pages, 2 figures, accepted to First Workshop on Machine Learning\n  with New Compute Paradigms at NeurIPS 2023",
      "repo_url": "#"
    },
    "2312.06372": {
      "paper_id": "2312.06372v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.06372v2",
      "paper_key": "2312.06372",
      "paper_title": "Ternary Spike: Learning Ternary Spikes for Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2312.06372v2",
      "paper_abstract": "The Spiking Neural Network (SNN), as one of the biologically inspired neural network infrastructures, has drawn increasing attention recently. It adopts binary spike activations to transmit information, thus the multiplications of activations and weights can be substituted by additions, which brings high energy efficiency. However, in the paper, we theoretically and experimentally prove that the binary spike activation map cannot carry enough information, thus causing information loss and resulting in accuracy decreasing. To handle the problem, we propose a ternary spike neuron to transmit information. The ternary spike neuron can also enjoy the event-driven and multiplication-free operation advantages of the binary spike neuron but will boost the information capacity. Furthermore, we also embed a trainable factor in the ternary spike neuron to learn the suitable spike amplitude, thus our SNN will adopt different spike amplitudes along layers, which can better suit the phenomenon that the membrane potential distributions are different along layers. To retain the efficiency of the vanilla ternary spike, the trainable ternary spike SNN will be converted to a standard one again via a re-parameterization technique in the inference. Extensive experiments with several popular network structures over static and dynamic datasets show that the ternary spike can consistently outperform state-of-the-art methods. Our code is open-sourced at https://github.com/yfguo91/Ternary-Spike.",
      "paper_authors": [
        "Yufei Guo",
        "Yuanpei Chen",
        "Xiaode Liu",
        "Weihang Peng",
        "Yuhan Zhang",
        "Xuhui Huang",
        "Zhe Ma"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-12-11",
      "update_time": "2023-12-17",
      "comments": "Accepted by AAAI2024",
      "repo_url": "https://github.com/yfguo91/ternary-spike"
    },
    "2312.05643": {
      "paper_id": "2312.05643v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.05643v1",
      "paper_key": "2312.05643",
      "paper_title": "NiSNN-A: Non-iterative Spiking Neural Networks with Attention with Application to Motor Imagery EEG Classification",
      "paper_url": "http://arxiv.org/abs/2312.05643v1",
      "paper_abstract": "Motor imagery, an important category in electroencephalogram (EEG) research, often intersects with scenarios demanding low energy consumption, such as portable medical devices and isolated environment operations. Traditional deep learning algorithms, despite their effectiveness, are characterized by significant computational demands accompanied by high energy usage. As an alternative, spiking neural networks (SNNs), inspired by the biological functions of the brain, emerge as a promising energy-efficient solution. However, SNNs typically exhibit lower accuracy than their counterpart convolutional neural networks (CNNs). Although attention mechanisms successfully increase network accuracy by focusing on relevant features, their integration in the SNN framework remains an open question. In this work, we combine the SNN and the attention mechanisms for the EEG classification, aiming to improve precision and reduce energy consumption. To this end, we first propose a Non-iterative Leaky Integrate-and-Fire (LIF) neuron model, overcoming the gradient issues in the traditional SNNs using the Iterative LIF neurons. Then, we introduce the sequence-based attention mechanisms to refine the feature map. We evaluated the proposed Non-iterative SNN with Attention (NiSNN-A) model on OpenBMI, a large-scale motor imagery dataset. Experiment results demonstrate that 1) our model outperforms other SNN models by achieving higher accuracy, 2) our model increases energy efficiency compared to the counterpart CNN models (i.e., by 2.27 times) while maintaining comparable accuracy.",
      "paper_authors": [
        "Chuhan Zhang",
        "Wei Pan",
        "Cosimo Della Santina"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-12-09",
      "update_time": "2023-12-09",
      "comments": null,
      "repo_url": "#"
    },
    "2312.05290": {
      "paper_id": "2312.05290v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.05290v1",
      "paper_key": "2312.05290",
      "paper_title": "Noise Adaptor in Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2312.05290v1",
      "paper_abstract": "Recent strides in low-latency spiking neural network (SNN) algorithms have drawn significant interest, particularly due to their event-driven computing nature and fast inference capability. One of the most efficient ways to construct a low-latency SNN is by converting a pre-trained, low-bit artificial neural network (ANN) into an SNN. However, this conversion process faces two main challenges: First, converting SNNs from low-bit ANNs can lead to ``occasional noise\" -- the phenomenon where occasional spikes are generated in spiking neurons where they should not be -- during inference, which significantly lowers SNN accuracy. Second, although low-latency SNNs initially show fast improvements in accuracy with time steps, these accuracy growths soon plateau, resulting in their peak accuracy lagging behind both full-precision ANNs and traditional ``long-latency SNNs'' that prioritize precision over speed.   In response to these two challenges, this paper introduces a novel technique named ``noise adaptor.'' Noise adaptor can model occasional noise during training and implicitly optimize SNN accuracy, particularly at high simulation times $T$. Our research utilizes the ResNet model for a comprehensive analysis of the impact of the noise adaptor on low-latency SNNs. The results demonstrate that our method outperforms the previously reported quant-ANN-to-SNN conversion technique. We achieved an accuracy of 95.95\\% within 4 time steps on CIFAR-10 using ResNet-18, and an accuracy of 74.37\\% within 64 time steps on ImageNet using ResNet-50. Remarkably, these results were obtained without resorting to any noise correction methods during SNN inference, such as negative spikes or two-stage SNN simulations. Our approach significantly boosts the peak accuracy of low-latency SNNs, bringing them on par with the accuracy of full-precision ANNs. Code will be open source.",
      "paper_authors": [
        "Chen Li",
        "Bipin Rajendran"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-12-08",
      "update_time": "2023-12-08",
      "comments": null,
      "repo_url": "#"
    },
    "2312.04840": {
      "paper_id": "2312.04840v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.04840v1",
      "paper_key": "2312.04840",
      "paper_title": "Analysis on Effects of Fault Elements in Memristive Neuromorphic Systems",
      "paper_url": "http://arxiv.org/abs/2312.04840v1",
      "paper_abstract": "Nowadays, neuromorphic systems based on Spiking Neural Networks (SNNs) attract attentions of many researchers. There are many studies to improve performances of neuromorphic systems. These studies have been showing satisfactory results. To magnify performances of neuromorphic systems, developing actual neuromorphic systems is essential. For developing them, memristors play key role due to their useful characteristics. Although memristors are essential for actual neuromorphic systems, they are vulnerable to faults. However, there are few studies analyzing effects of fault elements in neuromorphic systems using memristors. To solve this problem, we analyze performance of a memristive neuromorphic system with fault elements changing fault ratios, types, and positions. We choose neurons and synapses to inject faults. We inject two types of faults to synapses: SA0 and SA1 faults. The fault synapses appear in random and important positions. Through our analysis, we discover the following four interesting points. First, memristive characteristics increase vulnerability of neuromorphic systems to fault elements. Second, fault neuron ratios reducing performance sharply exist. Third, performance degradation by fault synapses depends on fault types. Finally, SA1 fault synapses improve performance when they appear in important positions.",
      "paper_authors": [
        "Hyun-Jong Lee",
        "Jae-Han Lim"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-12-08",
      "update_time": "2023-12-08",
      "comments": "8 pages, 7 figures, 5 tables, IJCAI 2023 GLOW\n  workshop(https://sites.google.com/view/glow-ijcai-23/home)",
      "repo_url": "#"
    },
    "2312.02659": {
      "paper_id": "2312.02659v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.02659v2",
      "paper_key": "2312.02659",
      "paper_title": "Supervised learning of spatial features with STDP and homeostasis using Spiking Neural Networks on SpiNNaker",
      "paper_url": "http://arxiv.org/abs/2312.02659v2",
      "paper_abstract": "Artificial Neural Networks (ANN) have gained significant popularity thanks to their ability to learn using the well-known backpropagation algorithm. Conversely, Spiking Neural Networks (SNNs), despite having broader capabilities than ANNs, have always posed challenges in the training phase. This paper shows a new method to perform supervised learning on SNNs, using Spike Timing Dependent Plasticity (STDP) and homeostasis, aiming at training the network to identify spatial patterns. Spatial patterns refer to spike patterns without a time component, where all spike events occur simultaneously. The method is tested using the SpiNNaker digital architecture. A SNN is trained to recognise one or multiple patterns and performance metrics are extracted to measure the performance of the network. Some considerations are drawn from the results showing that, in the case of a single trained pattern, the network behaves as the ideal detector, with 100% accuracy in detecting the trained pattern. However, as the number of trained patterns on a single network increases, the accuracy of identification is linked to the similarities between these patterns. This method of training an SNN to detect spatial patterns may be applied to pattern recognition in static images or traffic analysis in computer networks, where each network packet represents a spatial pattern. It will be stipulated that the homeostatic factor may enable the network to detect patterns with some degree of similarity, rather than only perfectly matching patterns.The principles outlined in this article serve as the fundamental building blocks for more complex systems that utilise both spatial and temporal patterns by converting specific features of input signals into spikes.One example of such a system is a computer network packet classifier, tasked with real-time identification of packet streams based on features within the packet content",
      "paper_authors": [
        "Sergio Davies",
        "Andrew Gait",
        "Andrew Rowley",
        "Alessandro Di Nuovo"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-12-05",
      "update_time": "2024-06-24",
      "comments": "14 pages, 6 figures (figure 6 has 9 sub-figures) for a total of 14\n  images, 10 tables, submitted to the Journal of Neural Networks",
      "repo_url": "#"
    },
    "2312.01742": {
      "paper_id": "2312.01742v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.01742v1",
      "paper_key": "2312.01742",
      "paper_title": "Fully Spiking Denoising Diffusion Implicit Models",
      "paper_url": "http://arxiv.org/abs/2312.01742v1",
      "paper_abstract": "Spiking neural networks (SNNs) have garnered considerable attention owing to their ability to run on neuromorphic devices with super-high speeds and remarkable energy efficiencies. SNNs can be used in conventional neural network-based time- and energy-consuming applications. However, research on generative models within SNNs remains limited, despite their advantages. In particular, diffusion models are a powerful class of generative models, whose image generation quality surpass that of the other generative models, such as GANs. However, diffusion models are characterized by high computational costs and long inference times owing to their iterative denoising feature. Therefore, we propose a novel approach fully spiking denoising diffusion implicit model (FSDDIM) to construct a diffusion model within SNNs and leverage the high speed and low energy consumption features of SNNs via synaptic current learning (SCL). SCL fills the gap in that diffusion models use a neural network to estimate real-valued parameters of a predefined probabilistic distribution, whereas SNNs output binary spike trains. The SCL enables us to complete the entire generative process of diffusion models exclusively using SNNs. We demonstrate that the proposed method outperforms the state-of-the-art fully spiking generative model.",
      "paper_authors": [
        "Ryo Watanabe",
        "Yusuke Mukuta",
        "Tatsuya Harada"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-12-04",
      "update_time": "2023-12-04",
      "comments": null,
      "repo_url": "https://github.com/mil-tokyo/FSDDIM"
    },
    "2312.01213": {
      "paper_id": "2312.01213v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.01213v1",
      "paper_key": "2312.01213",
      "paper_title": "Recent Advances in Scalable Energy-Efficient and Trustworthy Spiking Neural networks: from Algorithms to Technology",
      "paper_url": "http://arxiv.org/abs/2312.01213v1",
      "paper_abstract": "Neuromorphic computing and, in particular, spiking neural networks (SNNs) have become an attractive alternative to deep neural networks for a broad range of signal processing applications, processing static and/or temporal inputs from different sensory modalities, including audio and vision sensors. In this paper, we start with a description of recent advances in algorithmic and optimization innovations to efficiently train and scale low-latency, and energy-efficient spiking neural networks (SNNs) for complex machine learning applications. We then discuss the recent efforts in algorithm-architecture co-design that explores the inherent trade-offs between achieving high energy-efficiency and low latency while still providing high accuracy and trustworthiness. We then describe the underlying hardware that has been developed to leverage such algorithmic innovations in an efficient way. In particular, we describe a hybrid method to integrate significant portions of the model's computation within both memory components as well as the sensor itself. Finally, we discuss the potential path forward for research in building deployable SNN systems identifying key challenges in the algorithm-hardware-application co-design space with an emphasis on trustworthiness.",
      "paper_authors": [
        "Souvik Kundu",
        "Rui-Jie Zhu",
        "Akhilesh Jaiswal",
        "Peter A. Beerel"
      ],
      "primary_category": "cs.AR",
      "publish_time": "2023-12-02",
      "update_time": "2023-12-02",
      "comments": "5 pages, 3 figures",
      "repo_url": "#"
    },
    "2312.00919": {
      "paper_id": "2312.00919v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.00919v1",
      "paper_key": "2312.00919",
      "paper_title": "Rethinking Skip Connections in Spiking Neural Networks with Time-To-First-Spike Coding",
      "paper_url": "http://arxiv.org/abs/2312.00919v1",
      "paper_abstract": "Time-To-First-Spike (TTFS) coding in Spiking Neural Networks (SNNs) offers significant advantages in terms of energy efficiency, closely mimicking the behavior of biological neurons. In this work, we delve into the role of skip connections, a widely used concept in Artificial Neural Networks (ANNs), within the domain of SNNs with TTFS coding. Our focus is on two distinct types of skip connection architectures: (1) addition-based skip connections, and (2) concatenation-based skip connections. We find that addition-based skip connections introduce an additional delay in terms of spike timing. On the other hand, concatenation-based skip connections circumvent this delay but produce time gaps between after-convolution and skip connection paths, thereby restricting the effective mixing of information from these two paths. To mitigate these issues, we propose a novel approach involving a learnable delay for skip connections in the concatenation-based skip connection architecture. This approach successfully bridges the time gap between the convolutional and skip branches, facilitating improved information mixing. We conduct experiments on public datasets including MNIST and Fashion-MNIST, illustrating the advantage of the skip connection in TTFS coding architectures. Additionally, we demonstrate the applicability of TTFS coding on beyond image recognition tasks and extend it to scientific machine-learning tasks, broadening the potential uses of SNNs.",
      "paper_authors": [
        "Youngeun Kim",
        "Adar Kahana",
        "Ruokai Yin",
        "Yuhang Li",
        "Panos Stinis",
        "George Em Karniadakis",
        "Priyadarshini Panda"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2023-12-01",
      "update_time": "2023-12-01",
      "comments": null,
      "repo_url": "#"
    },
    "2312.00425": {
      "paper_id": "2312.00425v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.00425v2",
      "paper_key": "2312.00425",
      "paper_title": "Retina : Low-Power Eye Tracking with Event Camera and Spiking Hardware",
      "paper_url": "http://arxiv.org/abs/2312.00425v2",
      "paper_abstract": "This paper introduces a neuromorphic methodology for eye tracking, harnessing pure event data captured by a Dynamic Vision Sensor (DVS) camera. The framework integrates a directly trained Spiking Neuron Network (SNN) regression model and leverages a state-of-the-art low power edge neuromorphic processor - Speck, collectively aiming to advance the precision and efficiency of eye-tracking systems. First, we introduce a representative event-based eye-tracking dataset, \"Ini-30\", which was collected with two glass-mounted DVS cameras from thirty volunteers. Then,a SNN model, based on Integrate And Fire (IAF) neurons, named \"Retina\", is described , featuring only 64k parameters (6.63x fewer than the latest) and achieving pupil tracking error of only 3.24 pixels in a 64x64 DVS input. The continous regression output is obtained by means of convolution using a non-spiking temporal 1D filter slided across the output spiking layer. Finally, we evaluate Retina on the neuromorphic processor, showing an end-to-end power between 2.89-4.8 mW and a latency of 5.57-8.01 mS dependent on the time window. We also benchmark our model against the latest event-based eye-tracking method, \"3ET\", which was built upon event frames. Results show that Retina achieves superior precision with 1.24px less pupil centroid error and reduced computational complexity with 35 times fewer MAC operations. We hope this work will open avenues for further investigation of close-loop neuromorphic solutions and true event-based training pursuing edge performance.",
      "paper_authors": [
        "Pietro Bonazzi",
        "Sizhen Bian",
        "Giovanni Lippolis",
        "Yawei Li",
        "Sadique Sheik",
        "Michele Magno"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-12-01",
      "update_time": "2024-04-17",
      "comments": null,
      "repo_url": "https://github.com/pbonazzi/retina"
    },
    "2402.10214": {
      "paper_id": "2402.10214v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.10214v1",
      "paper_key": "2402.10214",
      "paper_title": "Astrocyte control bursting mode of spiking neuron network with memristor-implemented plasticity",
      "paper_url": "http://arxiv.org/abs/2402.10214v1",
      "paper_abstract": "A mathematical model of a spiking neuron network accompanied by astrocytes is considered. The network is composed of excitatory and inhibitory neurons with synaptic connections supplied by a memristor-based model of plasticity. Another mechanism for changing the synaptic connections involves astrocytic regulations using the concept of tripartite synapses. In the absence of memristor-based plasticity, the connections between these neurons drive the network dynamics into a burst mode, as observed in many experimental neurobiological studies when investigating living networks in neuronal cultures. The memristive plasticity implementing synaptic plasticity in inhibitory synapses results in a shift in network dynamics towards an asynchronous mode. Next,it is found that accounting for astrocytic regulation in glutamatergic excitatory synapses enable the restoration of 'normal' burst dynamics. The conditions and parameters of such astrocytic regulation's impact on burst dynamics established.",
      "paper_authors": [
        "Sergey V. Stasenko",
        "Alexey N. Mikhaylov",
        "Alexander A. Fedotov",
        "Vladimir A. Smirnov",
        "Victor B. Kazantsev"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2023-11-30",
      "update_time": "2023-11-30",
      "comments": null,
      "repo_url": "#"
    },
    "2311.18577": {
      "paper_id": "2311.18577v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.18577v1",
      "paper_key": "2311.18577",
      "paper_title": "Design Space and Variability Analysis of SOI MOSFET for Ultra-Low Power Band-to-Band Tunneling Neurons",
      "paper_url": "http://arxiv.org/abs/2311.18577v1",
      "paper_abstract": "Large spiking neural networks (SNNs) require ultra-low power and low variability hardware for neuromorphic computing applications. Recently, a band-to-band tunneling-based (BTBT) integrator, enabling sub-kHz operation of neurons with area and energy efficiency, was proposed. For an ultra-low power implementation of such neurons, a very low BTBT current is needed, so minimizing current without degrading neuronal properties is essential. Low variability is needed in the ultra-low current integrator to avoid network performance degradation in a large BTBT neuron-based SNN. To address this, we conducted design space and variability analysis in TCAD, utilizing a well-calibrated TCAD deck with experimental data from GlobalFoundries 32nm PD-SOI MOSFET. First, we discuss the physics-based explanation of the tunneling mechanism. Second, we explore the impact of device design parameters on SOI MOSFET performance, highlighting parameter sensitivities to tunneling current. With device parameters' optimization, we demonstrate a ~20x reduction in BTBT current compared to the experimental data. Finally, a variability analysis that includes the effects of random dopant fluctuations (RDF), oxide thickness variability (OTV), and channel-oxide interface traps DIT in the BTBT, SS, and ON regimes of operation is shown. The BTBT regime shows high sensitivity to the RDF and OTV as any variation in them directly modulates the tunnel length or the electric field at the drain-channel junction, whereas minimal sensitivity to DIT is observed.",
      "paper_authors": [
        "Jay Sonawane",
        "Shubham Patil",
        "Abhishek Kadam",
        "Ajay Kumar Singh",
        "Sandip Lashkare",
        "Veeresh Deshpande",
        "Udayan Ganguly"
      ],
      "primary_category": "physics.app-ph",
      "publish_time": "2023-11-30",
      "update_time": "2023-11-30",
      "comments": null,
      "repo_url": "#"
    },
    "2311.18340": {
      "paper_id": "2311.18340v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.18340v1",
      "paper_key": "2311.18340",
      "paper_title": "Neuromorphic Incremental on-chip Learning with Hebbian Weight Consolidation",
      "paper_url": "http://arxiv.org/abs/2311.18340v1",
      "paper_abstract": "As next-generation implantable brain-machine interfaces become pervasive on edge device, incrementally learning new tasks in bio-plasticity ways is urgently demanded for Neuromorphic chips. Due to the inherent characteristics of its structure, spiking neural networks are naturally well-suited for BMI-chips. Here we propose Hebbian Weight Consolidation, as well as an on-chip learning framework. HWC selectively masks synapse modifications for previous tasks, retaining them to store new knowledge from subsequent tasks while preserving the old knowledge. Leveraging the bio-plasticity of dendritic spines, the intrinsic self-organizing nature of Hebbian Weight Consolidation aligns naturally with the incremental learning paradigm, facilitating robust learning outcomes. By reading out spikes layer by layer and performing back-propagation on the external micro-controller unit, MLoC can efficiently accomplish on-chip learning. Experiments show that our HWC algorithm up to 23.19% outperforms lower bound that without incremental learning algorithm, particularly in more challenging monkey behavior decoding scenarios. Taking into account on-chip computing on Synsense Speck 2e chip, our proposed algorithm exhibits an improvement of 11.06%. This study demonstrates the feasibility of employing incremental learning for high-performance neural signal decoding in next-generation brain-machine interfaces.",
      "paper_authors": [
        "Zifan Ning",
        "Chaojin Chen",
        "Xiang Cheng",
        "Wangzi Yao",
        "Tielin Zhang",
        "Bo Xu"
      ],
      "primary_category": "cs.ET",
      "publish_time": "2023-11-30",
      "update_time": "2023-11-30",
      "comments": "12 pages, 6 figures",
      "repo_url": "#"
    },
    "2311.18134": {
      "paper_id": "2311.18134v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.18134v1",
      "paper_key": "2311.18134",
      "paper_title": "A computational model of behavioral adaptation to solve the credit assignment problem",
      "paper_url": "http://arxiv.org/abs/2311.18134v1",
      "paper_abstract": "The adaptive fitness of an organism in its ecological niche is highly reliant upon its ability to associate an environmental or internal stimulus with a behavior response through reinforcement. This simple but powerful observation has been successfully applied in a number of contexts within computational neuroscience and reinforcement learning to model both human and animal behaviors. However, a critical challenge faced by these models is the credit assignment problem which asks how past behavior comes to be associated with a delayed reinforcement signal. In this paper we reformulate the credit assignment problem to ask how past stimuli come to be linked to adaptive behavioral responses in the context of a simple neuronal circuit. We propose a biologically plausible variant of a spiking neural network which can model a wide variety of behavioral, learning, and evolutionary phenomena. Our model suggests one fundamental mechanism, potentially in use in the brains of both simple and complex organisms, that would allow it to associate a behavior with an adaptive response. We present results that showcase the model's versatility and biological plausibility in a number of tasks related to classical and operant conditioning including behavioral chaining. We then provide further simulations to demonstrate how adaptive behaviors such as reflexes and simple category detection may have evolved using our model. Our results indicate the potential for further modifications and extensions of our model to replicate more sophisticated and biologically plausible behavioral, learning, and intelligence phenomena found throughout the animal kingdom.",
      "paper_authors": [
        "Roy E. Clymer",
        "Sanjeev V. Namjoshi"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2023-11-29",
      "update_time": "2023-11-29",
      "comments": "18 pages, 9 figures",
      "repo_url": "#"
    },
    "2311.17383": {
      "paper_id": "2311.17383v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.17383v1",
      "paper_key": "2311.17383",
      "paper_title": "Dynamical manifold dimensionality as characterization measure of chimera states in bursting neuronal networks",
      "paper_url": "http://arxiv.org/abs/2311.17383v1",
      "paper_abstract": "Methods that distinguish dynamical regimes in networks of active elements make it possible to design the dynamics of models of realistic networks. A particularly salient example is partial synchronization, which may play a pivotal role in elucidating the dynamics of biological neural networks. Such emergent partial synchronization in structurally homogeneous networks is commonly denoted as chimera states. While several methods for detecting chimeras in networks of spiking neurons have been proposed, these are less effective when applied to networks of bursting neurons. Here we introduce the correlation dimension as a novel approach to identifying dynamic network states. To assess the viability of this new method, we study a network of intrinsically Hindmarsh-Rose neurons with non-local connections. In comparison to other measures of chimera states, the correlation dimension effectively characterizes chimeras in burst neurons, whether the incoherence arises in spikes or bursts. The generality of dimensionality measures inherent in the correlation dimension renders this approach applicable to any dynamic system, facilitating the comparison of simulated and experimental data. We anticipate that this methodology will enable the tuning and simulation of when modelling intricate network processes, contributing to a deeper understanding of neural dynamics.",
      "paper_authors": [
        "Olesia Dogonasheva",
        "Daniil Radushev",
        "Boris Gutkin",
        "Denis Zakharov"
      ],
      "primary_category": "nlin.AO",
      "publish_time": "2023-11-29",
      "update_time": "2023-11-29",
      "comments": null,
      "repo_url": "#"
    },
    "2311.16456": {
      "paper_id": "2311.16456v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.16456v1",
      "paper_key": "2311.16456",
      "paper_title": "Spiking Neural Networks with Dynamic Time Steps for Vision Transformers",
      "paper_url": "http://arxiv.org/abs/2311.16456v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) have emerged as a popular spatio-temporal computing paradigm for complex vision tasks. Recently proposed SNN training algorithms have significantly reduced the number of time steps (down to 1) for improved latency and energy efficiency, however, they target only convolutional neural networks (CNN). These algorithms, when applied on the recently spotlighted vision transformers (ViT), either require a large number of time steps or fail to converge. Based on analysis of the histograms of the ANN and SNN activation maps, we hypothesize that each ViT block has a different sensitivity to the number of time steps. We propose a novel training framework that dynamically allocates the number of time steps to each ViT module depending on a trainable score assigned to each timestep. In particular, we generate a scalar binary time step mask that filters spikes emitted by each neuron in a leaky-integrate-and-fire (LIF) layer. The resulting SNNs have high activation sparsity and require only accumulate operations (AC), except for the input embedding layer, in contrast to expensive multiply-and-accumulates (MAC) needed in traditional ViTs. This yields significant improvements in energy efficiency. We evaluate our training framework and resulting SNNs on image recognition tasks including CIFAR10, CIFAR100, and ImageNet with different ViT architectures. We obtain a test accuracy of 95.97% with 4.97 time steps with direct encoding on CIFAR10.",
      "paper_authors": [
        "Gourav Datta",
        "Zeyu Liu",
        "Anni Li",
        "Peter A. Beerel"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-28",
      "update_time": "2023-11-28",
      "comments": "Under review",
      "repo_url": "#"
    },
    "2311.15474": {
      "paper_id": "2311.15474v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.15474v1",
      "paper_key": "2311.15474",
      "paper_title": "Demonstration of Programmable Brain-Inspired Optoelectronic Neuron in Photonic Spiking Neural Network with Neural Heterogeneity",
      "paper_url": "http://arxiv.org/abs/2311.15474v1",
      "paper_abstract": "Photonic Spiking Neural Networks (PSNN) composed of the co-integrated CMOS and photonic elements can offer low loss, low power, highly-parallel, and high-throughput computing for brain-inspired neuromorphic systems. In addition, heterogeneity of neuron dynamics can also bring greater diversity and expressivity to brain-inspired networks, potentially allowing for the implementation of complex functions with fewer neurons. In this paper, we design, fabricate, and experimentally demonstrate an optoelectronic spiking neuron that can simultaneously achieve high programmability for heterogeneous biological neural networks and maintain high-speed computing. We demonstrate that our neuron can be programmed to tune four essential parameters of neuron dynamics under 1GSpike/s input spiking pattern signals. A single neuron circuit can be tuned to output three spiking patterns, including chattering behaviors. The PSNN consisting of the optoelectronic spiking neuron and a Mach-Zehnder interferometer (MZI) mesh synaptic network achieves 89.3% accuracy on the Iris dataset. Our neuron power consumption is 1.18 pJ/spike output, mainly limited by the power efficiency of the vertical-cavity-lasers, optical coupling efficiency, and the 45 nm CMOS platform used in this experiment, and is predicted to achieve 36.84 fJ/spike output with a 7 nm CMOS platform (e.g. ASAP7) integrated with silicon photonics containing on-chip micron-scale lasers.",
      "paper_authors": [
        "Yun-Jhu Lee",
        "Mehmet Berkay On",
        "Luis El Srouji",
        "Li Zhang",
        "Mahmoud Abdelghany",
        "S. J. Ben Yoo"
      ],
      "primary_category": "eess.SY",
      "publish_time": "2023-11-27",
      "update_time": "2023-11-27",
      "comments": null,
      "repo_url": "#"
    },
    "2311.14641": {
      "paper_id": "2311.14641v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.14641v1",
      "paper_key": "2311.14641",
      "paper_title": "Neuromorphic Intermediate Representation: A Unified Instruction Set for Interoperable Brain-Inspired Computing",
      "paper_url": "http://arxiv.org/abs/2311.14641v1",
      "paper_abstract": "Spiking neural networks and neuromorphic hardware platforms that emulate neural dynamics are slowly gaining momentum and entering main-stream usage. Despite a well-established mathematical foundation for neural dynamics, the implementation details vary greatly across different platforms. Correspondingly, there are a plethora of software and hardware implementations with their own unique technology stacks. Consequently, neuromorphic systems typically diverge from the expected computational model, which challenges the reproducibility and reliability across platforms. Additionally, most neuromorphic hardware is limited by its access via a single software frameworks with a limited set of training procedures. Here, we establish a common reference-frame for computations in neuromorphic systems, dubbed the Neuromorphic Intermediate Representation (NIR). NIR defines a set of computational primitives as idealized continuous-time hybrid systems that can be composed into graphs and mapped to and from various neuromorphic technology stacks. By abstracting away assumptions around discretization and hardware constraints, NIR faithfully captures the fundamental computation, while simultaneously exposing the exact differences between the evaluated implementation and the idealized mathematical formalism. We reproduce three NIR graphs across 7 neuromorphic simulators and 4 hardware platforms, demonstrating support for an unprecedented number of neuromorphic systems. With NIR, we decouple the evolution of neuromorphic hardware and software, ultimately increasing the interoperability between platforms and improving accessibility to neuromorphic technologies. We believe that NIR is an important step towards the continued study of brain-inspired hardware and bottom-up approaches aimed at an improved understanding of the computational underpinnings of nervous systems.",
      "paper_authors": [
        "Jens E. Pedersen",
        "Steven Abreu",
        "Matthias Jobst",
        "Gregor Lenz",
        "Vittorio Fra",
        "Felix C. Bauer",
        "Dylan R. Muir",
        "Peng Zhou",
        "Bernhard Vogginger",
        "Kade Heckel",
        "Gianvito Urgese",
        "Sadasivan Shankar",
        "Terrence C. Stewart",
        "Jason K. Eshraghian",
        "Sadique Sheik"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-11-24",
      "update_time": "2023-11-24",
      "comments": "NIR is available at https://github.com/neuromorphs/NIR",
      "repo_url": "https://github.com/neuromorphs/nir"
    },
    "2311.14447": {
      "paper_id": "2311.14447v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.14447v1",
      "paper_key": "2311.14447",
      "paper_title": "SNN Architecture for Differential Time Encoding Using Decoupled Processing Time",
      "paper_url": "http://arxiv.org/abs/2311.14447v1",
      "paper_abstract": "Spiking neural networks (SNNs) have gained attention in recent years due to their ability to handle sparse and event-based data better than regular artificial neural networks (ANNs). Since the structure of SNNs is less suited for typically used accelerators such as GPUs than conventional ANNs, there is a demand for custom hardware accelerators for processing SNNs. In the past, the main focus was on platforms that resemble the structure of multiprocessor systems. In this work, we propose a lightweight neuron layer architecture that allows network structures to be directly mapped onto digital hardware. Our approach is based on differential time coding of spike sequences and the decoupling of processing time and spike timing that allows the SNN to be processed on different hardware platforms. We present synthesis and performance results showing that this architecture can be implemented for networks of more than 1000 neurons with high clock speeds on a State-of-the-Art FPGA. We furthermore show results on the robustness of our approach to quantization. These results demonstrate that high-accuracy inference can be performed with bit widths as low as 4.",
      "paper_authors": [
        "Daniel Windhager",
        "Bernhard A. Moser",
        "Michael Lunglmayr"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2023-11-24",
      "update_time": "2023-11-24",
      "comments": null,
      "repo_url": "#"
    },
    "2311.14303": {
      "paper_id": "2311.14303v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.14303v2",
      "paper_key": "2311.14303",
      "paper_title": "RFI Detection with Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2311.14303v2",
      "paper_abstract": "Detecting and mitigating Radio Frequency Interference (RFI) is critical for enabling and maximising the scientific output of radio telescopes. The emergence of machine learning methods has led to their application in radio astronomy, and in RFI detection. Spiking Neural Networks (SNNs), inspired by biological systems, are well-suited for processing spatio-temporal data. This study introduces the first exploratory application of SNNs to an astronomical data-processing task, specifically RFI detection. We adapt the nearest-latent-neighbours (NLN) algorithm and auto-encoder architecture proposed by previous authors to SNN execution by direct ANN2SNN conversion, enabling simplified downstream RFI detection by sampling the naturally varying latent space from the internal spiking neurons. Our subsequent evaluation aims to determine whether SNNs are viable for future RFI detection schemes. We evaluate detection performance with the simulated HERA telescope and hand-labelled LOFAR observation dataset the original authors provided. We additionally evaluate detection performance with a new MeerKAT-inspired simulation dataset that provides a technical challenge for machine-learnt RFI detection methods. This dataset focuses on satellite-based RFI, an increasingly important class of RFI and is an additional contribution. Our approach remains competitive with existing methods in AUROC, AUPRC and F1 scores for the HERA dataset but exhibits difficulty in the LOFAR and Tabascal datasets. Our method maintains this accuracy while completely removing the compute and memory-intense latent sampling step found in NLN. This work demonstrates the viability of SNNs as a promising avenue for machine-learning-based RFI detection in radio telescopes by establishing a minimal performance baseline on traditional and nascent satellite-based RFI sources and is the first work to our knowledge to apply SNNs in astronomy.",
      "paper_authors": [
        "Nicholas J. Pritchard",
        "Andreas Wicenec",
        "Mohammed Bennamoun",
        "Richard Dodson"
      ],
      "primary_category": "astro-ph.IM",
      "publish_time": "2023-11-24",
      "update_time": "2024-03-22",
      "comments": "11 pages, 5 figures, 5 tables. Accepted for publication in PASA",
      "repo_url": "https://github.com/pritchardn/snn-nln"
    },
    "2311.14265": {
      "paper_id": "2311.14265v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.14265v2",
      "paper_key": "2311.14265",
      "paper_title": "Adaptive Calibration: A Unified Conversion Framework of Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2311.14265v2",
      "paper_abstract": "Spiking Neural Networks (SNNs) have emerged as a promising energy-efficient alternative to traditional Artificial Neural Networks (ANNs). Despite this, bridging the performance gap with ANNs in practical scenarios remains a significant challenge. This paper focuses on addressing the dual objectives of enhancing the performance and efficiency of SNNs through the established SNN Calibration conversion framework. Inspired by the biological nervous system, we propose a novel Adaptive-Firing Neuron Model (AdaFire) that dynamically adjusts firing patterns across different layers, substantially reducing conversion errors within limited timesteps. Moreover, to meet our efficiency objectives, we propose two novel strategies: an Sensitivity Spike Compression (SSC) technique and an Input-aware Adaptive Timesteps (IAT) technique. These techniques synergistically reduce both energy consumption and latency during the conversion process, thereby enhancing the overall efficiency of SNNs. Extensive experiments demonstrate our approach outperforms state-of-the-art SNNs methods, showcasing superior performance and efficiency in 2D, 3D, and event-driven classification, as well as object detection and segmentation tasks.",
      "paper_authors": [
        "Ziqing Wang",
        "Yuetong Fang",
        "Jiahang Cao",
        "Renjing Xu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-24",
      "update_time": "2024-03-16",
      "comments": "Under review",
      "repo_url": "https://github.com/bic-l/burst-ann2snn"
    },
    "2311.13186": {
      "paper_id": "2311.13186v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.13186v2",
      "paper_key": "2311.13186",
      "paper_title": "Applications of Spiking Neural Networks in Visual Place Recognition",
      "paper_url": "http://arxiv.org/abs/2311.13186v2",
      "paper_abstract": "In robotics, Spiking Neural Networks (SNNs) are increasingly recognized for their largely-unrealized potential energy efficiency and low latency particularly when implemented on neuromorphic hardware. Our paper highlights three advancements for SNNs in Visual Place Recognition (VPR). Firstly, we propose Modular SNNs, where each SNN represents a set of non-overlapping geographically distinct places, enabling scalable networks for large environments. Secondly, we present Ensembles of Modular SNNs, where multiple networks represent the same place, significantly enhancing accuracy compared to single-network models. Each of our Modular SNN modules is compact, comprising only 1500 neurons and 474k synapses, making them ideally suited for ensembling due to their small size. Lastly, we investigate the role of sequence matching in SNN-based VPR, a technique where consecutive images are used to refine place recognition. We analyze the responsiveness of SNNs to ensembling and sequence matching compared to other VPR techniques. Our contributions highlight the viability of SNNs for VPR, offering scalable and robust solutions, and paving the way for their application in various energy-sensitive robotic tasks.",
      "paper_authors": [
        "Somayeh Hussaini",
        "Michael Milford",
        "Tobias Fischer"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-22",
      "update_time": "2024-08-02",
      "comments": "20 pages, 10 figures, under review",
      "repo_url": "https://github.com/qvpr/vprsnn"
    },
    "2311.12449": {
      "paper_id": "2311.12449v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.12449v1",
      "paper_key": "2311.12449",
      "paper_title": "HPCNeuroNet: Advancing Neuromorphic Audio Signal Processing with Transformer-Enhanced Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2311.12449v1",
      "paper_abstract": "This paper presents a novel approach to neuromorphic audio processing by integrating the strengths of Spiking Neural Networks (SNNs), Transformers, and high-performance computing (HPC) into the HPCNeuroNet architecture. Utilizing the Intel N-DNS dataset, we demonstrate the system's capability to process diverse human vocal recordings across multiple languages and noise backgrounds. The core of our approach lies in the fusion of the temporal dynamics of SNNs with the attention mechanisms of Transformers, enabling the model to capture intricate audio patterns and relationships. Our architecture, HPCNeuroNet, employs the Short-Time Fourier Transform (STFT) for time-frequency representation, Transformer embeddings for dense vector generation, and SNN encoding/decoding mechanisms for spike train conversions. The system's performance is further enhanced by leveraging the computational capabilities of NVIDIA's GeForce RTX 3060 GPU and Intel's Core i9 12900H CPU. Additionally, we introduce a hardware implementation on the Xilinx VU37P HBM FPGA platform, optimizing for energy efficiency and real-time processing. The proposed accelerator achieves a throughput of 71.11 Giga-Operations Per Second (GOP/s) with a 3.55 W on-chip power consumption at 100 MHz. The comparison results with off-the-shelf devices and recent state-of-the-art implementations illustrate that the proposed accelerator has obvious advantages in terms of energy efficiency and design flexibility. Through design-space exploration, we provide insights into optimizing core capacities for audio tasks. Our findings underscore the transformative potential of integrating SNNs, Transformers, and HPC for neuromorphic audio processing, setting a new benchmark for future research and applications.",
      "paper_authors": [
        "Murat Isik",
        "Hiruna Vishwamith",
        "Kayode Inadagbo",
        "I. Can Dikmen"
      ],
      "primary_category": "eess.AS",
      "publish_time": "2023-11-21",
      "update_time": "2023-11-21",
      "comments": "Submitted to IEEE Transactions on Signal Processing",
      "repo_url": "#"
    },
    "2311.11853": {
      "paper_id": "2311.11853v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.11853v2",
      "paper_key": "2311.11853",
      "paper_title": "Asynchronous Bioplausible Neuron for SNN for Event Vision",
      "paper_url": "http://arxiv.org/abs/2311.11853v2",
      "paper_abstract": "Spiking Neural Networks (SNNs) offer a biologically inspired approach to computer vision that can lead to more efficient processing of visual data with reduced energy consumption. However, maintaining homeostasis within these networks is challenging, as it requires continuous adjustment of neural responses to preserve equilibrium and optimal processing efficiency amidst diverse and often unpredictable input signals. In response to these challenges, we propose the Asynchronous Bioplausible Neuron (ABN), a dynamic spike firing mechanism to auto-adjust the variations in the input signal. Comprehensive evaluation across various datasets demonstrates ABN's enhanced performance in image classification and segmentation, maintenance of neural equilibrium, and energy efficiency.",
      "paper_authors": [
        "Sanket Kachole",
        "Hussain Sajwani",
        "Fariborz Baghaei Naeini",
        "Dimitrios Makris",
        "Yahya Zweiri"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-11-20",
      "update_time": "2024-08-02",
      "comments": "10 pages",
      "repo_url": "#"
    },
    "2311.11390": {
      "paper_id": "2311.11390v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.11390v1",
      "paper_key": "2311.11390",
      "paper_title": "Addressing the speed-accuracy simulation trade-off for adaptive spiking neurons",
      "paper_url": "http://arxiv.org/abs/2311.11390v1",
      "paper_abstract": "The adaptive leaky integrate-and-fire (ALIF) model is fundamental within computational neuroscience and has been instrumental in studying our brains $\\textit{in silico}$. Due to the sequential nature of simulating these neural models, a commonly faced issue is the speed-accuracy trade-off: either accurately simulate a neuron using a small discretisation time-step (DT), which is slow, or more quickly simulate a neuron using a larger DT and incur a loss in simulation accuracy. Here we provide a solution to this dilemma, by algorithmically reinterpreting the ALIF model, reducing the sequential simulation complexity and permitting a more efficient parallelisation on GPUs. We computationally validate our implementation to obtain over a $50\\times$ training speedup using small DTs on synthetic benchmarks. We also obtained a comparable performance to the standard ALIF implementation on different supervised classification tasks - yet in a fraction of the training time. Lastly, we showcase how our model makes it possible to quickly and accurately fit real electrophysiological recordings of cortical neurons, where very fine sub-millisecond DTs are crucial for capturing exact spike timing.",
      "paper_authors": [
        "Luke Taylor",
        "Andrew J King",
        "Nicol S Harper"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-11-19",
      "update_time": "2023-11-19",
      "comments": "15 pages, 5 figures",
      "repo_url": "https://github.com/webstorms/blocks"
    },
    "2311.12060": {
      "paper_id": "2311.12060v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.12060v1",
      "paper_key": "2311.12060",
      "paper_title": "Pursing the Sparse Limitation of Spiking Deep Learning Structures",
      "paper_url": "http://arxiv.org/abs/2311.12060v1",
      "paper_abstract": "Spiking Neural Networks (SNNs), a novel brain-inspired algorithm, are garnering increased attention for their superior computation and energy efficiency over traditional artificial neural networks (ANNs). To facilitate deployment on memory-constrained devices, numerous studies have explored SNN pruning. However, these efforts are hindered by challenges such as scalability challenges in more complex architectures and accuracy degradation. Amidst these challenges, the Lottery Ticket Hypothesis (LTH) emerges as a promising pruning strategy. It posits that within dense neural networks, there exist winning tickets or subnetworks that are sparser but do not compromise performance. To explore a more structure-sparse and energy-saving model, we investigate the unique synergy of SNNs with LTH and design two novel spiking winning tickets to push the boundaries of sparsity within SNNs. Furthermore, we introduce an innovative algorithm capable of simultaneously identifying both weight and patch-level winning tickets, enabling the achievement of sparser structures without compromising on the final model's performance. Through comprehensive experiments on both RGB-based and event-based datasets, we demonstrate that our spiking lottery ticket achieves comparable or superior performance even when the model structure is extremely sparse.",
      "paper_authors": [
        "Hao Cheng",
        "Jiahang Cao",
        "Erjia Xiao",
        "Mengshu Sun",
        "Le Yang",
        "Jize Zhang",
        "Xue Lin",
        "Bhavya Kailkhura",
        "Kaidi Xu",
        "Renjing Xu"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-11-18",
      "update_time": "2023-11-18",
      "comments": null,
      "repo_url": "#"
    },
    "2311.10802": {
      "paper_id": "2311.10802v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.10802v1",
      "paper_key": "2311.10802",
      "paper_title": "Is Conventional SNN Really Efficient? A Perspective from Network Quantization",
      "paper_url": "http://arxiv.org/abs/2311.10802v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) have been widely praised for their high energy efficiency and immense potential. However, comprehensive research that critically contrasts and correlates SNNs with quantized Artificial Neural Networks (ANNs) remains scant, often leading to skewed comparisons lacking fairness towards ANNs. This paper introduces a unified perspective, illustrating that the time steps in SNNs and quantized bit-widths of activation values present analogous representations. Building on this, we present a more pragmatic and rational approach to estimating the energy consumption of SNNs. Diverging from the conventional Synaptic Operations (SynOps), we champion the \"Bit Budget\" concept. This notion permits an intricate discourse on strategically allocating computational and storage resources between weights, activation values, and temporal steps under stringent hardware constraints. Guided by the Bit Budget paradigm, we discern that pivoting efforts towards spike patterns and weight quantization, rather than temporal attributes, elicits profound implications for model performance. Utilizing the Bit Budget for holistic design consideration of SNNs elevates model performance across diverse data types, encompassing static imagery and neuromorphic datasets. Our revelations bridge the theoretical chasm between SNNs and quantized ANNs and illuminate a pragmatic trajectory for future endeavors in energy-efficient neural computations.",
      "paper_authors": [
        "Guobin Shen",
        "Dongcheng Zhao",
        "Tenglong Li",
        "Jindong Li",
        "Yi Zeng"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-11-17",
      "update_time": "2023-11-17",
      "comments": null,
      "repo_url": "#"
    },
    "2311.10411": {
      "paper_id": "2311.10411v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.10411v1",
      "paper_key": "2311.10411",
      "paper_title": "On exploiting the synaptic interaction properties to obtain frequency-specific neurons",
      "paper_url": "http://arxiv.org/abs/2311.10411v1",
      "paper_abstract": "Energy consumption remains the main limiting factors in many IoT applications. In particular, micro-controllers consume far too much power. In order to overcome this problem, new circuit designs have been proposed and the use of spiking neurons and analog computing has emerged as it allows a very significant consumption reduction. However, working in the analog domain brings difficulty to handle the sequential processing of incoming signals as is needed in many use cases. In this paper, we use a bio-inspired phenomenon called Interacting Synapses to produce a time filter, without using non-biological techniques such as synaptic delays. We propose a model of neuron and synapses that fire for a specific range of delays between two incoming spikes, but do not react when this Inter-Spike Timing is not in that range. We study the parameters of the model to understand how to choose them and adapt the Inter-Spike Timing. The originality of the paper is to propose a new way, in the analog domain, to deal with temporal sequences.",
      "paper_authors": [
        "Guillaume Marthe",
        "Claire Goursaud",
        "Romain Caz\u00e9",
        "Laurent Clavier"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-11-17",
      "update_time": "2023-11-17",
      "comments": null,
      "repo_url": "#"
    },
    "2311.09376": {
      "paper_id": "2311.09376v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.09376v1",
      "paper_key": "2311.09376",
      "paper_title": "DISTA: Denoising Spiking Transformer with intrinsic plasticity and spatiotemporal attention",
      "paper_url": "http://arxiv.org/abs/2311.09376v1",
      "paper_abstract": "Among the array of neural network architectures, the Vision Transformer (ViT) stands out as a prominent choice, acclaimed for its exceptional expressiveness and consistent high performance in various vision applications. Recently, the emerging Spiking ViT approach has endeavored to harness spiking neurons, paving the way for a more brain-inspired transformer architecture that thrives in ultra-low power operations on dedicated neuromorphic hardware. Nevertheless, this approach remains confined to spatial self-attention and doesn't fully unlock the potential of spiking neural networks. We introduce DISTA, a Denoising Spiking Transformer with Intrinsic Plasticity and SpatioTemporal Attention, designed to maximize the spatiotemporal computational prowess of spiking neurons, particularly for vision applications. DISTA explores two types of spatiotemporal attentions: intrinsic neuron-level attention and network-level attention with explicit memory. Additionally, DISTA incorporates an efficient nonlinear denoising mechanism to quell the noise inherent in computed spatiotemporal attention maps, thereby resulting in further performance gains. Our DISTA transformer undergoes joint training involving synaptic plasticity (i.e., weight tuning) and intrinsic plasticity (i.e., membrane time constant tuning) and delivers state-of-the-art performances across several static image and dynamic neuromorphic datasets. With only 6 time steps, DISTA achieves remarkable top-1 accuracy on CIFAR10 (96.26%) and CIFAR100 (79.15%), as well as 79.1% on CIFAR10-DVS using 10 time steps.",
      "paper_authors": [
        "Boxun Xu",
        "Hejia Geng",
        "Yuxuan Yin",
        "Peng Li"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-11-15",
      "update_time": "2023-11-15",
      "comments": null,
      "repo_url": "#"
    },
    "2311.09077": {
      "paper_id": "2311.09077v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.09077v3",
      "paper_key": "2311.09077",
      "paper_title": "Spiking NeRF: Representing the Real-World Geometry by a Discontinuous Representation",
      "paper_url": "http://arxiv.org/abs/2311.09077v3",
      "paper_abstract": "A crucial reason for the success of existing NeRF-based methods is to build a neural density field for the geometry representation via multiple perceptron layers (MLPs). MLPs are continuous functions, however, real geometry or density field is frequently discontinuous at the interface between the air and the surface. Such a contrary brings the problem of unfaithful geometry representation. To this end, this paper proposes spiking NeRF, which leverages spiking neurons and a hybrid Artificial Neural Network (ANN)-Spiking Neural Network (SNN) framework to build a discontinuous density field for faithful geometry representation. Specifically, we first demonstrate the reason why continuous density fields will bring inaccuracy. Then, we propose to use the spiking neurons to build a discontinuous density field. We conduct a comprehensive analysis for the problem of existing spiking neuron models and then provide the numerical relationship between the parameter of the spiking neuron and the theoretical accuracy of geometry. Based on this, we propose a bounded spiking neuron to build the discontinuous density field. Our method achieves SOTA performance. The source code and the supplementary material are available at https://github.com/liaozhanfeng/Spiking-NeRF.",
      "paper_authors": [
        "Zhanfeng Liao",
        "Qian Zheng",
        "Yan Liu",
        "Gang Pan"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-15",
      "update_time": "2024-08-23",
      "comments": null,
      "repo_url": "https://github.com/liaozhanfeng/spiking-nerf"
    },
    "2311.08806": {
      "paper_id": "2311.08806v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.08806v1",
      "paper_key": "2311.08806",
      "paper_title": "SparseSpikformer: A Co-Design Framework for Token and Weight Pruning in Spiking Transformer",
      "paper_url": "http://arxiv.org/abs/2311.08806v1",
      "paper_abstract": "As the third-generation neural network, the Spiking Neural Network (SNN) has the advantages of low power consumption and high energy efficiency, making it suitable for implementation on edge devices. More recently, the most advanced SNN, Spikformer, combines the self-attention module from Transformer with SNN to achieve remarkable performance. However, it adopts larger channel dimensions in MLP layers, leading to an increased number of redundant model parameters. To effectively decrease the computational complexity and weight parameters of the model, we explore the Lottery Ticket Hypothesis (LTH) and discover a very sparse ($\\ge$90%) subnetwork that achieves comparable performance to the original network. Furthermore, we also design a lightweight token selector module, which can remove unimportant background information from images based on the average spike firing rate of neurons, selecting only essential foreground image tokens to participate in attention calculation. Based on that, we present SparseSpikformer, a co-design framework aimed at achieving sparsity in Spikformer through token and weight pruning techniques. Experimental results demonstrate that our framework can significantly reduce 90% model parameters and cut down Giga Floating-Point Operations (GFLOPs) by 20% while maintaining the accuracy of the original model.",
      "paper_authors": [
        "Yue Liu",
        "Shanlin Xiao",
        "Bo Li",
        "Zhiyi Yu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-15",
      "update_time": "2023-11-15",
      "comments": null,
      "repo_url": "#"
    },
    "2311.14710": {
      "paper_id": "2311.14710v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.14710v1",
      "paper_key": "2311.14710",
      "paper_title": "Neuroscience inspired scientific machine learning (Part-2): Variable spiking wavelet neural operator",
      "paper_url": "http://arxiv.org/abs/2311.14710v1",
      "paper_abstract": "We propose, in this paper, a Variable Spiking Wavelet Neural Operator (VS-WNO), which aims to bridge the gap between theoretical and practical implementation of Artificial Intelligence (AI) algorithms for mechanics applications. With recent developments like the introduction of neural operators, AI's potential for being used in mechanics applications has increased significantly. However, AI's immense energy and resource requirements are a hurdle in its practical field use case. The proposed VS-WNO is based on the principles of spiking neural networks, which have shown promise in reducing the energy requirements of the neural networks. This makes possible the use of such algorithms in edge computing. The proposed VS-WNO utilizes variable spiking neurons, which promote sparse communication, thus conserving energy, and its use is further supported by its ability to tackle regression tasks, often faced in the field of mechanics. Various examples dealing with partial differential equations, like Burger's equation, Allen Cahn's equation, and Darcy's equation, have been shown. Comparisons have been shown against wavelet neural operator utilizing leaky integrate and fire neurons (direct and encoded inputs) and vanilla wavelet neural operator utilizing artificial neurons. The results produced illustrate the ability of the proposed VS-WNO to converge to ground truth while promoting sparse communication.",
      "paper_authors": [
        "Shailesh Garg",
        "Souvik Chakraborty"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-11-15",
      "update_time": "2023-11-15",
      "comments": null,
      "repo_url": "#"
    },
    "2311.09267": {
      "paper_id": "2311.09267v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.09267v1",
      "paper_key": "2311.09267",
      "paper_title": "Neuroscience inspired scientific machine learning (Part-1): Variable spiking neuron for regression",
      "paper_url": "http://arxiv.org/abs/2311.09267v1",
      "paper_abstract": "Redundant information transfer in a neural network can increase the complexity of the deep learning model, thus increasing its power consumption. We introduce in this paper a novel spiking neuron, termed Variable Spiking Neuron (VSN), which can reduce the redundant firing using lessons from biological neuron inspired Leaky Integrate and Fire Spiking Neurons (LIF-SN). The proposed VSN blends LIF-SN and artificial neurons. It garners the advantage of intermittent firing from the LIF-SN and utilizes the advantage of continuous activation from the artificial neuron. This property of the proposed VSN makes it suitable for regression tasks, which is a weak point for the vanilla spiking neurons, all while keeping the energy budget low. The proposed VSN is tested against both classification and regression tasks. The results produced advocate favorably towards the efficacy of the proposed spiking neuron, particularly for regression tasks.",
      "paper_authors": [
        "Shailesh Garg",
        "Souvik Chakraborty"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-11-15",
      "update_time": "2023-11-15",
      "comments": null,
      "repo_url": "#"
    },
    "2311.09266": {
      "paper_id": "2311.09266v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.09266v2",
      "paper_key": "2311.09266",
      "paper_title": "Adversarially Robust Spiking Neural Networks Through Conversion",
      "paper_url": "http://arxiv.org/abs/2311.09266v2",
      "paper_abstract": "Spiking neural networks (SNNs) provide an energy-efficient alternative to a variety of artificial neural network (ANN) based AI applications. As the progress in neuromorphic computing with SNNs expands their use in applications, the problem of adversarial robustness of SNNs becomes more pronounced. To the contrary of the widely explored end-to-end adversarial training based solutions, we address the limited progress in scalable robust SNN training methods by proposing an adversarially robust ANN-to-SNN conversion algorithm. Our method provides an efficient approach to embrace various computationally demanding robust learning objectives that have been proposed for ANNs. During a post-conversion robust finetuning phase, our method adversarially optimizes both layer-wise firing thresholds and synaptic connectivity weights of the SNN to maintain transferred robustness gains from the pre-trained ANN. We perform experimental evaluations in a novel setting proposed to rigorously assess the robustness of SNNs, where numerous adaptive adversarial attacks that account for the spike-based operation dynamics are considered. Results show that our approach yields a scalable state-of-the-art solution for adversarially robust deep SNNs with low-latency.",
      "paper_authors": [
        "Ozan \u00d6zdenizci",
        "Robert Legenstein"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-11-15",
      "update_time": "2024-04-12",
      "comments": "Transactions on Machine Learning Research (TMLR), 2024",
      "repo_url": "https://github.com/igitugraz/robustsnnconversion"
    },
    "2311.07787": {
      "paper_id": "2311.07787v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.07787v1",
      "paper_key": "2311.07787",
      "paper_title": "Hybrid Synaptic Structure for Spiking Neural Network Realization",
      "paper_url": "http://arxiv.org/abs/2311.07787v1",
      "paper_abstract": "Neural networks and neuromorphic computing play pivotal roles in deep learning and machine vision. Due to their dissipative nature and inherent limitations, traditional semiconductor-based circuits face challenges in realizing ultra-fast and low-power neural networks. However, the spiking behavior characteristic of single flux quantum (SFQ) circuits positions them as promising candidates for spiking neural networks (SNNs). Our previous work showcased a JJ-Soma design capable of operating at tens of gigahertz while consuming only a fraction of the power compared to traditional circuits, as documented in [1]. This paper introduces a compact SFQ-based synapse design that applies positive and negative weighted inputs to the JJ-Soma. Using an RSFQ synapse empowers us to replicate the functionality of a biological neuron, a crucial step in realizing a complete SNN. The JJ-Synapse can operate at ultra-high frequencies, exhibits orders of magnitude lower power consumption than CMOS counterparts, and can be conveniently fabricated using commercial Nb processes. Furthermore, the network's flexibility enables modifications by incorporating cryo-CMOS circuits for weight value adjustments. In our endeavor, we have successfully designed, fabricated, and partially tested the JJ-Synapse within our cryocooler system. Integration with the JJ-Soma further facilitates the realization of a high-speed inference SNN.",
      "paper_authors": [
        "Sasan Razmkhah",
        "Mustafa Altay Karamuftuoglu",
        "Ali Bozbey"
      ],
      "primary_category": "cond-mat.supr-con",
      "publish_time": "2023-11-13",
      "update_time": "2023-11-13",
      "comments": "7 pages, 10 figures",
      "repo_url": "#"
    },
    "2401.02429": {
      "paper_id": "2401.02429v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.02429v2",
      "paper_key": "2401.02429",
      "paper_title": "Brain-Inspired Spiking Neural Networks for Industrial Fault Diagnosis: A Survey, Challenges, and Opportunities",
      "paper_url": "http://arxiv.org/abs/2401.02429v2",
      "paper_abstract": "In recent decades, Industrial Fault Diagnosis (IFD) has emerged as a crucial discipline concerned with detecting and gathering vital information about industrial equipment's health condition, thereby facilitating the identification of failure types and severities. The pursuit of precise and effective fault recognition has garnered substantial attention, culminating in a focus on automating equipment monitoring to preclude safety accidents and reduce reliance on human labor. The advent of artificial neural networks (ANNs) has been instrumental in augmenting intelligent IFD algorithms, particularly in the context of big data. Despite these advancements, ANNs, being a simplified biomimetic neural network model, exhibit inherent limitations such as resource and data dependencies and restricted cognitive capabilities. To address these limitations, the third-generation Spiking Neural Network (SNN), founded on principles of Brain-inspired computing, has surfaced as a promising alternative. The SNN, characterized by its biological neuron dynamics and spiking information encoding, demonstrates exceptional potential in representing spatiotemporal features. Consequently, developing SNN-based IFD models has gained momentum, displaying encouraging performance. Nevertheless, this field lacks systematic surveys to illustrate the current situation, challenges, and future directions. Therefore, this paper systematically reviews the theoretical progress of SNN-based models to answer the question of what SNN is. Subsequently, it reviews and analyzes existing SNN-based IFD models to explain why SNN needs to be used and how to use it. More importantly, this paper systematically answers the challenges, solutions, and opportunities of SNN in IFD.",
      "paper_authors": [
        "Huan Wang",
        "Yan-Fu Li",
        "Konstantinos Gryllias"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-11-13",
      "update_time": "2024-06-04",
      "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible",
      "repo_url": "#"
    },
    "2311.07625": {
      "paper_id": "2311.07625v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.07625v2",
      "paper_key": "2311.07625",
      "paper_title": "Activity Sparsity Complements Weight Sparsity for Efficient RNN Inference",
      "paper_url": "http://arxiv.org/abs/2311.07625v2",
      "paper_abstract": "Artificial neural networks open up unprecedented machine learning capabilities at the cost of ever growing computational requirements. Sparsifying the parameters, often achieved through weight pruning, has been identified as a powerful technique to compress the number of model parameters and reduce the computational operations of neural networks. Yet, sparse activations, while omnipresent in both biological neural networks and deep learning systems, have not been fully utilized as a compression technique in deep learning. Moreover, the interaction between sparse activations and weight pruning is not fully understood. In this work, we demonstrate that activity sparsity can compose multiplicatively with parameter sparsity in a recurrent neural network model based on the GRU that is designed to be activity sparse. We achieve up to $20\\times$ reduction of computation while maintaining perplexities below $60$ on the Penn Treebank language modeling task. This magnitude of reduction has not been achieved previously with solely sparsely connected LSTMs, and the language modeling performance of our model has not been achieved previously with any sparsely activated recurrent neural networks or spiking neural networks. Neuromorphic computing devices are especially good at taking advantage of the dynamic activity sparsity, and our results provide strong evidence that making deep learning models activity sparse and porting them to neuromorphic devices can be a viable strategy that does not compromise on task performance. Our results also drive further convergence of methods from deep learning and neuromorphic computing for efficient machine learning.",
      "paper_authors": [
        "Rishav Mukherji",
        "Mark Sch\u00f6ne",
        "Khaleelulla Khan Nazeer",
        "Christian Mayr",
        "Anand Subramoney"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2023-11-13",
      "update_time": "2023-12-07",
      "comments": "Accepted to the First MLNCP Workshop @ NeurIPS 2023",
      "repo_url": "#"
    },
    "2311.06570": {
      "paper_id": "2311.06570v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.06570v3",
      "paper_key": "2311.06570",
      "paper_title": "SynA-ResNet: Spike-driven ResNet Achieved through OR Residual Connection",
      "paper_url": "http://arxiv.org/abs/2311.06570v3",
      "paper_abstract": "Spiking Neural Networks (SNNs) have garnered substantial attention in brain-like computing for their biological fidelity and the capacity to execute energy-efficient spike-driven operations. As the demand for heightened performance in SNNs surges, the trend towards training deeper networks becomes imperative, while residual learning stands as a pivotal method for training deep neural networks. In our investigation, we identified that the SEW-ResNet, a prominent representative of deep residual spiking neural networks, incorporates non-event-driven operations. To rectify this, we propose a novel training paradigm that first accumulates a large amount of redundant information through OR Residual Connection (ORRC), and then filters out the redundant information using the Synergistic Attention (SynA) module, which promotes feature extraction in the backbone while suppressing the influence of noise and useless features in the shortcuts. When integrating SynA into the network, we observed the phenomenon of \"natural pruning\", where after training, some or all of the shortcuts in the network naturally drop out without affecting the model's classification accuracy. This significantly reduces computational overhead and makes it more suitable for deployment on edge devices. Experimental results on various public datasets confirmed that the SynA-ResNet achieved single-sample classification with as little as 0.8 spikes per neuron. Moreover, when compared to other residual SNN models, it exhibited higher accuracy and up to a 28-fold reduction in energy consumption.",
      "paper_authors": [
        "Yimeng Shan",
        "Xuerui Qiu",
        "Rui-jie Zhu",
        "Jason K. Eshraghian",
        "Malu Zhang",
        "Haicheng Qu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-11",
      "update_time": "2024-07-08",
      "comments": "12 pages, 5 figures and 10 tables",
      "repo_url": "https://github.com/ym-shan/orrc-syna-natural-pruning"
    },
    "2311.06159": {
      "paper_id": "2311.06159v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.06159v1",
      "paper_key": "2311.06159",
      "paper_title": "Discrete synaptic events induce global oscillations in balanced neural networks",
      "paper_url": "http://arxiv.org/abs/2311.06159v1",
      "paper_abstract": "Neural dynamics is triggered by discrete synaptic inputs of finite amplitude. However, the neural response is usually obtained within the diffusion approximation (DA) representing the synaptic inputs as Gaussian noise. We derive a mean-field formalism encompassing synaptic shot-noise for sparse balanced networks of spiking neurons. For low (high) external drives (synaptic strengths) irregular global oscillations emerge via continuous and hysteretic transitions, correctly predicted by our approach, but not from the DA. These oscillations display frequencies in biologically relevant bands.",
      "paper_authors": [
        "Denis S. Goldobin",
        "Matteo di Volo",
        "Alessandro Torcini"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2023-11-10",
      "update_time": "2023-11-10",
      "comments": "6 pages, 3 figures",
      "repo_url": "#"
    },
    "2311.06074": {
      "paper_id": "2311.06074v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.06074v2",
      "paper_key": "2311.06074",
      "paper_title": "Two-compartment neuronal spiking model expressing brain-state specific apical-amplification, -isolation and -drive regimes",
      "paper_url": "http://arxiv.org/abs/2311.06074v2",
      "paper_abstract": "Mounting experimental evidence suggests that brain-state-specific neural mechanisms, supported by connectomic architectures, play a crucial role in integrating past and contextual knowledge with the current, incoming flow of evidence (e.g., from sensory systems). These mechanisms operate across multiple spatial and temporal scales, necessitating dedicated support at the levels of individual neurons and synapses. A notable feature within the neocortex is the structure of large, deep pyramidal neurons, which exhibit a distinctive separation between an apical dendritic compartment and a basal dendritic/perisomatic compartment. This separation is characterized by distinct patterns of incoming connections and brain-state-specific activation mechanisms, namely, apical amplification, isolation, and drive, which are associated with wakefulness, deeper NREM sleep stages, and REM sleep, respectively. The cognitive roles of apical mechanisms have been demonstrated in behaving animals. In contrast, classical models of learning in spiking networks are based on single-compartment neurons, lacking the ability to describe the integration of apical and basal/somatic information. This work aims to provide the computational community with a two-compartment spiking neuron model that incorporates features essential for supporting brain-state-specific learning. This model includes a piece-wise linear transfer function (ThetaPlanes) at the highest abstraction level, making it suitable for use in large-scale bio-inspired artificial intelligence systems. A machine learning evolutionary algorithm, guided by a set of fitness functions, selected the parameters that define neurons expressing the desired apical mechanisms.",
      "paper_authors": [
        "Elena Pastorelli",
        "Alper Yegenoglu",
        "Nicole Kolodziej",
        "Willem Wybo",
        "Francesco Simula",
        "Sandra Diaz",
        "Johan Frederik Storm",
        "Pier Stanislao Paolucci"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2023-11-10",
      "update_time": "2024-03-26",
      "comments": "23 pages, 9 figures (29 single images), 4 tables, paper",
      "repo_url": "#"
    },
    "2311.05442": {
      "paper_id": "2311.05442v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.05442v1",
      "paper_key": "2311.05442",
      "paper_title": "Firing rate distributions in plastic networks of spiking neurons",
      "paper_url": "http://arxiv.org/abs/2311.05442v1",
      "paper_abstract": "In recurrent networks of leaky integrate-and-fire (LIF) neurons, mean-field theory has proven successful in describing various statistical properties of neuronal activity at equilibrium, such as firing rate distributions. Mean-field theory has been applied to networks in which either the synaptic weights are homogeneous across synapses and the number of incoming connections of individual neurons is heterogeneous, or vice versa. Here we extend the previous mean-field formalisms to treat networks in which these two sources of structural heterogeneity occur simultaneously, including networks whose synapses are subject to plastic, activity-dependent modulation. The plasticity in our model is mediated by the introduction of one spike trace per neuron: a chemical signal that is released every time the neuron emits a spike and which is degraded over time. The temporal evolution of the trace is controlled by its degradation rate $r_p$ and by the neuron's underlying firing rate $\\nu$. When the ratio $\\alpha=\\nu / r_p$ tends to infinity, the trace can be rescaled to be a reliable estimation of the neuron's firing rate. In this regime, the value of any synaptic weight at equilibrium is a function of the pre- and post-synaptic firing rates, and this relation can be used in the mean-field formalism. The solution to the mean-field equations specifies the firing rate and synaptic weight distributions at equilibrium. These equations are exact in the limit of reliable traces but they already provide accurate results when the degradation rate lies within a reasonable range, as we show by comparison with simulations of the full neuronal dynamics in networks composed of excitatory and inhibitory LIF neurons. Overall, this work offers a way to explore and better understand the way in which plasticity shapes both activity and structure in neuronal networks.",
      "paper_authors": [
        "Marina Vegu\u00e9",
        "Antoine Allard",
        "Patrick Desrosiers"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2023-11-09",
      "update_time": "2023-11-09",
      "comments": "29 pages, 7 figures",
      "repo_url": "#"
    },
    "2311.05210": {
      "paper_id": "2311.05210v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.05210v1",
      "paper_key": "2311.05210",
      "paper_title": "From \"What\" to \"When\" -- a Spiking Neural Network Predicting Rare Events and Time to their Occurrence",
      "paper_url": "http://arxiv.org/abs/2311.05210v1",
      "paper_abstract": "In the reinforcement learning (RL) tasks, the ability to predict receiving reward in the near or more distant future means the ability to evaluate the current state as more or less close to the target state (labelled by the reward signal). In the present work, we utilize a spiking neural network (SNN) to predict time to the next target event (reward - in case of RL). In the context of SNNs, events are represented as spikes emitted by network neurons or input nodes. It is assumed that target events are indicated by spikes emitted by a special network input node. Using description of the current state encoded in the form of spikes from the other input nodes, the network should predict approximate time of the next target event. This research paper presents a novel approach to learning the corresponding predictive model by an SNN consisting of leaky integrate-and-fire (LIF) neurons. The proposed method leverages specially designed local synaptic plasticity rules and a novel columnar-layered SNN architecture. Similar to our previous works, this study places a strong emphasis on the hardware-friendliness of the proposed models, ensuring their efficient implementation on modern and future neuroprocessors. The approach proposed was tested on a simple reward prediction task in the context of one of the RL benchmark ATARI games, ping-pong. It was demonstrated that the SNN described in this paper gives superior prediction accuracy in comparison with precise machine learning techniques, such as decision tree algorithms and convolutional neural networks.",
      "paper_authors": [
        "Mikhail Kiselev"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-11-09",
      "update_time": "2023-11-09",
      "comments": "arXiv admin note: substantial text overlap with arXiv:2309.08476",
      "repo_url": "#"
    },
    "2311.05171": {
      "paper_id": "2311.05171v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.05171v1",
      "paper_key": "2311.05171",
      "paper_title": "Rethinking Residual Connection in Training Large-Scale Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2311.05171v1",
      "paper_abstract": "Spiking Neural Network (SNN) is known as the most famous brain-inspired model, but the non-differentiable spiking mechanism makes it hard to train large-scale SNNs. To facilitate the training of large-scale SNNs, many training methods are borrowed from Artificial Neural Networks (ANNs), among which deep residual learning is the most commonly used. But the unique features of SNNs make prior intuition built upon ANNs not available for SNNs. Although there are a few studies that have made some pioneer attempts on the topology of Spiking ResNet, the advantages of different connections remain unclear. To tackle this issue, we analyze the merits and limitations of various residual connections and empirically demonstrate our ideas with extensive experiments. Then, based on our observations, we abstract the best-performing connections into densely additive (DA) connection, extend such a concept to other topologies, and propose four architectures for training large-scale SNNs, termed DANet, which brings up to 13.24% accuracy gain on ImageNet. Besides, in order to present a detailed methodology for designing the topology of large-scale SNNs, we further conduct in-depth discussions on their applicable scenarios in terms of their performance on various scales of datasets and demonstrate their advantages over prior architectures. At a low training expense, our best-performing ResNet-50/101/152 obtain 73.71%/76.13%/77.22% top-1 accuracy on ImageNet with 4 time steps. We believe that this work shall give more insights for future works to design the topology of their networks and promote the development of large-scale SNNs. The code will be publicly available.",
      "paper_authors": [
        "Yudong Li",
        "Yunlin Lei",
        "Xu Yang"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-11-09",
      "update_time": "2023-11-09",
      "comments": null,
      "repo_url": "#"
    },
    "2311.04558": {
      "paper_id": "2311.04558v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.04558v1",
      "paper_key": "2311.04558",
      "paper_title": "Free-Space Optical Spiking Neural Network",
      "paper_url": "http://arxiv.org/abs/2311.04558v1",
      "paper_abstract": "Neuromorphic engineering has emerged as a promising avenue for developing brain-inspired computational systems. However, conventional electronic AI-based processors often encounter challenges related to processing speed and thermal dissipation. As an alternative, optical implementations of such processors have been proposed, capitalizing on the intrinsic information-processing capabilities of light. Within the realm of optical neuromorphic engineering, various optical neural networks (ONNs) have been explored. Among these, Spiking Neural Networks (SNNs) have exhibited notable success in emulating the computational principles of the human brain. Nevertheless, the integration of optical SNN processors has presented formidable obstacles, mainly when dealing with the computational demands of large datasets. In response to these challenges, we introduce a pioneering concept: the Free-space Optical deep Spiking Convolutional Neural Network (OSCNN). This novel approach draws inspiration from computational models of the human eye. We have meticulously designed various optical components within the OSCNN to tackle object detection tasks across prominent benchmark datasets, including MNIST, ETH 80, and Caltech. Our results demonstrate promising performance with minimal latency and power consumption compared to their electronic ONN counterparts. Additionally, we conducted several pertinent simulations, such as optical intensity to-latency conversion and synchronization. Of particular significance is the evaluation of the feature extraction layer, employing a Gabor filter bank, which stands to impact the practical deployment of diverse ONN architectures significantly.",
      "paper_authors": [
        "Reyhane Ahmadi",
        "Amirreza Ahmadnejad",
        "Somayyeh Koohi"
      ],
      "primary_category": "physics.optics",
      "publish_time": "2023-11-08",
      "update_time": "2023-11-08",
      "comments": null,
      "repo_url": "#"
    },
    "2311.04386": {
      "paper_id": "2311.04386v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.04386v1",
      "paper_key": "2311.04386",
      "paper_title": "Harnessing Manycore Processors with Distributed Memory for Accelerated Training of Sparse and Recurrent Models",
      "paper_url": "http://arxiv.org/abs/2311.04386v1",
      "paper_abstract": "Current AI training infrastructure is dominated by single instruction multiple data (SIMD) and systolic array architectures, such as Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs), that excel at accelerating parallel workloads and dense vector matrix multiplications. Potentially more efficient neural network models utilizing sparsity and recurrence cannot leverage the full power of SIMD processor and are thus at a severe disadvantage compared to today's prominent parallel architectures like Transformers and CNNs, thereby hindering the path towards more sustainable AI. To overcome this limitation, we explore sparse and recurrent model training on a massively parallel multiple instruction multiple data (MIMD) architecture with distributed local memory. We implement a training routine based on backpropagation through time (BPTT) for the brain-inspired class of Spiking Neural Networks (SNNs) that feature binary sparse activations. We observe a massive advantage in using sparse activation tensors with a MIMD processor, the Intelligence Processing Unit (IPU) compared to GPUs. On training workloads, our results demonstrate 5-10x throughput gains compared to A100 GPUs and up to 38x gains for higher levels of activation sparsity, without a significant slowdown in training convergence or reduction in final model performance. Furthermore, our results show highly promising trends for both single and multi IPU configurations as we scale up to larger model sizes. Our work paves the way towards more efficient, non-standard models via AI training hardware beyond GPUs, and competitive large scale SNN models.",
      "paper_authors": [
        "Jan Finkbeiner",
        "Thomas Gmeinder",
        "Mark Pupilli",
        "Alexander Titterton",
        "Emre Neftci"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-11-07",
      "update_time": "2023-11-07",
      "comments": null,
      "repo_url": "#"
    },
    "2311.16151": {
      "paper_id": "2311.16151v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.16151v1",
      "paper_key": "2311.16151",
      "paper_title": "Estimating Post-Synaptic Effects for Online Training of Feed-Forward SNNs",
      "paper_url": "http://arxiv.org/abs/2311.16151v1",
      "paper_abstract": "Facilitating online learning in spiking neural networks (SNNs) is a key step in developing event-based models that can adapt to changing environments and learn from continuous data streams in real-time. Although forward-mode differentiation enables online learning, its computational requirements restrict scalability. This is typically addressed through approximations that limit learning in deep models. In this study, we propose Online Training with Postsynaptic Estimates (OTPE) for training feed-forward SNNs, which approximates Real-Time Recurrent Learning (RTRL) by incorporating temporal dynamics not captured by current approximations, such as Online Training Through Time (OTTT) and Online Spatio-Temporal Learning (OSTL). We show improved scaling for multi-layer networks using a novel approximation of temporal effects on the subsequent layer's activity. This approximation incurs minimal overhead in the time and space complexity compared to similar algorithms, and the calculation of temporal effects remains local to each layer. We characterize the learning performance of our proposed algorithms on multiple SNN model configurations for rate-based and time-based encoding. OTPE exhibits the highest directional alignment to exact gradients, calculated with backpropagation through time (BPTT), in deep networks and, on time-based encoding, outperforms other approximate methods. We also observe sizeable gains in average performance over similar algorithms in offline training of Spiking Heidelberg Digits with equivalent hyper-parameters (OTTT/OSTL - 70.5%; OTPE - 75.2%; BPTT - 78.1%).",
      "paper_authors": [
        "Thomas Summe",
        "Clemens JS Schaefer",
        "Siddharth Joshi"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-11-07",
      "update_time": "2023-11-07",
      "comments": null,
      "repo_url": "https://github.com/intelligent-microsystems-lab/otpe"
    },
    "2311.16141": {
      "paper_id": "2311.16141v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.16141v2",
      "paper_key": "2311.16141",
      "paper_title": "Criticality-Guided Efficient Pruning in Spiking Neural Networks Inspired by Critical Brain Hypothesis",
      "paper_url": "http://arxiv.org/abs/2311.16141v2",
      "paper_abstract": "Spiking Neural Networks (SNNs) have gained considerable attention due to the energy-efficient and multiplication-free characteristics. The continuous growth in scale of deep SNNs poses challenges for model deployment. Network pruning reduces hardware resource requirements of model deployment by compressing the network scale. However, existing SNN pruning methods cause high pruning costs and performance loss because the pruning iterations amplify the training difficulty of SNNs. In this paper, inspired by the critical brain hypothesis in neuroscience, we propose a regeneration mechanism based on the neuron criticality for SNN pruning to enhance feature extraction and accelerate the pruning process. Firstly, we propose a low-cost metric for the criticality in SNNs. Then, we re-rank the pruned structures after pruning and regenerate those with higher criticality to obtain the critical network. Our method achieves higher performance than the current state-of-the-art (SOTA) method with up to 95.26% reduction of pruning cost. Moreover, we investigate the underlying mechanism of our method and find that it efficiently selects potential structures and learns the consistent feature representation.",
      "paper_authors": [
        "Shuo Chen",
        "Boxiao Liu",
        "Haihang You"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-11-05",
      "update_time": "2024-01-20",
      "comments": null,
      "repo_url": "#"
    },
    "2311.02110": {
      "paper_id": "2311.02110v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.02110v1",
      "paper_key": "2311.02110",
      "paper_title": "Feature Attribution Explanations for Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2311.02110v1",
      "paper_abstract": "Third-generation artificial neural networks, Spiking Neural Networks (SNNs), can be efficiently implemented on hardware. Their implementation on neuromorphic chips opens a broad range of applications, such as machine learning-based autonomous control and intelligent biomedical devices. In critical applications, however, insight into the reasoning of SNNs is important, thus SNNs need to be equipped with the ability to explain how decisions are reached. We present \\textit{Temporal Spike Attribution} (TSA), a local explanation method for SNNs. To compute the explanation, we aggregate all information available in model-internal variables: spike times and model weights. We evaluate TSA on artificial and real-world time series data and measure explanation quality w.r.t. multiple quantitative criteria. We find that TSA correctly identifies a small subset of input features relevant to the decision (i.e., is output-complete and compact) and generates similar explanations for similar inputs (i.e., is continuous). Further, our experiments show that incorporating the notion of \\emph{absent} spikes improves explanation quality. Our work can serve as a starting point for explainable SNNs, with future implementations on hardware yielding not only predictions but also explanations in a broad range of application scenarios. Source code is available at https://github.com/ElisaNguyen/tsa-explanations.",
      "paper_authors": [
        "Elisa Nguyen",
        "Meike Nauta",
        "Gwenn Englebienne",
        "Christin Seifert"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-11-02",
      "update_time": "2023-11-02",
      "comments": "Accepted at IEEE CogMI 2023, copyright final version IEEE",
      "repo_url": "https://github.com/elisanguyen/tsa-explanations"
    },
    "2310.19067": {
      "paper_id": "2310.19067v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.19067v1",
      "paper_key": "2310.19067",
      "paper_title": "Expanding memory in recurrent spiking networks",
      "paper_url": "http://arxiv.org/abs/2310.19067v1",
      "paper_abstract": "Recurrent spiking neural networks (RSNNs) are notoriously difficult to train because of the vanishing gradient problem that is enhanced by the binary nature of the spikes. In this paper, we review the ability of the current state-of-the-art RSNNs to solve long-term memory tasks, and show that they have strong constraints both in performance, and for their implementation on hardware analog neuromorphic processors. We present a novel spiking neural network that circumvents these limitations. Our biologically inspired neural network uses synaptic delays, branching factor regularization and a novel surrogate derivative for the spiking function. The proposed network proves to be more successful in using the recurrent connections on memory tasks.",
      "paper_authors": [
        "Ismael Balafrej",
        "Fabien Alibart",
        "Jean Rouat"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-10-29",
      "update_time": "2023-10-29",
      "comments": null,
      "repo_url": "#"
    },
    "2310.19062": {
      "paper_id": "2310.19062v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.19062v2",
      "paper_key": "2310.19062",
      "paper_title": "A multi-modal table tennis robot system",
      "paper_url": "http://arxiv.org/abs/2310.19062v2",
      "paper_abstract": "In recent years, robotic table tennis has become a popular research challenge for perception and robot control. Here, we present an improved table tennis robot system with high accuracy vision detection and fast robot reaction. Based on previous work, our system contains a KUKA robot arm with 6 DOF, with four frame-based cameras and two additional event-based cameras. We developed a novel calibration approach to calibrate this multimodal perception system. For table tennis, spin estimation is crucial. Therefore, we introduced a novel, and more accurate spin estimation approach. Finally, we show how combining the output of an event-based camera and a Spiking Neural Network (SNN) can be used for accurate ball detection.",
      "paper_authors": [
        "Andreas Ziegler",
        "Thomas Gossard",
        "Karl Vetter",
        "Jonas Tebbe",
        "Andreas Zell"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2023-10-29",
      "update_time": "2023-11-25",
      "comments": "Accepted for RoboLetics: Workshop on Robot Learning in Athletics\n  @CoRL 2023",
      "repo_url": "#"
    },
    "2310.16983": {
      "paper_id": "2310.16983v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.16983v1",
      "paper_key": "2310.16983",
      "paper_title": "WaLiN-GUI: a graphical and auditory tool for neuron-based encoding",
      "paper_url": "http://arxiv.org/abs/2310.16983v1",
      "paper_abstract": "Neuromorphic computing relies on spike-based, energy-efficient communication, inherently implying the need for conversion between real-valued (sensory) data and binary, sparse spiking representation. This is usually accomplished using the real valued data as current input to a spiking neuron model, and tuning the neuron's parameters to match a desired, often biologically inspired behaviour. We developed a tool, the WaLiN-GUI, that supports the investigation of neuron models and parameter combinations to identify suitable configurations for neuron-based encoding of sample-based data into spike trains. Due to the generalized LIF model implemented by default, next to the LIF and Izhikevich neuron models, many spiking behaviors can be investigated out of the box, thus offering the possibility of tuning biologically plausible responses to the input data. The GUI is provided open source and with documentation, being easy to extend with further neuron models and personalize with data analysis functions.",
      "paper_authors": [
        "Simon F. M\u00fcller-Cleve",
        "Fernando M. Quintana",
        "Vittorio Fra",
        "Pedro L. Galindo",
        "Fernando Perez-Pe\u00f1a",
        "Gianvito Urgese",
        "Chiara Bartolozzi"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-10-25",
      "update_time": "2023-10-25",
      "comments": "4 pages, 1 figure",
      "repo_url": "#"
    },
    "2310.16745": {
      "paper_id": "2310.16745v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.16745v1",
      "paper_key": "2310.16745",
      "paper_title": "Design Space Exploration of Sparsity-Aware Application-Specific Spiking Neural Network Accelerators",
      "paper_url": "http://arxiv.org/abs/2310.16745v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) offer a promising alternative to Artificial Neural Networks (ANNs) for deep learning applications, particularly in resource-constrained systems. This is largely due to their inherent sparsity, influenced by factors such as the input dataset, the length of the spike train, and the network topology. While a few prior works have demonstrated the advantages of incorporating sparsity into the hardware design, especially in terms of reducing energy consumption, the impact on hardware resources has not yet been explored. This is where design space exploration (DSE) becomes crucial, as it allows for the optimization of hardware performance by tailoring both the hardware and model parameters to suit specific application needs. However, DSE can be extremely challenging given the potentially large design space and the interplay of hardware architecture design choices and application-specific model parameters.   In this paper, we propose a flexible hardware design that leverages the sparsity of SNNs to identify highly efficient, application-specific accelerator designs. We develop a high-level, cycle-accurate simulation framework for this hardware and demonstrate the framework's benefits in enabling detailed and fine-grained exploration of SNN design choices, such as the layer-wise logical-to-hardware ratio (LHR). Our experimental results show that our design can (i) achieve up to $76\\%$ reduction in hardware resources and (ii) deliver a speed increase of up to $31.25\\times$, while requiring $27\\%$ fewer hardware resources compared to sparsity-oblivious designs. We further showcase the robustness of our framework by varying spike train lengths with different neuron population sizes to find the optimal trade-off points between accuracy and hardware latency.",
      "paper_authors": [
        "Ilkin Aliyev. Kama Svoboda",
        "Tosiron Adegbija"
      ],
      "primary_category": "cs.AR",
      "publish_time": "2023-10-25",
      "update_time": "2023-10-25",
      "comments": null,
      "repo_url": "https://github.com/githubofaliyev/snn-dse"
    },
    "2310.16675": {
      "paper_id": "2310.16675v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.16675v2",
      "paper_key": "2310.16675",
      "paper_title": "Agreeing to Stop: Reliable Latency-Adaptive Decision Making via Ensembles of Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2310.16675v2",
      "paper_abstract": "Spiking neural networks (SNNs) are recurrent models that can leverage sparsity in input time series to efficiently carry out tasks such as classification. Additional efficiency gains can be obtained if decisions are taken as early as possible as a function of the complexity of the input time series. The decision on when to stop inference and produce a decision must rely on an estimate of the current accuracy of the decision. Prior work demonstrated the use of conformal prediction (CP) as a principled way to quantify uncertainty and support adaptive-latency decisions in SNNs. In this paper, we propose to enhance the uncertainty quantification capabilities of SNNs by implementing ensemble models for the purpose of improving the reliability of stopping decisions. Intuitively, an ensemble of multiple models can decide when to stop more reliably by selecting times at which most models agree that the current accuracy level is sufficient. The proposed method relies on different forms of information pooling from ensemble models, and offers theoretical reliability guarantees. We specifically show that variational inference-based ensembles with p-variable pooling significantly reduce the average latency of state-of-the-art methods, while maintaining reliability guarantees.",
      "paper_authors": [
        "Jiechen Chen",
        "Sangwoo Park",
        "Osvaldo Simeone"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-10-25",
      "update_time": "2023-12-16",
      "comments": "Under review",
      "repo_url": "#"
    },
    "2310.16620": {
      "paper_id": "2310.16620v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.16620v1",
      "paper_key": "2310.16620",
      "paper_title": "SpikingJelly: An open-source machine learning infrastructure platform for spike-based intelligence",
      "paper_url": "http://arxiv.org/abs/2310.16620v1",
      "paper_abstract": "Spiking neural networks (SNNs) aim to realize brain-inspired intelligence on neuromorphic chips with high energy efficiency by introducing neural dynamics and spike properties. As the emerging spiking deep learning paradigm attracts increasing interest, traditional programming frameworks cannot meet the demands of the automatic differentiation, parallel computation acceleration, and high integration of processing neuromorphic datasets and deployment. In this work, we present the SpikingJelly framework to address the aforementioned dilemma. We contribute a full-stack toolkit for pre-processing neuromorphic datasets, building deep SNNs, optimizing their parameters, and deploying SNNs on neuromorphic chips. Compared to existing methods, the training of deep SNNs can be accelerated $11\\times$, and the superior extensibility and flexibility of SpikingJelly enable users to accelerate custom models at low costs through multilevel inheritance and semiautomatic code generation. SpikingJelly paves the way for synthesizing truly energy-efficient SNN-based machine intelligence systems, which will enrich the ecology of neuromorphic computing.",
      "paper_authors": [
        "Wei Fang",
        "Yanqi Chen",
        "Jianhao Ding",
        "Zhaofei Yu",
        "Timoth\u00e9e Masquelier",
        "Ding Chen",
        "Liwei Huang",
        "Huihui Zhou",
        "Guoqi Li",
        "Yonghong Tian"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-10-25",
      "update_time": "2023-10-25",
      "comments": "Accepted in Science Advances\n  (https://www.science.org/doi/10.1126/sciadv.adi1480)",
      "repo_url": "https://github.com/fangwei123456/spikingjelly"
    },
    "2310.16444": {
      "paper_id": "2310.16444v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.16444v1",
      "paper_key": "2310.16444",
      "paper_title": "How can neuromorphic hardware attain brain-like functional capabilities?",
      "paper_url": "http://arxiv.org/abs/2310.16444v1",
      "paper_abstract": "Research on neuromorphic computing is driven by the vision that we can emulate brain-like computing capability, learning capability, and energy-efficiency in novel hardware. Unfortunately, this vision has so far been pursued in a half-hearted manner. Most current neuromorphic hardware (NMHW) employs brain-like spiking neurons instead of standard artificial neurons. This is a good first step, which does improve the energy-efficiency of some computations, see \\citep{rao2022long} for one of many examples. But current architectures and training methods for networks of spiking neurons in NMHW are largely copied from artificial neural networks. Hence it is not surprising that they inherit many deficiencies of artificial neural networks, rather than attaining brain-like functional capabilities.   Of course, the brain is very complex, and we cannot implement all its details in NMHW. Instead, we need to focus on principles that are both easy to implement in NMHW and are likely to support brain-like functionality. The goal of this article is to highlight some of them.",
      "paper_authors": [
        "Wolfgang Maass"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-10-25",
      "update_time": "2023-10-25",
      "comments": null,
      "repo_url": "#"
    },
    "2310.15635": {
      "paper_id": "2310.15635v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.15635v1",
      "paper_key": "2310.15635",
      "paper_title": "Exploitation des propri{\u00e9}t{\u00e9}s de saturation synaptique pour obtenir un neurone {\u00e0} fr{\u00e9}quence sp{\u00e9}cifique",
      "paper_url": "http://arxiv.org/abs/2310.15635v1",
      "paper_abstract": "Energy consumption remains the main limiting factors in many promising IoT applications. In particular, micro-controllers consume far too much power. In order to overcome this problem, new circuit designs have been proposed and the use of spiking neurons and analog computing has emerged as it allows a very significant consumption reduction. However, working in the analog domain brings difficulty to handle the sequential processing of incoming signals as is needed in many use cases.In this paper, we propose to use a bio-inspired phenomenon called Interacting Synapses to produce a time filter. We propose a model of synapses that makes the neuron fire for a specific range of delays between two incoming spikes, but not react when this Inter-Spike Timing is not in that range. We study the parameters of the model to understand how to adapt the Inter-Spike Timing. The originality of the paper is to propose a new way, in the analog domain, to deal with temporal sequences.",
      "paper_authors": [
        "Guillaume Marthe",
        "Claire Goursaud"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2023-10-24",
      "update_time": "2023-10-24",
      "comments": "in French language",
      "repo_url": "#"
    },
    "2310.14978": {
      "paper_id": "2310.14978v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.14978v1",
      "paper_key": "2310.14978",
      "paper_title": "LC-TTFS: Towards Lossless Network Conversion for Spiking Neural Networks with TTFS Coding",
      "paper_url": "http://arxiv.org/abs/2310.14978v1",
      "paper_abstract": "The biological neurons use precise spike times, in addition to the spike firing rate, to communicate with each other. The time-to-first-spike (TTFS) coding is inspired by such biological observation. However, there is a lack of effective solutions for training TTFS-based spiking neural network (SNN). In this paper, we put forward a simple yet effective network conversion algorithm, which is referred to as LC-TTFS, by addressing two main problems that hinder an effective conversion from a high-performance artificial neural network (ANN) to a TTFS-based SNN. We show that our algorithm can achieve a near-perfect mapping between the activation values of an ANN and the spike times of an SNN on a number of challenging AI tasks, including image classification, image reconstruction, and speech enhancement. With TTFS coding, we can achieve up to orders of magnitude saving in computation over ANN and other rate-based SNNs. The study, therefore, paves the way for deploying ultra-low-power TTFS-based SNNs on power-constrained edge computing platforms.",
      "paper_authors": [
        "Qu Yang",
        "Malu Zhang",
        "Jibin Wu",
        "Kay Chen Tan",
        "Haizhou Li"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-10-23",
      "update_time": "2023-10-23",
      "comments": null,
      "repo_url": "#"
    },
    "2310.14839": {
      "paper_id": "2310.14839v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.14839v2",
      "paper_key": "2310.14839",
      "paper_title": "ESVAE: An Efficient Spiking Variational Autoencoder with Reparameterizable Poisson Spiking Sampling",
      "paper_url": "http://arxiv.org/abs/2310.14839v2",
      "paper_abstract": "In recent years, studies on image generation models of spiking neural networks (SNNs) have gained the attention of many researchers. Variational autoencoders (VAEs), as one of the most popular image generation models, have attracted a lot of work exploring their SNN implementation. Due to the constrained binary representation in SNNs, existing SNN VAE methods implicitly construct the latent space by an elaborated autoregressive network and use the network outputs as the sampling variables. However, this unspecified implicit representation of the latent space will increase the difficulty of generating high-quality images and introduces additional network parameters. In this paper, we propose an efficient spiking variational autoencoder (ESVAE) that constructs an interpretable latent space distribution and design a reparameterizable spiking sampling method. Specifically, we construct the prior and posterior of the latent space as a Poisson distribution using the firing rate of the spiking neurons. Subsequently, we propose a reparameterizable Poisson spiking sampling method, which is free from the additional network. Comprehensive experiments have been conducted, and the experimental results show that the proposed ESVAE outperforms previous SNN VAE methods in reconstructed & generated images quality. In addition, experiments demonstrate that ESVAE's encoder is able to retain the original image information more efficiently, and the decoder is more robust. The source code is available at https://github.com/QgZhan/ESVAE.",
      "paper_authors": [
        "Qiugang Zhan",
        "Ran Tao",
        "Xiurui Xie",
        "Guisong Liu",
        "Malu Zhang",
        "Huajin Tang",
        "Yang Yang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-10-23",
      "update_time": "2024-08-23",
      "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible",
      "repo_url": "https://github.com/qgzhan/esvae"
    },
    "2310.14621": {
      "paper_id": "2310.14621v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.14621v3",
      "paper_key": "2310.14621",
      "paper_title": "Spiking mode-based neural networks",
      "paper_url": "http://arxiv.org/abs/2310.14621v3",
      "paper_abstract": "Spiking neural networks play an important role in brain-like neuromorphic computations and in studying working mechanisms of neural circuits. One drawback of training a large scale spiking neural network is that updating all weights is quite expensive. Furthermore, after training, all information related to the computational task is hidden into the weight matrix, prohibiting us from a transparent understanding of circuit mechanisms. Therefore, in this work, we address these challenges by proposing a spiking mode-based training protocol, where the recurrent weight matrix is explained as a Hopfield-like multiplication of three matrices: input, output modes and a score matrix. The first advantage is that the weight is interpreted by input and output modes and their associated scores characterizing the importance of each decomposition term. The number of modes is thus adjustable, allowing more degrees of freedom for modeling the experimental data. This significantly reduces the training cost because of significantly reduced space complexity for learning. Training spiking networks is thus carried out in the mode-score space. The second advantage is that one can project the high dimensional neural activity (filtered spike train) in the state space onto the mode space which is typically of a low dimension, e.g., a few modes are sufficient to capture the shape of the underlying neural manifolds. We successfully apply our framework in two computational tasks -- digit classification and selective sensory integration tasks. Our method accelerate the training of spiking neural networks by a Hopfield-like decomposition, and moreover this training leads to low-dimensional attractor structures of high-dimensional neural dynamics.",
      "paper_authors": [
        "Zhanghan Lin",
        "Haiping Huang"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2023-10-23",
      "update_time": "2024-07-18",
      "comments": "30 pages, 10 figures, submitted to PRE",
      "repo_url": "https://github.com/linzhanghan/smnn"
    },
    "2310.14576": {
      "paper_id": "2310.14576v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.14576v2",
      "paper_key": "2310.14576",
      "paper_title": "Tensor Decomposition Based Attention Module for Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2310.14576v2",
      "paper_abstract": "The attention mechanism has been proven to be an effective way to improve spiking neural network (SNN). However, based on the fact that the current SNN input data flow is split into tensors to process on GPUs, none of the previous works consider the properties of tensors to implement an attention module. This inspires us to rethink current SNN from the perspective of tensor-relevant theories. Using tensor decomposition, we design the \\textit{projected full attention} (PFA) module, which demonstrates excellent results with linearly growing parameters. Specifically, PFA is composed by the \\textit{linear projection of spike tensor} (LPST) module and \\textit{attention map composing} (AMC) module. In LPST, we start by compressing the original spike tensor into three projected tensors using a single property-preserving strategy with learnable parameters for each dimension. Then, in AMC, we exploit the inverse procedure of the tensor decomposition process to combine the three tensors into the attention map using a so-called connecting factor. To validate the effectiveness of the proposed PFA module, we integrate it into the widely used VGG and ResNet architectures for classification tasks. Our method achieves state-of-the-art performance on both static and dynamic benchmark datasets, surpassing the existing SNN models with Transformer-based and CNN-based backbones.",
      "paper_authors": [
        "Haoyu Deng",
        "Ruijie Zhu",
        "Xuerui Qiu",
        "Yule Duan",
        "Malu Zhang",
        "Liangjian Deng"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2023-10-23",
      "update_time": "2024-04-11",
      "comments": "Accepted by Knowledge-Based Systems",
      "repo_url": "https://github.com/risingentropy/pfa"
    },
    "2310.13844": {
      "paper_id": "2310.13844v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.13844v1",
      "paper_key": "2310.13844",
      "paper_title": "Multi-level, Forming Free, Bulk Switching Trilayer RRAM for Neuromorphic Computing at the Edge",
      "paper_url": "http://arxiv.org/abs/2310.13844v1",
      "paper_abstract": "Resistive memory-based reconfigurable systems constructed by CMOS-RRAM integration hold great promise for low energy and high throughput neuromorphic computing. However, most RRAM technologies relying on filamentary switching suffer from variations and noise leading to computational accuracy loss, increased energy consumption, and overhead by expensive program and verify schemes. Low ON-state resistance of filamentary RRAM devices further increases the energy consumption due to high-current read and write operations, and limits the array size and parallel multiply & accumulate operations. High-forming voltages needed for filamentary RRAM are not compatible with advanced CMOS technology nodes. To address all these challenges, we developed a forming-free and bulk switching RRAM technology based on a trilayer metal-oxide stack. We systematically engineered a trilayer metal-oxide RRAM stack and investigated the switching characteristics of RRAM devices with varying thicknesses and oxygen vacancy distributions across the trilayer to achieve reliable bulk switching without any filament formation. We demonstrated bulk switching operation at megaohm regime with high current nonlinearity and programmed up to 100 levels without compliance current. We developed a neuromorphic compute-in-memory platform based on trilayer bulk RRAM crossbars by combining energy-efficient switched-capacitor voltage sensing circuits with differential encoding of weights to experimentally demonstrate high-accuracy matrix-vector multiplication. We showcased the computational capability of bulk RRAM crossbars by implementing a spiking neural network model for an autonomous navigation/racing task. Our work addresses challenges posed by existing RRAM technologies and paves the way for neuromorphic computing at the edge under strict size, weight, and power constraints.",
      "paper_authors": [
        "Jaeseoung Park",
        "Ashwani Kumar",
        "Yucheng Zhou",
        "Sangheon Oh",
        "Jeong-Hoon Kim",
        "Yuhan Shi",
        "Soumil Jain",
        "Gopabandhu Hota",
        "Amelie L. Nagle",
        "Catherine D. Schuman",
        "Gert Cauwenberghs",
        "Duygu Kuzum"
      ],
      "primary_category": "cs.ET",
      "publish_time": "2023-10-20",
      "update_time": "2023-10-20",
      "comments": null,
      "repo_url": "#"
    },
    "2310.10317": {
      "paper_id": "2310.10317v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.10317v1",
      "paper_key": "2310.10317",
      "paper_title": "Stochastic spin-orbit-torque synapse and its application in uncertainty quantification",
      "paper_url": "http://arxiv.org/abs/2310.10317v1",
      "paper_abstract": "Stochasticity plays a significant role in the low-power operation of a biological neural network. In an artificial neural network (ANN), stochasticity also contributes to critical functions such as the uncertainty quantification (UQ) for estimating the probability for the correctness of prediction. This UQ is vital for cutting-edge applications, including medical diagnostics, autopilots, and large language models. Thanks to high computing velocity and low dissipation, a spin-orbit-torque (SOT) device exhibits significant potential for implementing the UQ. However, up until now, the application of UQ for stochastic SOT devices remains unexplored. In this study, based on SOT-induced stochastic magnetic domain wall (DW) motion with varying velocity, we fabricated an SOT synapse that could emulate stochastic weight update following the Spike-Timing-Dependent-Plasticity (STDP) rule. Furthermore, we set up a stochastic Spiking-Neural-Network (SNN), which, when compared to its deterministic counterpart, demonstrates a clear advantage in quantifying uncertainty for diagnosing the type of breast tumor (benign or malignant).",
      "paper_authors": [
        "Cen Wang",
        "Guang Zeng",
        "Xinyu Wen",
        "Yuhui He",
        "Wei Luo",
        "Shiwei Chen",
        "Shiheng Liang",
        "Yue Zhang"
      ],
      "primary_category": "physics.app-ph",
      "publish_time": "2023-10-16",
      "update_time": "2023-10-16",
      "comments": null,
      "repo_url": "#"
    },
    "2310.08804": {
      "paper_id": "2310.08804v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.08804v1",
      "paper_key": "2310.08804",
      "paper_title": "Spiking Semantic Communication for Feature Transmission with HARQ",
      "paper_url": "http://arxiv.org/abs/2310.08804v1",
      "paper_abstract": "In Collaborative Intelligence (CI), the Artificial Intelligence (AI) model is divided between the edge and the cloud, with intermediate features being sent from the edge to the cloud for inference. Several deep learning-based Semantic Communication (SC) models have been proposed to reduce feature transmission overhead and mitigate channel noise interference. Previous research has demonstrated that Spiking Neural Network (SNN)-based SC models exhibit greater robustness on digital channels compared to Deep Neural Network (DNN)-based SC models. However, the existing SNN-based SC models require fixed time steps, resulting in fixed transmission bandwidths that cannot be adaptively adjusted based on channel conditions. To address this issue, this paper introduces a novel SC model called SNN-SC-HARQ, which combines the SNN-based SC model with the Hybrid Automatic Repeat Request (HARQ) mechanism. SNN-SC-HARQ comprises an SNN-based SC model that supports the transmission of features at varying bandwidths, along with a policy model that determines the appropriate bandwidth. Experimental results show that SNN-SC-HARQ can dynamically adjust the bandwidth according to the channel conditions without performance loss.",
      "paper_authors": [
        "Mengyang Wang",
        "Jiahui Li",
        "Mengyao Ma",
        "Xiaopeng Fan"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2023-10-13",
      "update_time": "2023-10-13",
      "comments": null,
      "repo_url": "#"
    },
    "2310.07824": {
      "paper_id": "2310.07824v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.07824v1",
      "paper_key": "2310.07824",
      "paper_title": "An On-Chip Trainable Neuron Circuit for SFQ-Based Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2310.07824v1",
      "paper_abstract": "We present an on-chip trainable neuron circuit. Our proposed circuit suits bio-inspired spike-based time-dependent data computation for training spiking neural networks (SNN). The thresholds of neurons can be increased or decreased depending on the desired application-specific spike generation rate. This mechanism provides us with a flexible design and scalable circuit structure. We demonstrate the trainable neuron structure under different operating scenarios. The circuits are designed and optimized for the MIT LL SFQ5ee fabrication process. Margin values for all parameters are above 25\\% with a 3GHz throughput for a 16-input neuron.",
      "paper_authors": [
        "Beyza Zeynep Ucpinar",
        "Mustafa Altay Karamuftuoglu",
        "Sasan Razmkhah",
        "Massoud Pedram"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-10-11",
      "update_time": "2023-10-11",
      "comments": "5 pages, 8 figures. The work was presented in EUCAS 2023",
      "repo_url": "#"
    },
    "2310.07189": {
      "paper_id": "2310.07189v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.07189v2",
      "paper_key": "2310.07189",
      "paper_title": "SpikePoint: An Efficient Point-based Spiking Neural Network for Event Cameras Action Recognition",
      "paper_url": "http://arxiv.org/abs/2310.07189v2",
      "paper_abstract": "Event cameras are bio-inspired sensors that respond to local changes in light intensity and feature low latency, high energy efficiency, and high dynamic range. Meanwhile, Spiking Neural Networks (SNNs) have gained significant attention due to their remarkable efficiency and fault tolerance. By synergistically harnessing the energy efficiency inherent in event cameras and the spike-based processing capabilities of SNNs, their integration could enable ultra-low-power application scenarios, such as action recognition tasks. However, existing approaches often entail converting asynchronous events into conventional frames, leading to additional data mapping efforts and a loss of sparsity, contradicting the design concept of SNNs and event cameras. To address this challenge, we propose SpikePoint, a novel end-to-end point-based SNN architecture. SpikePoint excels at processing sparse event cloud data, effectively extracting both global and local features through a singular-stage structure. Leveraging the surrogate training method, SpikePoint achieves high accuracy with few parameters and maintains low power consumption, specifically employing the identity mapping feature extractor on diverse datasets. SpikePoint achieves state-of-the-art (SOTA) performance on four event-based action recognition datasets using only 16 timesteps, surpassing other SNN methods. Moreover, it also achieves SOTA performance across all methods on three datasets, utilizing approximately 0.3\\% of the parameters and 0.5\\% of power consumption employed by artificial neural networks (ANNs). These results emphasize the significance of Point Cloud and pave the way for many ultra-low-power event-based data processing applications.",
      "paper_authors": [
        "Hongwei Ren",
        "Yue Zhou",
        "Yulong Huang",
        "Haotian Fu",
        "Xiaopeng Lin",
        "Jie Song",
        "Bojun Cheng"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-10-11",
      "update_time": "2024-01-23",
      "comments": "Accepted by ICLR 2024 (Spotlight)",
      "repo_url": "#"
    },
    "2310.06578": {
      "paper_id": "2310.06578v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.06578v1",
      "paper_key": "2310.06578",
      "paper_title": "Energy-Efficient Visual Search by Eye Movement and Low-Latency Spiking Neural Network",
      "paper_url": "http://arxiv.org/abs/2310.06578v1",
      "paper_abstract": "Human vision incorporates non-uniform resolution retina, efficient eye movement strategy, and spiking neural network (SNN) to balance the requirements in visual field size, visual resolution, energy cost, and inference latency. These properties have inspired interest in developing human-like computer vision. However, existing models haven't fully incorporated the three features of human vision, and their learned eye movement strategies haven't been compared with human's strategy, making the models' behavior difficult to interpret. Here, we carry out experiments to examine human visual search behaviors and establish the first SNN-based visual search model. The model combines an artificial retina with spiking feature extraction, memory, and saccade decision modules, and it employs population coding for fast and efficient saccade decisions. The model can learn either a human-like or a near-optimal fixation strategy, outperform humans in search speed and accuracy, and achieve high energy efficiency through short saccade decision latency and sparse activation. It also suggests that the human search strategy is suboptimal in terms of search speed. Our work connects modeling of vision in neuroscience and machine learning and sheds light on developing more energy-efficient computer vision algorithms.",
      "paper_authors": [
        "Yunhui Zhou",
        "Dongqi Han",
        "Yuguo Yu"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-10-10",
      "update_time": "2023-10-10",
      "comments": null,
      "repo_url": "#"
    },
    "2310.06488": {
      "paper_id": "2310.06488v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.06488v2",
      "paper_key": "2310.06488",
      "paper_title": "SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural Network",
      "paper_url": "http://arxiv.org/abs/2310.06488v2",
      "paper_abstract": "Spiking neural networks (SNNs) have demonstrated the capability to achieve comparable performance to deep neural networks (DNNs) in both visual and linguistic domains while offering the advantages of improved energy efficiency and adherence to biological plausibility. However, the extension of such single-modality SNNs into the realm of multimodal scenarios remains an unexplored territory. Drawing inspiration from the concept of contrastive language-image pre-training (CLIP), we introduce a novel framework, named SpikeCLIP, to address the gap between two modalities within the context of spike-based computing through a two-step recipe involving ``Alignment Pre-training + Dual-Loss Fine-tuning\". Extensive experiments demonstrate that SNNs achieve comparable results to their DNN counterparts while significantly reducing energy consumption across a variety of datasets commonly used for multimodal model evaluation. Furthermore, SpikeCLIP maintains robust performance in image classification tasks that involve class labels not predefined within specific categories.",
      "paper_authors": [
        "Tianlong Li",
        "Wenhao Liu",
        "Changze Lv",
        "Jianhan Xu",
        "Cenyuan Zhang",
        "Muling Wu",
        "Xiaoqing Zheng",
        "Xuanjing Huang"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-10-10",
      "update_time": "2023-10-12",
      "comments": null,
      "repo_url": "#"
    },
    "2310.06232": {
      "paper_id": "2310.06232v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.06232v1",
      "paper_key": "2310.06232",
      "paper_title": "Spiking PointNet: Spiking Neural Networks for Point Clouds",
      "paper_url": "http://arxiv.org/abs/2310.06232v1",
      "paper_abstract": "Recently, Spiking Neural Networks (SNNs), enjoying extreme energy efficiency, have drawn much research attention on 2D visual recognition and shown gradually increasing application potential. However, it still remains underexplored whether SNNs can be generalized to 3D recognition. To this end, we present Spiking PointNet in the paper, the first spiking neural model for efficient deep learning on point clouds. We discover that the two huge obstacles limiting the application of SNNs in point clouds are: the intrinsic optimization obstacle of SNNs that impedes the training of a big spiking model with large time steps, and the expensive memory and computation cost of PointNet that makes training a big spiking point model unrealistic. To solve the problems simultaneously, we present a trained-less but learning-more paradigm for Spiking PointNet with theoretical justifications and in-depth experimental analysis. In specific, our Spiking PointNet is trained with only a single time step but can obtain better performance with multiple time steps inference, compared to the one trained directly with multiple time steps. We conduct various experiments on ModelNet10, ModelNet40 to demonstrate the effectiveness of Spiking PointNet. Notably, our Spiking PointNet even can outperform its ANN counterpart, which is rare in the SNN field thus providing a potential research direction for the following work. Moreover, Spiking PointNet shows impressive speedup and storage saving in the training phase.",
      "paper_authors": [
        "Dayong Ren",
        "Zhe Ma",
        "Yuanpei Chen",
        "Weihang Peng",
        "Xiaode Liu",
        "Yuhan Zhang",
        "Yufei Guo"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-10-10",
      "update_time": "2023-10-10",
      "comments": "Accepted by NeurIPS",
      "repo_url": "https://github.com/dayongren/spiking-pointnet"
    },
    "2310.05868": {
      "paper_id": "2310.05868v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.05868v1",
      "paper_key": "2310.05868",
      "paper_title": "Bio-inspired computational memory model of the Hippocampus: an approach to a neuromorphic spike-based Content-Addressable Memory",
      "paper_url": "http://arxiv.org/abs/2310.05868v1",
      "paper_abstract": "The brain has computational capabilities that surpass those of modern systems, being able to solve complex problems efficiently in a simple way. Neuromorphic engineering aims to mimic biology in order to develop new systems capable of incorporating such capabilities. Bio-inspired learning systems continue to be a challenge that must be solved, and much work needs to be done in this regard. Among all brain regions, the hippocampus stands out as an autoassociative short-term memory with the capacity to learn and recall memories from any fragment of them. These characteristics make the hippocampus an ideal candidate for developing bio-inspired learning systems that, in addition, resemble content-addressable memories. Therefore, in this work we propose a bio-inspired spiking content-addressable memory model based on the CA3 region of the hippocampus with the ability to learn, forget and recall memories, both orthogonal and non-orthogonal, from any fragment of them. The model was implemented on the SpiNNaker hardware platform using Spiking Neural Networks. A set of experiments based on functional, stress and applicability tests were performed to demonstrate its correct functioning. This work presents the first hardware implementation of a fully-functional bio-inspired spiking hippocampal content-addressable memory model, paving the way for the development of future more complex neuromorphic systems.",
      "paper_authors": [
        "Daniel Casanueva-Morato",
        "Alvaro Ayuso-Martinez",
        "Juan P. Dominguez-Morales",
        "Angel Jimenez-Fernandez",
        "Gabriel Jimenez-Moreno"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-10-09",
      "update_time": "2023-10-09",
      "comments": "15 pages, 5 figures, journal, Spiking Neural Network",
      "repo_url": "https://github.com/dancasmor/an-aproach-to-a-spike-based-content-addressable-memory-bio-inspired-in-the-hippocampus"
    },
    "2310.05343": {
      "paper_id": "2310.05343v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.05343v1",
      "paper_key": "2310.05343",
      "paper_title": "Investigating Continuous Learning in Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2310.05343v1",
      "paper_abstract": "In this paper, the use of third-generation machine learning, also known as spiking neural network architecture, for continuous learning was investigated and compared to conventional models. The experimentation was divided into three separate phases. The first phase focused on training the conventional models via transfer learning. The second phase trains a Nengo model from their library. Lastly, each conventional model is converted into a spiking neural network and trained. Initial results from phase 1 are inline with known knowledge about continuous learning within current machine learning literature. All models were able to correctly identify the current classes, but they would immediately see a sharp performance drop in previous classes due to catastrophic forgetting. However, the SNN models were able to retain some information about previous classes. Although many of the previous classes were still identified as the current trained classes, the output probabilities showed a higher than normal value to the actual class. This indicates that the SNN models do have potential to overcome catastrophic forgetting but much work is still needed.",
      "paper_authors": [
        "C. Tanner Fredieu"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-10-09",
      "update_time": "2023-10-09",
      "comments": null,
      "repo_url": "#"
    },
    "2310.05022": {
      "paper_id": "2310.05022v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.05022v2",
      "paper_key": "2310.05022",
      "paper_title": "Fully Spiking Neural Network for Legged Robots",
      "paper_url": "http://arxiv.org/abs/2310.05022v2",
      "paper_abstract": "In recent years, legged robots based on deep reinforcement learning have made remarkable progress. Quadruped robots have demonstrated the ability to complete challenging tasks in complex environments and have been deployed in real-world scenarios to assist humans. Simultaneously, bipedal and humanoid robots have achieved breakthroughs in various demanding tasks. Current reinforcement learning methods can utilize diverse robot bodies and historical information to perform actions. However, prior research has not emphasized the speed and energy consumption of network inference, as well as the biological significance of the neural networks themselves. Most of the networks employed are traditional artificial neural networks that utilize multilayer perceptrons (MLP). In this paper, we successfully apply a novel Spiking Neural Network (SNN) to process legged robots, achieving outstanding results across a range of simulated terrains. SNN holds a natural advantage over traditional neural networks in terms of inference speed and energy consumption, and their pulse-form processing of body perception signals offers improved biological interpretability. Applying more biomimetic neural networks to legged robots can further reduce the heat dissipation and structural burden caused by the high power consumption of neural networks. To the best of our knowledge, this is the first work to implement SNN in legged robots.",
      "paper_authors": [
        "Xiaoyang Jiang",
        "Qiang Zhang",
        "Jingkai Sun",
        "Jiahang Cao",
        "Jingtong Ma",
        "Renjing Xu"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2023-10-08",
      "update_time": "2024-03-23",
      "comments": null,
      "repo_url": "#"
    },
    "2310.04043": {
      "paper_id": "2310.04043v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.04043v1",
      "paper_key": "2310.04043",
      "paper_title": "In the Blink of an Eye: Event-based Emotion Recognition",
      "paper_url": "http://arxiv.org/abs/2310.04043v1",
      "paper_abstract": "We introduce a wearable single-eye emotion recognition device and a real-time approach to recognizing emotions from partial observations of an emotion that is robust to changes in lighting conditions. At the heart of our method is a bio-inspired event-based camera setup and a newly designed lightweight Spiking Eye Emotion Network (SEEN). Compared to conventional cameras, event-based cameras offer a higher dynamic range (up to 140 dB vs. 80 dB) and a higher temporal resolution. Thus, the captured events can encode rich temporal cues under challenging lighting conditions. However, these events lack texture information, posing problems in decoding temporal information effectively. SEEN tackles this issue from two different perspectives. First, we adopt convolutional spiking layers to take advantage of the spiking neural network's ability to decode pertinent temporal information. Second, SEEN learns to extract essential spatial cues from corresponding intensity frames and leverages a novel weight-copy scheme to convey spatial attention to the convolutional spiking layers during training and inference. We extensively validate and demonstrate the effectiveness of our approach on a specially collected Single-eye Event-based Emotion (SEE) dataset. To the best of our knowledge, our method is the first eye-based emotion recognition method that leverages event-based cameras and spiking neural network.",
      "paper_authors": [
        "Haiwei Zhang",
        "Jiqing Zhang",
        "Bo Dong",
        "Pieter Peers",
        "Wenwei Wu",
        "Xiaopeng Wei",
        "Felix Heide",
        "Xin Yang"
      ],
      "primary_category": "cs.GR",
      "publish_time": "2023-10-06",
      "update_time": "2023-10-06",
      "comments": null,
      "repo_url": "https://github.com/zhanghaiwei1234/single-eye-emotion-recognition"
    },
    "2310.03918": {
      "paper_id": "2310.03918v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.03918v1",
      "paper_key": "2310.03918",
      "paper_title": "Unsupervised SFQ-Based Spiking Neural Network",
      "paper_url": "http://arxiv.org/abs/2310.03918v1",
      "paper_abstract": "Single Flux Quantum (SFQ) technology represents a groundbreaking advancement in computational efficiency and ultra-high-speed neuromorphic processing. The key features of SFQ technology, particularly data representation, transmission, and processing through SFQ pulses, closely mirror fundamental aspects of biological neural structures. Consequently, SFQ-based circuits emerge as an ideal candidate for realizing Spiking Neural Networks (SNNs). This study presents a proof-of-concept demonstration of an SFQ-based SNN architecture, showcasing its capacity for ultra-fast switching at remarkably low energy consumption per output activity. Notably, our work introduces innovative approaches: (i) We introduce a novel spike-timing-dependent plasticity mechanism to update synapses and to trace spike-activity by incorporating a leaky non-destructive readout circuit. (ii) We propose a novel method to dynamically regulate the threshold behavior of leaky integrate and fire superconductor neurons, enhancing the adaptability of our SNN architecture. (iii) Our research incorporates a novel winner-take-all mechanism, aligning with practical strategies for SNN development and enabling effective decision-making processes. The effectiveness of these proposed structural enhancements is evaluated by integrating high-level models into the BindsNET framework. By leveraging BindsNET, we model the online training of an SNN, integrating the novel structures into the learning process. To ensure the robustness and functionality of our circuits, we employ JoSIM for circuit parameter extraction and functional verification through simulation.",
      "paper_authors": [
        "Mustafa Altay Karamuftuoglu",
        "Beyza Zeynep Ucpinar",
        "Sasan Razmkhah",
        "Mehdi Kamal",
        "Massoud Pedram"
      ],
      "primary_category": "cs.ET",
      "publish_time": "2023-10-05",
      "update_time": "2023-10-05",
      "comments": null,
      "repo_url": "#"
    },
    "2310.03873": {
      "paper_id": "2310.03873v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.03873v1",
      "paper_key": "2310.03873",
      "paper_title": "Neuromorphic Robust Framework for Concurrent Estimation and Control in Dynamical Systems using Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2310.03873v1",
      "paper_abstract": "Concurrent estimation and control of robotic systems remains an ongoing challenge, where controllers rely on data extracted from states/parameters riddled with uncertainties and noises. Framework suitability hinges on task complexity and computational constraints, demanding a balance between computational efficiency and mission-critical accuracy. This study leverages recent advancements in neuromorphic computing, particularly spiking neural networks (SNNs), for estimation and control applications. Our presented framework employs a recurrent network of leaky integrate-and-fire (LIF) neurons, mimicking a linear quadratic regulator (LQR) through a robust filtering strategy, a modified sliding innovation filter (MSIF). Benefiting from both the robustness of MSIF and the computational efficiency of SNN, our framework customizes SNN weight matrices to match the desired system model without requiring training. Additionally, the network employs a biologically plausible firing rule similar to predictive coding. In the presence of uncertainties, we compare the SNN-LQR-MSIF with non-spiking LQR-MSIF and the optimal linear quadratic Gaussian (LQG) strategy. Evaluation across a workbench linear problem and a satellite rendezvous maneuver, implementing the Clohessy-Wiltshire (CW) model in space robotics, demonstrates that the SNN-LQR-MSIF achieves acceptable performance in computational efficiency, robustness, and accuracy. This positions it as a promising solution for addressing dynamic systems' concurrent estimation and control challenges in dynamic systems.",
      "paper_authors": [
        "Reza Ahmadvand",
        "Sarah Safura Sharif",
        "Yaser Mike Banad"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2023-10-05",
      "update_time": "2023-10-05",
      "comments": "12 pages, 15 figures",
      "repo_url": "#"
    },
    "2310.02772": {
      "paper_id": "2310.02772v6",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.02772v6",
      "paper_key": "2310.02772",
      "paper_title": "Spike Accumulation Forwarding for Effective Training of Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2310.02772v6",
      "paper_abstract": "In this article, we propose a new paradigm for training spiking neural networks (SNNs), spike accumulation forwarding (SAF). It is known that SNNs are energy-efficient but difficult to train. Consequently, many researchers have proposed various methods to solve this problem, among which online training through time (OTTT) is a method that allows inferring at each time step while suppressing the memory cost. However, to compute efficiently on GPUs, OTTT requires operations with spike trains and weighted summation of spike trains during forwarding. In addition, OTTT has shown a relationship with the Spike Representation, an alternative training method, though theoretical agreement with Spike Representation has yet to be proven. Our proposed method can solve these problems; namely, SAF can halve the number of operations during the forward process, and it can be theoretically proven that SAF is consistent with the Spike Representation and OTTT, respectively. Furthermore, we confirmed the above contents through experiments and showed that it is possible to reduce memory and training time while maintaining accuracy.",
      "paper_authors": [
        "Ryuji Saiin",
        "Tomoya Shirakawa",
        "Sota Yoshihara",
        "Yoshihide Sawada",
        "Hiroyuki Kusumoto"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-10-04",
      "update_time": "2024-06-28",
      "comments": "14 pages, 5 figures, Appendix:10 pages, 2 figures, v6:Published in\n  Transactions on Machine Learning Research",
      "repo_url": "#"
    },
    "2310.02364": {
      "paper_id": "2310.02364v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.02364v1",
      "paper_key": "2310.02364",
      "paper_title": "Graphene-Based Artificial Dendrites for Bio-Inspired Learning in Spiking Neuromorphic Systems",
      "paper_url": "http://arxiv.org/abs/2310.02364v1",
      "paper_abstract": "Analog neuromorphic computing systems emulate the parallelism and connectivity of the human brain, promising greater expressivity and energy efficiency compared to digital systems. Though many devices have emerged as candidates for artificial neurons and artificial synapses, there have been few device candidates for artificial dendrites. In this work, we report on biocompatible graphene-based artificial dendrites (GrADs) that can implement dendritic processing. By using a dual side-gate configuration, current applied through a Nafion membrane can be used to control device conductance across a trilayer graphene channel, showing spatiotemporal responses of leaky recurrent, alpha, and gaussian dendritic potentials. The devices can be variably connected to enable higher order neuronal responses, and we show through data-driven spiking neural network classification simulations that overall spiking activity is reduced by up to 15% without accuracy loss while low frequency operation is stabilized. This positions the GrADs as strong candidates for energy efficient bio-interfaced spiking neural networks.",
      "paper_authors": [
        "Samuel Liu",
        "Deji Akinwande",
        "Dmitry Kireev",
        "Jean Anne C. Incorvia"
      ],
      "primary_category": "cond-mat.mes-hall",
      "publish_time": "2023-10-03",
      "update_time": "2023-10-03",
      "comments": null,
      "repo_url": "#"
    },
    "2310.02361": {
      "paper_id": "2310.02361v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.02361v1",
      "paper_key": "2310.02361",
      "paper_title": "Event-Enhanced Multi-Modal Spiking Neural Network for Dynamic Obstacle Avoidance",
      "paper_url": "http://arxiv.org/abs/2310.02361v1",
      "paper_abstract": "Autonomous obstacle avoidance is of vital importance for an intelligent agent such as a mobile robot to navigate in its environment. Existing state-of-the-art methods train a spiking neural network (SNN) with deep reinforcement learning (DRL) to achieve energy-efficient and fast inference speed in complex/unknown scenes. These methods typically assume that the environment is static while the obstacles in real-world scenes are often dynamic. The movement of obstacles increases the complexity of the environment and poses a great challenge to the existing methods. In this work, we approach robust dynamic obstacle avoidance twofold. First, we introduce the neuromorphic vision sensor (i.e., event camera) to provide motion cues complementary to the traditional Laser depth data for handling dynamic obstacles. Second, we develop an DRL-based event-enhanced multimodal spiking actor network (EEM-SAN) that extracts information from motion events data via unsupervised representation learning and fuses Laser and event camera data with learnable thresholding. Experiments demonstrate that our EEM-SAN outperforms state-of-the-art obstacle avoidance methods by a significant margin, especially for dynamic obstacle avoidance.",
      "paper_authors": [
        "Yang Wang",
        "Bo Dong",
        "Yuji Zhang",
        "Yunduo Zhou",
        "Haiyang Mei",
        "Ziqi Wei",
        "Xin Yang"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2023-10-03",
      "update_time": "2023-10-03",
      "comments": "In Proceedings of the 31st ACM International Conference on Multimedia\n  (ACM MM 2023)",
      "repo_url": "#"
    },
    "2310.02068": {
      "paper_id": "2310.02068v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.02068v2",
      "paper_key": "2310.02068",
      "paper_title": "Well-posedness and numerical analysis of an elapsed time model with strongly coupled neural networks",
      "paper_url": "http://arxiv.org/abs/2310.02068v2",
      "paper_abstract": "The elapsed time equation is an age-structured model that describes dynamics of interconnected spiking neurons through the elapsed time since the last discharge, leading to many interesting questions on the evolution of the system from a mathematical and biological point of view. In this work, we first deal with the case when transmission after a spike is instantaneous and the case when there exists a distributed delay that depends on previous history of the system, which is a more realistic assumption. Then we study the well-posedness and the numerical analysis of the elapsed time models. For existence and uniqueness we improve the previous works by relaxing some hypothesis on the nonlinearity, including the strongly excitatory case, while for the numerical analysis we prove that the approximation given by the explicit upwind scheme converges to the solution of the non-linear problem. We also show some numerical simulations to compare the behavior of the system in the case of instantaneous transmission with the case of distributed delay under different parameters, leading to solutions with different asymptotic profiles.",
      "paper_authors": [
        "Mauricio Sepulveda",
        "Nicolas Torres",
        "Luis Miguel Villada"
      ],
      "primary_category": "math.AP",
      "publish_time": "2023-10-03",
      "update_time": "2024-04-09",
      "comments": null,
      "repo_url": "#"
    },
    "2310.02055": {
      "paper_id": "2310.02055v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.02055v1",
      "paper_key": "2310.02055",
      "paper_title": "Integrate-and-fire circuit for converting analog signals to spikes using phase encoding",
      "paper_url": "http://arxiv.org/abs/2310.02055v1",
      "paper_abstract": "Processing sensor data with spiking neural networks on digital neuromorphic chips requires converting continuous analog signals into spike pulses. Two strategies are promising for achieving low energy consumption and fast processing speeds in end-to-end neuromorphic applications. First, to directly encode analog signals to spikes to bypass the need for an analog-to-digital converter (ADC). Second, to use temporal encoding techniques to maximize the spike sparsity, which is a crucial parameter for fast and efficient neuromorphic processing. In this work, we propose an adaptive control of the refractory period of the leaky integrate-and-fire (LIF) neuron model for encoding continuous analog signals into a train of time-coded spikes. The LIF-based encoder generates phase-encoded spikes that are compatible with digital hardware. We implemented the neuron model on a physical circuit and tested it with different electric signals. A digital neuromorphic chip processed the generated spike trains and computed the signal's frequency spectrum using a spiking version of the Fourier transform. We tested the prototype circuit on electric signals up to 1 KHz. Thus, we provide an end-to-end neuromorphic application that generates the frequency spectrum of an electric signal without the need for an ADC or a digital signal processing algorithm.",
      "paper_authors": [
        "Javier Lopez-Randulfe",
        "Nico Reeb",
        "Alois Knoll"
      ],
      "primary_category": "cs.ET",
      "publish_time": "2023-10-03",
      "update_time": "2023-10-03",
      "comments": "13 pages",
      "repo_url": "#"
    },
    "2310.00564": {
      "paper_id": "2310.00564v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.00564v2",
      "paper_key": "2310.00564",
      "paper_title": "DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor",
      "paper_url": "http://arxiv.org/abs/2310.00564v2",
      "paper_abstract": "With the remarkable progress that technology has made, the need for processing data near the sensors at the edge has increased dramatically. The electronic systems used in these applications must process data continuously, in real-time, and extract relevant information using the smallest possible energy budgets. A promising approach for implementing always-on processing of sensory signals that supports on-demand, sparse, and edge-computing is to take inspiration from biological nervous system. Following this approach, we present a brain-inspired platform for prototyping real-time event-based Spiking Neural Networks (SNNs). The system proposed supports the direct emulation of dynamic and realistic neural processing phenomena such as short-term plasticity, NMDA gating, AMPA diffusion, homeostasis, spike frequency adaptation, conductance-based dendritic compartments and spike transmission delays. The analog circuits that implement such primitives are paired with a low latency asynchronous digital circuits for routing and mapping events. This asynchronous infrastructure enables the definition of different network architectures, and provides direct event-based interfaces to convert and encode data from event-based and continuous-signal sensors. Here we describe the overall system architecture, we characterize the mixed signal analog-digital circuits that emulate neural dynamics, demonstrate their features with experimental measurements, and present a low- and high-level software ecosystem that can be used for configuring the system. The flexibility to emulate different biologically plausible neural networks, and the chip's ability to monitor both population and single neuron signals in real-time, allow to develop and validate complex models of neural processing for both basic research and edge-computing applications.",
      "paper_authors": [
        "Ole Richter",
        "Chenxi Wu",
        "Adrian M. Whatley",
        "German K\u00f6stinger",
        "Carsten Nielsen",
        "Ning Qiao",
        "Giacomo Indiveri"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-10-01",
      "update_time": "2023-11-10",
      "comments": "*Ole Richter and Chenxi Wu contributed equally",
      "repo_url": "#"
    },
    "2309.16987": {
      "paper_id": "2309.16987v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.16987v1",
      "paper_key": "2309.16987",
      "paper_title": "SpikeMOT: Event-based Multi-Object Tracking with Sparse Motion Features",
      "paper_url": "http://arxiv.org/abs/2309.16987v1",
      "paper_abstract": "In comparison to conventional RGB cameras, the superior temporal resolution of event cameras allows them to capture rich information between frames, making them prime candidates for object tracking. Yet in practice, despite their theoretical advantages, the body of work on event-based multi-object tracking (MOT) remains in its infancy, especially in real-world settings where events from complex background and camera motion can easily obscure the true target motion. In this work, an event-based multi-object tracker, called SpikeMOT, is presented to address these challenges. SpikeMOT leverages spiking neural networks to extract sparse spatiotemporal features from event streams associated with objects. The resulting spike train representations are used to track the object movement at high frequency, while a simultaneous object detector provides updated spatial information of these objects at an equivalent frame rate. To evaluate the effectiveness of SpikeMOT, we introduce DSEC-MOT, the first large-scale event-based MOT benchmark incorporating fine-grained annotations for objects experiencing severe occlusions, frequent trajectory intersections, and long-term re-identification in real-world contexts. Extensive experiments employing DSEC-MOT and another event-based dataset, named FE240hz, demonstrate SpikeMOT's capability to achieve high tracking accuracy amidst challenging real-world scenarios, advancing the state-of-the-art in event-based multi-object tracking.",
      "paper_authors": [
        "Song Wang",
        "Zhu Wang",
        "Can Li",
        "Xiaojuan Qi",
        "Hayden Kwok-Hay So"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-09-29",
      "update_time": "2023-09-29",
      "comments": null,
      "repo_url": "#"
    },
    "2309.16795": {
      "paper_id": "2309.16795v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.16795v2",
      "paper_key": "2309.16795",
      "paper_title": "Ultra-low-power Image Classification on Neuromorphic Hardware",
      "paper_url": "http://arxiv.org/abs/2309.16795v2",
      "paper_abstract": "Spiking neural networks (SNNs) promise ultra-low-power applications by exploiting temporal and spatial sparsity. The number of binary activations, called spikes, is proportional to the power consumed when executed on neuromorphic hardware. Training such SNNs using backpropagation through time for vision tasks that rely mainly on spatial features is computationally costly. Training a stateless artificial neural network (ANN) to then convert the weights to an SNN is a straightforward alternative when it comes to image recognition datasets. Most conversion methods rely on rate coding in the SNN to represent ANN activation, which uses enormous amounts of spikes and, therefore, energy to encode information. Recently, temporal conversion methods have shown promising results requiring significantly fewer spikes per neuron, but sometimes complex neuron models. We propose a temporal ANN-to-SNN conversion method, which we call Quartz, that is based on the time to first spike (TTFS). Quartz achieves high classification accuracy and can be easily implemented on neuromorphic hardware while using the least amount of synaptic operations and memory accesses. It incurs a cost of two additional synapses per neuron compared to previous temporal conversion methods, which are readily available on neuromorphic hardware. We benchmark Quartz on MNIST, CIFAR10, and ImageNet in simulation to show the benefits of our method and follow up with an implementation on Loihi, a neuromorphic chip by Intel. We provide evidence that temporal coding has advantages in terms of power consumption, throughput, and latency for similar classification accuracy. Our code and models are publicly available.",
      "paper_authors": [
        "Gregor Lenz",
        "Garrick Orchard",
        "Sadique Sheik"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-09-28",
      "update_time": "2024-06-21",
      "comments": null,
      "repo_url": "https://github.com/biphasic/quartz-on-loihi"
    },
    "2309.16425": {
      "paper_id": "2309.16425v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.16425v1",
      "paper_key": "2309.16425",
      "paper_title": "Feed-forward and recurrent inhibition for compressing and classifying high dynamic range biosignals in spiking neural network architectures",
      "paper_url": "http://arxiv.org/abs/2309.16425v1",
      "paper_abstract": "Neuromorphic processors that implement Spiking Neural Networks (SNNs) using mixed-signal analog/digital circuits represent a promising technology for closed-loop real-time processing of biosignals. As in biology, to minimize power consumption, the silicon neurons' circuits are configured to fire with a limited dynamic range and with maximum firing rates restricted to a few tens or hundreds of Herz.   However, biosignals can have a very large dynamic range, so encoding them into spikes without saturating the neuron outputs represents an open challenge.   In this work, we present a biologically-inspired strategy for compressing this high-dynamic range in SNN architectures, using three adaptation mechanisms ubiquitous in the brain: spike-frequency adaptation at the single neuron level, feed-forward inhibitory connections from neurons belonging to the input layer, and Excitatory-Inhibitory (E-I) balance via recurrent inhibition among neurons in the output layer.   We apply this strategy to input biosignals encoded using both an asynchronous delta modulation method and an energy-based pulse-frequency modulation method.   We validate this approach in silico, simulating a simple network applied to a gesture classification task from surface EMG recordings.",
      "paper_authors": [
        "Rachel Sava",
        "Elisa Donati",
        "Giacomo Indiveri"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2023-09-28",
      "update_time": "2023-09-28",
      "comments": "5 pages, 7 figures, to be published in IEEE BioCAS 2023 Proceedings",
      "repo_url": "#"
    },
    "2309.16158": {
      "paper_id": "2309.16158v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.16158v1",
      "paper_key": "2309.16158",
      "paper_title": "FireFly v2: Advancing Hardware Support for High-Performance Spiking Neural Network with a Spatiotemporal FPGA Accelerator",
      "paper_url": "http://arxiv.org/abs/2309.16158v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) are expected to be a promising alternative to Artificial Neural Networks (ANNs) due to their strong biological interpretability and high energy efficiency. Specialized SNN hardware offers clear advantages over general-purpose devices in terms of power and performance. However, there's still room to advance hardware support for state-of-the-art (SOTA) SNN algorithms and improve computation and memory efficiency. As a further step in supporting high-performance SNNs on specialized hardware, we introduce FireFly v2, an FPGA SNN accelerator that can address the issue of non-spike operation in current SOTA SNN algorithms, which presents an obstacle in the end-to-end deployment onto existing SNN hardware. To more effectively align with the SNN characteristics, we design a spatiotemporal dataflow that allows four dimensions of parallelism and eliminates the need for membrane potential storage, enabling on-the-fly spike processing and spike generation. To further improve hardware acceleration performance, we develop a high-performance spike computing engine as a backend based on a systolic array operating at 500-600MHz. To the best of our knowledge, FireFly v2 achieves the highest clock frequency among all FPGA-based implementations. Furthermore, it stands as the first SNN accelerator capable of supporting non-spike operations, which are commonly used in advanced SNN algorithms. FireFly v2 has doubled the throughput and DSP efficiency when compared to our previous version of FireFly and it exhibits 1.33 times the DSP efficiency and 1.42 times the power efficiency compared to the current most advanced FPGA accelerators.",
      "paper_authors": [
        "Jindong Li",
        "Guobin Shen",
        "Dongcheng Zhao",
        "Qian Zhang",
        "Yi Zeng"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-09-28",
      "update_time": "2023-09-28",
      "comments": null,
      "repo_url": "#"
    },
    "2311.09225": {
      "paper_id": "2311.09225v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.09225v1",
      "paper_key": "2311.09225",
      "paper_title": "Autonomous Driving using Spiking Neural Networks on Dynamic Vision Sensor Data: A Case Study of Traffic Light Change Detection",
      "paper_url": "http://arxiv.org/abs/2311.09225v1",
      "paper_abstract": "Autonomous driving is a challenging task that has gained broad attention from both academia and industry. Current solutions using convolutional neural networks require large amounts of computational resources, leading to high power consumption. Spiking neural networks (SNNs) provide an alternative computation model to process information and make decisions. This biologically plausible model has the advantage of low latency and energy efficiency. Recent work using SNNs for autonomous driving mostly focused on simple tasks like lane keeping in simplified simulation environments. This project studies SNNs on photo-realistic driving scenes in the CARLA simulator, which is an important step toward using SNNs on real vehicles. The efficacy and generalizability of the method will be investigated.",
      "paper_authors": [
        "Xuelei Chen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-09-27",
      "update_time": "2023-09-27",
      "comments": null,
      "repo_url": "https://github.com/xueleichen/snn-dvs-carla"
    },
    "2309.15883": {
      "paper_id": "2309.15883v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.15883v1",
      "paper_key": "2309.15883",
      "paper_title": "Highly Efficient SNNs for High-speed Object Detection",
      "paper_url": "http://arxiv.org/abs/2309.15883v1",
      "paper_abstract": "The high biological properties and low energy consumption of Spiking Neural Networks (SNNs) have brought much attention in recent years. However, the converted SNNs generally need large time steps to achieve satisfactory performance, which will result in high inference latency and computational resources increase. In this work, we propose a highly efficient and fast SNN for object detection. First, we build an initial compact ANN by using quantization training method of convolution layer fold batch normalization layer and neural network modification. Second, we theoretically analyze how to obtain the low complexity SNN correctly. Then, we propose a scale-aware pseudoquantization scheme to guarantee the correctness of the compact ANN to SNN. Third, we propose a continuous inference scheme by using a Feed-Forward Integrate-and-Fire (FewdIF) neuron to realize high-speed object detection. Experimental results show that our efficient SNN can achieve 118X speedup on GPU with only 1.5MB parameters for object detection tasks. We further verify our SNN on FPGA platform and the proposed model can achieve 800+FPS object detection with extremely low latency.",
      "paper_authors": [
        "Nemin Qiu",
        "Zhiguo Li",
        "Yuan Li",
        "Chuang Zhu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-09-27",
      "update_time": "2023-09-27",
      "comments": null,
      "repo_url": "#"
    },
    "2309.15555": {
      "paper_id": "2309.15555v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.15555v1",
      "paper_key": "2309.15555",
      "paper_title": "Low Latency of object detection for spikng neural network",
      "paper_url": "http://arxiv.org/abs/2309.15555v1",
      "paper_abstract": "Spiking Neural Networks, as a third-generation neural network, are well-suited for edge AI applications due to their binary spike nature. However, when it comes to complex tasks like object detection, SNNs often require a substantial number of time steps to achieve high performance. This limitation significantly hampers the widespread adoption of SNNs in latency-sensitive edge devices. In this paper, our focus is on generating highly accurate and low-latency SNNs specifically for object detection. Firstly, we systematically derive the conversion between SNNs and ANNs and analyze how to improve the consistency between them: improving the spike firing rate and reducing the quantization error. Then we propose a structural replacement, quantization of ANN activation and residual fix to allevicate the disparity. We evaluate our method on challenging dataset MS COCO, PASCAL VOC and our spike dataset. The experimental results show that the proposed method achieves higher accuracy and lower latency compared to previous work Spiking-YOLO. The advantages of SNNs processing of spike signals are also demonstrated.",
      "paper_authors": [
        "Nemin Qiu",
        "Chuang Zhu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-09-27",
      "update_time": "2023-09-27",
      "comments": null,
      "repo_url": "#"
    },
    "2309.15090": {
      "paper_id": "2309.15090v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.15090v1",
      "paper_key": "2309.15090",
      "paper_title": "Single Biological Neurons as Temporally Precise Spatio-Temporal Pattern Recognizers",
      "paper_url": "http://arxiv.org/abs/2309.15090v1",
      "paper_abstract": "This PhD thesis is focused on the central idea that single neurons in the brain should be regarded as temporally precise and highly complex spatio-temporal pattern recognizers. This is opposed to the prevalent view of biological neurons as simple and mainly spatial pattern recognizers by most neuroscientists today. In this thesis, I will attempt to demonstrate that this is an important distinction, predominantly because the above-mentioned computational properties of single neurons have far-reaching implications with respect to the various brain circuits that neurons compose, and on how information is encoded by neuronal activity in the brain. Namely, that these particular \"low-level\" details at the single neuron level have substantial system-wide ramifications. In the introduction we will highlight the main components that comprise a neural microcircuit that can perform useful computations and illustrate the inter-dependence of these components from a system perspective. In chapter 1 we discuss the great complexity of the spatio-temporal input-output relationship of cortical neurons that are the result of morphological structure and biophysical properties of the neuron. In chapter 2 we demonstrate that single neurons can generate temporally precise output patterns in response to specific spatio-temporal input patterns with a very simple biologically plausible learning rule. In chapter 3, we use the differentiable deep network analog of a realistic cortical neuron as a tool to approximate the gradient of the output of the neuron with respect to its input and use this capability in an attempt to teach the neuron to perform nonlinear XOR operation. In chapter 4 we expand chapter 3 to describe extension of our ideas to neuronal networks composed of many realistic biological spiking neurons that represent either small microcircuits or entire brain regions.",
      "paper_authors": [
        "David Beniaguev"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2023-09-26",
      "update_time": "2023-09-26",
      "comments": null,
      "repo_url": "#"
    },
    "2309.14523": {
      "paper_id": "2309.14523v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.14523v1",
      "paper_key": "2309.14523",
      "paper_title": "Smooth Exact Gradient Descent Learning in Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2309.14523v1",
      "paper_abstract": "Artificial neural networks are highly successfully trained with backpropagation. For spiking neural networks, however, a similar gradient descent scheme seems prohibitive due to the sudden, disruptive (dis-)appearance of spikes. Here, we demonstrate exact gradient descent learning based on spiking dynamics that change only continuously. These are generated by neuron models whose spikes vanish and appear at the end of a trial, where they do not influence other neurons anymore. This also enables gradient-based spike addition and removal. We apply our learning scheme to induce and continuously move spikes to desired times, in single neurons and recurrent networks. Further, it achieves competitive performance in a benchmark task using deep, initially silent networks. Our results show how non-disruptive learning is possible despite discrete spikes.",
      "paper_authors": [
        "Christian Klos",
        "Raoul-Martin Memmesheimer"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2023-09-25",
      "update_time": "2023-09-25",
      "comments": null,
      "repo_url": "#"
    },
    "2309.13302": {
      "paper_id": "2309.13302v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.13302v3",
      "paper_key": "2309.13302",
      "paper_title": "Gaining the Sparse Rewards by Exploring Lottery Tickets in Spiking Neural Network",
      "paper_url": "http://arxiv.org/abs/2309.13302v3",
      "paper_abstract": "Deploying energy-efficient deep learning algorithms on computational-limited devices, such as robots, is still a pressing issue for real-world applications. Spiking Neural Networks (SNNs), a novel brain-inspired algorithm, offer a promising solution due to their low-latency and low-energy properties over traditional Artificial Neural Networks (ANNs). Despite their advantages, the dense structure of deep SNNs can still result in extra energy consumption. The Lottery Ticket Hypothesis (LTH) posits that within dense neural networks, there exist winning Lottery Tickets (LTs), namely sub-networks, that can be obtained without compromising performance. Inspired by this, this paper delves into the spiking-based LTs (SLTs), examining their unique properties and potential for extreme efficiency. Then, two significant sparse \\textbf{\\textit{Rewards}} are gained through comprehensive explorations and meticulous experiments on SLTs across various dense structures. Moreover, a sparse algorithm tailored for spiking transformer structure, which incorporates convolution operations into the Patch Embedding Projection (ConvPEP) module, has been proposed to achieve Multi-level Sparsity (MultiSp). MultiSp refers to (1) Patch number sparsity; (2) ConvPEP weights sparsity and binarization; and (3) ConvPEP activation layer binarization. Extensive experiments demonstrate that our method achieves extreme sparsity with only a slight performance decrease, paving the way for deploying energy-efficient neural networks in robotics and beyond.",
      "paper_authors": [
        "Hao Cheng",
        "Jiahang Cao",
        "Erjia Xiao",
        "Mengshu Sun",
        "Renjing Xu"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-09-23",
      "update_time": "2024-03-28",
      "comments": "This paper is under submission",
      "repo_url": "#"
    },
    "2309.12937": {
      "paper_id": "2309.12937v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.12937v1",
      "paper_key": "2309.12937",
      "paper_title": "Evolving Spiking Neural Networks to Mimic PID Control for Autonomous Blimps",
      "paper_url": "http://arxiv.org/abs/2309.12937v1",
      "paper_abstract": "In recent years, Artificial Neural Networks (ANN) have become a standard in robotic control. However, a significant drawback of large-scale ANNs is their increased power consumption. This becomes a critical concern when designing autonomous aerial vehicles, given the stringent constraints on power and weight. Especially in the case of blimps, known for their extended endurance, power-efficient control methods are essential. Spiking neural networks (SNN) can provide a solution, facilitating energy-efficient and asynchronous event-driven processing. In this paper, we have evolved SNNs for accurate altitude control of a non-neutrally buoyant indoor blimp, relying solely on onboard sensing and processing power. The blimp's altitude tracking performance significantly improved compared to prior research, showing reduced oscillations and a minimal steady-state error. The parameters of the SNNs were optimized via an evolutionary algorithm, using a Proportional-Derivative-Integral (PID) controller as the target signal. We developed two complementary SNN controllers while examining various hidden layer structures. The first controller responds swiftly to control errors, mitigating overshooting and oscillations, while the second minimizes steady-state errors due to non-neutral buoyancy-induced drift. Despite the blimp's drivetrain limitations, our SNN controllers ensured stable altitude control, employing only 160 spiking neurons.",
      "paper_authors": [
        "Tim Burgers",
        "Stein Stroobants",
        "Guido de Croon"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2023-09-22",
      "update_time": "2023-09-22",
      "comments": null,
      "repo_url": "#"
    },
    "2309.12761": {
      "paper_id": "2309.12761v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.12761v1",
      "paper_key": "2309.12761",
      "paper_title": "S3TC: Spiking Separated Spatial and Temporal Convolutions with Unsupervised STDP-based Learning for Action Recognition",
      "paper_url": "http://arxiv.org/abs/2309.12761v1",
      "paper_abstract": "Video analysis is a major computer vision task that has received a lot of attention in recent years. The current state-of-the-art performance for video analysis is achieved with Deep Neural Networks (DNNs) that have high computational costs and need large amounts of labeled data for training. Spiking Neural Networks (SNNs) have significantly lower computational costs (thousands of times) than regular non-spiking networks when implemented on neuromorphic hardware. They have been used for video analysis with methods like 3D Convolutional Spiking Neural Networks (3D CSNNs). However, these networks have a significantly larger number of parameters compared with spiking 2D CSNN. This, not only increases the computational costs, but also makes these networks more difficult to implement with neuromorphic hardware. In this work, we use CSNNs trained in an unsupervised manner with the Spike Timing-Dependent Plasticity (STDP) rule, and we introduce, for the first time, Spiking Separated Spatial and Temporal Convolutions (S3TCs) for the sake of reducing the number of parameters required for video analysis. This unsupervised learning has the advantage of not needing large amounts of labeled data for training. Factorizing a single spatio-temporal spiking convolution into a spatial and a temporal spiking convolution decreases the number of parameters of the network. We test our network with the KTH, Weizmann, and IXMAS datasets, and we show that S3TCs successfully extract spatio-temporal information from videos, while increasing the output spiking activity, and outperforming spiking 3D convolutions.",
      "paper_authors": [
        "Mireille El-Assal",
        "Pierre Tirilly",
        "Ioan Marius Bilasco"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-09-22",
      "update_time": "2023-09-22",
      "comments": null,
      "repo_url": "#"
    },
    "2309.10987": {
      "paper_id": "2309.10987v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.10987v3",
      "paper_key": "2309.10987",
      "paper_title": "SpikingNeRF: Making Bio-inspired Neural Networks See through the Real World",
      "paper_url": "http://arxiv.org/abs/2309.10987v3",
      "paper_abstract": "Spiking neural networks (SNNs) have been thriving on numerous tasks to leverage their promising energy efficiency and exploit their potentialities as biologically plausible intelligence. Meanwhile, the Neural Radiance Fields (NeRF) render high-quality 3D scenes with massive energy consumption, but few works delve into the energy-saving solution with a bio-inspired approach. In this paper, we propose SpikingNeRF, which aligns the radiance ray with the temporal dimension of SNN, to naturally accommodate the SNN to the reconstruction of Radiance Fields. Thus, the computation turns into a spike-based, multiplication-free manner, reducing the energy consumption. In SpikingNeRF, each sampled point on the ray is matched onto a particular time step, and represented in a hybrid manner where the voxel grids are maintained as well. Based on the voxel grids, sampled points are determined whether to be masked for better training and inference. However, this operation also incurs irregular temporal length. We propose the temporal padding strategy to tackle the masked samples to maintain regular temporal length, i.e., regular tensors, and the temporal condensing strategy to form a denser data structure for hardware-friendly computation. Extensive experiments on various datasets demonstrate that our method reduces the 70.79% energy consumption on average and obtains comparable synthesis quality with the ANN baseline.",
      "paper_authors": [
        "Xingting Yao",
        "Qinghao Hu",
        "Tielong Liu",
        "Zitao Mo",
        "Zeyu Zhu",
        "Zhengyang Zhuge",
        "Jian Cheng"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-09-20",
      "update_time": "2023-11-13",
      "comments": null,
      "repo_url": "#"
    },
    "2309.10225": {
      "paper_id": "2309.10225v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.10225v2",
      "paper_key": "2309.10225",
      "paper_title": "VPRTempo: A Fast Temporally Encoded Spiking Neural Network for Visual Place Recognition",
      "paper_url": "http://arxiv.org/abs/2309.10225v2",
      "paper_abstract": "Spiking Neural Networks (SNNs) are at the forefront of neuromorphic computing thanks to their potential energy-efficiency, low latencies, and capacity for continual learning. While these capabilities are well suited for robotics tasks, SNNs have seen limited adaptation in this field thus far. This work introduces a SNN for Visual Place Recognition (VPR) that is both trainable within minutes and queryable in milliseconds, making it well suited for deployment on compute-constrained robotic systems. Our proposed system, VPRTempo, overcomes slow training and inference times using an abstracted SNN that trades biological realism for efficiency. VPRTempo employs a temporal code that determines the timing of a single spike based on a pixel's intensity, as opposed to prior SNNs relying on rate coding that determined the number of spikes; improving spike efficiency by over 100%. VPRTempo is trained using Spike-Timing Dependent Plasticity and a supervised delta learning rule enforcing that each output spiking neuron responds to just a single place. We evaluate our system on the Nordland and Oxford RobotCar benchmark localization datasets, which include up to 27k places. We found that VPRTempo's accuracy is comparable to prior SNNs and the popular NetVLAD place recognition algorithm, while being several orders of magnitude faster and suitable for real-time deployment -- with inference speeds over 50 Hz on CPU. VPRTempo could be integrated as a loop closure component for online SLAM on resource-constrained systems such as space and underwater robots.",
      "paper_authors": [
        "Adam D. Hines",
        "Peter G. Stratton",
        "Michael Milford",
        "Tobias Fischer"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2023-09-19",
      "update_time": "2024-03-01",
      "comments": "8 pages, 3 figures, accepted to the IEEE International Conference on\n  Robotics and Automation (ICRA) 2024",
      "repo_url": "https://github.com/QVPR/VPRTempo"
    },
    "2309.09550": {
      "paper_id": "2309.09550v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.09550v2",
      "paper_key": "2309.09550",
      "paper_title": "Adaptive Reorganization of Neural Pathways for Continual Learning with Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2309.09550v2",
      "paper_abstract": "The human brain can self-organize rich and diverse sparse neural pathways to incrementally master hundreds of cognitive tasks. However, most existing continual learning algorithms for deep artificial and spiking neural networks are unable to adequately auto-regulate the limited resources in the network, which leads to performance drop along with energy consumption rise as the increase of tasks. In this paper, we propose a brain-inspired continual learning algorithm with adaptive reorganization of neural pathways, which employs Self-Organizing Regulation networks to reorganize the single and limited Spiking Neural Network (SOR-SNN) into rich sparse neural pathways to efficiently cope with incremental tasks. The proposed model demonstrates consistent superiority in performance, energy consumption, and memory capacity on diverse continual learning tasks ranging from child-like simple to complex tasks, as well as on generalized CIFAR100 and ImageNet datasets. In particular, the SOR-SNN model excels at learning more complex tasks as well as more tasks, and is able to integrate the past learned knowledge with the information from the current task, showing the backward transfer ability to facilitate the old tasks. Meanwhile, the proposed model exhibits self-repairing ability to irreversible damage and for pruned networks, could automatically allocate new pathway from the retained network to recover memory for forgotten knowledge.",
      "paper_authors": [
        "Bing Han",
        "Feifei Zhao",
        "Wenxuan Pan",
        "Zhaoya Zhao",
        "Xianqi Li",
        "Qingqun Kong",
        "Yi Zeng"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-09-18",
      "update_time": "2023-10-08",
      "comments": null,
      "repo_url": "#"
    },
    "2309.09469": {
      "paper_id": "2309.09469v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.09469v2",
      "paper_key": "2309.09469",
      "paper_title": "Spiking-LEAF: A Learnable Auditory front-end for Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2309.09469v2",
      "paper_abstract": "Brain-inspired spiking neural networks (SNNs) have demonstrated great potential for temporal signal processing. However, their performance in speech processing remains limited due to the lack of an effective auditory front-end. To address this limitation, we introduce Spiking-LEAF, a learnable auditory front-end meticulously designed for SNN-based speech processing. Spiking-LEAF combines a learnable filter bank with a novel two-compartment spiking neuron model called IHC-LIF. The IHC-LIF neurons draw inspiration from the structure of inner hair cells (IHC) and they leverage segregated dendritic and somatic compartments to effectively capture multi-scale temporal dynamics of speech signals. Additionally, the IHC-LIF neurons incorporate the lateral feedback mechanism along with spike regularization loss to enhance spike encoding efficiency. On keyword spotting and speaker identification tasks, the proposed Spiking-LEAF outperforms both SOTA spiking auditory front-ends and conventional real-valued acoustic features in terms of classification accuracy, noise robustness, and encoding efficiency.",
      "paper_authors": [
        "Zeyang Song",
        "Jibin Wu",
        "Malu Zhang",
        "Mike Zheng Shou",
        "Haizhou Li"
      ],
      "primary_category": "cs.SD",
      "publish_time": "2023-09-18",
      "update_time": "2024-03-23",
      "comments": "Accepted by ICASSP2024",
      "repo_url": "#"
    },
    "2309.09297": {
      "paper_id": "2309.09297v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.09297v2",
      "paper_key": "2309.09297",
      "paper_title": "Chasing Day and Night: Towards Robust and Efficient All-Day Object Detection Guided by an Event Camera",
      "paper_url": "http://arxiv.org/abs/2309.09297v2",
      "paper_abstract": "The ability to detect objects in all lighting (i.e., normal-, over-, and under-exposed) conditions is crucial for real-world applications, such as self-driving.Traditional RGB-based detectors often fail under such varying lighting conditions.Therefore, recent works utilize novel event cameras to supplement or guide the RGB modality; however, these methods typically adopt asymmetric network structures that rely predominantly on the RGB modality, resulting in limited robustness for all-day detection. In this paper, we propose EOLO, a novel object detection framework that achieves robust and efficient all-day detection by fusing both RGB and event modalities. Our EOLO framework is built based on a lightweight spiking neural network (SNN) to efficiently leverage the asynchronous property of events. Buttressed by it, we first introduce an Event Temporal Attention (ETA) module to learn the high temporal information from events while preserving crucial edge information. Secondly, as different modalities exhibit varying levels of importance under diverse lighting conditions, we propose a novel Symmetric RGB-Event Fusion (SREF) module to effectively fuse RGB-Event features without relying on a specific modality, thus ensuring a balanced and adaptive fusion for all-day detection. In addition, to compensate for the lack of paired RGB-Event datasets for all-day training and evaluation, we propose an event synthesis approach based on the randomized optical flow that allows for directly generating the event frame from a single exposure image. We further build two new datasets, E-MSCOCO and E-VOC based on the popular benchmarks MSCOCO and PASCAL VOC. Extensive experiments demonstrate that our EOLO outperforms the state-of-the-art detectors,e.g.,RENet,by a substantial margin (+3.74% mAP50) in all lighting conditions.Our code and datasets will be available at https://vlislab22.github.io/EOLO/",
      "paper_authors": [
        "Jiahang Cao",
        "Xu Zheng",
        "Yuanhuiyi Lyu",
        "Jiaxu Wang",
        "Renjing Xu",
        "Lin Wang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-09-17",
      "update_time": "2024-03-19",
      "comments": "Accepted by ICRA 2024",
      "repo_url": "#"
    },
    "2309.09025": {
      "paper_id": "2309.09025v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.09025v1",
      "paper_key": "2309.09025",
      "paper_title": "Efficient Privacy-Preserving Convolutional Spiking Neural Networks with FHE",
      "paper_url": "http://arxiv.org/abs/2309.09025v1",
      "paper_abstract": "With the rapid development of AI technology, we have witnessed numerous innovations and conveniences. However, along with these advancements come privacy threats and risks. Fully Homomorphic Encryption (FHE) emerges as a key technology for privacy-preserving computation, enabling computations while maintaining data privacy. Nevertheless, FHE has limitations in processing continuous non-polynomial functions as it is restricted to discrete integers and supports only addition and multiplication. Spiking Neural Networks (SNNs) operate on discrete spike signals, naturally aligning with the properties of FHE. In this paper, we present a framework called FHE-DiCSNN. This framework is based on the efficient TFHE scheme and leverages the discrete properties of SNNs to achieve high prediction performance on ciphertexts. Firstly, by employing bootstrapping techniques, we successfully implement computations of the Leaky Integrate-and-Fire neuron model on ciphertexts. Through bootstrapping, we can facilitate computations for SNNs of arbitrary depth. This framework can be extended to other spiking neuron models, providing a novel framework for the homomorphic evaluation of SNNs. Secondly, inspired by CNNs, we adopt convolutional methods to replace Poisson encoding. This not only enhances accuracy but also mitigates the issue of prolonged simulation time caused by random encoding. Furthermore, we employ engineering techniques to parallelize the computation of bootstrapping, resulting in a significant improvement in computational efficiency. Finally, we evaluate our model on the MNIST dataset. Experimental results demonstrate that, with the optimal parameter configuration, FHE-DiCSNN achieves an accuracy of 97.94% on ciphertexts, with a loss of only 0.53% compared to the original network's accuracy of 98.47%. Moreover, each prediction requires only 0.75 seconds of computation time",
      "paper_authors": [
        "Pengbo Li",
        "Huifang Huang",
        "Ting Gao",
        "Jin Guo",
        "Jinqiao Duan"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2023-09-16",
      "update_time": "2023-09-16",
      "comments": null,
      "repo_url": "#"
    },
    "2309.08476": {
      "paper_id": "2309.08476v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.08476v1",
      "paper_key": "2309.08476",
      "paper_title": "A Spiking Binary Neuron -- Detector of Causal Links",
      "paper_url": "http://arxiv.org/abs/2309.08476v1",
      "paper_abstract": "Causal relationship recognition is a fundamental operation in neural networks aimed at learning behavior, action planning, and inferring external world dynamics. This operation is particularly crucial for reinforcement learning (RL). In the context of spiking neural networks (SNNs), events are represented as spikes emitted by network neurons or input nodes. Detecting causal relationships within these events is essential for effective RL implementation. This research paper presents a novel approach to realize causal relationship recognition using a simple spiking binary neuron. The proposed method leverages specially designed synaptic plasticity rules, which are both straightforward and efficient. Notably, our approach accounts for the temporal aspects of detected causal links and accommodates the representation of spiking signals as single spikes or tight spike sequences (bursts), as observed in biological brains. Furthermore, this study places a strong emphasis on the hardware-friendliness of the proposed models, ensuring their efficient implementation on modern and future neuroprocessors. Being compared with precise machine learning techniques, such as decision tree algorithms and convolutional neural networks, our neuron demonstrates satisfactory accuracy despite its simplicity. In conclusion, we introduce a multi-neuron structure capable of operating in more complex environments with enhanced accuracy, making it a promising candidate for the advancement of RL applications in SNNs.",
      "paper_authors": [
        "Mikhail Kiselev",
        "Denis Larionov",
        "Andrey Urusov"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-09-15",
      "update_time": "2023-09-15",
      "comments": null,
      "repo_url": "#"
    },
    "2309.08232": {
      "paper_id": "2309.08232v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.08232v1",
      "paper_key": "2309.08232",
      "paper_title": "Astrocyte-Integrated Dynamic Function Exchange in Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2309.08232v1",
      "paper_abstract": "This paper presents an innovative methodology for improving the robustness and computational efficiency of Spiking Neural Networks (SNNs), a critical component in neuromorphic computing. The proposed approach integrates astrocytes, a type of glial cell prevalent in the human brain, into SNNs, creating astrocyte-augmented networks. To achieve this, we designed and implemented an astrocyte model in two distinct platforms: CPU/GPU and FPGA. Our FPGA implementation notably utilizes Dynamic Function Exchange (DFX) technology, enabling real-time hardware reconfiguration and adaptive model creation based on current operating conditions. The novel approach of leveraging astrocytes significantly improves the fault tolerance of SNNs, thereby enhancing their robustness. Notably, our astrocyte-augmented SNN displays near-zero latency and theoretically infinite throughput, implying exceptional computational efficiency. Through comprehensive comparative analysis with prior works, it's established that our model surpasses others in terms of neuron and synapse count while maintaining an efficient power consumption profile. These results underscore the potential of our methodology in shaping the future of neuromorphic computing, by providing robust and energy-efficient systems.",
      "paper_authors": [
        "Murat Isik",
        "Kayode Inadagbo"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-09-15",
      "update_time": "2023-09-15",
      "comments": "Accepted at 8th International Conference on Engineering of\n  Computer-based Systems",
      "repo_url": "#"
    },
    "2309.07641": {
      "paper_id": "2309.07641v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.07641v1",
      "paper_key": "2309.07641",
      "paper_title": "Reconfigurable neural spiking in bias field-free spin Hall nano oscillator",
      "paper_url": "http://arxiv.org/abs/2309.07641v1",
      "paper_abstract": "In this study, we theoretically investigate neuron-like spiking dynamics in an elliptic ferromagnet/heavy metal bilayer-based spin Hall nano oscillator (SHNO) in bias field-free condition, much suitable for practical realization of brain inspired computing schemes. We demonstrate regular periodic spiking with tunable frequency as well as the leaky-integrate-and-fire (LIF) behavior in a single SHNO by manipulating the pulse features of input current. The frequency of regular periodic spiking is tunable in a range of 0.5 GHz to 0.96 GHz (460 MHz bandwidth) through adjusting the magnitude of constant input dc current density. We further demonstrate the reconfigurability of spiking dynamics in response to a time varying input accomplished by continuously increasing the input current density as a linear function of time. Macrospin theory and micromagnetic simulation provide insights into the origin of bias field-free auto-oscillation and the spiking phenomena in our SHNO. In addition, we discuss how the shape anisotropy of the elliptic ferromagnet influence the bias field-free auto oscillation characteristics, including threshold current, frequency and transition from in-plane to out-of-plane precession. The SHNO operates below $10^{12} A/m^2$ input current density and exhibits a large auto-oscillation amplitude, ensuring high output power. We show that the threshold current density can be reduced by decreasing the ellipticity of the ferromagnet layer as well as enhancing the perpendicular magnetic anisotropy. These findings highlight the potential of bias field-free elliptic SHNO in designing power-efficient spiking neuron-based neuromorphic hardware.",
      "paper_authors": [
        "Sourabh Manna",
        "Rohit Medwal",
        "Rajdeep Singh Rawat"
      ],
      "primary_category": "physics.app-ph",
      "publish_time": "2023-09-14",
      "update_time": "2023-09-14",
      "comments": null,
      "repo_url": "#"
    },
    "2309.07535": {
      "paper_id": "2309.07535v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.07535v1",
      "paper_key": "2309.07535",
      "paper_title": "Spiking Dynamics in Dual Free Layer Perpendicular Magnetic Tunnel Junctions",
      "paper_url": "http://arxiv.org/abs/2309.07535v1",
      "paper_abstract": "Spintronic devices have recently attracted a lot of attention in the field of unconventional computing due to their non-volatility for short and long term memory, non-linear fast response and relatively small footprint. Here we report how voltage driven magnetization dynamics of dual free layer perpendicular magnetic tunnel junctions enable to emulate spiking neurons in hardware. The output spiking rate was controlled by varying the dc bias voltage across the device. The field-free operation of this two terminal device and its robustness against an externally applied magnetic field make it a suitable candidate to mimic neuron response in a dense Neural Network (NN). The small energy consumption of the device (4-16 pJ/spike) and its scalability are important benefits for embedded applications. This compact perpendicular magnetic tunnel junction structure could finally bring spiking neural networks (SNN) to sub-100nm size elements.",
      "paper_authors": [
        "Louis Farcis",
        "Bruno Teixeira",
        "Philippe Talatchian",
        "David Salomoni",
        "Ursula Ebels",
        "St\u00e9phane Auffret",
        "Bernard Dieny",
        "Frank Mizrahi",
        "Julie Grollier",
        "Ricardo Sousa",
        "Liliana Buda-Prejbeanu"
      ],
      "primary_category": "physics.app-ph",
      "publish_time": "2023-09-14",
      "update_time": "2023-09-14",
      "comments": null,
      "repo_url": "#"
    },
    "2309.06652": {
      "paper_id": "2309.06652v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.06652v1",
      "paper_key": "2309.06652",
      "paper_title": "Event-Driven Imaging in Turbid Media: A Confluence of Optoelectronics and Neuromorphic Computation",
      "paper_url": "http://arxiv.org/abs/2309.06652v1",
      "paper_abstract": "In this paper a new optical-computational method is introduced to unveil images of targets whose visibility is severely obscured by light scattering in dense, turbid media. The targets of interest are taken to be dynamic in that their optical properties are time-varying whether stationary in space or moving. The scheme, to our knowledge the first of its kind, is human vision inspired whereby diffuse photons collected from the turbid medium are first transformed to spike trains by a dynamic vision sensor as in the retina, and image reconstruction is then performed by a neuromorphic computing approach mimicking the brain. We combine benchtop experimental data in both reflection (backscattering) and transmission geometries with support from physics-based simulations to develop a neuromorphic computational model and then apply this for image reconstruction of different MNIST characters and image sets by a dedicated deep spiking neural network algorithm. Image reconstruction is achieved under conditions of turbidity where an original image is unintelligible to the human eye or a digital video camera, yet clearly and quantifiable identifiable when using the new neuromorphic computational approach.",
      "paper_authors": [
        "Ning Zhang",
        "Timothy Shea",
        "Arto Nurmikko"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-09-13",
      "update_time": "2023-09-13",
      "comments": null,
      "repo_url": "#"
    },
    "2311.16112": {
      "paper_id": "2311.16112v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.16112v1",
      "paper_key": "2311.16112",
      "paper_title": "Co-learning synaptic delays, weights and adaptation in spiking neural networks",
      "paper_url": "http://arxiv.org/abs/2311.16112v1",
      "paper_abstract": "Spiking neural networks (SNN) distinguish themselves from artificial neural networks (ANN) because of their inherent temporal processing and spike-based computations, enabling a power-efficient implementation in neuromorphic hardware. In this paper, we demonstrate that data processing with spiking neurons can be enhanced by co-learning the connection weights with two other biologically inspired neuronal features: 1) a set of parameters describing neuronal adaptation processes and 2) synaptic propagation delays. The former allows the spiking neuron to learn how to specifically react to incoming spikes based on its past. The trained adaptation parameters result in neuronal heterogeneity, which is found in the brain and also leads to a greater variety in available spike patterns. The latter enables to learn to explicitly correlate patterns that are temporally distanced. Synaptic delays reflect the time an action potential requires to travel from one neuron to another. We show that each of the co-learned features separately leads to an improvement over the baseline SNN and that the combination of both leads to state-of-the-art SNN results on all speech recognition datasets investigated with a simple 2-hidden layer feed-forward network. Our SNN outperforms the ANN on the neuromorpic datasets (Spiking Heidelberg Digits and Spiking Speech Commands), even with fewer trainable parameters. On the 35-class Google Speech Commands dataset, our SNN also outperforms a GRU of similar size. Our work presents brain-inspired improvements to SNN that enable them to excel over an equivalent ANN of similar size on tasks with rich temporal dynamics.",
      "paper_authors": [
        "Lucas Deckers",
        "Laurens Van Damme",
        "Ing Jyh Tsang",
        "Werner Van Leekwijck",
        "Steven Latr\u00e9"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-09-12",
      "update_time": "2023-09-12",
      "comments": "15 pages, 8 figures",
      "repo_url": "#"
    },
    "2309.05430": {
      "paper_id": "2309.05430v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.05430v1",
      "paper_key": "2309.05430",
      "paper_title": "Neuromorphic Auditory Perception by Neural Spiketrum",
      "paper_url": "http://arxiv.org/abs/2309.05430v1",
      "paper_abstract": "Neuromorphic computing holds the promise to achieve the energy efficiency and robust learning performance of biological neural systems. To realize the promised brain-like intelligence, it needs to solve the challenges of the neuromorphic hardware architecture design of biological neural substrate and the hardware amicable algorithms with spike-based encoding and learning. Here we introduce a neural spike coding model termed spiketrum, to characterize and transform the time-varying analog signals, typically auditory signals, into computationally efficient spatiotemporal spike patterns. It minimizes the information loss occurring at the analog-to-spike transformation and possesses informational robustness to neural fluctuations and spike losses. The model provides a sparse and efficient coding scheme with precisely controllable spike rate that facilitates training of spiking neural networks in various auditory perception tasks. We further investigate the algorithm-hardware co-designs through a neuromorphic cochlear prototype which demonstrates that our approach can provide a systematic solution for spike-based artificial intelligence by fully exploiting its advantages with spike-based computation.",
      "paper_authors": [
        "Huajin Tang",
        "Pengjie Gu",
        "Jayawan Wijekoon",
        "MHD Anas Alsakkal",
        "Ziming Wang",
        "Jiangrong Shen",
        "Rui Yan"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-09-11",
      "update_time": "2023-09-11",
      "comments": "This work has been submitted to the IEEE for possible publication",
      "repo_url": "#"
    },
    "2309.05345": {
      "paper_id": "2309.05345v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.05345v1",
      "paper_key": "2309.05345",
      "paper_title": "Empirical study on the efficiency of Spiking Neural Networks with axonal delays, and algorithm-hardware benchmarking",
      "paper_url": "http://arxiv.org/abs/2309.05345v1",
      "paper_abstract": "The role of axonal synaptic delays in the efficacy and performance of artificial neural networks has been largely unexplored. In step-based analog-valued neural network models (ANNs), the concept is almost absent. In their spiking neuroscience-inspired counterparts, there is hardly a systematic account of their effects on model performance in terms of accuracy and number of synaptic operations.This paper proposes a methodology for accounting for axonal delays in the training loop of deep Spiking Neural Networks (SNNs), intending to efficiently solve machine learning tasks on data with rich temporal dependencies. We then conduct an empirical study of the effects of axonal delays on model performance during inference for the Adding task, a benchmark for sequential regression, and for the Spiking Heidelberg Digits dataset (SHD), commonly used for evaluating event-driven models. Quantitative results on the SHD show that SNNs incorporating axonal delays instead of explicit recurrent synapses achieve state-of-the-art, over 90% test accuracy while needing less than half trainable synapses. Additionally, we estimate the required memory in terms of total parameters and energy consumption of accomodating such delay-trained models on a modern neuromorphic accelerator. These estimations are based on the number of synaptic operations and the reference GF-22nm FDX CMOS technology. As a result, we demonstrate that a reduced parameterization, which incorporates axonal delays, leads to approximately 90% energy and memory reduction in digital hardware implementations for a similar performance in the aforementioned task.",
      "paper_authors": [
        "Alberto Pati\u00f1o-Saucedo",
        "Amirreza Yousefzadeh",
        "Guangzhi Tang",
        "Federico Corradi",
        "Bernab\u00e9 Linares-Barranco",
        "Manolis Sifalakis"
      ],
      "primary_category": "cs.ET",
      "publish_time": "2023-09-11",
      "update_time": "2023-09-11",
      "comments": null,
      "repo_url": "#"
    },
    "2309.05263": {
      "paper_id": "2309.05263v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.05263v1",
      "paper_key": "2309.05263",
      "paper_title": "Brain-inspired Evolutionary Architectures for Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2309.05263v1",
      "paper_abstract": "The complex and unique neural network topology of the human brain formed through natural evolution enables it to perform multiple cognitive functions simultaneously. Automated evolutionary mechanisms of biological network structure inspire us to explore efficient architectural optimization for Spiking Neural Networks (SNNs). Instead of manually designed fixed architectures or hierarchical Network Architecture Search (NAS), this paper evolves SNNs architecture by incorporating brain-inspired local modular structure and global cross-module connectivity. Locally, the brain region-inspired module consists of multiple neural motifs with excitatory and inhibitory connections; Globally, we evolve free connections among modules, including long-term cross-module feedforward and feedback connections. We further introduce an efficient multi-objective evolutionary algorithm based on a few-shot performance predictor, endowing SNNs with high performance, efficiency and low energy consumption. Extensive experiments on static datasets (CIFAR10, CIFAR100) and neuromorphic datasets (CIFAR10-DVS, DVS128-Gesture) demonstrate that our proposed model boosts energy efficiency, archiving consistent and remarkable performance. This work explores brain-inspired neural architectures suitable for SNNs and also provides preliminary insights into the evolutionary mechanisms of biological neural networks in the human brain.",
      "paper_authors": [
        "Wenxuan Pan",
        "Feifei Zhao",
        "Zhuoya Zhao",
        "Yi Zeng"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-09-11",
      "update_time": "2023-09-11",
      "comments": null,
      "repo_url": "#"
    },
    "2309.04737": {
      "paper_id": "2309.04737v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.04737v3",
      "paper_key": "2309.04737",
      "paper_title": "Learning Spiking Neural Network from Easy to Hard task",
      "paper_url": "http://arxiv.org/abs/2309.04737v3",
      "paper_abstract": "Starting with small and simple concepts, and gradually introducing complex and difficult concepts is the natural process of human learning. Spiking Neural Networks (SNNs) aim to mimic the way humans process information, but current SNNs models treat all samples equally, which does not align with the principles of human learning and overlooks the biological plausibility of SNNs. To address this, we propose a CL-SNN model that introduces Curriculum Learning(CL) into SNNs, making SNNs learn more like humans and providing higher biological interpretability. CL is a training strategy that advocates presenting easier data to models before gradually introducing more challenging data, mimicking the human learning process. We use a confidence-aware loss to measure and process the samples with different difficulty levels. By learning the confidence of different samples, the model reduces the contribution of difficult samples to parameter optimization automatically. We conducted experiments on static image datasets MNIST, Fashion-MNIST, CIFAR10, and neuromorphic datasets N-MNIST, CIFAR10-DVS, DVS-Gesture. The results are promising. To our best knowledge, this is the first proposal to enhance the biologically plausibility of SNNs by introducing CL.",
      "paper_authors": [
        "Lingling Tang",
        "Jiangtao Hu",
        "Hua Yu",
        "Surui Liu",
        "Jielei Chu"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2023-09-09",
      "update_time": "2023-09-26",
      "comments": null,
      "repo_url": "#"
    },
    "2309.04426": {
      "paper_id": "2309.04426v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.04426v1",
      "paper_key": "2309.04426",
      "paper_title": "Advanced Computing and Related Applications Leveraging Brain-inspired Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2309.04426v1",
      "paper_abstract": "In the rapid evolution of next-generation brain-inspired artificial intelligence and increasingly sophisticated electromagnetic environment, the most bionic characteristics and anti-interference performance of spiking neural networks show great potential in terms of computational speed, real-time information processing, and spatio-temporal information processing. Data processing. Spiking neural network is one of the cores of brain-like artificial intelligence, which realizes brain-like computing by simulating the structure and information transfer mode of biological neural networks. This paper summarizes the strengths, weaknesses and applicability of five neuronal models and analyzes the characteristics of five network topologies; then reviews the spiking neural network algorithms and summarizes the unsupervised learning algorithms based on synaptic plasticity rules and four types of supervised learning algorithms from the perspectives of unsupervised learning and supervised learning; finally focuses on the review of brain-like neuromorphic chips under research at home and abroad. This paper is intended to provide learning concepts and research orientations for the peers who are new to the research field of spiking neural networks through systematic summaries.",
      "paper_authors": [
        "Lyuyang Sima",
        "Joseph Bucukovski",
        "Erwan Carlson",
        "Nicole L. Yien"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-09-08",
      "update_time": "2023-09-08",
      "comments": null,
      "repo_url": "#"
    },
    "2310.12985": {
      "paper_id": "2310.12985v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.12985v1",
      "paper_key": "2310.12985",
      "paper_title": "Enabling energy-Efficient object detection with surrogate gradient descent in spiking neural networks",
      "paper_url": "http://arxiv.org/abs/2310.12985v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) are a biologically plausible neural network model with significant advantages in both event-driven processing and spatio-temporal information processing, rendering SNNs an appealing choice for energyefficient object detection. However, the non-differentiability of the biological neuronal dynamics model presents a challenge during the training of SNNs. Furthermore, a suitable decoding strategy for object detection in SNNs is currently lacking. In this study, we introduce the Current Mean Decoding (CMD) method, which solves the regression problem to facilitate the training of deep SNNs for object detection tasks. Based on the gradient surrogate and CMD, we propose the SNN-YOLOv3 model for object detection. Our experiments demonstrate that SNN-YOLOv3 achieves a remarkable performance with an mAP of 61.87% on the PASCAL VOC dataset, requiring only 6 time steps. Compared to SpikingYOLO, we have managed to increase mAP by nearly 10% while reducing energy consumption by two orders of magnitude.",
      "paper_authors": [
        "Jilong Luo",
        "Shanlin Xiao",
        "Yinsheng Chen",
        "Zhiyi Yu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-09-07",
      "update_time": "2023-09-07",
      "comments": null,
      "repo_url": "#"
    },
    "2309.03641": {
      "paper_id": "2309.03641v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.03641v2",
      "paper_key": "2309.03641",
      "paper_title": "Spiking Structured State Space Model for Monaural Speech Enhancement",
      "paper_url": "http://arxiv.org/abs/2309.03641v2",
      "paper_abstract": "Speech enhancement seeks to extract clean speech from noisy signals. Traditional deep learning methods face two challenges: efficiently using information in long speech sequences and high computational costs. To address these, we introduce the Spiking Structured State Space Model (Spiking-S4). This approach merges the energy efficiency of Spiking Neural Networks (SNN) with the long-range sequence modeling capabilities of Structured State Space Models (S4), offering a compelling solution. Evaluation on the DNS Challenge and VoiceBank+Demand Datasets confirms that Spiking-S4 rivals existing Artificial Neural Network (ANN) methods but with fewer computational resources, as evidenced by reduced parameters and Floating Point Operations (FLOPs).",
      "paper_authors": [
        "Yu Du",
        "Xu Liu",
        "Yansong Chua"
      ],
      "primary_category": "cs.SD",
      "publish_time": "2023-09-07",
      "update_time": "2024-04-21",
      "comments": null,
      "repo_url": "#"
    },
    "2309.03388": {
      "paper_id": "2309.03388v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.03388v1",
      "paper_key": "2309.03388",
      "paper_title": "Are SNNs Truly Energy-efficient? $-$ A Hardware Perspective",
      "paper_url": "http://arxiv.org/abs/2309.03388v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) have gained attention for their energy-efficient machine learning capabilities, utilizing bio-inspired activation functions and sparse binary spike-data representations. While recent SNN algorithmic advances achieve high accuracy on large-scale computer vision tasks, their energy-efficiency claims rely on certain impractical estimation metrics. This work studies two hardware benchmarking platforms for large-scale SNN inference, namely SATA and SpikeSim. SATA is a sparsity-aware systolic-array accelerator, while SpikeSim evaluates SNNs implemented on In-Memory Computing (IMC) based analog crossbars. Using these tools, we find that the actual energy-efficiency improvements of recent SNN algorithmic works differ significantly from their estimated values due to various hardware bottlenecks. We identify and address key roadblocks to efficient SNN deployment on hardware, including repeated computations & data movements over timesteps, neuronal module overhead, and vulnerability of SNNs towards crossbar non-idealities.",
      "paper_authors": [
        "Abhiroop Bhattacharjee",
        "Ruokai Yin",
        "Abhishek Moitra",
        "Priyadarshini Panda"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-09-06",
      "update_time": "2023-09-06",
      "comments": "5 pages",
      "repo_url": "#"
    },
    "2309.02111": {
      "paper_id": "2309.02111v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.02111v1",
      "paper_key": "2309.02111",
      "paper_title": "HW/SW Codesign for Robust and Efficient Binarized SNNs by Capacitor Minimization",
      "paper_url": "http://arxiv.org/abs/2309.02111v1",
      "paper_abstract": "Using accelerators based on analog computing is an efficient way to process the immensely large workloads in Neural Networks (NNs). One example of an analog computing scheme for NNs is Integrate-and-Fire (IF) Spiking Neural Networks (SNNs). However, to achieve high inference accuracy in IF-SNNs, the analog hardware needs to represent current-based multiply-accumulate (MAC) levels as spike times, for which a large membrane capacitor needs to be charged for a certain amount of time. A large capacitor results in high energy use, considerable area cost, and long latency, constituting one of the major bottlenecks in analog IF-SNN implementations. In this work, we propose a HW/SW Codesign method, called CapMin, for capacitor size minimization in analog computing IF-SNNs. CapMin minimizes the capacitor size by reducing the number of spike times needed for accurate operation of the HW, based on the absolute frequency of MAC level occurrences in the SW. To increase the operation of IF-SNNs to current variation, we propose the method CapMin-V, which trades capacitor size for protection based on the reduced capacitor size found in CapMin. In our experiments, CapMin achieves more than a 14$\\times$ reduction in capacitor size over the state of the art, while CapMin-V achieves increased variation tolerance in the IF-SNN operation, requiring only a small increase in capacitor size.",
      "paper_authors": [
        "Mikail Yayla",
        "Simon Thomann",
        "Ming-Liang Wei",
        "Chia-Lin Yang",
        "Jian-Jia Chen",
        "Hussam Amrouch"
      ],
      "primary_category": "cs.AR",
      "publish_time": "2023-09-05",
      "update_time": "2023-09-05",
      "comments": "9 pages, 9 figures",
      "repo_url": "https://github.com/myay/SPICE-Torch"
    },
    "2309.00241": {
      "paper_id": "2309.00241v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.00241v1",
      "paper_key": "2309.00241",
      "paper_title": "Spiking based Cellular Learning Automata (SCLA) algorithm for mobile robot motion formulation",
      "paper_url": "http://arxiv.org/abs/2309.00241v1",
      "paper_abstract": "In this paper a new method called SCLA which stands for Spiking based Cellular Learning Automata is proposed for a mobile robot to get to the target from any random initial point. The proposed method is a result of the integration of both cellular automata and spiking neural networks. The environment consists of multiple squares of the same size and the robot only observes the neighboring squares of its current square. It should be stated that the robot only moves either up and down or right and left. The environment returns feedback to the learning automata to optimize its decision making in the next steps resulting in cellular automata training. Simultaneously a spiking neural network is trained to implement long term improvements and reductions on the paths. The results show that the integration of both cellular automata and spiking neural network ends up in reinforcing the proper paths and training time reduction at the same time.",
      "paper_authors": [
        "Vahid Pashaei Rad",
        "Vahid Azimi Rad",
        "Saleh Valizadeh Sotubadi"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2023-09-01",
      "update_time": "2023-09-01",
      "comments": null,
      "repo_url": "#"
    },
    "2309.03221": {
      "paper_id": "2309.03221v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.03221v1",
      "paper_key": "2309.03221",
      "paper_title": "SPAIC: A sub-$\u03bc$W/Channel, 16-Channel General-Purpose Event-Based Analog Front-End with Dual-Mode Encoders",
      "paper_url": "http://arxiv.org/abs/2309.03221v1",
      "paper_abstract": "Low-power event-based analog front-ends (AFE) are a crucial component required to build efficient end-to-end neuromorphic processing systems for edge computing. Although several neuromorphic chips have been developed for implementing spiking neural networks (SNNs) and solving a wide range of sensory processing tasks, there are only a few general-purpose analog front-end devices that can be used to convert analog sensory signals into spikes and interfaced to neuromorphic processors. In this work, we present a novel, highly configurable analog front-end chip, denoted as SPAIC (signal-to-spike converter for analog AI computation), that offers a general-purpose dual-mode analog signal-to-spike encoding with delta modulation and pulse frequency modulation, with tunable frequency bands. The ASIC is designed in a 180 nm process. It supports and encodes a wide variety of signals spanning 4 orders of magnitude in frequency, and provides an event-based output that is compatible with existing neuromorphic processors. We validated the ASIC for its functions and present initial silicon measurement results characterizing the basic building blocks of the chip.",
      "paper_authors": [
        "Shyam Narayanan",
        "Matteo Cartiglia",
        "Arianna Rubino",
        "Charles Lego",
        "Charlotte Frenkel",
        "Giacomo Indiveri"
      ],
      "primary_category": "cs.AR",
      "publish_time": "2023-08-31",
      "update_time": "2023-08-31",
      "comments": "5 pages, 10 figures, Accepted for lecture at IEEE BioCAS Conference\n  2023",
      "repo_url": "#"
    },
    "2308.16372": {
      "paper_id": "2308.16372v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.16372v1",
      "paper_key": "2308.16372",
      "paper_title": "Artificial to Spiking Neural Networks Conversion for Scientific Machine Learning",
      "paper_url": "http://arxiv.org/abs/2308.16372v1",
      "paper_abstract": "We introduce a method to convert Physics-Informed Neural Networks (PINNs), commonly used in scientific machine learning, to Spiking Neural Networks (SNNs), which are expected to have higher energy efficiency compared to traditional Artificial Neural Networks (ANNs). We first extend the calibration technique of SNNs to arbitrary activation functions beyond ReLU, making it more versatile, and we prove a theorem that ensures the effectiveness of the calibration. We successfully convert PINNs to SNNs, enabling computational efficiency for diverse regression tasks in solving multiple differential equations, including the unsteady Navier-Stokes equations. We demonstrate great gains in terms of overall efficiency, including Separable PINNs (SPINNs), which accelerate the training process. Overall, this is the first work of this kind and the proposed method achieves relatively good accuracy with low spike rates.",
      "paper_authors": [
        "Qian Zhang",
        "Chenxi Wu",
        "Adar Kahana",
        "Youngeun Kim",
        "Yuhang Li",
        "George Em Karniadakis",
        "Priyadarshini Panda"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-08-31",
      "update_time": "2023-08-31",
      "comments": null,
      "repo_url": "#"
    },
    "2308.15754": {
      "paper_id": "2308.15754v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.15754v1",
      "paper_key": "2308.15754",
      "paper_title": "A Deep Dive into the Design Space of a Dynamically Reconfigurable Cryogenic Spiking Neuron",
      "paper_url": "http://arxiv.org/abs/2308.15754v1",
      "paper_abstract": "Spiking neural network offers the most bio-realistic approach to mimic the parallelism and compactness of the human brain. A spiking neuron is the central component of an SNN which generates information-encoded spikes. We present a comprehensive design space analysis of the superconducting memristor (SM)-based electrically reconfigurable cryogenic neuron. A superconducting nanowire (SNW) connected in parallel with an SM function as a dual-frequency oscillator and two of these oscillators can be coupled to design a dynamically tunable spiking neuron. The same neuron topology was previously proposed where a fixed resistance was used in parallel with the SNW. Replacing the fixed resistance with the SM provides an additional tuning knob with four distinct combinations of SM resistances, which improves the reconfigurability by up to ~70%. Utilizing an external bias current (Ibias), the spike frequency can be modulated up to ~3.5 times. Two distinct spike amplitudes (~1V and ~1.8 V) are also achieved. Here, we perform a systematic sensitivity analysis and show that the reconfigurability can be further tuned by choosing a higher input current strength. By performing a 500-point Monte Carlo variation analysis, we find that the spike amplitude is more variation robust than spike frequency and the variation robustness can be further improved by choosing a higher Ibias. Our study provides valuable insights for further exploration of materials and circuit level modification of the neuron that will be useful for system-level incorporation of the neuron circuit",
      "paper_authors": [
        "Md Mazharul Islam",
        "Shamiul Alam",
        "Catherine D Schuman",
        "Md Shafayat Hossain",
        "Ahmedullah Aziz"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-08-30",
      "update_time": "2023-08-30",
      "comments": null,
      "repo_url": "#"
    },
    "2308.15390": {
      "paper_id": "2308.15390v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.15390v1",
      "paper_key": "2308.15390",
      "paper_title": "Bayesian Integration of Information Using Top-Down Modulated WTA Networks",
      "paper_url": "http://arxiv.org/abs/2308.15390v1",
      "paper_abstract": "Winner Take All (WTA) circuits a type of Spiking Neural Networks (SNN) have been suggested as facilitating the brain's ability to process information in a Bayesian manner. Research has shown that WTA circuits are capable of approximating hierarchical Bayesian models via Expectation Maximization (EM). So far, research in this direction has focused on bottom up processes. This is contrary to neuroscientific evidence that shows that, besides bottom up processes, top down processes too play a key role in information processing by the human brain. Several functions ascribed to top down processes include direction of attention, adjusting for expectations, facilitation of encoding and recall of learned information, and imagery. This paper explores whether WTA circuits are suitable for further integrating information represented in separate WTA networks. Furthermore, it explores whether, and under what circumstances, top down processes can improve WTA network performance with respect to inference and learning. The results show that WTA circuits are capable of integrating the probabilistic information represented by other WTA networks, and that top down processes can improve a WTA network's inference and learning performance. Notably, it is able to do this according to key neuromorphic principles, making it ideal for low-latency and energy efficient implementation on neuromorphic hardware.",
      "paper_authors": [
        "Otto van der Himst",
        "Leila Bagheriye",
        "Johan Kwisthout"
      ],
      "primary_category": "cs.AI",
      "publish_time": "2023-08-29",
      "update_time": "2023-08-29",
      "comments": null,
      "repo_url": "https://github.com/grottoh/wta-network"
    },
    "2308.15150": {
      "paper_id": "2308.15150v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.15150v1",
      "paper_key": "2308.15150",
      "paper_title": "Unleashing the Potential of Spiking Neural Networks for Sequential Modeling with Contextual Embedding",
      "paper_url": "http://arxiv.org/abs/2308.15150v1",
      "paper_abstract": "The human brain exhibits remarkable abilities in integrating temporally distant sensory inputs for decision-making. However, existing brain-inspired spiking neural networks (SNNs) have struggled to match their biological counterpart in modeling long-term temporal relationships. To address this problem, this paper presents a novel Contextual Embedding Leaky Integrate-and-Fire (CE-LIF) spiking neuron model. Specifically, the CE-LIF model incorporates a meticulously designed contextual embedding component into the adaptive neuronal firing threshold, thereby enhancing the memory storage of spiking neurons and facilitating effective sequential modeling. Additionally, theoretical analysis is provided to elucidate how the CE-LIF model enables long-term temporal credit assignment. Remarkably, when compared to state-of-the-art recurrent SNNs, feedforward SNNs comprising the proposed CE-LIF neurons demonstrate superior performance across extensive sequential modeling tasks in terms of classification accuracy, network convergence speed, and memory capacity.",
      "paper_authors": [
        "Xinyi Chen",
        "Jibin Wu",
        "Huajin Tang",
        "Qinyuan Ren",
        "Kay Chen Tan"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-08-29",
      "update_time": "2023-08-29",
      "comments": null,
      "repo_url": "#"
    },
    "2308.15122": {
      "paper_id": "2308.15122v4",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.15122v4",
      "paper_key": "2308.15122",
      "paper_title": "SpikeBERT: A Language Spikformer Learned from BERT with Knowledge Distillation",
      "paper_url": "http://arxiv.org/abs/2308.15122v4",
      "paper_abstract": "Spiking neural networks (SNNs) offer a promising avenue to implement deep neural networks in a more energy-efficient way. However, the network architectures of existing SNNs for language tasks are still simplistic and relatively shallow, and deep architectures have not been fully explored, resulting in a significant performance gap compared to mainstream transformer-based networks such as BERT. To this end, we improve a recently-proposed spiking Transformer (i.e., Spikformer) to make it possible to process language tasks and propose a two-stage knowledge distillation method for training it, which combines pre-training by distilling knowledge from BERT with a large collection of unlabelled texts and fine-tuning with task-specific instances via knowledge distillation again from the BERT fine-tuned on the same training examples. Through extensive experimentation, we show that the models trained with our method, named SpikeBERT, outperform state-of-the-art SNNs and even achieve comparable results to BERTs on text classification tasks for both English and Chinese with much less energy consumption. Our code is available at https://github.com/Lvchangze/SpikeBERT.",
      "paper_authors": [
        "Changze Lv",
        "Tianlong Li",
        "Jianhan Xu",
        "Chenxi Gu",
        "Zixuan Ling",
        "Cenyuan Zhang",
        "Xiaoqing Zheng",
        "Xuanjing Huang"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2023-08-29",
      "update_time": "2024-02-21",
      "comments": null,
      "repo_url": "https://github.com/Lvchangze/SpikeBERT"
    },
    "2308.13250": {
      "paper_id": "2308.13250v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.13250v3",
      "paper_key": "2308.13250",
      "paper_title": "TC-LIF: A Two-Compartment Spiking Neuron Model for Long-Term Sequential Modelling",
      "paper_url": "http://arxiv.org/abs/2308.13250v3",
      "paper_abstract": "The identification of sensory cues associated with potential opportunities and dangers is frequently complicated by unrelated events that separate useful cues by long delays. As a result, it remains a challenging task for state-of-the-art spiking neural networks (SNNs) to establish long-term temporal dependency between distant cues. To address this challenge, we propose a novel biologically inspired Two-Compartment Leaky Integrate-and-Fire spiking neuron model, dubbed TC-LIF. The proposed model incorporates carefully designed somatic and dendritic compartments that are tailored to facilitate learning long-term temporal dependencies. Furthermore, a theoretical analysis is provided to validate the effectiveness of TC-LIF in propagating error gradients over an extended temporal duration. Our experimental results, on a diverse range of temporal classification tasks, demonstrate superior temporal classification capability, rapid training convergence, and high energy efficiency of the proposed TC-LIF model. Therefore, this work opens up a myriad of opportunities for solving challenging temporal processing tasks on emerging neuromorphic computing systems. Our code is publicly available at https://github.com/ZhangShimin1/TC-LIF.",
      "paper_authors": [
        "Shimin Zhang",
        "Qu Yang",
        "Chenxiang Ma",
        "Jibin Wu",
        "Haizhou Li",
        "Kay Chen Tan"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-08-25",
      "update_time": "2024-02-17",
      "comments": "arXiv admin note: substantial text overlap with arXiv:2307.07231",
      "repo_url": "https://github.com/zhangshimin1/tc-lif"
    },
    "2308.12529": {
      "paper_id": "2308.12529v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.12529v1",
      "paper_key": "2308.12529",
      "paper_title": "Privacy-Preserving Discretized Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2308.12529v1",
      "paper_abstract": "The rapid development of artificial intelligence has brought considerable convenience, yet also introduces significant security risks. One of the research hotspots is to balance data privacy and utility in the real world of artificial intelligence. The present second-generation artificial neural networks have made tremendous advances, but some big models could have really high computational costs. The third-generation neural network, SNN (Spiking Neural Network), mimics real neurons by using discrete spike signals, whose sequences exhibit strong sparsity, providing advantages such as low energy consumption and high efficiency. In this paper, we construct a framework to evaluate the homomorphic computation of SNN named FHE-DiSNN that enables SNN to achieve good prediction performance on encrypted data. First, benefitting from the discrete nature of spike signals, our proposed model avoids the errors introduced by discretizing activation functions. Second, by applying bootstrapping, we design new private preserving functions FHE-Fire and FHE-Reset, through which noise can be refreshed, allowing us to evaluate SNN for an arbitrary number of operations. Furthermore, We improve the computational efficiency of FHE-DiSNN while maintaining a high level of accuracy. Finally, we evaluate our model on the MNIST dataset. The experiments show that FHE-DiSNN with 30 neurons in the hidden layer achieves a minimum prediction accuracy of 94.4%. Under optimal parameters, it achieves a 95.1% accuracy, with only a 0.6% decrease compared to the original SNN (95.7%). These results demonstrate the superiority of SNN over second-generation neural networks for homomorphic evaluation.",
      "paper_authors": [
        "Pengbo Li",
        "Ting Gao",
        "Huifang Huang",
        "Jiani Cheng",
        "Shuhong Gao",
        "Zhigang Zeng",
        "Jinqiao Duan"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2023-08-24",
      "update_time": "2023-08-24",
      "comments": null,
      "repo_url": "#"
    },
    "2310.06842": {
      "paper_id": "2310.06842v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.06842v1",
      "paper_key": "2310.06842",
      "paper_title": "Computational models of object motion detectors accelerated using FPGA technology",
      "paper_url": "http://arxiv.org/abs/2310.06842v1",
      "paper_abstract": "This PhD research introduces three key contributions in the domain of object motion detection:   Multi-Hierarchical Spiking Neural Network (MHSNN): A specialized four-layer Spiking Neural Network (SNN) architecture inspired by vertebrate retinas. Trained on custom lab-generated images, it exhibited 6.75% detection error for horizontal and vertical movements. While non-scalable, MHSNN laid the foundation for further advancements. Hybrid Sensitive Motion Detector (HSMD): Enhancing Dynamic Background Subtraction (DBS) using a tailored three-layer SNN, stabilizing foreground data to enhance object motion detection. Evaluated on standard datasets, HSMD outperformed OpenCV-based methods, excelling in four categories across eight metrics. It maintained real-time processing (13.82-13.92 fps) on a high-performance computer but showed room for hardware optimisation. Neuromorphic Hybrid Sensitive Motion Detector (NeuroHSMD): Building upon HSMD, this adaptation implemented the SNN component on dedicated hardware (FPGA). OpenCL simplified FPGA design and enabled portability. NeuroHSMD demonstrated an 82% speedup over HSMD, achieving 28.06-28.71 fps on CDnet2012 and CDnet2014 datasets.   These contributions collectively represent significant advancements in object motion detection, from a biologically inspired neural network design to an optimized hardware implementation that outperforms existing methods in accuracy and processing speed.",
      "paper_authors": [
        "Pedro Machado"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-08-23",
      "update_time": "2023-08-23",
      "comments": "PhD thesis",
      "repo_url": "#"
    },
    "2308.12063": {
      "paper_id": "2308.12063v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.12063v2",
      "paper_key": "2308.12063",
      "paper_title": "Learning the Plasticity: Plasticity-Driven Learning Framework in Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2308.12063v2",
      "paper_abstract": "The evolution of the human brain has led to the development of complex synaptic plasticity, enabling dynamic adaptation to a constantly evolving world. This progress inspires our exploration into a new paradigm for Spiking Neural Networks (SNNs): a Plasticity-Driven Learning Framework (PDLF). This paradigm diverges from traditional neural network models that primarily focus on direct training of synaptic weights, leading to static connections that limit adaptability in dynamic environments. Instead, our approach delves into the heart of synaptic behavior, prioritizing the learning of plasticity rules themselves. This shift in focus from weight adjustment to mastering the intricacies of synaptic change offers a more flexible and dynamic pathway for neural networks to evolve and adapt. Our PDLF does not merely adapt existing concepts of functional and Presynaptic-Dependent Plasticity but redefines them, aligning closely with the dynamic and adaptive nature of biological learning. This reorientation enhances key cognitive abilities in artificial intelligence systems, such as working memory and multitasking capabilities, and demonstrates superior adaptability in complex, real-world scenarios. Moreover, our framework sheds light on the intricate relationships between various forms of plasticity and cognitive functions, thereby contributing to a deeper understanding of the brain's learning mechanisms. Integrating this groundbreaking plasticity-centric approach in SNNs marks a significant advancement in the fusion of neuroscience and artificial intelligence. It paves the way for developing AI systems that not only learn but also adapt in an ever-changing world, much like the human brain.",
      "paper_authors": [
        "Guobin Shen",
        "Dongcheng Zhao",
        "Yiting Dong",
        "Yang Li",
        "Feifei Zhao",
        "Yi Zeng"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-08-23",
      "update_time": "2024-02-01",
      "comments": null,
      "repo_url": "#"
    },
    "2308.12053": {
      "paper_id": "2308.12053v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.12053v1",
      "paper_key": "2308.12053",
      "paper_title": "Layer-wise Feedback Propagation",
      "paper_url": "http://arxiv.org/abs/2308.12053v1",
      "paper_abstract": "In this paper, we present Layer-wise Feedback Propagation (LFP), a novel training approach for neural-network-like predictors that utilizes explainability, specifically Layer-wise Relevance Propagation(LRP), to assign rewards to individual connections based on their respective contributions to solving a given task. This differs from traditional gradient descent, which updates parameters towards anestimated loss minimum. LFP distributes a reward signal throughout the model without the need for gradient computations. It then strengthens structures that receive positive feedback while reducingthe influence of structures that receive negative feedback. We establish the convergence of LFP theoretically and empirically, and demonstrate its effectiveness in achieving comparable performance to gradient descent on various models and datasets. Notably, LFP overcomes certain limitations associated with gradient-based methods, such as reliance on meaningful derivatives. We further investigate how the different LRP-rules can be extended to LFP, what their effects are on training, as well as potential applications, such as training models with no meaningful derivatives, e.g., step-function activated Spiking Neural Networks (SNNs), or for transfer learning, to efficiently utilize existing knowledge.",
      "paper_authors": [
        "Leander Weber",
        "Jim Berend",
        "Alexander Binder",
        "Thomas Wiegand",
        "Wojciech Samek",
        "Sebastian Lapuschkin"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2023-08-23",
      "update_time": "2023-08-23",
      "comments": null,
      "repo_url": "#"
    },
    "2308.11152": {
      "paper_id": "2308.11152v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.11152v1",
      "paper_key": "2308.11152",
      "paper_title": "Energy-Efficient On-Board Radio Resource Management for Satellite Communications via Neuromorphic Computing",
      "paper_url": "http://arxiv.org/abs/2308.11152v1",
      "paper_abstract": "The latest satellite communication (SatCom) missions are characterized by a fully reconfigurable on-board software-defined payload, capable of adapting radio resources to the temporal and spatial variations of the system traffic. As pure optimization-based solutions have shown to be computationally tedious and to lack flexibility, machine learning (ML)-based methods have emerged as promising alternatives. We investigate the application of energy-efficient brain-inspired ML models for on-board radio resource management. Apart from software simulation, we report extensive experimental results leveraging the recently released Intel Loihi 2 chip. To benchmark the performance of the proposed model, we implement conventional convolutional neural networks (CNN) on a Xilinx Versal VCK5000, and provide a detailed comparison of accuracy, precision, recall, and energy efficiency for different traffic demands. Most notably, for relevant workloads, spiking neural networks (SNNs) implemented on Loihi 2 yield higher accuracy, while reducing power consumption by more than 100$\\times$ as compared to the CNN-based reference platform. Our findings point to the significant potential of neuromorphic computing and SNNs in supporting on-board SatCom operations, paving the way for enhanced efficiency and sustainability in future SatCom systems.",
      "paper_authors": [
        "Flor Ortiz",
        "Nicolas Skatchkovsky",
        "Eva Lagunas",
        "Wallace A. Martins",
        "Geoffrey Eappen",
        "Saed Daoud",
        "Osvaldo Simeone",
        "Bipin Rajendran",
        "Symeon Chatzinotas"
      ],
      "primary_category": "eess.SY",
      "publish_time": "2023-08-22",
      "update_time": "2023-08-22",
      "comments": "currently under review at IEEE Transactions on Machine Learning in\n  Communications and Networking",
      "repo_url": "#"
    },
    "2308.10873": {
      "paper_id": "2308.10873v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.10873v3",
      "paper_key": "2308.10873",
      "paper_title": "SpikingBERT: Distilling BERT to Train Spiking Language Models Using Implicit Differentiation",
      "paper_url": "http://arxiv.org/abs/2308.10873v3",
      "paper_abstract": "Large language Models (LLMs), though growing exceedingly powerful, comprises of orders of magnitude less neurons and synapses than the human brain. However, it requires significantly more power/energy to operate. In this work, we propose a novel bio-inspired spiking language model (LM) which aims to reduce the computational cost of conventional LMs by drawing motivation from the synaptic information flow in the brain. In this paper, we demonstrate a framework that leverages the average spiking rate of neurons at equilibrium to train a neuromorphic spiking LM using implicit differentiation technique, thereby overcoming the non-differentiability problem of spiking neural network (SNN) based algorithms without using any type of surrogate gradient. The steady-state convergence of the spiking neurons also allows us to design a spiking attention mechanism, which is critical in developing a scalable spiking LM. Moreover, the convergence of average spiking rate of neurons at equilibrium is utilized to develop a novel ANN-SNN knowledge distillation based technique wherein we use a pre-trained BERT model as \"teacher\" to train our \"student\" spiking architecture. While the primary architecture proposed in this paper is motivated by BERT, the technique can be potentially extended to different kinds of LLMs. Our work is the first one to demonstrate the performance of an operational spiking LM architecture on multiple different tasks in the GLUE benchmark.",
      "paper_authors": [
        "Malyaban Bal",
        "Abhronil Sengupta"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-08-21",
      "update_time": "2024-02-18",
      "comments": "Accepted at AAAI 2024",
      "repo_url": "https://github.com/neurocomplab-psu/spikingbert"
    },
    "2308.10373": {
      "paper_id": "2308.10373v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.10373v3",
      "paper_key": "2308.10373",
      "paper_title": "HoSNN: Adversarially-Robust Homeostatic Spiking Neural Networks with Adaptive Firing Thresholds",
      "paper_url": "http://arxiv.org/abs/2308.10373v3",
      "paper_abstract": "While spiking neural networks (SNNs) offer a promising neurally-inspired model of computation, they are vulnerable to adversarial attacks. We present the first study that draws inspiration from neural homeostasis to design a threshold-adapting leaky integrate-and-fire (TA-LIF) neuron model and utilize TA-LIF neurons to construct the adversarially robust homeostatic SNNs (HoSNNs) for improved robustness. The TA-LIF model incorporates a self-stabilizing dynamic thresholding mechanism, offering a local feedback control solution to the minimization of each neuron's membrane potential error caused by adversarial disturbance. Theoretical analysis demonstrates favorable dynamic properties of TA-LIF neurons in terms of the bounded-input bounded-output stability and suppressed time growth of membrane potential error, underscoring their superior robustness compared with the standard LIF neurons. When trained with weak FGSM attacks (attack budget = 2/255) and tested with much stronger PGD attacks (attack budget = 8/255), our HoSNNs significantly improve model accuracy on several datasets: from 30.54% to 74.91% on FashionMNIST, from 0.44% to 35.06% on SVHN, from 0.56% to 42.63% on CIFAR10, from 0.04% to 16.66% on CIFAR100, over the conventional LIF-based SNNs.",
      "paper_authors": [
        "Hejia Geng",
        "Peng Li"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-08-20",
      "update_time": "2024-05-31",
      "comments": null,
      "repo_url": "https://github.com/stonezwr/TSSL-BP"
    },
    "2308.10187": {
      "paper_id": "2308.10187v4",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.10187v4",
      "paper_key": "2308.10187",
      "paper_title": "Spiking-Diffusion: Vector Quantized Discrete Diffusion Model with Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2308.10187v4",
      "paper_abstract": "Spiking neural networks (SNNs) have tremendous potential for energy-efficient neuromorphic chips due to their binary and event-driven architecture. SNNs have been primarily used in classification tasks, but limited exploration on image generation tasks. To fill the gap, we propose a Spiking-Diffusion model, which is based on the vector quantized discrete diffusion model. First, we develop a vector quantized variational autoencoder with SNNs (VQ-SVAE) to learn a discrete latent space for images. In VQ-SVAE, image features are encoded using both the spike firing rate and postsynaptic potential, and an adaptive spike generator is designed to restore embedding features in the form of spike trains. Next, we perform absorbing state diffusion in the discrete latent space and construct a spiking diffusion image decoder (SDID) with SNNs to denoise the image. Our work is the first to build the diffusion model entirely from SNN layers. Experimental results on MNIST, FMNIST, KMNIST, Letters, and Cifar10 demonstrate that Spiking-Diffusion outperforms the existing SNN-based generation model. We achieve FIDs of 37.50, 91.98, 59.23, 67.41, and 120.5 on the above datasets respectively, with reductions of 58.60\\%, 18.75\\%, 64.51\\%, 29.75\\%, and 44.88\\% in FIDs compared with the state-of-art work. Our code will be available at \\url{https://github.com/Arktis2022/Spiking-Diffusion}.",
      "paper_authors": [
        "Mingxuan Liu",
        "Jie Gan",
        "Rui Wen",
        "Tao Li",
        "Yongli Chen",
        "Hong Chen"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-08-20",
      "update_time": "2023-09-22",
      "comments": "Under Review",
      "repo_url": "https://github.com/Arktis2022/Spiking-Diffusion"
    },
    "2308.09455": {
      "paper_id": "2308.09455v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.09455v1",
      "paper_key": "2308.09455",
      "paper_title": "Artificial-Spiking Hierarchical Networks for Vision-Language Representation Learning",
      "paper_url": "http://arxiv.org/abs/2308.09455v1",
      "paper_abstract": "With the success of self-supervised learning, multimodal foundation models have rapidly adapted a wide range of downstream tasks driven by vision and language (VL) pretraining. State-of-the-art methods achieve impressive performance by pre-training on large-scale datasets. However, bridging the semantic gap between the two modalities remains a nonnegligible challenge for VL tasks. In this work, we propose an efficient computation framework for multimodal alignment by introducing a novel visual semantic module to further improve the performance of the VL tasks. Specifically, we propose a flexible model, namely Artificial-Spiking Hierarchical Networks (ASH-Nets), which combines the complementary advantages of Artificial neural networks (ANNs) and Spiking neural networks (SNNs) to enrich visual semantic representations. In particular, a visual concrete encoder and a semantic abstract encoder are constructed to learn continuous and discrete latent variables to enhance the flexibility of semantic encoding. Considering the spatio-temporal properties of SNNs modeling, we introduce a contrastive learning method to optimize the inputs of similar samples. This can improve the computational efficiency of the hierarchical network, while the augmentation of hard samples is beneficial to the learning of visual representations. Furthermore, the Spiking to Text Uni-Alignment Learning (STUA) pre-training method is proposed, which only relies on text features to enhance the encoding ability of abstract semantics. We validate the performance on multiple well-established downstream VL tasks. Experiments show that the proposed ASH-Nets achieve competitive results.",
      "paper_authors": [
        "Yeming Chen",
        "Siyu Zhang",
        "Yaoru Sun",
        "Weijian Liang",
        "Haoran Wang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-08-18",
      "update_time": "2023-08-18",
      "comments": null,
      "repo_url": "#"
    },
    "2308.09049": {
      "paper_id": "2308.09049v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.09049v1",
      "paper_key": "2308.09049",
      "paper_title": "Development of an interface for digital neuromorphic hardware based on an FPGA",
      "paper_url": "http://arxiv.org/abs/2308.09049v1",
      "paper_abstract": "Exploring and understanding the functioning of the human brain is one of the greatest challenges for current research. Neuromorphic engineering tries to address this challenge by abstracting biological mechanisms and translating them into technology. Via the abstraction process and experiments with the resulting technical system, an attempt is made to obtain information about the biological counterpart. One subsection of Neuromorphic Engineering (NE) are Spiking Neural Networks (SNN), which describe the structures of the human brain more and more closely than Artificial Neural Networks (ANN). Together with their dedicated hardware, SNNs provide a good platform for developing new algorithms for information processing. In the context of these neuromorphic hardware platforms, this paper aims to develop an interface for a digital hardware platform (SPINN-3 Development Board) to enable the use of industrial or conventional sensors and thus create new approaches for experimental research. The basis for this endeavor is a Field Programmable Gate Array (FPGA), which is placed as a gateway between the sensors and the neuromorphic hardware. Overall, the developed system provides a robust solution for a wide variety of investigations related to neuromorphic hardware and SNNs. Furthermore, the solution also offers suitable possibilities to monitor all processes within the system in order to obtain suitable measurements, which can be examined in search of meaningful results.",
      "paper_authors": [
        "Ren\u00e9 Harmann",
        "Lukas Sohlbach",
        "Fernando Perez-Pe\u00f1a",
        "Karsten Schmidt"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2023-08-17",
      "update_time": "2023-08-17",
      "comments": "Accepted for publication with Proceedings of the Unified Conference\n  of DAMAS, InCoME and TEPEN Conferences (UNIfied 2023), Springer Nature",
      "repo_url": "#"
    },
    "2308.08649": {
      "paper_id": "2308.08649v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.08649v1",
      "paper_key": "2308.08649",
      "paper_title": "Towards Zero Memory Footprint Spiking Neural Network Training",
      "paper_url": "http://arxiv.org/abs/2308.08649v1",
      "paper_abstract": "Biologically-inspired Spiking Neural Networks (SNNs), processing information using discrete-time events known as spikes rather than continuous values, have garnered significant attention due to their hardware-friendly and energy-efficient characteristics. However, the training of SNNs necessitates a considerably large memory footprint, given the additional storage requirements for spikes or events, leading to a complex structure and dynamic setup. In this paper, to address memory constraint in SNN training, we introduce an innovative framework, characterized by a remarkably low memory footprint. We \\textbf{(i)} design a reversible SNN node that retains a high level of accuracy. Our design is able to achieve a $\\mathbf{58.65\\times}$ reduction in memory usage compared to the current SNN node. We \\textbf{(ii)} propose a unique algorithm to streamline the backpropagation process of our reversible SNN node. This significantly trims the backward Floating Point Operations Per Second (FLOPs), thereby accelerating the training process in comparison to current reversible layer backpropagation method. By using our algorithm, the training time is able to be curtailed by $\\mathbf{23.8\\%}$ relative to existing reversible layer architectures.",
      "paper_authors": [
        "Bin Lei",
        "Sheng Lin",
        "Pei-Hung Lin",
        "Chunhua Liao",
        "Caiwen Ding"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-08-16",
      "update_time": "2023-08-16",
      "comments": null,
      "repo_url": "#"
    },
    "2308.08359": {
      "paper_id": "2308.08359v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.08359v1",
      "paper_key": "2308.08359",
      "paper_title": "Membrane Potential Batch Normalization for Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2308.08359v1",
      "paper_abstract": "As one of the energy-efficient alternatives of conventional neural networks (CNNs), spiking neural networks (SNNs) have gained more and more interest recently. To train the deep models, some effective batch normalization (BN) techniques are proposed in SNNs. All these BNs are suggested to be used after the convolution layer as usually doing in CNNs. However, the spiking neuron is much more complex with the spatio-temporal dynamics. The regulated data flow after the BN layer will be disturbed again by the membrane potential updating operation before the firing function, i.e., the nonlinear activation. Therefore, we advocate adding another BN layer before the firing function to normalize the membrane potential again, called MPBN. To eliminate the induced time cost of MPBN, we also propose a training-inference-decoupled re-parameterization technique to fold the trained MPBN into the firing threshold. With the re-parameterization technique, the MPBN will not introduce any extra time burden in the inference. Furthermore, the MPBN can also adopt the element-wised form, while these BNs after the convolution layer can only use the channel-wised form. Experimental results show that the proposed MPBN performs well on both popular non-spiking static and neuromorphic datasets. Our code is open-sourced at \\href{https://github.com/yfguo91/MPBN}{MPBN}.",
      "paper_authors": [
        "Yufei Guo",
        "Yuhan Zhang",
        "Yuanpei Chen",
        "Weihang Peng",
        "Xiaode Liu",
        "Liwen Zhang",
        "Xuhui Huang",
        "Zhe Ma"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-08-16",
      "update_time": "2023-08-16",
      "comments": "Accepted by ICCV2023",
      "repo_url": "https://github.com/yfguo91/mpbn"
    },
    "2308.08227": {
      "paper_id": "2308.08227v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.08227v1",
      "paper_key": "2308.08227",
      "paper_title": "Inherent Redundancy in Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2308.08227v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) are well known as a promising energy-efficient alternative to conventional artificial neural networks. Subject to the preconceived impression that SNNs are sparse firing, the analysis and optimization of inherent redundancy in SNNs have been largely overlooked, thus the potential advantages of spike-based neuromorphic computing in accuracy and energy efficiency are interfered. In this work, we pose and focus on three key questions regarding the inherent redundancy in SNNs. We argue that the redundancy is induced by the spatio-temporal invariance of SNNs, which enhances the efficiency of parameter utilization but also invites lots of noise spikes. Further, we analyze the effect of spatio-temporal invariance on the spatio-temporal dynamics and spike firing of SNNs. Then, motivated by these analyses, we propose an Advance Spatial Attention (ASA) module to harness SNNs' redundancy, which can adaptively optimize their membrane potential distribution by a pair of individual spatial attention sub-modules. In this way, noise spike features are accurately regulated. Experimental results demonstrate that the proposed method can significantly drop the spike firing with better performance than state-of-the-art SNN baselines. Our code is available in \\url{https://github.com/BICLab/ASA-SNN}.",
      "paper_authors": [
        "Man Yao",
        "Jiakui Hu",
        "Guangshe Zhao",
        "Yaoyuan Wang",
        "Ziyang Zhang",
        "Bo Xu",
        "Guoqi Li"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-08-16",
      "update_time": "2023-08-16",
      "comments": "Accepted by ICCV2023",
      "repo_url": "https://github.com/biclab/asa-snn"
    },
    "2308.08222": {
      "paper_id": "2308.08222v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.08222v2",
      "paper_key": "2308.08222",
      "paper_title": "HyperSNN: A new efficient and robust deep learning model for resource constrained control applications",
      "paper_url": "http://arxiv.org/abs/2308.08222v2",
      "paper_abstract": "In light of the increasing adoption of edge computing in areas such as intelligent furniture, robotics, and smart homes, this paper introduces HyperSNN, an innovative method for control tasks that uses spiking neural networks (SNNs) in combination with hyperdimensional computing. HyperSNN substitutes expensive 32-bit floating point multiplications with 8-bit integer additions, resulting in reduced energy consumption while enhancing robustness and potentially improving accuracy. Our model was tested on AI Gym benchmarks, including Cartpole, Acrobot, MountainCar, and Lunar Lander. HyperSNN achieves control accuracies that are on par with conventional machine learning methods but with only 1.36% to 9.96% of the energy expenditure. Furthermore, our experiments showed increased robustness when using HyperSNN. We believe that HyperSNN is especially suitable for interactive, mobile, and wearable devices, promoting energy-efficient and robust system design. Furthermore, it paves the way for the practical implementation of complex algorithms like model predictive control (MPC) in real-world industrial scenarios.",
      "paper_authors": [
        "Zhanglu Yan",
        "Shida Wang",
        "Kaiwen Tang",
        "Weng-Fai Wong"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2023-08-16",
      "update_time": "2023-08-17",
      "comments": null,
      "repo_url": "#"
    },
    "2308.08218": {
      "paper_id": "2308.08218v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.08218v2",
      "paper_key": "2308.08218",
      "paper_title": "Expressivity of Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2308.08218v2",
      "paper_abstract": "The synergy between spiking neural networks and neuromorphic hardware holds promise for the development of energy-efficient AI applications. Inspired by this potential, we revisit the foundational aspects to study the capabilities of spiking neural networks where information is encoded in the firing time of neurons. Under the Spike Response Model as a mathematical model of a spiking neuron with a linear response function, we compare the expressive power of artificial and spiking neural networks, where we initially show that they realize piecewise linear mappings. In contrast to ReLU networks, we prove that spiking neural networks can realize both continuous and discontinuous functions. Moreover, we provide complexity bounds on the size of spiking neural networks to emulate multi-layer (ReLU) neural networks. Restricting to the continuous setting, we also establish complexity bounds in the reverse direction for one-layer spiking neural networks.",
      "paper_authors": [
        "Manjot Singh",
        "Adalbert Fono",
        "Gitta Kutyniok"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-08-16",
      "update_time": "2024-03-15",
      "comments": null,
      "repo_url": "#"
    },
    "2308.06787": {
      "paper_id": "2308.06787v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.06787v1",
      "paper_key": "2308.06787",
      "paper_title": "RMP-Loss: Regularizing Membrane Potential Distribution for Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2308.06787v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) as one of the biology-inspired models have received much attention recently. It can significantly reduce energy consumption since they quantize the real-valued membrane potentials to 0/1 spikes to transmit information thus the multiplications of activations and weights can be replaced by additions when implemented on hardware. However, this quantization mechanism will inevitably introduce quantization error, thus causing catastrophic information loss. To address the quantization error problem, we propose a regularizing membrane potential loss (RMP-Loss) to adjust the distribution which is directly related to quantization error to a range close to the spikes. Our method is extremely simple to implement and straightforward to train an SNN. Furthermore, it is shown to consistently outperform previous state-of-the-art methods over different network architectures and datasets.",
      "paper_authors": [
        "Yufei Guo",
        "Xiaode Liu",
        "Yuanpei Chen",
        "Liwen Zhang",
        "Weihang Peng",
        "Yuhan Zhang",
        "Xuhui Huang",
        "Zhe Ma"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-08-13",
      "update_time": "2023-08-13",
      "comments": "Accepted by ICCV2023",
      "repo_url": "#"
    },
    "2308.06582": {
      "paper_id": "2308.06582v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.06582v2",
      "paper_key": "2308.06582",
      "paper_title": "Gated Attention Coding for Training High-performance and Efficient Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2308.06582v2",
      "paper_abstract": "Spiking neural networks (SNNs) are emerging as an energy-efficient alternative to traditional artificial neural networks (ANNs) due to their unique spike-based event-driven nature. Coding is crucial in SNNs as it converts external input stimuli into spatio-temporal feature sequences. However, most existing deep SNNs rely on direct coding that generates powerless spike representation and lacks the temporal dynamics inherent in human vision. Hence, we introduce Gated Attention Coding (GAC), a plug-and-play module that leverages the multi-dimensional gated attention unit to efficiently encode inputs into powerful representations before feeding them into the SNN architecture. GAC functions as a preprocessing layer that does not disrupt the spike-driven nature of the SNN, making it amenable to efficient neuromorphic hardware implementation with minimal modifications. Through an observer model theoretical analysis, we demonstrate GAC's attention mechanism improves temporal dynamics and coding efficiency. Experiments on CIFAR10/100 and ImageNet datasets demonstrate that GAC achieves state-of-the-art accuracy with remarkable efficiency. Notably, we improve top-1 accuracy by 3.10\\% on CIFAR100 with only 6-time steps and 1.07\\% on ImageNet while reducing energy usage to 66.9\\% of the previous works. To our best knowledge, it is the first time to explore the attention-based dynamic coding scheme in deep SNNs, with exceptional effectiveness and efficiency on large-scale datasets.The Code is available at https://github.com/bollossom/GAC.",
      "paper_authors": [
        "Xuerui Qiu",
        "Rui-Jie Zhu",
        "Yuhong Chou",
        "Zhaorui Wang",
        "Liang-jian Deng",
        "Guoqi Li"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-08-12",
      "update_time": "2024-06-04",
      "comments": "Accepted by Proceedings of the AAAI Conference on Artificial\n  Intelligence 38 (AAAI 24)",
      "repo_url": "https://github.com/bollossom/GAC"
    },
    "2308.05636": {
      "paper_id": "2308.05636v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.05636v2",
      "paper_key": "2308.05636",
      "paper_title": "A Homomorphic Encryption Framework for Privacy-Preserving Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2308.05636v2",
      "paper_abstract": "Machine learning (ML) is widely used today, especially through deep neural networks (DNNs), however, increasing computational load and resource requirements have led to cloud-based solutions. To address this problem, a new generation of networks called Spiking Neural Networks (SNN) has emerged, which mimic the behavior of the human brain to improve efficiency and reduce energy consumption. These networks often process large amounts of sensitive information, such as confidential data, and thus privacy issues arise. Homomorphic encryption (HE) offers a solution, allowing calculations to be performed on encrypted data without decrypting it. This research compares traditional DNNs and SNNs using the Brakerski/Fan-Vercauteren (BFV) encryption scheme. The LeNet-5 model, a widely-used convolutional architecture, is used for both DNN and SNN models based on the LeNet-5 architecture, and the networks are trained and compared using the FashionMNIST dataset. The results show that SNNs using HE achieve up to 40% higher accuracy than DNNs for low values of the plaintext modulus t, although their execution time is longer due to their time-coding nature with multiple time-steps.",
      "paper_authors": [
        "Farzad Nikfam",
        "Raffaele Casaburi",
        "Alberto Marchisio",
        "Maurizio Martina",
        "Muhammad Shafique"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2023-08-10",
      "update_time": "2023-10-12",
      "comments": null,
      "repo_url": "#"
    },
    "2308.04749": {
      "paper_id": "2308.04749v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.04749v1",
      "paper_key": "2308.04749",
      "paper_title": "Enhancing Efficient Continual Learning with Dynamic Structure Development of Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2308.04749v1",
      "paper_abstract": "Children possess the ability to learn multiple cognitive tasks sequentially, which is a major challenge toward the long-term goal of artificial general intelligence. Existing continual learning frameworks are usually applicable to Deep Neural Networks (DNNs) and lack the exploration on more brain-inspired, energy-efficient Spiking Neural Networks (SNNs). Drawing on continual learning mechanisms during child growth and development, we propose Dynamic Structure Development of Spiking Neural Networks (DSD-SNN) for efficient and adaptive continual learning. When learning a sequence of tasks, the DSD-SNN dynamically assigns and grows new neurons to new tasks and prunes redundant neurons, thereby increasing memory capacity and reducing computational overhead. In addition, the overlapping shared structure helps to quickly leverage all acquired knowledge to new tasks, empowering a single network capable of supporting multiple incremental tasks (without the separate sub-network mask for each task). We validate the effectiveness of the proposed model on multiple class incremental learning and task incremental learning benchmarks. Extensive experiments demonstrated that our model could significantly improve performance, learning speed and memory capacity, and reduce computational overhead. Besides, our DSD-SNN model achieves comparable performance with the DNNs-based methods, and significantly outperforms the state-of-the-art (SOTA) performance for existing SNNs-based continual learning methods.",
      "paper_authors": [
        "Bing Han",
        "Feifei Zhao",
        "Yi Zeng",
        "Wenxuan Pan",
        "Guobin Shen"
      ],
      "primary_category": "cs.AI",
      "publish_time": "2023-08-09",
      "update_time": "2023-08-09",
      "comments": null,
      "repo_url": "https://github.com/braincog-x/brain-cog"
    },
    "2308.04672": {
      "paper_id": "2308.04672v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.04672v1",
      "paper_key": "2308.04672",
      "paper_title": "Resource Constrained Model Compression via Minimax Optimization for Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2308.04672v1",
      "paper_abstract": "Brain-inspired Spiking Neural Networks (SNNs) have the characteristics of event-driven and high energy-efficient, which are different from traditional Artificial Neural Networks (ANNs) when deployed on edge devices such as neuromorphic chips. Most previous work focuses on SNNs training strategies to improve model performance and brings larger and deeper network architectures. It is difficult to deploy these complex networks on resource-limited edge devices directly. To meet such demand, people compress SNNs very cautiously to balance the performance and the computation efficiency. Existing compression methods either iteratively pruned SNNs using weights norm magnitude or formulated the problem as a sparse learning optimization. We propose an improved end-to-end Minimax optimization method for this sparse learning problem to better balance the model performance and the computation efficiency. We also demonstrate that jointly applying compression and finetuning on SNNs is better than sequentially, especially for extreme compression ratios. The compressed SNN models achieved state-of-the-art (SOTA) performance on various benchmark datasets and architectures. Our code is available at https://github.com/chenjallen/Resource-Constrained-Compression-on-SNN.",
      "paper_authors": [
        "Jue Chen",
        "Huan Yuan",
        "Jianchao Tan",
        "Bin Chen",
        "Chengru Song",
        "Di Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-08-09",
      "update_time": "2023-08-09",
      "comments": "ACM MM 2023",
      "repo_url": "https://github.com/chenjallen/resource-constrained-compression-on-snn"
    },
    "2308.04369": {
      "paper_id": "2308.04369v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.04369v2",
      "paper_key": "2308.04369",
      "paper_title": "SSTFormer: Bridging Spiking Neural Network and Memory Support Transformer for Frame-Event based Recognition",
      "paper_url": "http://arxiv.org/abs/2308.04369v2",
      "paper_abstract": "Event camera-based pattern recognition is a newly arising research topic in recent years. Current researchers usually transform the event streams into images, graphs, or voxels, and adopt deep neural networks for event-based classification. Although good performance can be achieved on simple event recognition datasets, however, their results may be still limited due to the following two issues. Firstly, they adopt spatial sparse event streams for recognition only, which may fail to capture the color and detailed texture information well. Secondly, they adopt either Spiking Neural Networks (SNN) for energy-efficient recognition with suboptimal results, or Artificial Neural Networks (ANN) for energy-intensive, high-performance recognition. However, seldom of them consider achieving a balance between these two aspects. In this paper, we formally propose to recognize patterns by fusing RGB frames and event streams simultaneously and propose a new RGB frame-event recognition framework to address the aforementioned issues. The proposed method contains four main modules, i.e., memory support Transformer network for RGB frame encoding, spiking neural network for raw event stream encoding, multi-modal bottleneck fusion module for RGB-Event feature aggregation, and prediction head. Due to the scarce of RGB-Event based classification dataset, we also propose a large-scale PokerEvent dataset which contains 114 classes, and 27102 frame-event pairs recorded using a DVS346 event camera. Extensive experiments on two RGB-Event based classification datasets fully validated the effectiveness of our proposed framework. We hope this work will boost the development of pattern recognition by fusing RGB frames and event streams. Both our dataset and source code of this work will be released at https://github.com/Event-AHU/SSTFormer.",
      "paper_authors": [
        "Xiao Wang",
        "Zongzhen Wu",
        "Yao Rong",
        "Lin Zhu",
        "Bo Jiang",
        "Jin Tang",
        "Yonghong Tian"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-08-08",
      "update_time": "2024-02-05",
      "comments": "In Peer Review",
      "repo_url": "https://github.com/event-ahu/sstformer"
    },
    "2308.04171": {
      "paper_id": "2308.04171v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.04171v1",
      "paper_key": "2308.04171",
      "paper_title": "Core interface optimization for multi-core neuromorphic processors",
      "paper_url": "http://arxiv.org/abs/2308.04171v1",
      "paper_abstract": "Hardware implementations of Spiking Neural Networks (SNNs) represent a promising approach to edge-computing for applications that require low-power and low-latency, and which cannot resort to external cloud-based computing services. However, most solutions proposed so far either support only relatively small networks, or take up significant hardware resources, to implement large networks. To realize large-scale and scalable SNNs it is necessary to develop an efficient asynchronous communication and routing fabric that enables the design of multi-core architectures. In particular the core interface that manages inter-core spike communication is a crucial component as it represents the bottleneck of Power-Performance-Area (PPA) especially for the arbitration architecture and the routing memory. In this paper we present an arbitration mechanism with the corresponding asynchronous encoding pipeline circuits, based on hierarchical arbiter trees. The proposed scheme reduces the latency by more than 70% in sparse-event mode, compared to the state-of-the-art arbitration architectures, with lower area cost. The routing memory makes use of asynchronous Content Addressable Memory (CAM) with Current Sensing Completion Detection (CSCD), which saves approximately 46% energy, and achieves a 40% increase in throughput against conventional asynchronous CAM using configurable delay lines, at the cost of only a slight increase in area. In addition as it radically reduces the core interface resources in multi-core neuromorphic processors, the arbitration architecture and CAM architecture we propose can be also applied to a wide range of general asynchronous circuits and systems.",
      "paper_authors": [
        "Zhe Su",
        "Hyunjung Hwang",
        "Tristan Torchet",
        "Giacomo Indiveri"
      ],
      "primary_category": "cs.AR",
      "publish_time": "2023-08-08",
      "update_time": "2023-08-08",
      "comments": null,
      "repo_url": "#"
    },
    "2308.02194": {
      "paper_id": "2308.02194v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.02194v2",
      "paper_key": "2308.02194",
      "paper_title": "Paired Competing Neurons Improving STDP Supervised Local Learning In Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2308.02194v2",
      "paper_abstract": "Direct training of Spiking Neural Networks (SNNs) on neuromorphic hardware has the potential to significantly reduce the energy consumption of artificial neural network training. SNNs trained with Spike Timing-Dependent Plasticity (STDP) benefit from gradient-free and unsupervised local learning, which can be easily implemented on ultra-low-power neuromorphic hardware. However, classification tasks cannot be performed solely with unsupervised STDP. In this paper, we propose Stabilized Supervised STDP (S2-STDP), a supervised STDP learning rule to train the classification layer of an SNN equipped with unsupervised STDP for feature extraction. S2-STDP integrates error-modulated weight updates that align neuron spikes with desired timestamps derived from the average firing time within the layer. Then, we introduce a training architecture called Paired Competing Neurons (PCN) to further enhance the learning capabilities of our classification layer trained with S2-STDP. PCN associates each class with paired neurons and encourages neuron specialization toward target or non-target samples through intra-class competition. We evaluate our methods on image recognition datasets, including MNIST, Fashion-MNIST, and CIFAR-10. Results show that our methods outperform state-of-the-art supervised STDP learning rules, for comparable architectures and numbers of neurons. Further analysis demonstrates that the use of PCN enhances the performance of S2-STDP, regardless of the hyperparameter set and without introducing any additional hyperparameters.",
      "paper_authors": [
        "Gaspard Goupy",
        "Pierre Tirilly",
        "Ioan Marius Bilasco"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-08-04",
      "update_time": "2024-04-27",
      "comments": null,
      "repo_url": "#"
    },
    "2308.01241": {
      "paper_id": "2308.01241v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.01241v1",
      "paper_key": "2308.01241",
      "paper_title": "Digital Twin Brain: a simulation and assimilation platform for whole human brain",
      "paper_url": "http://arxiv.org/abs/2308.01241v1",
      "paper_abstract": "In this work, we present a computing platform named digital twin brain (DTB) that can simulate spiking neuronal networks of the whole human brain scale and more importantly, a personalized biological brain structure. In comparison to most brain simulations with a homogeneous global structure, we highlight that the sparseness, couplingness and heterogeneity in the sMRI, DTI and PET data of the brain has an essential impact on the efficiency of brain simulation, which is proved from the scaling experiments that the DTB of human brain simulation is communication-intensive and memory-access intensive computing systems rather than computation-intensive. We utilize a number of optimization techniques to balance and integrate the computation loads and communication traffics from the heterogeneous biological structure to the general GPU-based HPC and achieve leading simulation performance for the whole human brain-scaled spiking neuronal networks. On the other hand, the biological structure, equipped with a mesoscopic data assimilation, enables the DTB to investigate brain cognitive function by a reverse-engineering method, which is demonstrated by a digital experiment of visual evaluation on the DTB. Furthermore, we believe that the developing DTB will be a promising powerful platform for a large of research orients including brain-inspiredintelligence, rain disease medicine and brain-machine interface.",
      "paper_authors": [
        "Wenlian Lu",
        "Longbin Zeng",
        "Xin Du",
        "Wenyong Zhang",
        "Shitong Xiang",
        "Huarui Wang",
        "Jiexiang Wang",
        "Mingda Ji",
        "Yubo Hou",
        "Minglong Wang",
        "Yuhao Liu",
        "Zhongyu Chen",
        "Qibao Zheng",
        "Ningsheng Xu",
        "Jianfeng Feng"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-08-02",
      "update_time": "2023-08-02",
      "comments": "12 pages, 11 figures",
      "repo_url": "#"
    },
    "2308.02557": {
      "paper_id": "2308.02557v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.02557v2",
      "paper_key": "2308.02557",
      "paper_title": "Attention-free Spikformer: Mixing Spike Sequences with Simple Linear Transforms",
      "paper_url": "http://arxiv.org/abs/2308.02557v2",
      "paper_abstract": "By integrating the self-attention capability and the biological properties of Spiking Neural Networks (SNNs), Spikformer applies the flourishing Transformer architecture to SNNs design. It introduces a Spiking Self-Attention (SSA) module to mix sparse visual features using spike-form Query, Key, and Value, resulting in the State-Of-The-Art (SOTA) performance on numerous datasets compared to previous SNN-like frameworks. In this paper, we demonstrate that the Spikformer architecture can be accelerated by replacing the SSA with an unparameterized Linear Transform (LT) such as Fourier and Wavelet transforms. These transforms are utilized to mix spike sequences, reducing the quadratic time complexity to log-linear time complexity. They alternate between the frequency and time domains to extract sparse visual features, showcasing powerful performance and efficiency. We conduct extensive experiments on image classification using both neuromorphic and static datasets. The results indicate that compared to the SOTA Spikformer with SSA, Spikformer with LT achieves higher Top-1 accuracy on neuromorphic datasets (i.e., CIFAR10-DVS and DVS128 Gesture) and comparable Top-1 accuracy on static datasets (i.e., CIFAR-10 and CIFAR-100). Furthermore, Spikformer with LT achieves approximately 29-51% improvement in training speed, 61-70% improvement in inference speed, and reduces memory usage by 4-26% due to not requiring learnable parameters.",
      "paper_authors": [
        "Qingyu Wang",
        "Duzhen Zhang",
        "Tielin Zhang",
        "Bo Xu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-08-02",
      "update_time": "2023-08-17",
      "comments": "Under Review",
      "repo_url": "#"
    },
    "2308.00787": {
      "paper_id": "2308.00787v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.00787v1",
      "paper_key": "2308.00787",
      "paper_title": "Evaluating Spiking Neural Network On Neuromorphic Platform For Human Activity Recognition",
      "paper_url": "http://arxiv.org/abs/2308.00787v1",
      "paper_abstract": "Energy efficiency and low latency are crucial requirements for designing wearable AI-empowered human activity recognition systems, due to the hard constraints of battery operations and closed-loop feedback. While neural network models have been extensively compressed to match the stringent edge requirements, spiking neural networks and event-based sensing are recently emerging as promising solutions to further improve performance due to their inherent energy efficiency and capacity to process spatiotemporal data in very low latency. This work aims to evaluate the effectiveness of spiking neural networks on neuromorphic processors in human activity recognition for wearable applications. The case of workout recognition with wrist-worn wearable motion sensors is used as a study. A multi-threshold delta modulation approach is utilized for encoding the input sensor data into spike trains to move the pipeline into the event-based approach. The spikes trains are then fed to a spiking neural network with direct-event training, and the trained model is deployed on the research neuromorphic platform from Intel, Loihi, to evaluate energy and latency efficiency. Test results show that the spike-based workouts recognition system can achieve a comparable accuracy (87.5\\%) comparable to the popular milliwatt RISC-V bases multi-core processor GAP8 with a traditional neural network ( 88.1\\%) while achieving two times better energy-delay product (0.66 \\si{\\micro\\joule\\second} vs. 1.32 \\si{\\micro\\joule\\second}).",
      "paper_authors": [
        "Sizhen Bian",
        "Michele Magno"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-08-01",
      "update_time": "2023-08-01",
      "comments": null,
      "repo_url": "https://github.com/zhaxidele/har-with-snn"
    },
    "2308.00558": {
      "paper_id": "2308.00558v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.00558v1",
      "paper_key": "2308.00558",
      "paper_title": "Gradient Scaling on Deep Spiking Neural Networks with Spike-Dependent Local Information",
      "paper_url": "http://arxiv.org/abs/2308.00558v1",
      "paper_abstract": "Deep spiking neural networks (SNNs) are promising neural networks for their model capacity from deep neural network architecture and energy efficiency from SNNs' operations. To train deep SNNs, recently, spatio-temporal backpropagation (STBP) with surrogate gradient was proposed. Although deep SNNs have been successfully trained with STBP, they cannot fully utilize spike information. In this work, we proposed gradient scaling with local spike information, which is the relation between pre- and post-synaptic spikes. Considering the causality between spikes, we could enhance the training performance of deep SNNs. According to our experiments, we could achieve higher accuracy with lower spikes by adopting the gradient scaling on image classification tasks, such as CIFAR10 and CIFAR100.",
      "paper_authors": [
        "Seongsik Park",
        "Jeonghee Jo",
        "Jongkil Park",
        "Yeonjoo Jeong",
        "Jaewook Kim",
        "Suyoun Lee",
        "Joon Young Kwak",
        "Inho Kim",
        "Jong-Keuk Park",
        "Kyeong Seok Lee",
        "Gye Weon Hwang",
        "Hyun Jae Jang"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-08-01",
      "update_time": "2023-08-01",
      "comments": "ICML-23 Localized Learning Workshop: Decentralized Model Updates via\n  Non-Global Objectives",
      "repo_url": "#"
    },
    "2308.00421": {
      "paper_id": "2308.00421v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.00421v3",
      "paper_key": "2308.00421",
      "paper_title": "Effect of Synaptic Heterogeneity on Neuronal Coordination",
      "paper_url": "http://arxiv.org/abs/2308.00421v3",
      "paper_abstract": "Recent advancements in measurement techniques have resulted in an increasing amount of data on neural activities recorded in parallel, revealing largely heterogeneous correlation patterns across neurons. Yet, the mechanistic origin of this heterogeneity is largely unknown because existing theoretical approaches linking structure and dynamics in neural circuits are restricted to population-averaged connectivity and activity. Here we present a systematic inclusion of heterogeneity in network connectivity to derive quantitative predictions for neuron-resolved covariances and their statistics in spiking neural networks. Our study shows that the heterogeneity in covariances is not a result of variability in single-neuron firing statistics but stems from the ubiquitously observed sparsity and variability of connections in brain networks. Linear-response theory maps these features to the effective connectivity between neurons, which in turn determines neuronal covariances. Beyond-mean-field tools reveal that synaptic heterogeneity modulates the variability of covariances and thus the complexity of neuronal coordination across many orders of magnitude.",
      "paper_authors": [
        "Moritz Layer",
        "Moritz Helias",
        "David Dahmen"
      ],
      "primary_category": "cond-mat.dis-nn",
      "publish_time": "2023-08-01",
      "update_time": "2024-04-04",
      "comments": null,
      "repo_url": "#"
    },
    "2307.16236": {
      "paper_id": "2307.16236v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.16236v1",
      "paper_key": "2307.16236",
      "paper_title": "Synaptic Plasticity Models and Bio-Inspired Unsupervised Deep Learning: A Survey",
      "paper_url": "http://arxiv.org/abs/2307.16236v1",
      "paper_abstract": "Recently emerged technologies based on Deep Learning (DL) achieved outstanding results on a variety of tasks in the field of Artificial Intelligence (AI). However, these encounter several challenges related to robustness to adversarial inputs, ecological impact, and the necessity of huge amounts of training data. In response, researchers are focusing more and more interest on biologically grounded mechanisms, which are appealing due to the impressive capabilities exhibited by biological brains. This survey explores a range of these biologically inspired models of synaptic plasticity, their application in DL scenarios, and the connections with models of plasticity in Spiking Neural Networks (SNNs). Overall, Bio-Inspired Deep Learning (BIDL) represents an exciting research direction, aiming at advancing not only our current technologies but also our understanding of intelligence.",
      "paper_authors": [
        "Gabriele Lagani",
        "Fabrizio Falchi",
        "Claudio Gennaro",
        "Giuseppe Amato"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-07-30",
      "update_time": "2023-07-30",
      "comments": null,
      "repo_url": "#"
    },
    "2307.16235": {
      "paper_id": "2307.16235v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.16235v1",
      "paper_key": "2307.16235",
      "paper_title": "Spiking Neural Networks and Bio-Inspired Supervised Deep Learning: A Survey",
      "paper_url": "http://arxiv.org/abs/2307.16235v1",
      "paper_abstract": "For a long time, biology and neuroscience fields have been a great source of inspiration for computer scientists, towards the development of Artificial Intelligence (AI) technologies. This survey aims at providing a comprehensive review of recent biologically-inspired approaches for AI. After introducing the main principles of computation and synaptic plasticity in biological neurons, we provide a thorough presentation of Spiking Neural Network (SNN) models, and we highlight the main challenges related to SNN training, where traditional backprop-based optimization is not directly applicable. Therefore, we discuss recent bio-inspired training methods, which pose themselves as alternatives to backprop, both for traditional and spiking networks. Bio-Inspired Deep Learning (BIDL) approaches towards advancing the computational capabilities and biological plausibility of current models.",
      "paper_authors": [
        "Gabriele Lagani",
        "Fabrizio Falchi",
        "Claudio Gennaro",
        "Giuseppe Amato"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-07-30",
      "update_time": "2023-07-30",
      "comments": null,
      "repo_url": "#"
    },
    "2307.14464": {
      "paper_id": "2307.14464v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.14464v1",
      "paper_key": "2307.14464",
      "paper_title": "Single Channel Speech Enhancement Using U-Net Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2307.14464v1",
      "paper_abstract": "Speech enhancement (SE) is crucial for reliable communication devices or robust speech recognition systems. Although conventional artificial neural networks (ANN) have demonstrated remarkable performance in SE, they require significant computational power, along with high energy costs. In this paper, we propose a novel approach to SE using a spiking neural network (SNN) based on a U-Net architecture. SNNs are suitable for processing data with a temporal dimension, such as speech, and are known for their energy-efficient implementation on neuromorphic hardware. As such, SNNs are thus interesting candidates for real-time applications on devices with limited resources. The primary objective of the current work is to develop an SNN-based model with comparable performance to a state-of-the-art ANN model for SE. We train a deep SNN using surrogate-gradient-based optimization and evaluate its performance using perceptual objective tests under different signal-to-noise ratios and real-world noise conditions. Our results demonstrate that the proposed energy-efficient SNN model outperforms the Intel Neuromorphic Deep Noise Suppression Challenge (Intel N-DNS Challenge) baseline solution and achieves acceptable performance compared to an equivalent ANN model.",
      "paper_authors": [
        "Abir Riahi",
        "\u00c9ric Plourde"
      ],
      "primary_category": "cs.SD",
      "publish_time": "2023-07-26",
      "update_time": "2023-07-26",
      "comments": null,
      "repo_url": "https://github.com/riaa3102/SESNNet"
    },
    "2307.14077": {
      "paper_id": "2307.14077v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.14077v1",
      "paper_key": "2307.14077",
      "paper_title": "Learning heterogeneous delays in a layer of spiking neurons for fast motion detection",
      "paper_url": "http://arxiv.org/abs/2307.14077v1",
      "paper_abstract": "The precise timing of spikes emitted by neurons plays a crucial role in shaping the response of efferent biological neurons. This temporal dimension of neural activity holds significant importance in understanding information processing in neurobiology, especially for the performance of neuromorphic hardware, such as event-based cameras. Nonetheless, many artificial neural models disregard this critical temporal dimension of neural activity. In this study, we present a model designed to efficiently detect temporal spiking motifs using a layer of spiking neurons equipped with heterogeneous synaptic delays. Our model capitalizes on the diverse synaptic delays present on the dendritic tree, enabling specific arrangements of temporally precise synaptic inputs to synchronize upon reaching the basal dendritic tree. We formalize this process as a time-invariant logistic regression, which can be trained using labeled data. To demonstrate its practical efficacy, we apply the model to naturalistic videos transformed into event streams, simulating the output of the biological retina or event-based cameras. To evaluate the robustness of the model in detecting visual motion, we conduct experiments by selectively pruning weights and demonstrate that the model remains efficient even under significantly reduced workloads. In conclusion, by providing a comprehensive, event-driven computational building block, the incorporation of heterogeneous delays has the potential to greatly improve the performance of future spiking neural network algorithms, particularly in the context of neuromorphic chips.",
      "paper_authors": [
        "Antoine Grimaldi",
        "Laurent U Perrinet"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2023-07-26",
      "update_time": "2023-07-26",
      "comments": null,
      "repo_url": "https://github.com/spikeai/2023_grimaldiperrinet_heterogeneousdelaysnn"
    },
    "2307.12900": {
      "paper_id": "2307.12900v5",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.12900v5",
      "paper_key": "2307.12900",
      "paper_title": "Automotive Object Detection via Learning Sparse Events by Spiking Neurons",
      "paper_url": "http://arxiv.org/abs/2307.12900v5",
      "paper_abstract": "Event-based sensors, distinguished by their high temporal resolution of 1 $\\mathrm{\\mu}\\text{s}$ and a dynamic range of 120 $\\text{dB}$, stand out as ideal tools for deployment in fast-paced settings like vehicles and drones. Traditional object detection techniques that utilize Artificial Neural Networks (ANNs) face challenges due to the sparse and asynchronous nature of the events these sensors capture. In contrast, Spiking Neural Networks (SNNs) offer a promising alternative, providing a temporal representation that is inherently aligned with event-based data. This paper explores the unique membrane potential dynamics of SNNs and their ability to modulate sparse events. We introduce an innovative spike-triggered adaptive threshold mechanism designed for stable training. Building on these insights, we present a specialized spiking feature pyramid network (SpikeFPN) optimized for automotive event-based object detection. Comprehensive evaluations demonstrate that SpikeFPN surpasses both traditional SNNs and advanced ANNs enhanced with attention mechanisms. Evidently, SpikeFPN achieves a mean Average Precision (mAP) of 0.477 on the GEN1 Automotive Detection (GAD) benchmark dataset, marking significant increases over the selected SNN baselines. Moreover, the efficient design of SpikeFPN ensures robust performance while optimizing computational resources, attributed to its innate sparse computation capabilities. Source codes are publicly accessible at https://github.com/EMI-Group/spikefpn.",
      "paper_authors": [
        "Hu Zhang",
        "Yanchen Li",
        "Luziwei Leng",
        "Kaiwei Che",
        "Qian Liu",
        "Qinghai Guo",
        "Jianxing Liao",
        "Ran Cheng"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-07-24",
      "update_time": "2024-06-11",
      "comments": "IEEE Transactions on Cognitive and Developmental Systems",
      "repo_url": "#"
    },
    "2307.13007": {
      "paper_id": "2307.13007v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.13007v1",
      "paper_key": "2307.13007",
      "paper_title": "Sparse-firing regularization methods for spiking neural networks with time-to-first spike coding",
      "paper_url": "http://arxiv.org/abs/2307.13007v1",
      "paper_abstract": "The training of multilayer spiking neural networks (SNNs) using the error backpropagation algorithm has made significant progress in recent years. Among the various training schemes, the error backpropagation method that directly uses the firing time of neurons has attracted considerable attention because it can realize ideal temporal coding. This method uses time-to-first spike (TTFS) coding, in which each neuron fires at most once, and this restriction on the number of firings enables information to be processed at a very low firing frequency. This low firing frequency increases the energy efficiency of information processing in SNNs, which is important not only because of its similarity with information processing in the brain, but also from an engineering point of view. However, only an upper limit has been provided for TTFS-coded SNNs, and the information-processing capability of SNNs at lower firing frequencies has not been fully investigated. In this paper, we propose two spike timing-based sparse-firing (SSR) regularization methods to further reduce the firing frequency of TTFS-coded SNNs. The first is the membrane potential-aware SSR (M-SSR) method, which has been derived as an extreme form of the loss function of the membrane potential value. The second is the firing condition-aware SSR (F-SSR) method, which is a regularization function obtained from the firing conditions. Both methods are characterized by the fact that they only require information about the firing timing and associated weights. The effects of these regularization methods were investigated on the MNIST, Fashion-MNIST, and CIFAR-10 datasets using multilayer perceptron networks and convolutional neural network structures.",
      "paper_authors": [
        "Yusuke Sakemi",
        "Kakei Yamamoto",
        "Takeo Hosomi",
        "Kazuyuki Aihara"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-07-24",
      "update_time": "2023-07-24",
      "comments": null,
      "repo_url": "#"
    },
    "2307.11844": {
      "paper_id": "2307.11844v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.11844v2",
      "paper_key": "2307.11844",
      "paper_title": "Bio-realistic Neural Network Implementation on Loihi 2 with Izhikevich Neurons",
      "paper_url": "http://arxiv.org/abs/2307.11844v2",
      "paper_abstract": "In this paper, we presented a bio-realistic basal ganglia neural network and its integration into Intel's Loihi neuromorphic processor to perform simple Go/No-Go task. To incorporate more bio-realistic and diverse set of neuron dynamics, we used Izhikevich neuron model, implemented as microcode, instead of Leaky-Integrate and Fire (LIF) neuron model that has built-in support on Loihi. This work aims to demonstrate the feasibility of implementing computationally efficient custom neuron models on Loihi for building spiking neural networks (SNNs) that features these custom neurons to realize bio-realistic neural networks.",
      "paper_authors": [
        "Recep Bu\u011fra Uluda\u011f",
        "Serhat \u00c7a\u011fda\u015f",
        "Yavuz Selim \u0130\u015fler",
        "Neslihan Serap \u015eeng\u00f6r",
        "Ismail Akturk"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-07-21",
      "update_time": "2023-07-28",
      "comments": null,
      "repo_url": "#"
    },
    "2307.11555": {
      "paper_id": "2307.11555v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.11555v2",
      "paper_key": "2307.11555",
      "paper_title": "Accurate Detection of Spiking Motifs by Learning Heterogeneous Delays of a Spiking Neural Network",
      "paper_url": "http://arxiv.org/abs/2307.11555v2",
      "paper_abstract": "Recently, interest has grown in exploring the hypothesis that neural activity conveys information through precise spiking motifs. To investigate this phenomenon, various algorithms have been proposed to detect such motifs in Single Unit Activity (SUA) recorded from populations of neurons. In this study, we present a novel detection model based on the inversion of a generative model of raster plot synthesis. Using this generative model, we derive an optimal detection procedure that takes the form of logistic regression combined with temporal convolution. A key advantage of this model is its differentiability, which allows us to formulate a supervised learning approach using a gradient descent on the binary cross-entropy loss. To assess the model's ability to detect spiking motifs in synthetic data, we first perform numerical evaluations. This analysis highlights the advantages of using spiking motifs over traditional firing rate based population codes. We then successfully demonstrate that our learning method can recover synthetically generated spiking motifs, indicating its potential for further applications. In the future, we aim to extend this method to real neurobiological data, where the ground truth is unknown, to explore and detect spiking motifs in a more natural and biologically relevant context.",
      "paper_authors": [
        "Laurent U Perrinet"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2023-07-21",
      "update_time": "2023-09-25",
      "comments": "ICANN 2023 Special Session on Recent Advances in Spiking Neural\n  Networks - Conference paper",
      "repo_url": "https://github.com/laurentperrinet/2023-09-27_HDSNN-ICANN"
    },
    "2307.11411": {
      "paper_id": "2307.11411v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.11411v3",
      "paper_key": "2307.11411",
      "paper_title": "Deep Directly-Trained Spiking Neural Networks for Object Detection",
      "paper_url": "http://arxiv.org/abs/2307.11411v3",
      "paper_abstract": "Spiking neural networks (SNNs) are brain-inspired energy-efficient models that encode information in spatiotemporal dynamics. Recently, deep SNNs trained directly have shown great success in achieving high performance on classification tasks with very few time steps. However, how to design a directly-trained SNN for the regression task of object detection still remains a challenging problem. To address this problem, we propose EMS-YOLO, a novel directly-trained SNN framework for object detection, which is the first trial to train a deep SNN with surrogate gradients for object detection rather than ANN-SNN conversion strategies. Specifically, we design a full-spike residual block, EMS-ResNet, which can effectively extend the depth of the directly-trained SNN with low power consumption. Furthermore, we theoretically analyze and prove the EMS-ResNet could avoid gradient vanishing or exploding. The results demonstrate that our approach outperforms the state-of-the-art ANN-SNN conversion methods (at least 500 time steps) in extremely fewer time steps (only 4 time steps). It is shown that our model could achieve comparable performance to the ANN with the same architecture while consuming 5.83 times less energy on the frame-based COCO Dataset and the event-based Gen1 Dataset.",
      "paper_authors": [
        "Qiaoyi Su",
        "Yuhong Chou",
        "Yifan Hu",
        "Jianing Li",
        "Shijie Mei",
        "Ziyang Zhang",
        "Guoqi Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-07-21",
      "update_time": "2023-07-27",
      "comments": "Accepted by ICCV2023",
      "repo_url": "https://github.com/BICLab/EMS-YOLO"
    },
    "2307.11349": {
      "paper_id": "2307.11349v5",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.11349v5",
      "paper_key": "2307.11349",
      "paper_title": "EV-Planner: Energy-Efficient Robot Navigation via Event-Based Physics-Guided Neuromorphic Planner",
      "paper_url": "http://arxiv.org/abs/2307.11349v5",
      "paper_abstract": "Vision-based object tracking is an essential precursor to performing autonomous aerial navigation in order to avoid obstacles. Biologically inspired neuromorphic event cameras are emerging as a powerful alternative to frame-based cameras, due to their ability to asynchronously detect varying intensities (even in poor lighting conditions), high dynamic range, and robustness to motion blur. Spiking neural networks (SNNs) have gained traction for processing events asynchronously in an energy-efficient manner. On the other hand, physics-based artificial intelligence (AI) has gained prominence recently, as they enable embedding system knowledge via physical modeling inside traditional analog neural networks (ANNs). In this letter, we present an event-based physics-guided neuromorphic planner (EV-Planner) to perform obstacle avoidance using neuromorphic event cameras and physics-based AI. We consider the task of autonomous drone navigation where the mission is to detect moving gates and fly through them while avoiding a collision. We use event cameras to perform object detection using a shallow spiking neural network in an unsupervised fashion. Utilizing the physical equations of the brushless DC motors present in the drone rotors, we train a lightweight energy-aware physics-guided neural network (PgNN) with depth inputs. This predicts the optimal flight time responsible for generating near-minimum energy paths. We spawn the drone in the Gazebo simulator and implement a sensor-fused vision-to-planning neuro-symbolic framework using Robot Operating System (ROS). Simulation results for safe collision-free flight trajectories are presented with performance analysis, ablation study and potential future research directions",
      "paper_authors": [
        "Sourav Sanyal",
        "Rohan Kumar Manna",
        "Kaushik Roy"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2023-07-21",
      "update_time": "2024-01-03",
      "comments": "accepted for publication at IEEE Robotics and Automation Letters",
      "repo_url": "https://github.com/souravsanyal06/ev-planner"
    },
    "2307.11314": {
      "paper_id": "2307.11314v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.11314v1",
      "paper_key": "2307.11314",
      "paper_title": "Neuromorphic Online Learning for Spatiotemporal Patterns with a Forward-only Timeline",
      "paper_url": "http://arxiv.org/abs/2307.11314v1",
      "paper_abstract": "Spiking neural networks (SNNs) are bio-plausible computing models with high energy efficiency. The temporal dynamics of neurons and synapses enable them to detect temporal patterns and generate sequences. While Backpropagation Through Time (BPTT) is traditionally used to train SNNs, it is not suitable for online learning of embedded applications due to its high computation and memory cost as well as extended latency. Previous works have proposed online learning algorithms, but they often utilize highly simplified spiking neuron models without synaptic dynamics and reset feedback, resulting in subpar performance. In this work, we present Spatiotemporal Online Learning for Synaptic Adaptation (SOLSA), specifically designed for online learning of SNNs composed of Leaky Integrate and Fire (LIF) neurons with exponentially decayed synapses and soft reset. The algorithm not only learns the synaptic weight but also adapts the temporal filters associated to the synapses. Compared to the BPTT algorithm, SOLSA has much lower memory requirement and achieves a more balanced temporal workload distribution. Moreover, SOLSA incorporates enhancement techniques such as scheduled weight update, early stop training and adaptive synapse filter, which speed up the convergence and enhance the learning performance. When compared to other non-BPTT based SNN learning, SOLSA demonstrates an average learning accuracy improvement of 14.2%. Furthermore, compared to BPTT, SOLSA achieves a 5% higher average learning accuracy with a 72% reduction in memory cost.",
      "paper_authors": [
        "Zhenhang Zhang",
        "Jingang Jin",
        "Haowen Fang",
        "Qinru Qiu"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-07-21",
      "update_time": "2023-07-21",
      "comments": "9 pages,8 figures",
      "repo_url": "#"
    },
    "2307.11242": {
      "paper_id": "2307.11242v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.11242v1",
      "paper_key": "2307.11242",
      "paper_title": "On-Sensor Data Filtering using Neuromorphic Computing for High Energy Physics Experiments",
      "paper_url": "http://arxiv.org/abs/2307.11242v1",
      "paper_abstract": "This work describes the investigation of neuromorphic computing-based spiking neural network (SNN) models used to filter data from sensor electronics in high energy physics experiments conducted at the High Luminosity Large Hadron Collider. We present our approach for developing a compact neuromorphic model that filters out the sensor data based on the particle's transverse momentum with the goal of reducing the amount of data being sent to the downstream electronics. The incoming charge waveforms are converted to streams of binary-valued events, which are then processed by the SNN. We present our insights on the various system design choices - from data encoding to optimal hyperparameters of the training algorithm - for an accurate and compact SNN optimized for hardware deployment. Our results show that an SNN trained with an evolutionary algorithm and an optimized set of hyperparameters obtains a signal efficiency of about 91% with nearly half as many parameters as a deep neural network.",
      "paper_authors": [
        "Shruti R. Kulkarni",
        "Aaron Young",
        "Prasanna Date",
        "Narasinga Rao Miniskar",
        "Jeffrey S. Vetter",
        "Farah Fahim",
        "Benjamin Parpillon",
        "Jennet Dickinson",
        "Nhan Tran",
        "Jieun Yoo",
        "Corrinne Mills",
        "Morris Swartz",
        "Petar Maksimovic",
        "Catherine D. Schuman",
        "Alice Bean"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-07-20",
      "update_time": "2023-07-20",
      "comments": "Manuscript accepted at ICONS'23",
      "repo_url": "#"
    },
    "2307.10974": {
      "paper_id": "2307.10974v4",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.10974v4",
      "paper_key": "2307.10974",
      "paper_title": "Deep Multi-Threshold Spiking-UNet for Image Processing",
      "paper_url": "http://arxiv.org/abs/2307.10974v4",
      "paper_abstract": "U-Net, known for its simple yet efficient architecture, is widely utilized for image processing tasks and is particularly suitable for deployment on neuromorphic chips. This paper introduces the novel concept of Spiking-UNet for image processing, which combines the power of Spiking Neural Networks (SNNs) with the U-Net architecture. To achieve an efficient Spiking-UNet, we face two primary challenges: ensuring high-fidelity information propagation through the network via spikes and formulating an effective training strategy. To address the issue of information loss, we introduce multi-threshold spiking neurons, which improve the efficiency of information transmission within the Spiking-UNet. For the training strategy, we adopt a conversion and fine-tuning pipeline that leverage pre-trained U-Net models. During the conversion process, significant variability in data distribution across different parts is observed when utilizing skip connections. Therefore, we propose a connection-wise normalization method to prevent inaccurate firing rates. Furthermore, we adopt a flow-based training method to fine-tune the converted models, reducing time steps while preserving performance. Experimental results show that, on image segmentation and denoising, our Spiking-UNet achieves comparable performance to its non-spiking counterpart, surpassing existing SNN methods. Compared with the converted Spiking-UNet without fine-tuning, our Spiking-UNet reduces inference time by approximately 90\\%. This research broadens the application scope of SNNs in image processing and is expected to inspire further exploration in the field of neuromorphic engineering. The code for our Spiking-UNet implementation is available at https://github.com/SNNresearch/Spiking-UNet.",
      "paper_authors": [
        "Hebei Li",
        "Yueyi Zhang",
        "Zhiwei Xiong",
        "Xiaoyan Sun"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-07-20",
      "update_time": "2024-04-11",
      "comments": "Accepted in NeuroComputing",
      "repo_url": "https://github.com/snnresearch/spiking-unet"
    },
    "2307.08116": {
      "paper_id": "2307.08116v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.08116v2",
      "paper_key": "2307.08116",
      "paper_title": "Scaling Limits of Memristor-Based Routers for Asynchronous Neuromorphic Systems",
      "paper_url": "http://arxiv.org/abs/2307.08116v2",
      "paper_abstract": "Multi-core neuromorphic systems typically use on-chip routers to transmit spikes among cores. These routers require significant memory resources and consume a large part of the overall system's energy budget. A promising alternative approach to using standard CMOS and SRAM-based routers is to exploit the features of memristive crossbar arrays and use them as programmable switch-matrices that route spikes. However, the scaling of these crossbar arrays presents physical challenges, such as \"IR drop\" on the metal lines due to the parasitic resistance, and leakage current accumulation on multiple active memristors in their \"off\" state. While reliability challenges of this type have been extensively studied in synchronous systems for compute-in-memory matrix-vector multiplication (MVM) accelerators and storage class memory, little effort has been devoted so far to characterizing the scaling limits of memristor-based crossbar routers. Here, we study the challenges of memristive crossbar arrays, when used as routing channels to transmit spikes in asynchronous Spiking Neural Network (SNN) hardware. We validate our analytical findings with experimental results obtained from a 4K-ReRAM chip which demonstrates its functionality as a routing crossbar. We determine the functionality bounds on the routing due to the IR drop and leak problem, based on theoretical modeling, circuit simulations for a 22nm FDSOI technology, and experimental measurements. This work highlights the limitations of this approach and provides useful guidelines for engineering the memristor device properties in memristive crossbar routers for multi-core asynchronous neuromorphic systems.",
      "paper_authors": [
        "Junren Chen",
        "Siyao Yang",
        "Huaqiang Wu",
        "Giacomo Indiveri",
        "Melika Payvand"
      ],
      "primary_category": "cs.ET",
      "publish_time": "2023-07-16",
      "update_time": "2023-12-21",
      "comments": "5 pages, 11 figures",
      "repo_url": "#"
    },
    "2307.07963": {
      "paper_id": "2307.07963v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.07963v1",
      "paper_key": "2307.07963",
      "paper_title": "Enhancing Energy Efficiency and Reliability in Autonomous Systems Estimation using Neuromorphic Approach",
      "paper_url": "http://arxiv.org/abs/2307.07963v1",
      "paper_abstract": "Energy efficiency and reliability have long been crucial factors for ensuring cost-effective and safe missions in autonomous systems computers. With the rapid evolution of industries such as space robotics and advanced air mobility, the demand for these low size, weight, and power (SWaP) computers has grown significantly. This study focuses on introducing an estimation framework based on spike coding theories and spiking neural networks (SNN), leveraging the efficiency and scalability of neuromorphic computers. Therefore, we propose an SNN-based Kalman filter (KF), a fundamental and widely adopted optimal strategy for well-defined linear systems. Furthermore, based on the modified sliding innovation filter (MSIF) we present a robust strategy called SNN-MSIF. Notably, the weight matrices of the networks are designed according to the system model, eliminating the need for learning. To evaluate the effectiveness of the proposed strategies, we compare them to their algorithmic counterparts, namely the KF and the MSIF, using Monte Carlo simulations. Additionally, we assess the robustness of SNN-MSIF by comparing it to SNN-KF in the presence of modeling uncertainties and neuron loss. Our results demonstrate the applicability of the proposed methods and highlight the superior performance of SNN-MSIF in terms of accuracy and robustness. Furthermore, the spiking pattern observed from the networks serves as evidence of the energy efficiency achieved by the proposed methods, as they exhibited an impressive reduction of approximately 97 percent in emitted spikes compared to possible spikes.",
      "paper_authors": [
        "Reza Ahmadvand",
        "Sarah Safura Sharif",
        "Yaser Mike Banad"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2023-07-16",
      "update_time": "2023-07-16",
      "comments": "10 pages, 14 figures",
      "repo_url": "#"
    },
    "2307.07869": {
      "paper_id": "2307.07869v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.07869v1",
      "paper_key": "2307.07869",
      "paper_title": "Custom DNN using Reward Modulated Inverted STDP Learning for Temporal Pattern Recognition",
      "paper_url": "http://arxiv.org/abs/2307.07869v1",
      "paper_abstract": "Temporal spike recognition plays a crucial role in various domains, including anomaly detection, keyword spotting and neuroscience. This paper presents a novel algorithm for efficient temporal spike pattern recognition on sparse event series data. The algorithm leverages a combination of reward-modulatory behavior, Hebbian and anti-Hebbian based learning methods to identify patterns in dynamic datasets with short intervals of training. The algorithm begins with a preprocessing step, where the input data is rationalized and translated to a feature-rich yet sparse spike time series data. Next, a linear feed forward spiking neural network processes this data to identify a trained pattern. Finally, the next layer performs a weighted check to ensure the correct pattern has been detected.To evaluate the performance of the proposed algorithm, it was trained on a complex dataset containing spoken digits with spike information and its output compared to state-of-the-art.",
      "paper_authors": [
        "Vijay Shankaran Vivekanand",
        "Rajkumar Kubendran"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-07-15",
      "update_time": "2023-07-15",
      "comments": null,
      "repo_url": "#"
    },
    "2307.07231": {
      "paper_id": "2307.07231v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.07231v1",
      "paper_key": "2307.07231",
      "paper_title": "Long Short-term Memory with Two-Compartment Spiking Neuron",
      "paper_url": "http://arxiv.org/abs/2307.07231v1",
      "paper_abstract": "The identification of sensory cues associated with potential opportunities and dangers is frequently complicated by unrelated events that separate useful cues by long delays. As a result, it remains a challenging task for state-of-the-art spiking neural networks (SNNs) to identify long-term temporal dependencies since bridging the temporal gap necessitates an extended memory capacity. To address this challenge, we propose a novel biologically inspired Long Short-Term Memory Leaky Integrate-and-Fire spiking neuron model, dubbed LSTM-LIF. Our model incorporates carefully designed somatic and dendritic compartments that are tailored to retain short- and long-term memories. The theoretical analysis further confirms its effectiveness in addressing the notorious vanishing gradient problem. Our experimental results, on a diverse range of temporal classification tasks, demonstrate superior temporal classification capability, rapid training convergence, strong network generalizability, and high energy efficiency of the proposed LSTM-LIF model. This work, therefore, opens up a myriad of opportunities for resolving challenging temporal processing tasks on emerging neuromorphic computing machines.",
      "paper_authors": [
        "Shimin Zhang",
        "Qu Yang",
        "Chenxiang Ma",
        "Jibin Wu",
        "Haizhou Li",
        "Kay Chen Tan"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-07-14",
      "update_time": "2023-07-14",
      "comments": null,
      "repo_url": "#"
    },
    "2307.07136": {
      "paper_id": "2307.07136v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.07136v1",
      "paper_key": "2307.07136",
      "paper_title": "SLSSNN: High energy efficiency spike-train level spiking neural networks with spatio-temporal conversion",
      "paper_url": "http://arxiv.org/abs/2307.07136v1",
      "paper_abstract": "Brain-inspired spiking neuron networks (SNNs) have attracted widespread research interest due to their low power features, high biological plausibility, and strong spatiotemporal information processing capability. Although adopting a surrogate gradient (SG) makes the non-differentiability SNN trainable, achieving comparable accuracy for ANNs and keeping low-power features simultaneously is still tricky. In this paper, we proposed an energy-efficient spike-train level spiking neural network (SLSSNN) with low computational cost and high accuracy. In the SLSSNN, spatio-temporal conversion blocks (STCBs) are applied to replace the convolutional and ReLU layers to keep the low power features of SNNs and improve accuracy. However, SLSSNN cannot adopt backpropagation algorithms directly due to the non-differentiability nature of spike trains. We proposed a suitable learning rule for SLSSNNs by deducing the equivalent gradient of STCB. We evaluate the proposed SLSSNN on static and neuromorphic datasets, including Fashion-Mnist, Cifar10, Cifar100, TinyImageNet, and DVS-Cifar10. The experiment results show that our proposed SLSSNN outperforms the state-of-the-art accuracy on nearly all datasets, using fewer time steps and being highly energy-efficient.",
      "paper_authors": [
        "Changqing Xu",
        "Yi Liu",
        "Yintang Yang"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-07-14",
      "update_time": "2023-07-14",
      "comments": null,
      "repo_url": "#"
    },
    "2307.08501": {
      "paper_id": "2307.08501v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.08501v1",
      "paper_key": "2307.08501",
      "paper_title": "Corticomorphic Hybrid CNN-SNN Architecture for EEG-based Low-footprint Low-latency Auditory Attention Detection",
      "paper_url": "http://arxiv.org/abs/2307.08501v1",
      "paper_abstract": "In a multi-speaker \"cocktail party\" scenario, a listener can selectively attend to a speaker of interest. Studies into the human auditory attention network demonstrate cortical entrainment to speech envelopes resulting in highly correlated Electroencephalography (EEG) measurements. Current trends in EEG-based auditory attention detection (AAD) using artificial neural networks (ANN) are not practical for edge-computing platforms due to longer decision windows using several EEG channels, with higher power consumption and larger memory footprint requirements. Nor are ANNs capable of accurately modeling the brain's top-down attention network since the cortical organization is complex and layer. In this paper, we propose a hybrid convolutional neural network-spiking neural network (CNN-SNN) corticomorphic architecture, inspired by the auditory cortex, which uses EEG data along with multi-speaker speech envelopes to successfully decode auditory attention with low latency down to 1 second, using only 8 EEG electrodes strategically placed close to the auditory cortex, at a significantly higher accuracy of 91.03%, compared to the state-of-the-art. Simultaneously, when compared to a traditional CNN reference model, our model uses ~15% fewer parameters at a lower bit precision resulting in ~57% memory footprint reduction. The results show great promise for edge-computing in brain-embedded devices, like smart hearing aids.",
      "paper_authors": [
        "Richard Gall",
        "Deniz Kocanaogullari",
        "Murat Akcakaya",
        "Deniz Erdogmus",
        "Rajkumar Kubendran"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2023-07-13",
      "update_time": "2023-07-13",
      "comments": null,
      "repo_url": "#"
    },
    "2307.06084": {
      "paper_id": "2307.06084v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.06084v1",
      "paper_key": "2307.06084",
      "paper_title": "Neuromorphic analog circuits for robust on-chip always-on learning in spiking neural networks",
      "paper_url": "http://arxiv.org/abs/2307.06084v1",
      "paper_abstract": "Mixed-signal neuromorphic systems represent a promising solution for solving extreme-edge computing tasks without relying on external computing resources. Their spiking neural network circuits are optimized for processing sensory data on-line in continuous-time. However, their low precision and high variability can severely limit their performance. To address this issue and improve their robustness to inhomogeneities and noise in both their internal state variables and external input signals, we designed on-chip learning circuits with short-term analog dynamics and long-term tristate discretization mechanisms. An additional hysteretic stop-learning mechanism is included to improve stability and automatically disable weight updates when necessary, to enable continuous always-on learning. We designed a spiking neural network with these learning circuits in a prototype chip using a 180 nm CMOS technology. Simulation and silicon measurement results from the prototype chip are presented. These circuits enable the construction of large-scale spiking neural networks with online learning capabilities for real-world edge computing tasks.",
      "paper_authors": [
        "Arianna Rubino",
        "Matteo Cartiglia",
        "Melika Payvand",
        "Giacomo Indiveri"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-07-12",
      "update_time": "2023-07-12",
      "comments": null,
      "repo_url": "#"
    },
    "2307.05225": {
      "paper_id": "2307.05225v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.05225v2",
      "paper_key": "2307.05225",
      "paper_title": "Energy Efficient Personalized Hand-Gesture Recognition with Neuromorphic Computing",
      "paper_url": "http://arxiv.org/abs/2307.05225v2",
      "paper_abstract": "Hand gestures are a form of non-verbal communication that is used in social interaction and it is therefore required for more natural human-robot interaction. Neuromorphic (brain-inspired) computing offers a low-power solution for Spiking neural networks (SNNs) that can be used for the classification and recognition of gestures. This article introduces the preliminary results of a novel methodology for training spiking convolutional neural networks for hand-gesture recognition so that a humanoid robot with integrated neuromorphic hardware will be able to personalise the interaction with a user according to the shown hand gesture. It also describes other approaches that could improve the overall performance of the model.",
      "paper_authors": [
        "Muhammad Aitsam",
        "Alessandro Di Nuovo"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2023-07-11",
      "update_time": "2023-07-25",
      "comments": null,
      "repo_url": "#"
    },
    "2307.04356": {
      "paper_id": "2307.04356v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.04356v2",
      "paper_key": "2307.04356",
      "paper_title": "InfLoR-SNN: Reducing Information Loss for Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2307.04356v2",
      "paper_abstract": "The Spiking Neural Network (SNN) has attracted more and more attention recently. It adopts binary spike signals to transmit information. Benefitting from the information passing paradigm of SNNs, the multiplications of activations and weights can be replaced by additions, which are more energy-efficient. However, its \"Hard Reset\" mechanism for the firing activity would ignore the difference among membrane potentials when the membrane potential is above the firing threshold, causing information loss. Meanwhile, quantifying the membrane potential to 0/1 spikes at the firing instants will inevitably introduce the quantization error thus bringing about information loss too. To address these problems, we propose to use the \"Soft Reset\" mechanism for the supervised training-based SNNs, which will drive the membrane potential to a dynamic reset potential according to its magnitude, and Membrane Potential Rectifier (MPR) to reduce the quantization error via redistributing the membrane potential to a range close to the spikes. Results show that the SNNs with the \"Soft Reset\" mechanism and MPR outperform their vanilla counterparts on both static and dynamic datasets.",
      "paper_authors": [
        "Yufei Guo",
        "Yuanpei Chen",
        "Liwen Zhang",
        "Xiaode Liu",
        "Xinyi Tong",
        "Yuanyuan Ou",
        "Xuhui Huang",
        "Zhe Ma"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-07-10",
      "update_time": "2023-08-18",
      "comments": "Accepted by ECCV2022",
      "repo_url": "#"
    },
    "2307.04054": {
      "paper_id": "2307.04054v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.04054v2",
      "paper_key": "2307.04054",
      "paper_title": "Deep Unsupervised Learning Using Spike-Timing-Dependent Plasticity",
      "paper_url": "http://arxiv.org/abs/2307.04054v2",
      "paper_abstract": "Spike-Timing-Dependent Plasticity (STDP) is an unsupervised learning mechanism for Spiking Neural Networks (SNNs) that has received significant attention from the neuromorphic hardware community. However, scaling such local learning techniques to deeper networks and large-scale tasks has remained elusive. In this work, we investigate a Deep-STDP framework where a rate-based convolutional network, that can be deployed in a neuromorphic setting, is trained in tandem with pseudo-labels generated by the STDP clustering process on the network outputs. We achieve $24.56\\%$ higher accuracy and $3.5\\times$ faster convergence speed at iso-accuracy on a 10-class subset of the Tiny ImageNet dataset in contrast to a $k$-means clustering approach.",
      "paper_authors": [
        "Sen Lu",
        "Abhronil Sengupta"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-07-08",
      "update_time": "2024-03-16",
      "comments": null,
      "repo_url": "#"
    },
    "2307.03910": {
      "paper_id": "2307.03910v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.03910v1",
      "paper_key": "2307.03910",
      "paper_title": "A Survey of Spiking Neural Network Accelerator on FPGA",
      "paper_url": "http://arxiv.org/abs/2307.03910v1",
      "paper_abstract": "Due to the ability to implement customized topology, FPGA is increasingly used to deploy SNNs in both embedded and high-performance applications. In this paper, we survey state-of-the-art SNN implementations and their applications on FPGA. We collect the recent widely-used spiking neuron models, network structures, and signal encoding formats, followed by the enumeration of related hardware design schemes for FPGA-based SNN implementations. Compared with the previous surveys, this manuscript enumerates the application instances that applied the above-mentioned technical schemes in recent research. Based on that, we discuss the actual acceleration potential of implementing SNN on FPGA. According to our above discussion, the upcoming trends are discussed in this paper and give a guideline for further advancement in related subjects.",
      "paper_authors": [
        "Murat Isik"
      ],
      "primary_category": "cs.AR",
      "publish_time": "2023-07-08",
      "update_time": "2023-07-08",
      "comments": null,
      "repo_url": "#"
    },
    "2307.02947": {
      "paper_id": "2307.02947v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.02947v2",
      "paper_key": "2307.02947",
      "paper_title": "A Neuromorphic Architecture for Reinforcement Learning from Real-Valued Observations",
      "paper_url": "http://arxiv.org/abs/2307.02947v2",
      "paper_abstract": "Reinforcement Learning (RL) provides a powerful framework for decision-making in complex environments. However, implementing RL in hardware-efficient and bio-inspired ways remains a challenge. This paper presents a novel Spiking Neural Network (SNN) architecture for solving RL problems with real-valued observations. The proposed model incorporates multi-layered event-based clustering, with the addition of Temporal Difference (TD)-error modulation and eligibility traces, building upon prior work. An ablation study confirms the significant impact of these components on the proposed model's performance. A tabular actor-critic algorithm with eligibility traces and a state-of-the-art Proximal Policy Optimization (PPO) algorithm are used as benchmarks. Our network consistently outperforms the tabular approach and successfully discovers stable control policies on classic RL environments: mountain car, cart-pole, and acrobot. The proposed model offers an appealing trade-off in terms of computational and hardware implementation requirements. The model does not require an external memory buffer nor a global error gradient computation, and synaptic updates occur online, driven by local learning rules and a broadcasted TD-error signal. Thus, this work contributes to the development of more hardware-efficient RL solutions.",
      "paper_authors": [
        "Sergio F. Chevtchenko",
        "Yeshwanth Bethi",
        "Teresa B. Ludermir",
        "Saeed Afshar"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-07-06",
      "update_time": "2023-08-08",
      "comments": null,
      "repo_url": "#"
    },
    "2307.02659": {
      "paper_id": "2307.02659v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.02659v1",
      "paper_key": "2307.02659",
      "paper_title": "Modelling Spontaneous Firing Activity of the Motor Cortex in a Spiking Neural Network with Random and Local Connectivity",
      "paper_url": "http://arxiv.org/abs/2307.02659v1",
      "paper_abstract": "Computational models of cortical activity provide insight into the mechanisms of higher-order processing in the human brain including planning, perception and the control of movement. Activity in the cortex is ongoing even in the absence of sensory input or discernible movements and is thought to be linked to the topology of cortical circuitry. However, the connectivity and its functional role in the generation of spatio-temporal firing patterns and cortical computations are still unknown. Movement of the body is a key function of the brain, with the motor cortex the main cortical area implicated in the generation of movement. We built a spiking neural network model of the motor cortex which incorporates a laminar structure and circuitry based on a previous cortical model. A local connectivity scheme was implemented to introduce more physiological plausibility to the cortex model, and the effect on the rates, distributions and irregularity of neuronal firing was compared to the original random connectivity method and experimental data. Local connectivity increased the distribution of and overall rate of neuronal firing. It also resulted in the irregularity of firing being more similar to those observed in experimental measurements. The larger variability in dynamical behaviour of the local connectivity model suggests that the topological structure of the connections in neuronal population plays a significant role in firing patterns during spontaneous activity. This model took steps towards replicating the macroscopic network of the motor cortex, replicating realistic spatiotemporal firing to shed light on information coding in the cortex. Large scale computational models such as this one can capture how structure and function relate to observable neuronal firing behaviour, and investigates the underlying computational mechanisms of the brain.",
      "paper_authors": [
        "Lysea Haggie",
        "Thor Besier",
        "Angus McMorland"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2023-07-05",
      "update_time": "2023-07-05",
      "comments": null,
      "repo_url": "https://github.com/munozatabi/motorcortex"
    },
    "2307.01694": {
      "paper_id": "2307.01694v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.01694v1",
      "paper_key": "2307.01694",
      "paper_title": "Spike-driven Transformer",
      "paper_url": "http://arxiv.org/abs/2307.01694v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) provide an energy-efficient deep learning option due to their unique spike-based event-driven (i.e., spike-driven) paradigm. In this paper, we incorporate the spike-driven paradigm into Transformer by the proposed Spike-driven Transformer with four unique properties: 1) Event-driven, no calculation is triggered when the input of Transformer is zero; 2) Binary spike communication, all matrix multiplications associated with the spike matrix can be transformed into sparse additions; 3) Self-attention with linear complexity at both token and channel dimensions; 4) The operations between spike-form Query, Key, and Value are mask and addition. Together, there are only sparse addition operations in the Spike-driven Transformer. To this end, we design a novel Spike-Driven Self-Attention (SDSA), which exploits only mask and addition operations without any multiplication, and thus having up to $87.2\\times$ lower computation energy than vanilla self-attention. Especially in SDSA, the matrix multiplication between Query, Key, and Value is designed as the mask operation. In addition, we rearrange all residual connections in the vanilla Transformer before the activation functions to ensure that all neurons transmit binary spike signals. It is shown that the Spike-driven Transformer can achieve 77.1\\% top-1 accuracy on ImageNet-1K, which is the state-of-the-art result in the SNN field. The source code is available at https://github.com/BICLab/Spike-Driven-Transformer.",
      "paper_authors": [
        "Man Yao",
        "Jiakui Hu",
        "Zhaokun Zhou",
        "Li Yuan",
        "Yonghong Tian",
        "Bo Xu",
        "Guoqi Li"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-07-04",
      "update_time": "2023-07-04",
      "comments": null,
      "repo_url": "https://github.com/biclab/spike-driven-transformer"
    },
    "2307.00771": {
      "paper_id": "2307.00771v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.00771v1",
      "paper_key": "2307.00771",
      "paper_title": "Resistive memory-based zero-shot liquid state machine for multimodal event data learning",
      "paper_url": "http://arxiv.org/abs/2307.00771v1",
      "paper_abstract": "The human brain is a complex spiking neural network (SNN) that learns multimodal signals in a zero-shot manner by generalizing existing knowledge. Remarkably, the brain achieves this with minimal power consumption, using event-based signals that propagate within its structure. However, mimicking the human brain in neuromorphic hardware presents both hardware and software challenges. Hardware limitations, such as the slowdown of Moore's law and the von Neumann bottleneck, hinder the efficiency of digital computers. On the software side, SNNs are known for their difficult training, especially when learning multimodal signals. To overcome these challenges, we propose a hardware-software co-design that combines a fixed and random liquid state machine (LSM) SNN encoder with trainable artificial neural network (ANN) projections. The LSM is physically implemented using analogue resistive memory, leveraging the inherent stochasticity of resistive switching to generate random weights. This highly efficient and nanoscale in-memory computing approach effectively addresses the von Neumann bottleneck and the slowdown of Moore's law. The ANN projections are implemented digitally, allowing for easy optimization using contrastive loss, which helps to overcome the difficulties associated with SNN training. We experimentally implement this co-design on a 40nm 256Kb in-memory computing macro. We first demonstrate LSM-based event encoding through supervised classification and linear probing on the N-MNIST and N-TIDIGITS datasets.",
      "paper_authors": [
        "Ning Lin",
        "Shaocong Wang",
        "Yi Li",
        "Bo Wang",
        "Shuhui Shi",
        "Yangu He",
        "Woyu Zhang",
        "Yifei Yu",
        "Yue Zhang",
        "Xiaojuan Qi",
        "Xiaoming Chen",
        "Hao Jiang",
        "Xumeng Zhang",
        "Peng Lin",
        "Xiaoxin Xu",
        "Qi Liu",
        "Zhongrui Wang",
        "Dashan Shang",
        "Ming Liu"
      ],
      "primary_category": "cs.ET",
      "publish_time": "2023-07-03",
      "update_time": "2023-07-03",
      "comments": null,
      "repo_url": "#"
    },
    "2307.00293": {
      "paper_id": "2307.00293v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.00293v2",
      "paper_key": "2307.00293",
      "paper_title": "AutoST: Training-free Neural Architecture Search for Spiking Transformers",
      "paper_url": "http://arxiv.org/abs/2307.00293v2",
      "paper_abstract": "Spiking Transformers have gained considerable attention because they achieve both the energy efficiency of Spiking Neural Networks (SNNs) and the high capacity of Transformers. However, the existing Spiking Transformer architectures, derived from Artificial Neural Networks (ANNs), exhibit a notable architectural gap, resulting in suboptimal performance compared to their ANN counterparts. Manually discovering optimal architectures is time-consuming. To address these limitations, we introduce AutoST, a training-free NAS method for Spiking Transformers, to rapidly identify high-performance Spiking Transformer architectures. Unlike existing training-free NAS methods, which struggle with the non-differentiability and high sparsity inherent in SNNs, we propose to utilize Floating-Point Operations (FLOPs) as a performance metric, which is independent of model computations and training dynamics, leading to a stronger correlation with performance. Our extensive experiments show that AutoST models outperform state-of-the-art manually or automatically designed SNN architectures on static and neuromorphic datasets. Full code, model, and data are released for reproduction.",
      "paper_authors": [
        "Ziqing Wang",
        "Qidong Zhao",
        "Jinku Cui",
        "Xu Liu",
        "Dongkuan Xu"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2023-07-01",
      "update_time": "2023-12-14",
      "comments": "ICASSP 2024",
      "repo_url": "https://github.com/alexandrewang915/autost"
    },
    "2408.14917": {
      "paper_id": "2408.14917v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.14917v1",
      "paper_key": "2408.14917",
      "paper_title": "PMSN: A Parallel Multi-compartment Spiking Neuron for Multi-scale Temporal Processing",
      "paper_url": "http://arxiv.org/abs/2408.14917v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) hold great potential to realize brain-inspired, energy-efficient computational systems. However, current SNNs still fall short in terms of multi-scale temporal processing compared to their biological counterparts. This limitation has resulted in poor performance in many pattern recognition tasks with information that varies across different timescales. To address this issue, we put forward a novel spiking neuron model called Parallel Multi-compartment Spiking Neuron (PMSN). The PMSN emulates biological neurons by incorporating multiple interacting substructures and allows for flexible adjustment of the substructure counts to effectively represent temporal information across diverse timescales. Additionally, to address the computational burden associated with the increased complexity of the proposed model, we introduce two parallelization techniques that decouple the temporal dependencies of neuronal updates, enabling parallelized training across different time steps. Our experimental results on a wide range of pattern recognition tasks demonstrate the superiority of PMSN. It outperforms other state-of-the-art spiking neuron models in terms of its temporal processing capacity, training speed, and computation cost. Specifically, compared with the commonly used Leaky Integrate-and-Fire neuron, PMSN offers a simulation acceleration of over 10 $\\times$ and a 30 % improvement in accuracy on Sequential CIFAR10 dataset, while maintaining comparable computational cost.",
      "paper_authors": [
        "Xinyi Chen",
        "Jibin Wu",
        "Chenxiang Ma",
        "Yinsong Yan",
        "Yujie Wu",
        "Kay Chen Tan"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-08-27",
      "update_time": "2024-08-27",
      "comments": null,
      "repo_url": "#"
    },
    "2408.14909": {
      "paper_id": "2408.14909v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.14909v1",
      "paper_key": "2408.14909",
      "paper_title": "SpikingSSMs: Learning Long Sequences with Sparse and Parallel Spiking State Space Models",
      "paper_url": "http://arxiv.org/abs/2408.14909v1",
      "paper_abstract": "Known as low energy consumption networks, spiking neural networks (SNNs) have gained a lot of attention within the past decades. While SNNs are increasing competitive with artificial neural networks (ANNs) for vision tasks, they are rarely used for long sequence tasks, despite their intrinsic temporal dynamics. In this work, we develop spiking state space models (SpikingSSMs) for long sequence learning by leveraging on the sequence learning abilities of state space models (SSMs). Inspired by dendritic neuron structure, we hierarchically integrate neuronal dynamics with the original SSM block, meanwhile realizing sparse synaptic computation. Furthermore, to solve the conflict of event-driven neuronal dynamics with parallel computing, we propose a light-weight surrogate dynamic network which accurately predicts the after-reset membrane potential and compatible to learnable thresholds, enabling orders of acceleration in training speed compared with conventional iterative methods. On the long range arena benchmark task, SpikingSSM achieves competitive performance to state-of-the-art SSMs meanwhile realizing on average 90\\% of network sparsity. On language modeling, our network significantly surpasses existing spiking large language models (spikingLLMs) on the WikiText-103 dataset with only a third of the model size, demonstrating its potential as backbone architecture for low computation cost LLMs.",
      "paper_authors": [
        "Shuaijie Shen",
        "Chao Wang",
        "Renzhuo Huang",
        "Yan Zhong",
        "Qinghai Guo",
        "Zhichao Lu",
        "Jianguo Zhang",
        "Luziwei Leng"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-08-27",
      "update_time": "2024-08-27",
      "comments": null,
      "repo_url": "#"
    },
    "2408.14437": {
      "paper_id": "2408.14437v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.14437v1",
      "paper_key": "2408.14437",
      "paper_title": "Sparsity-Aware Hardware-Software Co-Design of Spiking Neural Networks: An Overview",
      "paper_url": "http://arxiv.org/abs/2408.14437v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) are inspired by the sparse and event-driven nature of biological neural processing, and offer the potential for ultra-low-power artificial intelligence. However, realizing their efficiency benefits requires specialized hardware and a co-design approach that effectively leverages sparsity. We explore the hardware-software co-design of sparse SNNs, examining how sparsity representation, hardware architectures, and training techniques influence hardware efficiency. We analyze the impact of static and dynamic sparsity, discuss the implications of different neuron models and encoding schemes, and investigate the need for adaptability in hardware designs. Our work aims to illuminate the path towards embedded neuromorphic systems that fully exploit the computational advantages of sparse SNNs.",
      "paper_authors": [
        "Ilkin Aliyev",
        "Kama Svoboda",
        "Tosiron Adegbija",
        "Jean-Marc Fellous"
      ],
      "primary_category": "cs.AR",
      "publish_time": "2024-08-26",
      "update_time": "2024-08-26",
      "comments": null,
      "repo_url": "#"
    },
    "2408.13996": {
      "paper_id": "2408.13996v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.13996v2",
      "paper_key": "2408.13996",
      "paper_title": "Research Advances and New Paradigms for Biology-inspired Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2408.13996v2",
      "paper_abstract": "Spiking neural networks (SNNs) are gaining popularity in the computational simulation and artificial intelligence fields owing to their biological plausibility and computational efficiency. This paper explores the historical development of SNN and concludes that these two fields are intersecting and merging rapidly. Following the successful application of Dynamic Vision Sensors (DVS) and Dynamic Audio Sensors (DAS), SNNs have found some proper paradigms, such as continuous visual signal tracking, automatic speech recognition, and reinforcement learning for continuous control, that have extensively supported their key features, including spike encoding, neuronal heterogeneity, specific functional circuits, and multiscale plasticity. Compared to these real-world paradigms, the brain contains a spiking version of the biology-world paradigm, which exhibits a similar level of complexity and is usually considered a mirror of the real world. Considering the projected rapid development of invasive and parallel Brain-Computer Interface (BCI), as well as the new BCI-based paradigms that include online pattern recognition and stimulus control of biological spike trains, SNNs naturally leverage their advantages in energy efficiency, robustness, and flexibility. The biological brain has inspired the present study of SNNs and effective SNN machine-learning algorithms, which can help enhance neuroscience discoveries in the brain by applying them to the new BCI paradigm. Such two-way interactions with positive feedback can accelerate brain science research and brain-inspired intelligence technology.",
      "paper_authors": [
        "Tianyu Zheng",
        "Liyuan Han",
        "Tielin Zhang"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-08-26",
      "update_time": "2024-08-28",
      "comments": null,
      "repo_url": "#"
    },
    "2408.13379": {
      "paper_id": "2408.13379v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.13379v1",
      "paper_key": "2408.13379",
      "paper_title": "N-DriverMotion: Driver motion learning and prediction using an event-based camera and directly trained spiking neural networks",
      "paper_url": "http://arxiv.org/abs/2408.13379v1",
      "paper_abstract": "Driver motion recognition is a principal factor in ensuring the safety of driving systems. This paper presents a novel system for learning and predicting driver motions and an event-based high-resolution (1280x720) dataset, N-DriverMotion, newly collected to train on a neuromorphic vision system. The system comprises an event-based camera that generates the first high-resolution driver motion dataset representing spike inputs and efficient spiking neural networks (SNNs) that are effective in training and predicting the driver's gestures. The event dataset consists of 13 driver motion categories classified by direction (front, side), illumination (bright, moderate, dark), and participant. A novel simplified four-layer convolutional spiking neural network (CSNN) that we proposed was directly trained using the high-resolution dataset without any time-consuming preprocessing. This enables efficient adaptation to on-device SNNs for real-time inference on high-resolution event-based streams. Compared with recent gesture recognition systems adopting neural networks for vision processing, the proposed neuromorphic vision system achieves comparable accuracy, 94.04\\%, in recognizing driver motions with the CSNN architecture. Our proposed CSNN and the dataset can be used to develop safer and more efficient driver monitoring systems for autonomous vehicles or edge devices requiring an efficient neural network architecture.",
      "paper_authors": [
        "Hyo Jong Chung",
        "Byungkon Kang",
        "Yoonseok Yang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-23",
      "update_time": "2024-08-23",
      "comments": "10 pages, 5 figures",
      "repo_url": "#"
    },
    "2408.14499": {
      "paper_id": "2408.14499v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.14499v1",
      "paper_key": "2408.14499",
      "paper_title": "SHEDAD: SNN-Enhanced District Heating Anomaly Detection for Urban Substations",
      "paper_url": "http://arxiv.org/abs/2408.14499v1",
      "paper_abstract": "District Heating (DH) systems are essential for energy-efficient urban heating. However, despite the advancements in automated fault detection and diagnosis (FDD), DH still faces challenges in operational faults that impact efficiency. This study introduces the Shared Nearest Neighbor Enhanced District Heating Anomaly Detection (SHEDAD) approach, designed to approximate the DH network topology and allow for local anomaly detection without disclosing sensitive information, such as substation locations. The approach leverages a multi-adaptive k-Nearest Neighbor (k-NN) graph to improve the initial neighborhood creation. Moreover, it introduces a merging technique that reduces noise and eliminates trivial edges. We use the Median Absolute Deviation (MAD) and modified z-scores to flag anomalous substations. The results reveal that SHEDAD outperforms traditional clustering methods, achieving significantly lower intra-cluster variance and distance. Additionally, SHEDAD effectively isolates and identifies two distinct categories of anomalies: supply temperatures and substation performance. We identified 30 anomalous substations and reached a sensitivity of approximately 65\\% and specificity of approximately 97\\%. By focusing on this subset of poor-performing substations in the network, SHEDAD enables more targeted and effective maintenance interventions, which can reduce energy usage while optimizing network performance.",
      "paper_authors": [
        "Jonne van Dreven",
        "Abbas Cheddad",
        "Sadi Alawadi",
        "Ahmad Nauman Ghazi",
        "Jad Al Koussa",
        "Dirk Vanhoudt"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-08-23",
      "update_time": "2024-08-23",
      "comments": "12 pages, 5 figures, FMEC2024",
      "repo_url": "#"
    },
    "2408.04583": {
      "paper_id": "2408.04583v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.04583v1",
      "paper_key": "2408.04583",
      "paper_title": "Unveiling the Power of Sparse Neural Networks for Feature Selection",
      "paper_url": "http://arxiv.org/abs/2408.04583v1",
      "paper_abstract": "Sparse Neural Networks (SNNs) have emerged as powerful tools for efficient feature selection. Leveraging the dynamic sparse training (DST) algorithms within SNNs has demonstrated promising feature selection capabilities while drastically reducing computational overheads. Despite these advancements, several critical aspects remain insufficiently explored for feature selection. Questions persist regarding the choice of the DST algorithm for network training, the choice of metric for ranking features/neurons, and the comparative performance of these methods across diverse datasets when compared to dense networks. This paper addresses these gaps by presenting a comprehensive systematic analysis of feature selection with sparse neural networks. Moreover, we introduce a novel metric considering sparse neural network characteristics, which is designed to quantify feature importance within the context of SNNs. Our findings show that feature selection with SNNs trained with DST algorithms can achieve, on average, more than $50\\%$ memory and $55\\%$ FLOPs reduction compared to the dense networks, while outperforming them in terms of the quality of the selected features. Our code and the supplementary material are available on GitHub (\\url{https://github.com/zahraatashgahi/Neuron-Attribution}).",
      "paper_authors": [
        "Zahra Atashgahi",
        "Tennison Liu",
        "Mykola Pechenizkiy",
        "Raymond Veldhuis",
        "Decebal Constantin Mocanu",
        "Mihaela van der Schaar"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-08-08",
      "update_time": "2024-08-08",
      "comments": null,
      "repo_url": "https://github.com/zahraatashgahi/neuron-attribution"
    },
    "2407.20597": {
      "paper_id": "2407.20597v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.20597v1",
      "paper_key": "2407.20597",
      "paper_title": "Joint Diffusion Processes as an Inductive Bias in Sheaf Neural Networks",
      "paper_url": "http://arxiv.org/abs/2407.20597v1",
      "paper_abstract": "Sheaf Neural Networks (SNNs) naturally extend Graph Neural Networks (GNNs) by endowing a cellular sheaf over the graph, equipping nodes and edges with vector spaces and defining linear mappings between them. While the attached geometric structure has proven to be useful in analyzing heterophily and oversmoothing, so far the methods by which the sheaf is computed do not always guarantee a good performance in such settings. In this work, drawing inspiration from opinion dynamics concepts, we propose two novel sheaf learning approaches that (i) provide a more intuitive understanding of the involved structure maps, (ii) introduce a useful inductive bias for heterophily and oversmoothing, and (iii) infer the sheaf in a way that does not scale with the number of features, thus using fewer learnable parameters than existing methods. In our evaluation, we show the limitations of the real-world benchmarks used so far on SNNs, and design a new synthetic task -- leveraging the symmetries of n-dimensional ellipsoids -- that enables us to better assess the strengths and weaknesses of sheaf-based models. Our extensive experimentation on these novel datasets reveals valuable insights into the scenarios and contexts where SNNs in general -- and our proposed approaches in particular -- can be beneficial.",
      "paper_authors": [
        "Ferran Hernandez Caralt",
        "Guillermo Bern\u00e1rdez Gil",
        "Iulia Duta",
        "Pietro Li\u00f2",
        "Eduard Alarc\u00f3n Cot"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-07-30",
      "update_time": "2024-07-30",
      "comments": null,
      "repo_url": "#"
    },
    "2407.08861": {
      "paper_id": "2407.08861v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.08861v1",
      "paper_key": "2407.08861",
      "paper_title": "A Hybrid Spiking-Convolutional Neural Network Approach for Advancing Machine Learning Models",
      "paper_url": "http://arxiv.org/abs/2407.08861v1",
      "paper_abstract": "In this article, we propose a novel standalone hybrid Spiking-Convolutional Neural Network (SC-NN) model and test on using image inpainting tasks. Our approach uses the unique capabilities of SNNs, such as event-based computation and temporal processing, along with the strong representation learning abilities of CNNs, to generate high-quality inpainted images. The model is trained on a custom dataset specifically designed for image inpainting, where missing regions are created using masks. The hybrid model consists of SNNConv2d layers and traditional CNN layers. The SNNConv2d layers implement the leaky integrate-and-fire (LIF) neuron model, capturing spiking behavior, while the CNN layers capture spatial features. In this study, a mean squared error (MSE) loss function demonstrates the training process, where a training loss value of 0.015, indicates accurate performance on the training set and the model achieved a validation loss value as low as 0.0017 on the testing set. Furthermore, extensive experimental results demonstrate state-of-the-art performance, showcasing the potential of integrating temporal dynamics and feature extraction in a single network for image inpainting.",
      "paper_authors": [
        "Sanaullah",
        "Kaushik Roy",
        "Ulrich R\u00fcckert",
        "Thorsten Jungeblut"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-07-11",
      "update_time": "2024-07-11",
      "comments": "7 Pages, 3 figures, and 2 tables",
      "repo_url": "#"
    },
    "2407.06333": {
      "paper_id": "2407.06333v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.06333v2",
      "paper_key": "2407.06333",
      "paper_title": "A third-order finite difference weighted essentially non-oscillatory scheme with shallow neural network",
      "paper_url": "http://arxiv.org/abs/2407.06333v2",
      "paper_abstract": "In this paper, we introduce the finite difference weighted essentially non-oscillatory (WENO) scheme based on the neural network for hyperbolic conservation laws. We employ the supervised learning and design two loss functions, one with the mean squared error and the other with the mean squared logarithmic error, where the WENO3-JS weights are computed as the labels. Each loss function consists of two components where the first component compares the difference between the weights from the neural network and WENO3-JS weights, while the second component matches the output weights of the neural network and the linear weights. The former of the loss function enforces the neural network to follow the WENO properties, implying that there is no need for the post-processing layer. Additionally the latter leads to better performance around discontinuities. As a neural network structure, we choose the shallow neural network (SNN) for computational efficiency with the Delta layer consisting of the normalized undivided differences. These constructed WENO3-SNN schemes show the outperformed results in one-dimensional examples and improved behavior in two-dimensional examples, compared with the simulations from WENO3-JS and WENO3-Z.",
      "paper_authors": [
        "Kwanghyuk Park",
        "Xinjuan Chen",
        "Dongjin Lee",
        "Jiaxi Gu",
        "Jae-Hun Jung"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-07-08",
      "update_time": "2024-07-10",
      "comments": null,
      "repo_url": "#"
    },
    "2406.04178": {
      "paper_id": "2406.04178v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.04178v1",
      "paper_key": "2406.04178",
      "paper_title": "Encoding Semantic Priors into the Weights of Implicit Neural Representation",
      "paper_url": "http://arxiv.org/abs/2406.04178v1",
      "paper_abstract": "Implicit neural representation (INR) has recently emerged as a promising paradigm for signal representations, which takes coordinates as inputs and generates corresponding signal values. Since these coordinates contain no semantic features, INR fails to take any semantic information into consideration. However, semantic information has been proven critical in many vision tasks, especially for visual signal representation. This paper proposes a reparameterization method termed as SPW, which encodes the semantic priors to the weights of INR, thus making INR contain semantic information implicitly and enhancing its representational capacity. Specifically, SPW uses the Semantic Neural Network (SNN) to extract both low- and high-level semantic information of the target visual signal and generates the semantic vector, which is input into the Weight Generation Network (WGN) to generate the weights of INR model. Finally, INR uses the generated weights with semantic priors to map the coordinates to the signal values. After training, we only retain the generated weights while abandoning both SNN and WGN, thus SPW introduces no extra costs in inference. Experimental results show that SPW can improve the performance of various INR models significantly on various tasks, including image fitting, CT reconstruction, MRI reconstruction, and novel view synthesis. Further experiments illustrate that model with SPW has lower weight redundancy and learns more novel representations, validating the effectiveness of SPW.",
      "paper_authors": [
        "Zhicheng Cai",
        "Qiu Shen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-06",
      "update_time": "2024-06-06",
      "comments": "ICME 2024",
      "repo_url": "#"
    },
    "2405.20088": {
      "paper_id": "2405.20088v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.20088v1",
      "paper_key": "2405.20088",
      "paper_title": "Personalized Predictions from Population Level Experiments: A Study on Alzheimer's Disease",
      "paper_url": "http://arxiv.org/abs/2405.20088v1",
      "paper_abstract": "The purpose of this article is to infer patient level outcomes from population level randomized control trials (RCTs). In this pursuit, we utilize the recently proposed synthetic nearest neighbors (SNN) estimator. At its core, SNN leverages information across patients to impute missing data associated with each patient of interest. We focus on two types of missing data: (i) unrecorded outcomes from discontinuing the assigned treatments and (ii) unobserved outcomes associated with unassigned treatments. Data imputation in the former powers and de-biases RCTs, while data imputation in the latter simulates \"synthetic RCTs\" to predict the outcomes for each patient under every treatment. The SNN estimator is interpretable, transparent, and causally justified under a broad class of missing data scenarios. Relative to several standard methods, we empirically find that SNN performs well for the above two applications using Phase 3 clinical trial data on patients with Alzheimer's Disease. Our findings directly suggest that SNN can tackle a current pain point within the clinical trial workflow on patient dropouts and serve as a new tool towards the development of precision medicine. Building on our insights, we discuss how SNN can further generalize to real-world applications.",
      "paper_authors": [
        "Dennis Shen",
        "Anish Agarwal",
        "Vishal Misra",
        "Bjoern Schelter",
        "Devavrat Shah",
        "Helen Shiells",
        "Claude Wischik"
      ],
      "primary_category": "stat.AP",
      "publish_time": "2024-05-30",
      "update_time": "2024-05-30",
      "comments": null,
      "repo_url": "https://github.com/deshen24/syntheticNN"
    },
    "2405.16769": {
      "paper_id": "2405.16769v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.16769v1",
      "paper_key": "2405.16769",
      "paper_title": "Learning phase transitions by siamese neural network",
      "paper_url": "http://arxiv.org/abs/2405.16769v1",
      "paper_abstract": "The wide application of machine learning (ML) techniques in statistics physics has presented new avenues for research in this field. In this paper, we introduce a semi-supervised learning method based on Siamese Neural Networks (SNN), trying to explore the potential of neural network (NN) in the study of critical behaviors beyond the approaches of supervised and unsupervised learning. By focusing on the (1+1) dimensional bond directed percolation (DP) model of nonequilibrium phase transition, we use the SNN to predict the critical values and critical exponents of the system. Different from traditional ML methods, the input of SNN is a set of configuration data pairs and the output prediction is similarity, which prompts to find an anchor point of data for pair comparison during the test. In our study, during test we set different bond probability $p$ as anchors, and discuss the impact of the configurations at this anchors on predictions. More, we use an iterative method to find the optimal training interval to make the algorithm more efficient, and the prediction results are comparable to other ML methods.",
      "paper_authors": [
        "Jianmin Shen",
        "Shiyang Chen",
        "Feiyi Liu",
        "Youju Liu",
        "Wei Li"
      ],
      "primary_category": "physics.comp-ph",
      "publish_time": "2024-05-27",
      "update_time": "2024-05-27",
      "comments": "14 pages, 9 figures",
      "repo_url": "#"
    },
    "2405.15540": {
      "paper_id": "2405.15540v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.15540v1",
      "paper_key": "2405.15540",
      "paper_title": "Bundle Neural Networks for message diffusion on graphs",
      "paper_url": "http://arxiv.org/abs/2405.15540v1",
      "paper_abstract": "The dominant paradigm for learning on graph-structured data is message passing. Despite being a strong inductive bias, the local message passing mechanism suffers from pathological issues such as over-smoothing, over-squashing, and limited node-level expressivity. To address these limitations we propose Bundle Neural Networks (BuNN), a new type of GNN that operates via message diffusion over flat vector bundles - structures analogous to connections on Riemannian manifolds that augment the graph by assigning to each node a vector space and an orthogonal map. A BuNN layer evolves the features according to a diffusion-type partial differential equation. When discretized, BuNNs are a special case of Sheaf Neural Networks (SNNs), a recently proposed MPNN capable of mitigating over-smoothing. The continuous nature of message diffusion enables BuNNs to operate on larger scales of the graph and, therefore, to mitigate over-squashing. Finally, we prove that BuNN can approximate any feature transformation over nodes on any (potentially infinite) family of graphs given injective positional encodings, resulting in universal node-level expressivity. We support our theory via synthetic experiments and showcase the strong empirical performance of BuNNs over a range of real-world tasks, achieving state-of-the-art results on several standard benchmarks in transductive and inductive settings.",
      "paper_authors": [
        "Jacob Bamberger",
        "Federico Barbero",
        "Xiaowen Dong",
        "Michael Bronstein"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-05-24",
      "update_time": "2024-05-24",
      "comments": null,
      "repo_url": "#"
    },
    "2405.12379": {
      "paper_id": "2405.12379v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.12379v1",
      "paper_key": "2405.12379",
      "paper_title": "Measurement dependence can enhance security in a quantum network",
      "paper_url": "http://arxiv.org/abs/2405.12379v1",
      "paper_abstract": "Network Nonlocality is an advanced study of quantum nonlocality that comprises network structure beyond Bell's theorem. The development of quantum networks has the potential to bring a lot of technological applications in sevaral quantum information processing tasks. Here, we are focusing on how the role of the independence of the measurement choices of the end parties in a network works and can be used to enhance the security in a quantum network. In both three-parties two-sources bilocal network and four-parties three-sources star network scenarios, we are able to show, a practical way to understand the relaxation of the assumptions to enhance a real security protocol if someone wants to breach in a network communications. Theoratically, we have proved that by relaxing the independence of the measurement choices of only one end party we can create a Standard Network Nonlocality(SNN) and more stronger Full Network Nonlocality(FNN) and we can get maximum quantum violation by the classical no-signalling local model. We are able to distinguish between two types of network nonlocality in the sense that the FNN is stronger than SNN, i.e., FNN states all the sources in a network need to distribute nonlocal resources.",
      "paper_authors": [
        "Amit Kundu",
        "Debasis Sarkar"
      ],
      "primary_category": "quant-ph",
      "publish_time": "2024-05-20",
      "update_time": "2024-05-20",
      "comments": "18 pages, 5 figures, 2 tables, latex2e, comments welcome",
      "repo_url": "#"
    },
    "2405.02949": {
      "paper_id": "2405.02949v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.02949v1",
      "paper_key": "2405.02949",
      "paper_title": "K+K- photoproduction in ultra$-$peripheral Pb$--$Pb collisions",
      "paper_url": "http://arxiv.org/abs/2405.02949v1",
      "paper_abstract": "In ultra-peripheral collisions (UPCs) of relativistic heavy ions, photoproduction occurs when a photon emitted from one nucleus interacts with the other nucleus from the opposing beam, producing particles in the final state. Measurements of \\KK photoproduction probe interactions and couplings between the $\\phi (1020)$ and charged kaons with photons and nuclear targets. We report exclusive \\KK photoproduction cross section at midrapidity in \\PbPb collisions at \\snn = 5.02 TeV, which is measured for the first time in UPCs.",
      "paper_authors": [
        "Minjung Kim"
      ],
      "primary_category": "nucl-ex",
      "publish_time": "2024-05-05",
      "update_time": "2024-05-05",
      "comments": "Proceedings for the contribution presented at the 1st International\n  Workshop on the physics of Ultra Peripheral Collisions (UPC 2023)",
      "repo_url": "#"
    },
    "2408.15663": {
      "paper_id": "2408.15663v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.15663v1",
      "paper_key": "2408.15663",
      "paper_title": "NeuroVE: Brain-inspired Linear-Angular Velocity Estimation with Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2408.15663v1",
      "paper_abstract": "Vision-based ego-velocity estimation is a fundamental problem in robot state estimation. However, the constraints of frame-based cameras, including motion blur and insufficient frame rates in dynamic settings, readily lead to the failure of conventional velocity estimation techniques. Mammals exhibit a remarkable ability to accurately estimate their ego-velocity during aggressive movement. Hence, integrating this capability into robots shows great promise for addressing these challenges. In this paper, we propose a brain-inspired framework for linear-angular velocity estimation, dubbed NeuroVE. The NeuroVE framework employs an event camera to capture the motion information and implements spiking neural networks (SNNs) to simulate the brain's spatial cells' function for velocity estimation. We formulate the velocity estimation as a time-series forecasting problem. To this end, we design an Astrocyte Leaky Integrate-and-Fire (ALIF) neuron model to encode continuous values. Additionally, we have developed an Astrocyte Spiking Long Short-term Memory (ASLSTM) structure, which significantly improves the time-series forecasting capabilities, enabling an accurate estimate of ego-velocity. Results from both simulation and real-world experiments indicate that NeuroVE has achieved an approximate 60% increase in accuracy compared to other SNN-based approaches.",
      "paper_authors": [
        "Xiao Li",
        "Xieyuanli Chen",
        "Ruibin Guo",
        "Yujie Wu",
        "Zongtan Zhou",
        "Fangwen Yu",
        "Huimin Lu"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-08-28",
      "update_time": "2024-08-28",
      "comments": null,
      "repo_url": "#"
    },
    "2408.15578": {
      "paper_id": "2408.15578v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.15578v1",
      "paper_key": "2408.15578",
      "paper_title": "FireFly-S: Exploiting Dual-Side Sparsity for Spiking Neural Networks Acceleration with Reconfigurable Spatial Architecture",
      "paper_url": "http://arxiv.org/abs/2408.15578v1",
      "paper_abstract": "Spiking Neural Networks (SNNs), with their brain-inspired structure using discrete spikes instead of continuous activations, are gaining attention for their potential of efficient processing on neuromorphic chips. While current SNN hardware accelerators often prioritize temporal spike sparsity, exploiting sparse synaptic weights offers significant untapped potential for even greater efficiency. To address this, we propose FireFly-S, a Sparse extension of the FireFly series. This co-optimized software-hardware design focusing on leveraging dual-side sparsity for acceleration. On the software side, we propose a novel algorithmic optimization framework that combines gradient rewiring for pruning and modified Learned Step Size Quantization (LSQ) tailored for SNNs, which achieves remarkable weight sparsity exceeding 85\\% and enables efficient 4-bit quantization with negligible accuracy loss. On the hardware side, we present an efficient dual-side sparsity detector employing a Bitmap-based sparse decoding logic to pinpoint the positions of non-zero weights and input spikes. The logic allows for the direct bypassing of redundant computations, thereby enhancing computational efficiency. Different from the overlay architecture adopted by previous FireFly series, we adopt a spatial architecture with inter-layer pipelining that can fully exploit the nature of Field-Programmable Gate Arrays (FPGAs). A spatial-temporal dataflow is also proposed to support such inter-layer pipelining and avoid long-term temporal dependencies. In experiments conducted on the MNIST, DVS-Gesture and CIFAR-10 datasets, the FireFly-S model achieves 85-95\\% sparsity with 4-bit quantization and the hardware accelerator effectively leverages the dual-side sparsity, delivering outstanding performance metrics of 10,047 FPS/W on MNIST, 3,683 FPS/W on DVS-Gesture, and 2,327 FPS/W on CIFAR-10.",
      "paper_authors": [
        "Tenglong Li",
        "Jindong Li",
        "Guobin Shen",
        "Dongcheng Zhao",
        "Qian Zhang",
        "Yi Zeng"
      ],
      "primary_category": "cs.AR",
      "publish_time": "2024-08-28",
      "update_time": "2024-08-28",
      "comments": null,
      "repo_url": "#"
    },
    "2408.16754": {
      "paper_id": "2408.16754v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.16754v1",
      "paper_key": "2408.16754",
      "paper_title": "A compact neuromorphic system for ultra energy-efficient, on-device robot localization",
      "paper_url": "http://arxiv.org/abs/2408.16754v1",
      "paper_abstract": "Neuromorphic computing offers a transformative pathway to overcome the computational and energy challenges faced in deploying robotic localization and navigation systems at the edge. Visual place recognition, a critical component for navigation, is often hampered by the high resource demands of conventional systems, making them unsuitable for small-scale robotic platforms which still require to perform complex, long-range tasks. Although neuromorphic approaches offer potential for greater efficiency, real-time edge deployment remains constrained by the complexity and limited scalability of bio-realistic networks. Here, we demonstrate a neuromorphic localization system that performs accurate place recognition in up to 8km of traversal using models as small as 180 KB with 44k parameters, while consuming less than 1% of the energy required by conventional methods. Our Locational Encoding with Neuromorphic Systems (LENS) integrates spiking neural networks, an event-based dynamic vision sensor, and a neuromorphic processor within a single SPECK(TM) chip, enabling real-time, energy-efficient localization on a hexapod robot. LENS represents the first fully neuromorphic localization system capable of large-scale, on-device deployment, setting a new benchmark for energy efficient robotic place recognition.",
      "paper_authors": [
        "Adam D. Hines",
        "Michael Milford",
        "Tobias Fischer"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-08-29",
      "update_time": "2024-08-29",
      "comments": "28 pages, 4 main figures, 4 supplementary figures, 1 supplementary\n  table, and 1 movie. Under review",
      "repo_url": "https://github.com/AdamDHines/LENS"
    },
    "2408.16564": {
      "paper_id": "2408.16564v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.16564v1",
      "paper_key": "2408.16564",
      "paper_title": "Human-Inspired Audio-Visual Speech Recognition: Spike Activity, Cueing Interaction and Causal Processing",
      "paper_url": "http://arxiv.org/abs/2408.16564v1",
      "paper_abstract": "Humans naturally perform audiovisual speech recognition (AVSR), enhancing the accuracy and robustness by integrating auditory and visual information. Spiking neural networks (SNNs), which mimic the brain's information-processing mechanisms, are well-suited for emulating the human capability of AVSR. Despite their potential, research on SNNs for AVSR is scarce, with most existing audio-visual multimodal methods focused on object or digit recognition. These models simply integrate features from both modalities, neglecting their unique characteristics and interactions. Additionally, they often rely on future information for current processing, which increases recognition latency and limits real-time applicability. Inspired by human speech perception, this paper proposes a novel human-inspired SNN named HI-AVSNN for AVSR, incorporating three key characteristics: cueing interaction, causal processing and spike activity. For cueing interaction, we propose a visual-cued auditory attention module (VCA2M) that leverages visual cues to guide attention to auditory features. We achieve causal processing by aligning the SNN's temporal dimension with that of visual and auditory features and applying temporal masking to utilize only past and current information. To implement spike activity, in addition to using SNNs, we leverage the event camera to capture lip movement as spikes, mimicking the human retina and providing efficient visual data. We evaluate HI-AVSNN on an audiovisual speech recognition dataset combining the DVS-Lip dataset with its corresponding audio samples. Experimental results demonstrate the superiority of our proposed fusion method, outperforming existing audio-visual SNN fusion methods and achieving a 2.27% improvement in accuracy over the only existing SNN-based AVSR method.",
      "paper_authors": [
        "Qianhui Liu",
        "Jiadong Wang",
        "Yang Wang",
        "Xin Yang",
        "Gang Pan",
        "Haizhou Li"
      ],
      "primary_category": "cs.MM",
      "publish_time": "2024-08-29",
      "update_time": "2024-08-29",
      "comments": null,
      "repo_url": "#"
    },
    "2408.16467": {
      "paper_id": "2408.16467v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.16467v1",
      "paper_key": "2408.16467",
      "paper_title": "Spiking Diffusion Models",
      "paper_url": "http://arxiv.org/abs/2408.16467v1",
      "paper_abstract": "Recent years have witnessed Spiking Neural Networks (SNNs) gaining attention for their ultra-low energy consumption and high biological plausibility compared with traditional Artificial Neural Networks (ANNs). Despite their distinguished properties, the application of SNNs in the computationally intensive field of image generation is still under exploration. In this paper, we propose the Spiking Diffusion Models (SDMs), an innovative family of SNN-based generative models that excel in producing high-quality samples with significantly reduced energy consumption. In particular, we propose a Temporal-wise Spiking Mechanism (TSM) that allows SNNs to capture more temporal features from a bio-plasticity perspective. In addition, we propose a threshold-guided strategy that can further improve the performances by up to 16.7% without any additional training. We also make the first attempt to use the ANN-SNN approach for SNN-based generation tasks. Extensive experimental results reveal that our approach not only exhibits comparable performance to its ANN counterpart with few spiking time steps, but also outperforms previous SNN-based generative models by a large margin. Moreover, we also demonstrate the high-quality generation ability of SDM on large-scale datasets, e.g., LSUN bedroom. This development marks a pivotal advancement in the capabilities of SNN-based generation, paving the way for future research avenues to realize low-energy and low-latency generative applications. Our code is available at https://github.com/AndyCao1125/SDM.",
      "paper_authors": [
        "Jiahang Cao",
        "Hanzhong Guo",
        "Ziqing Wang",
        "Deming Zhou",
        "Hao Cheng",
        "Qiang Zhang",
        "Renjing Xu"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-08-29",
      "update_time": "2024-08-29",
      "comments": "Accepted by IEEE Transactions on Artificial Intelligence",
      "repo_url": "https://github.com/andycao1125/sdm"
    },
    "2408.16096": {
      "paper_id": "2408.16096v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.16096v1",
      "paper_key": "2408.16096",
      "paper_title": "Accelerating Sensor Fusion in Neuromorphic Computing: A Case Study on Loihi-2",
      "paper_url": "http://arxiv.org/abs/2408.16096v1",
      "paper_abstract": "In our study, we utilized Intel's Loihi-2 neuromorphic chip to enhance sensor fusion in fields like robotics and autonomous systems, focusing on datasets such as AIODrive, Oxford Radar RobotCar, D-Behavior (D-Set), nuScenes by Motional, and Comma2k19. Our research demonstrated that Loihi-2, using spiking neural networks, significantly outperformed traditional computing methods in speed and energy efficiency. Compared to conventional CPUs and GPUs, Loihi-2 showed remarkable energy efficiency, being over 100 times more efficient than a CPU and nearly 30 times more than a GPU. Additionally, our Loihi-2 implementation achieved faster processing speeds on various datasets, marking a substantial advancement over existing state-of-the-art implementations. This paper also discusses the specific challenges encountered during the implementation and optimization processes, providing insights into the architectural innovations of Loihi-2 that contribute to its superior performance.",
      "paper_authors": [
        "Murat Isik",
        "Karn Tiwari",
        "Muhammed Burak Eryilmaz",
        "I. Can Dikmen"
      ],
      "primary_category": "cs.AR",
      "publish_time": "2024-08-28",
      "update_time": "2024-08-28",
      "comments": "Accepted at 2024 IEEE High Performance Extreme Computing",
      "repo_url": "#"
    },
    "2408.17245": {
      "paper_id": "2408.17245v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.17245v1",
      "paper_key": "2408.17245",
      "paper_title": "Stepwise Weighted Spike Coding for Deep Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2408.17245v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) seek to mimic the spiking behavior of biological neurons and are expected to play a key role in the advancement of neural computing and artificial intelligence. The efficiency of SNNs is often determined by the neural coding schemes. Existing coding schemes either cause huge delays and energy consumption or necessitate intricate neuron models and training techniques. To address these issues, we propose a novel Stepwise Weighted Spike (SWS) coding scheme to enhance the encoding of information in spikes. This approach compresses the spikes by weighting the significance of the spike in each step of neural computation, achieving high performance and low energy consumption. A Ternary Self-Amplifying (TSA) neuron model with a silent period is proposed for supporting SWS-based computing, aimed at minimizing the residual error resulting from stepwise weighting in neural computation. Our experimental results show that the SWS coding scheme outperforms the existing neural coding schemes in very deep SNNs, and significantly reduces operations and latency.",
      "paper_authors": [
        "Yiwen Gu",
        "Junchuan Gu",
        "Haibin Shen",
        "Kejie Huang"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-08-30",
      "update_time": "2024-08-30",
      "comments": null,
      "repo_url": "#"
    },
    "2407.03372": {
      "paper_id": "2407.03372v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.03372v3",
      "paper_key": "2407.03372",
      "paper_title": "Physics-informed Neural Networks for Heterogeneous Poroelastic Media",
      "paper_url": "http://arxiv.org/abs/2407.03372v3",
      "paper_abstract": "This study presents a novel physics-informed neural network (PINN) framework for modeling poroelasticity in heterogeneous media with material interfaces. The approach introduces a composite neural network (CoNN) where separate neural networks predict displacement and pressure variables for each material. While sharing identical activation functions, these networks are independently trained for all other parameters. To address challenges posed by heterogeneous material interfaces, the CoNN is integrated with the Interface-PINNs or I-PINNs framework (Sarma et al. 2024, https://dx.doi.org/10.1016/j.cma.2024.117135), allowing different activation functions across material interfaces. This ensures accurate approximation of discontinuous solution fields and gradients. Performance and accuracy of this combined architecture were evaluated against the conventional PINNs approach, a single neural network (SNN) architecture, and the eXtended PINNs (XPINNs) framework through two one-dimensional benchmark examples with discontinuous material properties. The results show that the proposed CoNN with I-PINNs architecture achieves an RMSE that is two orders of magnitude better than the conventional PINNs approach and is at least 40 times faster than the SNN framework. Compared to XPINNs, the proposed method achieves an RMSE at least one order of magnitude better and is 40% faster.",
      "paper_authors": [
        "Sumanta Roy",
        "Chandrasekhar Annavarapu",
        "Pratanu Roy",
        "Dakshina Murthy Valiveti"
      ],
      "primary_category": "eess.SY",
      "publish_time": "2024-07-01",
      "update_time": "2024-09-02",
      "comments": "34 pages, 12 figures, 3 tables",
      "repo_url": "#"
    },
    "2409.02842": {
      "paper_id": "2409.02842v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.02842v1",
      "paper_key": "2409.02842",
      "paper_title": "SNNAX -- Spiking Neural Networks in JAX",
      "paper_url": "http://arxiv.org/abs/2409.02842v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) simulators are essential tools to prototype biologically inspired models and neuromorphic hardware architectures and predict their performance. For such a tool, ease of use and flexibility are critical, but so is simulation speed especially given the complexity inherent to simulating SNN. Here, we present SNNAX, a JAX-based framework for simulating and training such models with PyTorch-like intuitiveness and JAX-like execution speed. SNNAX models are easily extended and customized to fit the desired model specifications and target neuromorphic hardware. Additionally, SNNAX offers key features for optimizing the training and deployment of SNNs such as flexible automatic differentiation and just-in-time compilation. We evaluate and compare SNNAX to other commonly used machine learning (ML) frameworks used for programming SNNs. We provide key performance metrics, best practices, documented examples for simulating SNNs in SNNAX, and implement several benchmarks used in the literature.",
      "paper_authors": [
        "Jamie Lohoff",
        "Jan Finkbeiner",
        "Emre Neftci"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-09-04",
      "update_time": "2024-09-04",
      "comments": null,
      "repo_url": "#"
    },
    "2409.02680": {
      "paper_id": "2409.02680v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.02680v1",
      "paper_key": "2409.02680",
      "paper_title": "A Low-Cost Real-Time Spiking System for Obstacle Detection based on Ultrasonic Sensors and Rate Coding",
      "paper_url": "http://arxiv.org/abs/2409.02680v1",
      "paper_abstract": "Since the advent of mobile robots, obstacle detection has been a topic of great interest. It has also been a subject of study in neuroscience, where flying insects and bats could be considered two of the most interesting cases in terms of vision-based and sound-based mechanisms for obstacle detection, respectively. Currently, many studies focus on vision-based obstacle detection, but not many can be found regarding sound-based obstacle detection. This work focuses on the latter approach, which also makes use of a Spiking Neural Network to exploit the advantages of these architectures and achieve an approach closer to biology. The complete system was tested through a series of experiments that confirm the validity of the spiking architecture for obstacle detection. It is empirically demonstrated that, when the distance between the robot and the obstacle decreases, the output firing rate of the system increases in response as expected, and vice versa. Therefore, there is a direct relation between the two. Furthermore, there is a distance threshold between detectable and undetectable objects which is also empirically measured in this work. An in-depth study on how this system works at low level based on the Inter-Spike Interval concept was performed, which may be useful in the future development of applications based on spiking filters.",
      "paper_authors": [
        "Alvaro Ayuso-Martinez",
        "Daniel Casanueva-Morato",
        "Juan Pedro Dominguez-Morales",
        "Angel Jimenez-Fernandez",
        "Gabriel Jimenez-Moreno"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-09-04",
      "update_time": "2024-09-04",
      "comments": "22 pages, 8 figures",
      "repo_url": "#"
    },
    "2409.02238": {
      "paper_id": "2409.02238v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.02238v1",
      "paper_key": "2409.02238",
      "paper_title": "Internal Representations in Spiking Neural Networks, criticality and the Renormalization Group",
      "paper_url": "http://arxiv.org/abs/2409.02238v1",
      "paper_abstract": "Optimal information processing in peripheral sensory systems has been associated in several examples to the signature of a critical or near critical state. Furthermore, cortical systems have also been described to be in a critical state in both wake and anesthetized experimental models, both {\\it in vitro} and {\\it in vivo}. We investigate whether a similar signature characterizes the internal representations (IR) of a multilayer (deep) spiking artificial neural network performing computationally simple but meaningful cognitive tasks, using a methodology inspired in the biological setup, with cortical implanted electrodes in rats, either freely behaving or under different levels of anesthesia. The increase of the characteristic time of the decay of the correlation of fluctuations of the IR, found when the network input changes, are indications of a broad-tailed distribution of IR fluctuations. The broad tails are present even when the network is not yet capable of performing the classification tasks, either due to partial training or to the effect of a low dose of anesthesia in a simple model. However, we don't find enough evidence of power law distributions of avalanche size and duration. We interpret the results from a renormalization group perspective to point out that despite having broad tails, this is not related to a critical transition but rather similar to fluctuations driven by the reversal of the magnetic field in a ferromagnetic system. Another example of persistent correlation of fluctuations of a non critical system is constructed, where a particle undergoes Brownian motion on a slowly varying potential.",
      "paper_authors": [
        "Jo\u00e3o Henrique de Sant'Ana",
        "Nestor Caticha"
      ],
      "primary_category": "physics.bio-ph",
      "publish_time": "2024-09-03",
      "update_time": "2024-09-03",
      "comments": null,
      "repo_url": "#"
    },
    "2409.01762": {
      "paper_id": "2409.01762v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.01762v1",
      "paper_key": "2409.01762",
      "paper_title": "Decoding finger velocity from cortical spike trains with recurrent spiking neural networks",
      "paper_url": "http://arxiv.org/abs/2409.01762v1",
      "paper_abstract": "Invasive cortical brain-machine interfaces (BMIs) can significantly improve the life quality of motor-impaired patients. Nonetheless, externally mounted pedestals pose an infection risk, which calls for fully implanted systems. Such systems, however, must meet strict latency and energy constraints while providing reliable decoding performance. While recurrent spiking neural networks (RSNNs) are ideally suited for ultra-low-power, low-latency processing on neuromorphic hardware, it is unclear whether they meet the above requirements. To address this question, we trained RSNNs to decode finger velocity from cortical spike trains (CSTs) of two macaque monkeys. First, we found that a large RSNN model outperformed existing feedforward spiking neural networks (SNNs) and artificial neural networks (ANNs) in terms of their decoding accuracy. We next developed a tiny RSNN with a smaller memory footprint, low firing rates, and sparse connectivity. Despite its reduced computational requirements, the resulting model performed substantially better than existing SNN and ANN decoders. Our results thus demonstrate that RSNNs offer competitive CST decoding performance under tight resource constraints and are promising candidates for fully implanted ultra-low-power BMIs with the potential to revolutionize patient care.",
      "paper_authors": [
        "Tengjun Liu",
        "Julia Gygax",
        "Julian Rossbroich",
        "Yansong Chua",
        "Shaomin Zhang",
        "Friedemann Zenke"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2024-09-03",
      "update_time": "2024-09-03",
      "comments": "5 pages, 2 figures. This work has been submitted to the IEEE BioCAS\n  2024 conference",
      "repo_url": "https://github.com/fmi-basel/neural-decoding-RSNN"
    },
    "2409.02146": {
      "paper_id": "2409.02146v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.02146v1",
      "paper_key": "2409.02146",
      "paper_title": "Brain-Inspired Online Adaptation for Remote Sensing with Spiking Neural Network",
      "paper_url": "http://arxiv.org/abs/2409.02146v1",
      "paper_abstract": "On-device computing, or edge computing, is becoming increasingly important for remote sensing, particularly in applications like deep network-based perception on on-orbit satellites and unmanned aerial vehicles (UAVs). In these scenarios, two brain-like capabilities are crucial for remote sensing models: (1) high energy efficiency, allowing the model to operate on edge devices with limited computing resources, and (2) online adaptation, enabling the model to quickly adapt to environmental variations, weather changes, and sensor drift. This work addresses these needs by proposing an online adaptation framework based on spiking neural networks (SNNs) for remote sensing. Starting with a pretrained SNN model, we design an efficient, unsupervised online adaptation algorithm, which adopts an approximation of the BPTT algorithm and only involves forward-in-time computation that significantly reduces the computational complexity of SNN adaptation learning. Besides, we propose an adaptive activation scaling scheme to boost online SNN adaptation performance, particularly in low time-steps. Furthermore, for the more challenging remote sensing detection task, we propose a confidence-based instance weighting scheme, which substantially improves adaptation performance in the detection task. To our knowledge, this work is the first to address the online adaptation of SNNs. Extensive experiments on seven benchmark datasets across classification, segmentation, and detection tasks demonstrate that our proposed method significantly outperforms existing domain adaptation and domain generalization approaches under varying weather conditions. The proposed method enables energy-efficient and fast online adaptation on edge devices, and has much potential in applications such as remote perception on on-orbit satellites and UAV.",
      "paper_authors": [
        "Dexin Duan",
        "Peilin liu",
        "Fei Wen"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-03",
      "update_time": "2024-09-03",
      "comments": null,
      "repo_url": "#"
    },
    "2409.01564": {
      "paper_id": "2409.01564v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.01564v1",
      "paper_key": "2409.01564",
      "paper_title": "ReSpike: Residual Frames-based Hybrid Spiking Neural Networks for Efficient Action Recognition",
      "paper_url": "http://arxiv.org/abs/2409.01564v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) have emerged as a compelling, energy-efficient alternative to traditional Artificial Neural Networks (ANNs) for static image tasks such as image classification and segmentation. However, in the more complex video classification domain, SNN-based methods fall considerably short of ANN-based benchmarks due to the challenges in processing dense frame sequences. To bridge this gap, we propose ReSpike, a hybrid framework that synergizes the strengths of ANNs and SNNs to tackle action recognition tasks with high accuracy and low energy cost. By decomposing film clips into spatial and temporal components, i.e., RGB image Key Frames and event-like Residual Frames, ReSpike leverages ANN for learning spatial information and SNN for learning temporal information. In addition, we propose a multi-scale cross-attention mechanism for effective feature fusion. Compared to state-of-the-art SNN baselines, our ReSpike hybrid architecture demonstrates significant performance improvements (e.g., >30% absolute accuracy improvement on HMDB-51, UCF-101, and Kinetics-400). Furthermore, ReSpike achieves comparable performance with prior ANN approaches while bringing better accuracy-energy tradeoff.",
      "paper_authors": [
        "Shiting Xiao",
        "Yuhang Li",
        "Youngeun Kim",
        "Donghyun Lee",
        "Priyadarshini Panda"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-03",
      "update_time": "2024-09-03",
      "comments": null,
      "repo_url": "#"
    },
    "2409.01230": {
      "paper_id": "2409.01230v5",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.01230v5",
      "paper_key": "2409.01230",
      "paper_title": "CoLaNET -- A Spiking Neural Network with Columnar Layered Architecture for Classification",
      "paper_url": "http://arxiv.org/abs/2409.01230v5",
      "paper_abstract": "In the present paper, I describe a spiking neural network (SNN) architecture which, can be used in wide range of supervised learning classification tasks. It is assumed, that all participating signals (the classified object description, correct class label and SNN decision) have spiking nature. The distinctive feature of this architecture is a combination of prototypical network structures corresponding to different classes and significantly distinctive instances of one class (=columns) and functionally differing populations of neurons inside columns (=layers). The other distinctive feature is a novel combination of anti-Hebbian and dopamine-modulated plasticity. The plasticity rules are local and do not use the backpropagation principle. Besides that, as in my previous studies, I was guided by the requirement that the all neuron/plasticity models should be easily implemented on modern neurochips. I illustrate the high performance of my network on a task related to model-based reinforcement learning, namely, evaluation of proximity of an external world state to the target state.",
      "paper_authors": [
        "Mikhail Kiselev"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-09-02",
      "update_time": "2024-09-14",
      "comments": null,
      "repo_url": "#"
    },
    "2409.00552": {
      "paper_id": "2409.00552v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.00552v1",
      "paper_key": "2409.00552",
      "paper_title": "Digit Recognition using Multimodal Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2409.00552v1",
      "paper_abstract": "Spiking neural networks (SNNs) are the third generation of neural networks that are biologically inspired to process data in a fashion that emulates the exchange of signals in the brain. Within the Computer Vision community SNNs have garnered significant attention due in large part to the availability of event-based sensors that produce a spatially resolved spike train in response to changes in scene radiance. SNNs are used to process event-based data due to their neuromorphic nature. The proposed work examines the neuromorphic advantage of fusing multiple sensory inputs in classification tasks. Specifically we study the performance of a SNN in digit classification by passing in a visual modality branch (Neuromorphic-MNIST [N-MNIST]) and an auditory modality branch (Spiking Heidelberg Digits [SHD]) from datasets that were created using event-based sensors to generate a series of time-dependent events. It is observed that multi-modal SNNs outperform unimodal visual and unimodal auditory SNNs. Furthermore, it is observed that the process of sensory fusion is insensitive to the depth at which the visual and auditory branches are combined. This work achieves a 98.43% accuracy on the combined N-MNIST and SHD dataset using a multimodal SNN that concatenates the visual and auditory branches at a late depth.",
      "paper_authors": [
        "William Bjorndahl",
        "Jack Easton",
        "Austin Modoff",
        "Eric C. Larson",
        "Joseph Camp",
        "Prasanna Rangarajan"
      ],
      "primary_category": "eess.AS",
      "publish_time": "2024-08-31",
      "update_time": "2024-08-31",
      "comments": "4 pages, 2 figures, submitted to 2025 IEEE International Conference\n  on Acoustics, Speech, and Signal Processing",
      "repo_url": "#"
    },
    "2409.02111": {
      "paper_id": "2409.02111v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.02111v1",
      "paper_key": "2409.02111",
      "paper_title": "Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions",
      "paper_url": "http://arxiv.org/abs/2409.02111v1",
      "paper_abstract": "Deep learning has revolutionized artificial intelligence (AI), achieving remarkable progress in fields such as computer vision, speech recognition, and natural language processing. Moreover, the recent success of large language models (LLMs) has fueled a surge in research on large-scale neural networks. However, the escalating demand for computing resources and energy consumption has prompted the search for energy-efficient alternatives. Inspired by the human brain, spiking neural networks (SNNs) promise energy-efficient computation with event-driven spikes. To provide future directions toward building energy-efficient large SNN models, we present a survey of existing methods for developing deep spiking neural networks, with a focus on emerging Spiking Transformers. Our main contributions are as follows: (1) an overview of learning methods for deep spiking neural networks, categorized by ANN-to-SNN conversion and direct training with surrogate gradients; (2) an overview of network architectures for deep spiking neural networks, categorized by deep convolutional neural networks (DCNNs) and Transformer architecture; and (3) a comprehensive comparison of state-of-the-art deep SNNs with a focus on emerging Spiking Transformers. We then further discuss and outline future directions toward large-scale SNNs.",
      "paper_authors": [
        "Yangfan Hu",
        "Qian Zheng",
        "Guoqi Li",
        "Huajin Tang",
        "Gang Pan"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-08-19",
      "update_time": "2024-08-19",
      "comments": null,
      "repo_url": "#"
    },
    "2409.00044": {
      "paper_id": "2409.00044v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.00044v1",
      "paper_key": "2409.00044",
      "paper_title": "A More Accurate Approximation of Activation Function with Few Spikes Neurons",
      "paper_url": "http://arxiv.org/abs/2409.00044v1",
      "paper_abstract": "Recent deep neural networks (DNNs), such as diffusion models [1], have faced high computational demands. Thus, spiking neural networks (SNNs) have attracted lots of attention as energy-efficient neural networks. However, conventional spiking neurons, such as leaky integrate-and-fire neurons, cannot accurately represent complex non-linear activation functions, such as Swish [2]. To approximate activation functions with spiking neurons, few spikes (FS) neurons were proposed [3], but the approximation performance was limited due to the lack of training methods considering the neurons. Thus, we propose tendency-based parameter initialization (TBPI) to enhance the approximation of activation function with FS neurons, exploiting temporal dependencies initializing the training parameters.",
      "paper_authors": [
        "Dayena Jeong",
        "Jaewoo Park",
        "Jeonghee Jo",
        "Jongkil Park",
        "Jaewook Kim",
        "Hyun Jae Jang",
        "Suyoun Lee",
        "Seongsik Park"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-08-19",
      "update_time": "2024-08-19",
      "comments": "IJCAI Workshop on Human Brain and Artificial Intelligence (HBAI) 2024",
      "repo_url": "#"
    },
    "2409.00021": {
      "paper_id": "2409.00021v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.00021v1",
      "paper_key": "2409.00021",
      "paper_title": "TACOS: Task Agnostic Continual Learning in Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2409.00021v1",
      "paper_abstract": "Catastrophic interference, the loss of previously learned information when learning new information, remains a major challenge in machine learning. Since living organisms do not seem to suffer from this problem, researchers have taken inspiration from biology to improve memory retention in artificial intelligence systems. However, previous attempts to use bio-inspired mechanisms have typically resulted in systems that rely on task boundary information during training and/or explicit task identification during inference, information that is not available in real-world scenarios. Here, we show that neuro-inspired mechanisms such as synaptic consolidation and metaplasticity can mitigate catastrophic interference in a spiking neural network, using only synapse-local information, with no need for task awareness, and with a fixed memory size that does not need to be increased when training on new tasks. Our model, TACOS, combines neuromodulation with complex synaptic dynamics to enable new learning while protecting previous information. We evaluate TACOS on sequential image recognition tasks and demonstrate its effectiveness in reducing catastrophic interference. Our results show that TACOS outperforms existing regularization techniques in domain-incremental learning scenarios. We also report the results of an ablation study to elucidate the contribution of each neuro-inspired mechanism separately.",
      "paper_authors": [
        "Nicholas Soures",
        "Peter Helfer",
        "Anurag Daram",
        "Tej Pandit",
        "Dhireesha Kudithipudi"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-08-16",
      "update_time": "2024-08-16",
      "comments": null,
      "repo_url": "#"
    },
    "2409.03508": {
      "paper_id": "2409.03508v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.03508v1",
      "paper_key": "2409.03508",
      "paper_title": "Revealing Untapped DSP Optimization Potentials for FPGA-Based Systolic Matrix Engines",
      "paper_url": "http://arxiv.org/abs/2409.03508v1",
      "paper_abstract": "Systolic architectures are widely embraced by neural network accelerators for their superior performance in highly parallelized computation. The DSP48E2s serve as dedicated arithmetic blocks in Xilinx Ultrascale series FPGAs and constitute a fundamental component in FPGA-based systolic matrix engines. Harnessing the full potential of DSP48E2s in architectural design can result in significant performance enhancements for systolic architectures on Ultrascale series FPGAs. This paper unveils several previously untapped DSP optimization techniques capable of further enhancing FPGA-based systolic matrix engines. We apply these techniques to two well-known systolic architectures: Google TPUv1 and Xilinx Vitis AI DPU. With the proposed techniques, our design achieves substantial resource and power reduction compared to the open-source TPUv1 FPGA implementation and the Vitis AI DPU implementation in the same parallelism setting. We also demonstrate the applicability of our techniques to neuromorphic hardware for supporting spiking neural network acceleration.",
      "paper_authors": [
        "Jindong Li",
        "Tenglong Li",
        "Guobin Shen",
        "Dongcheng Zhao",
        "Qian Zhang",
        "Yi Zeng"
      ],
      "primary_category": "cs.AR",
      "publish_time": "2024-09-05",
      "update_time": "2024-09-05",
      "comments": "Accepted by FPL2024",
      "repo_url": "#"
    },
    "2409.03368": {
      "paper_id": "2409.03368v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.03368v1",
      "paper_key": "2409.03368",
      "paper_title": "Training-free Conversion of Pretrained ANNs to SNNs for Low-Power and High-Performance Applications",
      "paper_url": "http://arxiv.org/abs/2409.03368v1",
      "paper_abstract": "Spiking Neural Networks (SNNs) have emerged as a promising substitute for Artificial Neural Networks (ANNs) due to their advantages of fast inference and low power consumption. However, the lack of efficient training algorithms has hindered their widespread adoption. Existing supervised learning algorithms for SNNs require significantly more memory and time than their ANN counterparts. Even commonly used ANN-SNN conversion methods necessitate re-training of ANNs to enhance conversion efficiency, incurring additional computational costs. To address these challenges, we propose a novel training-free ANN-SNN conversion pipeline. Our approach directly converts pre-trained ANN models into high-performance SNNs without additional training. The conversion pipeline includes a local-learning-based threshold balancing algorithm, which enables efficient calculation of the optimal thresholds and fine-grained adjustment of threshold value by channel-wise scaling. We demonstrate the scalability of our framework across three typical computer vision tasks: image classification, semantic segmentation, and object detection. This showcases its applicability to both classification and regression tasks. Moreover, we have evaluated the energy consumption of the converted SNNs, demonstrating their superior low-power advantage compared to conventional ANNs. Our training-free algorithm outperforms existing methods, highlighting its practical applicability and efficiency. This approach simplifies the deployment of SNNs by leveraging open-source pre-trained ANN models and neuromorphic hardware, enabling fast, low-power inference with negligible performance reduction.",
      "paper_authors": [
        "Tong Bu",
        "Maohua Li",
        "Zhaofei Yu"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-09-05",
      "update_time": "2024-09-05",
      "comments": null,
      "repo_url": "#"
    },
    "2409.04428": {
      "paper_id": "2409.04428v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.04428v1",
      "paper_key": "2409.04428",
      "paper_title": "Hybrid Spiking Neural Networks for Low-Power Intra-Cortical Brain-Machine Interfaces",
      "paper_url": "http://arxiv.org/abs/2409.04428v1",
      "paper_abstract": "Intra-cortical brain-machine interfaces (iBMIs) have the potential to dramatically improve the lives of people with paraplegia by restoring their ability to perform daily activities. However, current iBMIs suffer from scalability and mobility limitations due to bulky hardware and wiring. Wireless iBMIs offer a solution but are constrained by a limited data rate. To overcome this challenge, we are investigating hybrid spiking neural networks for embedded neural decoding in wireless iBMIs. The networks consist of a temporal convolution-based compression followed by recurrent processing and a final interpolation back to the original sequence length. As recurrent units, we explore gated recurrent units (GRUs), leaky integrate-and-fire (LIF) neurons, and a combination of both - spiking GRUs (sGRUs) and analyze their differences in terms of accuracy, footprint, and activation sparsity. To that end, we train decoders on the \"Nonhuman Primate Reaching with Multichannel Sensorimotor Cortex Electrophysiology\" dataset and evaluate it using the NeuroBench framework, targeting both tracks of the IEEE BioCAS Grand Challenge on Neural Decoding. Our approach achieves high accuracy in predicting velocities of primate reaching movements from multichannel primary motor cortex recordings while maintaining a low number of synaptic operations, surpassing the current baseline models in the NeuroBench framework. This work highlights the potential of hybrid neural networks to facilitate wireless iBMIs with high decoding precision and a substantial increase in the number of monitored neurons, paving the way toward more advanced neuroprosthetic technologies.",
      "paper_authors": [
        "Alexandru Vasilache",
        "Jann Krausse",
        "Klaus Knobloch",
        "Juergen Becker"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-06",
      "update_time": "2024-09-06",
      "comments": "This work has been accepted at the 2024 IEEE Biomedical Circuits and\n  Systems Conference",
      "repo_url": "#"
    },
    "2409.04082": {
      "paper_id": "2409.04082v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.04082v1",
      "paper_key": "2409.04082",
      "paper_title": "SDformerFlow: Spatiotemporal swin spikeformer for event-based optical flow estimation",
      "paper_url": "http://arxiv.org/abs/2409.04082v1",
      "paper_abstract": "Event cameras generate asynchronous and sparse event streams capturing changes in light intensity. They offer significant advantages over conventional frame-based cameras, such as a higher dynamic range and an extremely faster data rate, making them particularly useful in scenarios involving fast motion or challenging lighting conditions. Spiking neural networks (SNNs) share similar asynchronous and sparse characteristics and are well-suited for processing data from event cameras. Inspired by the potential of transformers and spike-driven transformers (spikeformers) in other computer vision tasks, we propose two solutions for fast and robust optical flow estimation for event cameras: STTFlowNet and SDformerFlow. STTFlowNet adopts a U-shaped artificial neural network (ANN) architecture with spatiotemporal shifted window self-attention (swin) transformer encoders, while SDformerFlow presents its fully spiking counterpart, incorporating swin spikeformer encoders. Furthermore, we present two variants of the spiking version with different neuron models. Our work is the first to make use of spikeformers for dense optical flow estimation. We conduct end-to-end training for all models using supervised learning. Our results yield state-of-the-art performance among SNN-based event optical flow methods on both the DSEC and MVSEC datasets, and show significant reduction in power consumption compared to the equivalent ANNs.",
      "paper_authors": [
        "Yi Tian",
        "Juan Andrade-Cetto"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-06",
      "update_time": "2024-09-06",
      "comments": "Under Review",
      "repo_url": "https://github.com/yitian97/SDformerFlow"
    },
    "2409.05726": {
      "paper_id": "2409.05726v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05726v1",
      "paper_key": "2409.05726",
      "paper_title": "Optical Spiking Neurons Enable High-Speed and Energy-Efficient Optical Neural Networks",
      "paper_url": "http://arxiv.org/abs/2409.05726v1",
      "paper_abstract": "Optical neural networks (ONNs) perform extensive computations using photons instead of electrons, resulting in passively energy-efficient and low-latency computing. Among various ONNs, the diffractive optical neural networks (DONNs) particularly excel in energy efficiency, bandwidth, and parallelism, therefore attract considerable attention. However, their performance is limited by the inherent constraints of traditional frame-based sensors, which process and produce dense and redundant information at low operating frequency. Inspired by the spiking neurons in human neural system, which utilize a thresholding mechanism to transmit information sparsely and efficiently, we propose integrating a threshold-locking method into neuromorphic vision sensors to generate sparse and binary information, achieving microsecond-level accurate perception similar to human spiking neurons. By introducing novel Binary Dual Adaptive Training (BAT) and Optically Parallel Mixture of Experts (OPMoE) inference methods, the high-speed, spike-based diffractive optical neural network (S2NN) demonstrates an ultra-fast operating speed of 3649 FPS, which is 30 fold faster than that of reported DONNs, delivering a remarkable computational speed of 417.96 TOPS and a system energy efficiency of 12.6 TOPS/W. Our work demonstrates the potential of incorporating neuromorphic architecture to facilitate optical neural network applications in real-world scenarios for both low-level and high-level machine vision tasks.",
      "paper_authors": [
        "Bo Xu",
        "Zefeng Huang",
        "Yuetong Fang",
        "Xin Wang",
        "Bojun Cheng",
        "Shaoliang Yu",
        "Zhongrui Wang",
        "Renjing Xu"
      ],
      "primary_category": "physics.optics",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": null,
      "repo_url": "#"
    },
    "2409.05610": {
      "paper_id": "2409.05610v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05610v1",
      "paper_key": "2409.05610",
      "paper_title": "SpikingRx: From Neural to Spiking Receiver",
      "paper_url": "http://arxiv.org/abs/2409.05610v1",
      "paper_abstract": "In this work, we propose an energy efficient neuromorphic receiver to replace multiple signal-processing blocks at the receiver by a Spiking Neural Network (SNN) based module, called SpikingRx. We propose a deep convolutional SNN with spike-element-wise ResNet layers which takes a whole OFDM grid compliant with 5G specifications and provides soft outputs for decoded bits that can be used as log-likelihood ratios. We propose to employ the surrogate gradient descent method for training the SpikingRx and focus on its generalizability and robustness to quantization. Moreover, the interpretability of the proposed SpikingRx is studied by a comprehensive ablation study. Our extensive numerical simulations show that SpikingRx is capable of achieving significant block error rate performance gain compared to conventional 5G receivers and similar performance compared to its traditional NN-based counterparts with approximately 9x less energy consumption.",
      "paper_authors": [
        "Ankit Gupta",
        "Onur Dizdar",
        "Yun Chen",
        "Stephen Wang"
      ],
      "primary_category": "cs.IT",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": null,
      "repo_url": "#"
    },
    "2409.05386": {
      "paper_id": "2409.05386v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05386v1",
      "paper_key": "2409.05386",
      "paper_title": "Predictive Coding with Spiking Neural Networks: a Survey",
      "paper_url": "http://arxiv.org/abs/2409.05386v1",
      "paper_abstract": "In this article, we review a class of neuro-mimetic computational models that we place under the label of spiking predictive coding. Specifically, we review the general framework of predictive processing in the context of neurons that emit discrete action potentials, i.e., spikes. Theoretically, we structure our survey around how prediction errors are represented, which results in an organization of historical neuromorphic generalizations that is centered around three broad classes of approaches: prediction errors in explicit groups of error neurons, in membrane potentials, and implicit prediction error encoding. Furthermore, we examine some applications of spiking predictive coding that utilize more energy-efficient, edge-computing hardware platforms. Finally, we highlight important future directions and challenges in this emerging line of inquiry in brain-inspired computing. Building on the prior results of work in computational cognitive neuroscience, machine intelligence, and neuromorphic engineering, we hope that this review of neuromorphic formulations and implementations of predictive coding will encourage and guide future research and development in this emerging research area.",
      "paper_authors": [
        "Antony W. N'dri",
        "William Gebhardt",
        "C\u00e9line Teuli\u00e8re",
        "Fleur Zeldenrust",
        "Rajesh P. N. Rao",
        "Jochen Triesch",
        "Alexander Ororbia"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": null,
      "repo_url": "#"
    },
    "2409.05349": {
      "paper_id": "2409.05349v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05349v1",
      "paper_key": "2409.05349",
      "paper_title": "On the Convergence Analysis of Over-Parameterized Variational Autoencoders: A Neural Tangent Kernel Perspective",
      "paper_url": "http://arxiv.org/abs/2409.05349v1",
      "paper_abstract": "Variational Auto-Encoders (VAEs) have emerged as powerful probabilistic models for generative tasks. However, their convergence properties have not been rigorously proven. The challenge of proving convergence is inherently difficult due to the highly non-convex nature of the training objective and the implementation of a Stochastic Neural Network (SNN) within VAE architectures. This paper addresses these challenges by characterizing the optimization trajectory of SNNs utilized in VAEs through the lens of Neural Tangent Kernel (NTK) techniques. These techniques govern the optimization and generalization behaviors of ultra-wide neural networks. We provide a mathematical proof of VAE convergence under mild assumptions, thus advancing the theoretical understanding of VAE optimization dynamics. Furthermore, we establish a novel connection between the optimization problem faced by over-parameterized SNNs and the Kernel Ridge Regression (KRR) problem. Our findings not only contribute to the theoretical foundation of VAEs but also open new avenues for investigating the optimization of generative models using advanced kernel methods. Our theoretical claims are verified by experimental simulations.",
      "paper_authors": [
        "Li Wang",
        "Wei Huang"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "Accepted by Machine Learning journal",
      "repo_url": "#"
    },
    "2409.04978": {
      "paper_id": "2409.04978v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.04978v1",
      "paper_key": "2409.04978",
      "paper_title": "Time-independent Spiking Neuron via Membrane Potential Estimation for Efficient Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2409.04978v1",
      "paper_abstract": "The computational inefficiency of spiking neural networks (SNNs) is primarily due to the sequential updates of membrane potential, which becomes more pronounced during extended encoding periods compared to artificial neural networks (ANNs). This highlights the need to parallelize SNN computations effectively to leverage available hardware parallelism. To address this, we propose Membrane Potential Estimation Parallel Spiking Neurons (MPE-PSN), a parallel computation method for spiking neurons that enhances computational efficiency by enabling parallel processing while preserving the intrinsic dynamic characteristics of SNNs. Our approach exhibits promise for enhancing computational efficiency, particularly under conditions of elevated neuron density. Empirical experiments demonstrate that our method achieves state-of-the-art (SOTA) accuracy and efficiency on neuromorphic datasets without requiring additional training parameters. Codes are available at~\\url{https://github.com/chrazqee/MPE-PSN}.",
      "paper_authors": [
        "Hanqi Chen",
        "Lixing Yu",
        "Shaojie Zhan",
        "Penghui Yao",
        "Jiankun Shao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-08",
      "comments": null,
      "repo_url": "https://github.com/chrazqee/mpe-psn"
    },
    "2409.07833": {
      "paper_id": "2409.07833v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07833v1",
      "paper_key": "2409.07833",
      "paper_title": "Classifying Images with CoLaNET Spiking Neural Network -- the MNIST Example",
      "paper_url": "http://arxiv.org/abs/2409.07833v1",
      "paper_abstract": "In the present paper, it is shown how the columnar/layered CoLaNET spiking neural network (SNN) architecture can be used in supervised learning image classification tasks. Image pixel brightness is coded by the spike count during image presentation period. Image class label is indicated by activity of special SNN input nodes (one node per class). The CoLaNET classification accuracy is evaluated on the MNIST benchmark. It is demonstrated that CoLaNET is almost as accurate as the most advanced machine learning algorithms (not using convolutional approach).",
      "paper_authors": [
        "Mikhail Kiselev"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "arXiv admin note: substantial text overlap with arXiv:2409.01230",
      "repo_url": "#"
    },
    "2409.07776": {
      "paper_id": "2409.07776v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07776v1",
      "paper_key": "2409.07776",
      "paper_title": "Training Spiking Neural Networks via Augmented Direct Feedback Alignment",
      "paper_url": "http://arxiv.org/abs/2409.07776v1",
      "paper_abstract": "Spiking neural networks (SNNs), the models inspired by the mechanisms of real neurons in the brain, transmit and represent information by employing discrete action potentials or spikes. The sparse, asynchronous properties of information processing make SNNs highly energy efficient, leading to SNNs being promising solutions for implementing neural networks in neuromorphic devices. However, the nondifferentiable nature of SNN neurons makes it a challenge to train them. The current training methods of SNNs that are based on error backpropagation (BP) and precisely designing surrogate gradient are difficult to implement and biologically implausible, hindering the implementation of SNNs on neuromorphic devices. Thus, it is important to train SNNs with a method that is both physically implementatable and biologically plausible. In this paper, we propose using augmented direct feedback alignment (aDFA), a gradient-free approach based on random projection, to train SNNs. This method requires only partial information of the forward process during training, so it is easy to implement and biologically plausible. We systematically demonstrate the feasibility of the proposed aDFA-SNNs scheme, propose its effective working range, and analyze its well-performing settings by employing genetic algorithm. We also analyze the impact of crucial features of SNNs on the scheme, thus demonstrating its superiority and stability over BP and conventional direct feedback alignment. Our scheme can achieve competitive performance without accurate prior knowledge about the utilized system, thus providing a valuable reference for physically training SNNs.",
      "paper_authors": [
        "Yongbo Zhang",
        "Katsuma Inoue",
        "Mitsumasa Nakajima",
        "Toshikazu Hashimoto",
        "Yasuo Kuniyoshi",
        "Kohei Nakajima"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "20 pages, 8 figures, 2 tables",
      "repo_url": "#"
    },
    "2409.08698": {
      "paper_id": "2409.08698v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08698v1",
      "paper_key": "2409.08698",
      "paper_title": "Efficient FPGA Implementation of an Optimized SNN-based DFE for Optical Communications",
      "paper_url": "http://arxiv.org/abs/2409.08698v1",
      "paper_abstract": "The ever-increasing demand for higher data rates in communication systems intensifies the need for advanced non-linear equalizers capable of higher performance. Recently artificial neural networks (ANNs) were introduced as a viable candidate for advanced non-linear equalizers, as they outperform traditional methods. However, they are computationally complex and therefore power hungry. Spiking neural networks (SNNs) started to gain attention as an energy-efficient alternative to ANNs. Recent works proved that they can outperform ANNs at this task. In this work, we explore the design space of an SNN-based decision-feedback equalizer (DFE) to reduce its computational complexity for an efficient implementation on field programmable gate array (FPGA). Our Results prove that it achieves higher communication performance than ANN-based DFE at roughly the same throughput and at 25X higher energy efficiency.",
      "paper_authors": [
        "Mohamed Moursi",
        "Jonas Ney",
        "Bilal Hammoud",
        "Norbert Wehn"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "accepted for publication in IEEE Middle East Conference on\n  Communications and Networking (MECOM 2024). November, 2024",
      "repo_url": "#"
    },
    "2409.08389": {
      "paper_id": "2409.08389v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08389v1",
      "paper_key": "2409.08389",
      "paper_title": "Higher-Order Topological Directionality and Directed Simplicial Neural Networks",
      "paper_url": "http://arxiv.org/abs/2409.08389v1",
      "paper_abstract": "Topological Deep Learning (TDL) has emerged as a paradigm to process and learn from signals defined on higher-order combinatorial topological spaces, such as simplicial or cell complexes. Although many complex systems have an asymmetric relational structure, most TDL models forcibly symmetrize these relationships. In this paper, we first introduce a novel notion of higher-order directionality and we then design Directed Simplicial Neural Networks (Dir-SNNs) based on it. Dir-SNNs are message-passing networks operating on directed simplicial complexes able to leverage directed and possibly asymmetric interactions among the simplices. To our knowledge, this is the first TDL model using a notion of higher-order directionality. We theoretically and empirically prove that Dir-SNNs are more expressive than their directed graph counterpart in distinguishing isomorphic directed graphs. Experiments on a synthetic source localization task demonstrate that Dir-SNNs outperform undirected SNNs when the underlying complex is directed, and perform comparably when the underlying complex is undirected.",
      "paper_authors": [
        "Manuel Lecha",
        "Andrea Cavallo",
        "Francesca Dominici",
        "Elvin Isufi",
        "Claudio Battiloro"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "7 pages, 8 figures, 1 table",
      "repo_url": "https://github.com/andrea-cavallo-98/DirSNN"
    },
    "2409.08290": {
      "paper_id": "2409.08290v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08290v1",
      "paper_key": "2409.08290",
      "paper_title": "Reconsidering the energy efficiency of spiking neural networks",
      "paper_url": "http://arxiv.org/abs/2409.08290v1",
      "paper_abstract": "Spiking neural networks (SNNs) are generally regarded as more energy-efficient because they do not use multiplications. However, most SNN works only consider the counting of additions to evaluate energy consumption, neglecting other overheads such as memory accesses and data movement operations. This oversight can lead to a misleading perception of efficiency, especially when state-of-the-art SNN accelerators operate with very small time window sizes. In this paper, we present a detailed comparison of the energy consumption of artificial neural networks (ANNs) and SNNs from a hardware perspective. We provide accurate formulas for energy consumption based on classical multi-level memory hierarchy architectures, commonly used neuromorphic dataflow architectures, and our proposed improved spatial-dataflow architecture. Our research demonstrates that to achieve comparable accuracy and greater energy efficiency than ANNs, SNNs require strict limitations on both time window size T and sparsity s. For instance, with the VGG16 model and a fixed T of 6, the neuron sparsity rate must exceed 93% to ensure energy efficiency across most architectures. Inspired by our findings, we explore strategies to enhance energy efficiency by increasing sparsity. We introduce two regularization terms during training that constrain weights and activations, effectively boosting the sparsity rate. Our experiments on the CIFAR-10 dataset, using T of 6, show that our SNNs consume 69% of the energy used by optimized ANNs on spatial-dataflow architectures, while maintaining an SNN accuracy of 94.18%. This framework, developed using PyTorch, is publicly available for use and further research.",
      "paper_authors": [
        "Zhanglu Yan",
        "Zhenyu Bai",
        "Weng-Fai Wong"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-08-29",
      "update_time": "2024-08-29",
      "comments": null,
      "repo_url": "#"
    },
    "2409.09196": {
      "paper_id": "2409.09196v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09196v1",
      "paper_key": "2409.09196",
      "paper_title": "Are Sparse Neural Networks Better Hard Sample Learners?",
      "paper_url": "http://arxiv.org/abs/2409.09196v1",
      "paper_abstract": "While deep learning has demonstrated impressive progress, it remains a daunting challenge to learn from hard samples as these samples are usually noisy and intricate. These hard samples play a crucial role in the optimal performance of deep neural networks. Most research on Sparse Neural Networks (SNNs) has focused on standard training data, leaving gaps in understanding their effectiveness on complex and challenging data. This paper's extensive investigation across scenarios reveals that most SNNs trained on challenging samples can often match or surpass dense models in accuracy at certain sparsity levels, especially with limited data. We observe that layer-wise density ratios tend to play an important role in SNN performance, particularly for methods that train from scratch without pre-trained initialization. These insights enhance our understanding of SNNs' behavior and potential for efficient learning approaches in data-centric AI. Our code is publicly available at: \\url{https://github.com/QiaoXiao7282/hard_sample_learners}.",
      "paper_authors": [
        "Qiao Xiao",
        "Boqian Wu",
        "Lu Yin",
        "Christopher Neil Gadzinski",
        "Tianjin Huang",
        "Mykola Pechenizkiy",
        "Decebal Constantin Mocanu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "Accepted at British Machine Vision Conference (BMVC 2024)",
      "repo_url": "#"
    },
    "2409.11263": {
      "paper_id": "2409.11263v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11263v1",
      "paper_key": "2409.11263",
      "paper_title": "Bio-Inspired Mamba: Temporal Locality and Bioplausible Learning in Selective State Space Models",
      "paper_url": "http://arxiv.org/abs/2409.11263v1",
      "paper_abstract": "This paper introduces Bio-Inspired Mamba (BIM), a novel online learning framework for selective state space models that integrates biological learning principles with the Mamba architecture. BIM combines Real-Time Recurrent Learning (RTRL) with Spike-Timing-Dependent Plasticity (STDP)-like local learning rules, addressing the challenges of temporal locality and biological plausibility in training spiking neural networks. Our approach leverages the inherent connection between backpropagation through time and STDP, offering a computationally efficient alternative that maintains the ability to capture long-range dependencies. We evaluate BIM on language modeling, speech recognition, and biomedical signal analysis tasks, demonstrating competitive performance against traditional methods while adhering to biological learning principles. Results show improved energy efficiency and potential for neuromorphic hardware implementation. BIM not only advances the field of biologically plausible machine learning but also provides insights into the mechanisms of temporal information processing in biological neural networks.",
      "paper_authors": [
        "Jiahao Qin"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "17 pages, 1 figure, 2 tables",
      "repo_url": "#"
    },
    "2409.11195": {
      "paper_id": "2409.11195v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11195v1",
      "paper_key": "2409.11195",
      "paper_title": "SDP: Spiking Diffusion Policy for Robotic Manipulation with Learnable Channel-Wise Membrane Thresholds",
      "paper_url": "http://arxiv.org/abs/2409.11195v1",
      "paper_abstract": "This paper introduces a Spiking Diffusion Policy (SDP) learning method for robotic manipulation by integrating Spiking Neurons and Learnable Channel-wise Membrane Thresholds (LCMT) into the diffusion policy model, thereby enhancing computational efficiency and achieving high performance in evaluated tasks. Specifically, the proposed SDP model employs the U-Net architecture as the backbone for diffusion learning within the Spiking Neural Network (SNN). It strategically places residual connections between the spike convolution operations and the Leaky Integrate-and-Fire (LIF) nodes, thereby preventing disruptions to the spiking states. Additionally, we introduce a temporal encoding block and a temporal decoding block to transform static and dynamic data with timestep $T_S$ into each other, enabling the transmission of data within the SNN in spike format. Furthermore, we propose LCMT to enable the adaptive acquisition of membrane potential thresholds, thereby matching the conditions of varying membrane potentials and firing rates across channels and avoiding the cumbersome process of manually setting and tuning hyperparameters. Evaluating the SDP model on seven distinct tasks with SNN timestep $T_S=4$, we achieve results comparable to those of the ANN counterparts, along with faster convergence speeds than the baseline SNN method. This improvement is accompanied by a reduction of 94.3\\% in dynamic energy consumption estimated on 45nm hardware.",
      "paper_authors": [
        "Zhixing Hou",
        "Maoxu Gao",
        "Hang Yu",
        "Mengyu Yang",
        "Chio-In Ieong"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": null,
      "repo_url": "#"
    },
    "2409.10887": {
      "paper_id": "2409.10887v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10887v1",
      "paper_key": "2409.10887",
      "paper_title": "Contrastive Learning in Memristor-based Neuromorphic Systems",
      "paper_url": "http://arxiv.org/abs/2409.10887v1",
      "paper_abstract": "Spiking neural networks, the third generation of artificial neural networks, have become an important family of neuron-based models that sidestep many of the key limitations facing modern-day backpropagation-trained deep networks, including their high energy inefficiency and long-criticized biological implausibility. In this work, we design and investigate a proof-of-concept instantiation of contrastive-signal-dependent plasticity (CSDP), a neuromorphic form of forward-forward-based, backpropagation-free learning. Our experimental simulations demonstrate that a hardware implementation of CSDP is capable of learning simple logic functions without the need to resort to complex gradient calculations.",
      "paper_authors": [
        "Cory Merkel",
        "Alexander Ororbia"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "Accepted in SiPS 2024",
      "repo_url": "#"
    },
    "2409.10532": {
      "paper_id": "2409.10532v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10532v1",
      "paper_key": "2409.10532",
      "paper_title": "Slug Mobile: Test-Bench for RL Testing",
      "paper_url": "http://arxiv.org/abs/2409.10532v1",
      "paper_abstract": "Sim-to real gap in Reinforcement Learning is when a model trained in a simulator does not translate to the real world. This is a problem for Autonomous Vehicles (AVs) as vehicle dynamics can vary from simulation to reality, and also from vehicle to vehicle. Slug Mobile is a one tenth scale autonomous vehicle created to help address the sim-to-real gap for AVs by acting as a test-bench to develop models that can easily scale from one vehicle to another. In addition to traditional sensors found in other one tenth scale AVs, we have also included a Dynamic Vision Sensor so we can train Spiking Neural Networks running on neuromorphic hardware.",
      "paper_authors": [
        "Jonathan Wellington Morris",
        "Vishrut Shah",
        "Alex Besanceney",
        "Daksh Shah",
        "Leilani H. Gilpin"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-08-31",
      "update_time": "2024-08-31",
      "comments": "Submitted to BayLearn 2024",
      "repo_url": "#"
    },
    "2409.11619": {
      "paper_id": "2409.11619v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11619v1",
      "paper_key": "2409.11619",
      "paper_title": "Hyperspectral Image Classification Based on Faster Residual Multi-branch Spiking Neural Network",
      "paper_url": "http://arxiv.org/abs/2409.11619v1",
      "paper_abstract": "Convolutional neural network (CNN) performs well in Hyperspectral Image (HSI) classification tasks, but its high energy consumption and complex network structure make it difficult to directly apply it to edge computing devices. At present, spiking neural networks (SNN) have developed rapidly in HSI classification tasks due to their low energy consumption and event driven characteristics. However, it usually requires a longer time step to achieve optimal accuracy. In response to the above problems, this paper builds a spiking neural network (SNN-SWMR) based on the leaky integrate-and-fire (LIF) neuron model for HSI classification tasks. The network uses the spiking width mixed residual (SWMR) module as the basic unit to perform feature extraction operations. The spiking width mixed residual module is composed of spiking mixed convolution (SMC), which can effectively extract spatial-spectral features. Secondly, this paper designs a simple and efficient arcsine approximate derivative (AAD), which solves the non-differentiable problem of spike firing by fitting the Dirac function. Through AAD, we can directly train supervised spike neural networks. Finally, this paper conducts comparative experiments with multiple advanced HSI classification algorithms based on spiking neural networks on six public hyperspectral data sets. Experimental results show that the AAD function has strong robustness and a good fitting effect. Meanwhile, compared with other algorithms, SNN-SWMR requires a time step reduction of about 84%, training time, and testing time reduction of about 63% and 70% at the same accuracy. This study solves the key problem of SNN based HSI classification algorithms, which has important practical significance for promoting the practical application of HSI classification algorithms in edge devices such as spaceborne and airborne devices.",
      "paper_authors": [
        "Yang Liu",
        "Yahui Li",
        "Rui Li",
        "Liming Zhou",
        "Lanxue Dang",
        "Huiyu Mu",
        "Qiang Ge"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "15pages,12figures",
      "repo_url": "#"
    },
    "2409.11612": {
      "paper_id": "2409.11612v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11612v1",
      "paper_key": "2409.11612",
      "paper_title": "Hardware-Friendly Implementation of Physical Reservoir Computing with CMOS-based Time-domain Analog Spiking Neurons",
      "paper_url": "http://arxiv.org/abs/2409.11612v1",
      "paper_abstract": "This paper introduces an analog spiking neuron that utilizes time-domain information, i.e., a time interval of two signal transitions and a pulse width, to construct a spiking neural network (SNN) for a hardware-friendly physical reservoir computing (RC) on a complementary metal-oxide-semiconductor (CMOS) platform. A neuron with leaky integrate-and-fire is realized by employing two voltage-controlled oscillators (VCOs) with opposite sensitivities to the internal control voltage, and the neuron connection structure is restricted by the use of only 4 neighboring neurons on the 2-dimensional plane to feasibly construct a regular network topology. Such a system enables us to compose an SNN with a counter-based readout circuit, which simplifies the hardware implementation of the SNN. Moreover, another technical advantage thanks to the bottom-up integration is the capability of dynamically capturing every neuron state in the network, which can significantly contribute to finding guidelines on how to enhance the performance for various computational tasks in temporal information processing. Diverse nonlinear physical dynamics needed for RC can be realized by collective behavior through dynamic interaction between neurons, like coupled oscillators, despite the simple network structure. With behavioral system-level simulations, we demonstrate physical RC through short-term memory and exclusive OR tasks, and the spoken digit recognition task with an accuracy of 97.7% as well. Our system is considerably feasible for practical applications and also can be a useful platform for studying the mechanism of physical RC.",
      "paper_authors": [
        "Nanako Kimura",
        "Ckristian Duran",
        "Zolboo Byambadorj",
        "Ryosho Nakane",
        "Tetsuya Iizuka"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": null,
      "repo_url": "#"
    },
    "2409.11567": {
      "paper_id": "2409.11567v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11567v1",
      "paper_key": "2409.11567",
      "paper_title": "Inferno: An Extensible Framework for Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2409.11567v1",
      "paper_abstract": "This paper introduces Inferno, a software library built on top of PyTorch that is designed to meet distinctive challenges of using spiking neural networks (SNNs) for machine learning tasks. We describe the architecture of Inferno and key differentiators that make it uniquely well-suited to these tasks. We show how Inferno supports trainable heterogeneous delays on both CPUs and GPUs, and how Inferno enables a \"write once, apply everywhere\" development methodology for novel models and techniques. We compare Inferno's performance to BindsNET, a library aimed at machine learning with SNNs, and Brian2/Brian2CUDA which is popular in neuroscience. Among several examples, we show how the design decisions made by Inferno facilitate easily implementing the new methods of Nadafian and Ganjtabesh in delay learning with spike-timing dependent plasticity.",
      "paper_authors": [
        "Marissa Dominijanni"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "Source code is available at https://github.com/mdominijanni/inferno\n  and documentation can be viewed at https://docs.inferno-ai.dev/ in a\n  browsable form",
      "repo_url": "https://github.com/mdominijanni/inferno"
    }
  },
  "Infrared Small Target Detection": {
    "2408.09615": {
      "paper_id": "2408.09615v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.09615v1",
      "paper_key": "2408.09615",
      "paper_title": "The First Competition on Resource-Limited Infrared Small Target Detection Challenge: Methods and Results",
      "paper_url": "http://arxiv.org/abs/2408.09615v1",
      "paper_abstract": "In this paper, we briefly summarize the first competition on resource-limited infrared small target detection (namely, LimitIRSTD). This competition has two tracks, including weakly-supervised infrared small target detection (Track 1) and lightweight infrared small target detection (Track 2). 46 and 60 teams successfully registered and took part in Tracks 1 and Track 2, respectively. The top-performing methods and their results in each track are described with details. This competition inspires the community to explore the tough problems in the application of infrared small target detection, and ultimately promote the deployment of this technology under limited resource.",
      "paper_authors": [
        "Boyang Li",
        "Xinyi Ying",
        "Ruojing Li",
        "Yongxian Liu",
        "Yangsi Shi",
        "Miao Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-18",
      "update_time": "2024-08-18",
      "comments": null,
      "repo_url": "https://github.com/xinyiying/basicirstd"
    },
    "2408.08191": {
      "paper_id": "2408.08191v4",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.08191v4",
      "paper_key": "2408.08191",
      "paper_title": "Beyond Full Label: Single-Point Prompt for Infrared Small Target Label Generation",
      "paper_url": "http://arxiv.org/abs/2408.08191v4",
      "paper_abstract": "In this work, we make the first attempt to construct a learning-based single-point annotation paradigm for infrared small target label generation (IRSTLG). Our intuition is that label generation requires just one more point prompt than target detection: IRSTLG can be regarded as an infrared small target detection (IRSTD) task with the target location hint. Based on this insight, we introduce an energy double guided single-point prompt (EDGSP) framework, which adeptly transforms the target detection network into a refined label generation method. Specifically, the proposed EDGSP includes: 1) target energy initialization (TEI) to create a foundational outline for sufficient shape evolution of pseudo label, 2) double prompt embedding (DPE) for rapid localization of interested regions and reinforcement of individual differences to avoid label adhesion, and 3) bounding box-based matching (BBM) to eliminate false alarms. Experimental results show that pseudo labels generated by three baselines equipped with EDGSP achieve 100% object-level probability of detection (Pd) and 0% false-alarm rate (Fa) on SIRST, NUDT-SIRST, and IRSTD-1k datasets, with a pixel-level intersection over union (IoU) improvement of 13.28% over state-of-the-art (SOTA) label generation methods. In the practical application of downstream IRSTD, EDGSP realizes, for the first time, a single-point generated pseudo mask beyond the full label. Even with coarse single-point annotations, it still achieves 99.5% performance of full labeling.",
      "paper_authors": [
        "Shuai Yuan",
        "Hanlin Qin",
        "Renke Kou",
        "Xiang Yan",
        "Zechuan Li",
        "Chenxu Peng",
        "Abd-Krim Seghouane"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-15",
      "update_time": "2024-08-20",
      "comments": null,
      "repo_url": "https://github.com/xdfai/edgsp"
    },
    "2408.03717": {
      "paper_id": "2408.03717v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.03717v1",
      "paper_key": "2408.03717",
      "paper_title": "Pick of the Bunch: Detecting Infrared Small Targets Beyond Hit-Miss Trade-Offs via Selective Rank-Aware Attention",
      "paper_url": "http://arxiv.org/abs/2408.03717v1",
      "paper_abstract": "Infrared small target detection faces the inherent challenge of precisely localizing dim targets amidst complex background clutter. Traditional approaches struggle to balance detection precision and false alarm rates. To break this dilemma, we propose SeRankDet, a deep network that achieves high accuracy beyond the conventional hit-miss trade-off, by following the ``Pick of the Bunch'' principle. At its core lies our Selective Rank-Aware Attention (SeRank) module, employing a non-linear Top-K selection process that preserves the most salient responses, preventing target signal dilution while maintaining constant complexity. Furthermore, we replace the static concatenation typical in U-Net structures with our Large Selective Feature Fusion (LSFF) module, a dynamic fusion strategy that empowers SeRankDet with adaptive feature integration, enhancing its ability to discriminate true targets from false alarms. The network's discernment is further refined by our Dilated Difference Convolution (DDC) module, which merges differential convolution aimed at amplifying subtle target characteristics with dilated convolution to expand the receptive field, thereby substantially improving target-background separation. Despite its lightweight architecture, the proposed SeRankDet sets new benchmarks in state-of-the-art performance across multiple public datasets. The code is available at https://github.com/GrokCV/SeRankDet.",
      "paper_authors": [
        "Yimian Dai",
        "Peiwen Pan",
        "Yulei Qian",
        "Yuxuan Li",
        "Xiang Li",
        "Jian Yang",
        "Huan Wan"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-07",
      "update_time": "2024-08-07",
      "comments": null,
      "repo_url": "https://github.com/grokcv/serankdet"
    },
    "2408.02780": {
      "paper_id": "2408.02780v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.02780v1",
      "paper_key": "2408.02780",
      "paper_title": "LR-Net: A Lightweight and Robust Network for Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2408.02780v1",
      "paper_abstract": "Limited by equipment limitations and the lack of target intrinsic features, existing infrared small target detection methods have difficulty meeting actual comprehensive performance requirements. Therefore, we propose an innovative lightweight and robust network (LR-Net), which abandons the complex structure and achieves an effective balance between detection accuracy and resource consumption. Specifically, to ensure the lightweight and robustness, on the one hand, we construct a lightweight feature extraction attention (LFEA) module, which can fully extract target features and strengthen information interaction across channels. On the other hand, we construct a simple refined feature transfer (RFT) module. Compared with direct cross-layer connections, the RFT module can improve the network's feature refinement extraction capability with little resource consumption. Meanwhile, to solve the problem of small target loss in high-level feature maps, on the one hand, we propose a low-level feature distribution (LFD) strategy to use low-level features to supplement the information of high-level features. On the other hand, we introduce an efficient simplified bilinear interpolation attention module (SBAM) to promote the guidance constraints of low-level features on high-level features and the fusion of the two. In addition, We abandon the traditional resizing method and adopt a new training and inference cropping strategy, which is more robust to datasets with multi-scale samples. Extensive experimental results show that our LR-Net achieves state-of-the-art (SOTA) performance. Notably, on the basis of the proposed LR-Net, we achieve 3rd place in the \"ICPR 2024 Resource-Limited Infrared Small Target Detection Challenge Track 2: Lightweight Infrared Small Target Detection\".",
      "paper_authors": [
        "Chuang Yu",
        "Yunpeng Liu",
        "Jinmiao Zhao",
        "Zelin Shi"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-05",
      "update_time": "2024-08-05",
      "comments": null,
      "repo_url": "#"
    },
    "2408.02773": {
      "paper_id": "2408.02773v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.02773v1",
      "paper_key": "2408.02773",
      "paper_title": "Refined Infrared Small Target Detection Scheme with Single-Point Supervision",
      "paper_url": "http://arxiv.org/abs/2408.02773v1",
      "paper_abstract": "Recently, infrared small target detection with single-point supervision has attracted extensive attention. However, the detection accuracy of existing methods has difficulty meeting actual needs. Therefore, we propose an innovative refined infrared small target detection scheme with single-point supervision, which has excellent segmentation accuracy and detection rate. Specifically, we introduce label evolution with single point supervision (LESPS) framework and explore the performance of various excellent infrared small target detection networks based on this framework. Meanwhile, to improve the comprehensive performance, we construct a complete post-processing strategy. On the one hand, to improve the segmentation accuracy, we use a combination of test-time augmentation (TTA) and conditional random field (CRF) for post-processing. On the other hand, to improve the detection rate, we introduce an adjustable sensitivity (AS) strategy for post-processing, which fully considers the advantages of multiple detection results and reasonably adds some areas with low confidence to the fine segmentation image in the form of centroid points. In addition, to further improve the performance and explore the characteristics of this task, on the one hand, we construct and find that a multi-stage loss is helpful for fine-grained detection. On the other hand, we find that a reasonable sliding window cropping strategy for test samples has better performance for actual multi-size samples. Extensive experimental results show that the proposed scheme achieves state-of-the-art (SOTA) performance. Notably, the proposed scheme won the third place in the \"ICPR 2024 Resource-Limited Infrared Small Target Detection Challenge Track 1: Weakly Supervised Infrared Small Target Detection\".",
      "paper_authors": [
        "Jinmiao Zhao",
        "Zelin Shi",
        "Chuang Yu",
        "Yunpeng Liu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-05",
      "update_time": "2024-08-05",
      "comments": null,
      "repo_url": "#"
    },
    "2408.01976": {
      "paper_id": "2408.01976v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.01976v2",
      "paper_key": "2408.01976",
      "paper_title": "Single-Point Supervised High-Resolution Dynamic Network for Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2408.01976v2",
      "paper_abstract": "Infrared small target detection (IRSTD) tasks are extremely challenging for two main reasons: 1) it is difficult to obtain accurate labelling information that is critical to existing methods, and 2) infrared (IR) small target information is easily lost in deep networks. To address these issues, we propose a single-point supervised high-resolution dynamic network (SSHD-Net). In contrast to existing methods, we achieve state-of-the-art (SOTA) detection performance using only single-point supervision. Specifically, we first design a high-resolution cross-feature extraction module (HCEM), that achieves bi-directional feature interaction through stepped feature cascade channels (SFCC). It balances network depth and feature resolution to maintain deep IR small-target information. Secondly, the effective integration of global and local features is achieved through the dynamic coordinate fusion module (DCFM), which enhances the anti-interference ability in complex backgrounds. In addition, we introduce the high-resolution multilevel residual module (HMRM) to enhance the semantic information extraction capability. Finally, we design the adaptive target localization detection head (ATLDH) to improve detection accuracy. Experiments on the publicly available datasets NUDT-SIRST and IRSTD-1k demonstrate the effectiveness of our method. Compared to other SOTA methods, our method can achieve better detection performance with only a single point of supervision.",
      "paper_authors": [
        "Jing Wu",
        "Rixiang Ni",
        "Feng Huang",
        "Zhaobing Qiu",
        "Liqiong Chen",
        "Changhai Luo",
        "Yunxiang Li",
        "Youli Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-04",
      "update_time": "2024-08-08",
      "comments": null,
      "repo_url": "#"
    },
    "2407.20090": {
      "paper_id": "2407.20090v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.20090v1",
      "paper_key": "2407.20090",
      "paper_title": "Infrared Small Target Detection based on Adjustable Sensitivity Strategy and Multi-Scale Fusion",
      "paper_url": "http://arxiv.org/abs/2407.20090v1",
      "paper_abstract": "Recently, deep learning-based single-frame infrared small target (SIRST) detection technology has made significant progress. However, existing infrared small target detection methods are often optimized for a fixed image resolution, a single wavelength, or a specific imaging system, limiting their breadth and flexibility in practical applications. Therefore, we propose a refined infrared small target detection scheme based on an adjustable sensitivity (AS) strategy and multi-scale fusion. Specifically, a multi-scale model fusion framework based on multi-scale direction-aware network (MSDA-Net) is constructed, which uses input images of multiple scales to train multiple models and fuses them. Multi-scale fusion helps characterize the shape, edge, and texture features of the target from different scales, making the model more accurate and reliable in locating the target. At the same time, we fully consider the characteristics of the infrared small target detection task and construct an edge enhancement difficulty mining (EEDM) loss. The EEDM loss helps alleviate the problem of category imbalance and guides the network to pay more attention to difficult target areas and edge features during training. In addition, we propose an adjustable sensitivity strategy for post-processing. This strategy significantly improves the detection rate of infrared small targets while ensuring segmentation accuracy. Extensive experimental results show that the proposed scheme achieves the best performance. Notably, this scheme won the first prize in the PRCV 2024 wide-area infrared small target detection competition.",
      "paper_authors": [
        "Jinmiao Zhao",
        "Zelin Shi",
        "Chuang Yu",
        "Yunpeng Liu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-29",
      "update_time": "2024-07-29",
      "comments": null,
      "repo_url": "#"
    },
    "2407.20078": {
      "paper_id": "2407.20078v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.20078v1",
      "paper_key": "2407.20078",
      "paper_title": "Background Semantics Matter: Cross-Task Feature Exchange Network for Clustered Infrared Small Target Detection With Sky-Annotated Dataset",
      "paper_url": "http://arxiv.org/abs/2407.20078v1",
      "paper_abstract": "Infrared small target detection poses unique challenges due to the scarcity of intrinsic target features and the abundance of similar background distractors. We argue that background semantics play a pivotal role in distinguishing visually similar objects for this task. To address this, we introduce a new task -- clustered infrared small target detection, and present DenseSIRST, a novel benchmark dataset that provides per-pixel semantic annotations for background regions, enabling the transition from sparse to dense target detection. Leveraging this dataset, we propose the Background-Aware Feature Exchange Network (BAFE-Net), which transforms the detection paradigm from a single task focused on the foreground to a multi-task architecture that jointly performs target detection and background semantic segmentation. BAFE-Net introduces a cross-task feature hard-exchange mechanism to embed target and background semantics between the two tasks. Furthermore, we propose the Background-Aware Gaussian Copy-Paste (BAG-CP) method, which selectively pastes small targets into sky regions during training, avoiding the creation of false alarm targets in complex non-sky backgrounds. Extensive experiments validate the effectiveness of BAG-CP and BAFE-Net in improving target detection accuracy while reducing false alarms. The DenseSIRST dataset, code, and trained models are available at https://github.com/GrokCV/BAFE-Net.",
      "paper_authors": [
        "Yimian Dai",
        "Mengxuan Xiao",
        "Yiming Zhu",
        "Huan Wang",
        "Kehua Guo",
        "Jian Yang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-29",
      "update_time": "2024-07-29",
      "comments": null,
      "repo_url": "https://github.com/grokcv/bafe-net"
    },
    "2407.15369": {
      "paper_id": "2407.15369v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.15369v1",
      "paper_key": "2407.15369",
      "paper_title": "Sparse Prior Is Not All You Need: When Differential Directionality Meets Saliency Coherence for Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2407.15369v1",
      "paper_abstract": "Infrared small target detection is crucial for the efficacy of infrared search and tracking systems. Current tensor decomposition methods emphasize representing small targets with sparsity but struggle to separate targets from complex backgrounds due to insufficient use of intrinsic directional information and reduced target visibility during decomposition. To address these challenges, this study introduces a Sparse Differential Directionality prior (SDD) framework. SDD leverages the distinct directional characteristics of targets to differentiate them from the background, applying mixed sparse constraints on the differential directional images and continuity difference matrix of the temporal component, both derived from Tucker decomposition. We further enhance target detectability with a saliency coherence strategy that intensifies target contrast against the background during hierarchical decomposition. A Proximal Alternating Minimization-based (PAM) algorithm efficiently solves our proposed model. Experimental results on several real-world datasets validate our method's effectiveness, outperforming ten state-of-the-art methods in target detection and clutter suppression. Our code is available at https://github.com/GrokCV/SDD.",
      "paper_authors": [
        "Fei Zhou",
        "Maixia Fu",
        "Yulei Qian",
        "Jian Yang",
        "Yimian Dai"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-22",
      "update_time": "2024-07-22",
      "comments": "Submitted to IEEE TIM, Minor Revision",
      "repo_url": "https://github.com/grokcv/sdd"
    },
    "2407.07520": {
      "paper_id": "2407.07520v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.07520v1",
      "paper_key": "2407.07520",
      "paper_title": "IRSAM: Advancing Segment Anything Model for Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2407.07520v1",
      "paper_abstract": "The recent Segment Anything Model (SAM) is a significant advancement in natural image segmentation, exhibiting potent zero-shot performance suitable for various downstream image segmentation tasks. However, directly utilizing the pretrained SAM for Infrared Small Target Detection (IRSTD) task falls short in achieving satisfying performance due to a notable domain gap between natural and infrared images. Unlike a visible light camera, a thermal imager reveals an object's temperature distribution by capturing infrared radiation. Small targets often show a subtle temperature transition at the object's boundaries. To address this issue, we propose the IRSAM model for IRSTD, which improves SAM's encoder-decoder architecture to learn better feature representation of infrared small objects. Specifically, we design a Perona-Malik diffusion (PMD)-based block and incorporate it into multiple levels of SAM's encoder to help it capture essential structural features while suppressing noise. Additionally, we devise a Granularity-Aware Decoder (GAD) to fuse the multi-granularity feature from the encoder to capture structural information that may be lost in long-distance modeling. Extensive experiments on the public datasets, including NUAA-SIRST, NUDT-SIRST, and IRSTD-1K, validate the design choice of IRSAM and its significant superiority over representative state-of-the-art methods. The source code are available at: github.com/IPIC-Lab/IRSAM.",
      "paper_authors": [
        "Mingjin Zhang",
        "Yuchun Wang",
        "Jie Guo",
        "Yunsong Li",
        "Xinbo Gao",
        "Jing Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-10",
      "update_time": "2024-07-10",
      "comments": "18 pages, 8 figures, to be published in ECCV2024",
      "repo_url": "https://github.com/ipic-lab/irsam"
    },
    "2406.13445": {
      "paper_id": "2406.13445v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.13445v1",
      "paper_key": "2406.13445",
      "paper_title": "Lost in UNet: Improving Infrared Small Target Detection by Underappreciated Local Features",
      "paper_url": "http://arxiv.org/abs/2406.13445v1",
      "paper_abstract": "Many targets are often very small in infrared images due to the long-distance imaging meachnism. UNet and its variants, as popular detection backbone networks, downsample the local features early and cause the irreversible loss of these local features, leading to both the missed and false detection of small targets in infrared images. We propose HintU, a novel network to recover the local features lost by various UNet-based methods for effective infrared small target detection. HintU has two key contributions. First, it introduces the \"Hint\" mechanism for the first time, i.e., leveraging the prior knowledge of target locations to highlight critical local features. Second, it improves the mainstream UNet-based architecture to preserve target pixels even after downsampling. HintU can shift the focus of various networks (e.g., vanilla UNet, UNet++, UIUNet, MiM+, and HCFNet) from the irrelevant background pixels to a more restricted area from the beginning. Experimental results on three datasets NUDT-SIRST, SIRSTv2 and IRSTD1K demonstrate that HintU enhances the performance of existing methods with only an additional 1.88 ms cost (on RTX Titan). Additionally, the explicit constraints of HintU enhance the generalization ability of UNet-based methods. Code is available at https://github.com/Wuzhou-Quan/HintU.",
      "paper_authors": [
        "Wuzhou Quan",
        "Wei Zhao",
        "Weiming Wang",
        "Haoran Xie",
        "Fu Lee Wang",
        "Mingqiang Wei"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-19",
      "update_time": "2024-06-19",
      "comments": null,
      "repo_url": "https://github.com/wuzhou-quan/hintu"
    },
    "2406.06949": {
      "paper_id": "2406.06949v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.06949v2",
      "paper_key": "2406.06949",
      "paper_title": "Triple-domain Feature Learning with Frequency-aware Memory Enhancement for Moving Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2406.06949v2",
      "paper_abstract": "As a sub-field of object detection, moving infrared small target detection presents significant challenges due to tiny target sizes and low contrast against backgrounds. Currently-existing methods primarily rely on the features extracted only from spatio-temporal domain. Frequency domain has hardly been concerned yet, although it has been widely applied in image processing. To extend feature source domains and enhance feature representation, we propose a new Triple-domain Strategy (Tridos) with the frequency-aware memory enhancement on spatio-temporal domain for infrared small target detection. In this scheme, it effectively detaches and enhances frequency features by a local-global frequency-aware module with Fourier transform. Inspired by human visual system, our memory enhancement is designed to capture the spatial relations of infrared targets among video frames. Furthermore, it encodes temporal dynamics motion features via differential learning and residual enhancing. Additionally, we further design a residual compensation to reconcile possible cross-domain feature mismatches. To our best knowledge, proposed Tridos is the first work to explore infrared target feature learning comprehensively in spatio-temporal-frequency domains. The extensive experiments on three datasets (i.e., DAUB, ITSDT-15K and IRDST) validate that our triple-domain infrared feature learning scheme could often be obviously superior to state-of-the-art ones. Source codes are available at https://github.com/UESTC-nnLab/Tridos.",
      "paper_authors": [
        "Weiwei Duan",
        "Luping Ji",
        "Shengjia Chen",
        "Sicheng Zhu",
        "Mao Ye"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-11",
      "update_time": "2024-09-05",
      "comments": "This paper has accepted IEEE TGRS",
      "repo_url": "https://github.com/uestc-nnlab/tridos"
    },
    "2406.02037": {
      "paper_id": "2406.02037v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.02037v1",
      "paper_key": "2406.02037",
      "paper_title": "Multi-Scale Direction-Aware Network for Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2406.02037v1",
      "paper_abstract": "Infrared small target detection faces the problem that it is difficult to effectively separate the background and the target. Existing deep learning-based methods focus on appearance features and ignore high-frequency directional features. Therefore, we propose a multi-scale direction-aware network (MSDA-Net), which is the first attempt to integrate the high-frequency directional features of infrared small targets as domain prior knowledge into neural networks. Specifically, an innovative multi-directional feature awareness (MDFA) module is constructed, which fully utilizes the prior knowledge of targets and emphasizes the focus on high-frequency directional features. On this basis, combined with the multi-scale local relation learning (MLRL) module, a multi-scale direction-aware (MSDA) module is further constructed. The MSDA module promotes the full extraction of local relations at different scales and the full perception of key features in different directions. Meanwhile, a high-frequency direction injection (HFDI) module without training parameters is constructed to inject the high-frequency directional information of the original image into the network. This helps guide the network to pay attention to detailed information such as target edges and shapes. In addition, we propose a feature aggregation (FA) structure that aggregates multi-level features to solve the problem of small targets disappearing in deep feature maps. Furthermore, a lightweight feature alignment fusion (FAF) module is constructed, which can effectively alleviate the pixel offset existing in multi-level feature map fusion. Extensive experimental results show that our MSDA-Net achieves state-of-the-art (SOTA) results on the public NUDT-SIRST, SIRST and IRSTD-1k datasets.",
      "paper_authors": [
        "Jinmiao Zhao",
        "Zelin Shi",
        "Chuang Yu",
        "Yunpeng Liu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-04",
      "update_time": "2024-06-04",
      "comments": null,
      "repo_url": "#"
    },
    "2406.00632": {
      "paper_id": "2406.00632v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.00632v1",
      "paper_key": "2406.00632",
      "paper_title": "Diff-Mosaic: Augmenting Realistic Representations in Infrared Small Target Detection via Diffusion Prior",
      "paper_url": "http://arxiv.org/abs/2406.00632v1",
      "paper_abstract": "Recently, researchers have proposed various deep learning methods to accurately detect infrared targets with the characteristics of indistinct shape and texture. Due to the limited variety of infrared datasets, training deep learning models with good generalization poses a challenge. To augment the infrared dataset, researchers employ data augmentation techniques, which often involve generating new images by combining images from different datasets. However, these methods are lacking in two respects. In terms of realism, the images generated by mixup-based methods lack realism and are difficult to effectively simulate complex real-world scenarios. In terms of diversity, compared with real-world scenes, borrowing knowledge from another dataset inherently has a limited diversity. Currently, the diffusion model stands out as an innovative generative approach. Large-scale trained diffusion models have a strong generative prior that enables real-world modeling of images to generate diverse and realistic images. In this paper, we propose Diff-Mosaic, a data augmentation method based on the diffusion model. This model effectively alleviates the challenge of diversity and realism of data augmentation methods via diffusion prior. Specifically, our method consists of two stages. Firstly, we introduce an enhancement network called Pixel-Prior, which generates highly coordinated and realistic Mosaic images by harmonizing pixels. In the second stage, we propose an image enhancement strategy named Diff-Prior. This strategy utilizes diffusion priors to model images in the real-world scene, further enhancing the diversity and realism of the images. Extensive experiments have demonstrated that our approach significantly improves the performance of the detection network. The code is available at https://github.com/YupeiLin2388/Diff-Mosaic",
      "paper_authors": [
        "Yukai Shi",
        "Yupei Lin",
        "Pengxu Wei",
        "Xiaoyu Xian",
        "Tianshui Chen",
        "Liang Lin"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-02",
      "update_time": "2024-06-02",
      "comments": null,
      "repo_url": "https://github.com/yupeilin2388/diff-mosaic"
    },
    "2403.19366": {
      "paper_id": "2403.19366v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.19366v1",
      "paper_key": "2403.19366",
      "paper_title": "Infrared Small Target Detection with Scale and Location Sensitivity",
      "paper_url": "http://arxiv.org/abs/2403.19366v1",
      "paper_abstract": "Recently, infrared small target detection (IRSTD) has been dominated by deep-learning-based methods. However, these methods mainly focus on the design of complex model structures to extract discriminative features, leaving the loss functions for IRSTD under-explored. For example, the widely used Intersection over Union (IoU) and Dice losses lack sensitivity to the scales and locations of targets, limiting the detection performance of detectors. In this paper, we focus on boosting detection performance with a more effective loss but a simpler model structure. Specifically, we first propose a novel Scale and Location Sensitive (SLS) loss to handle the limitations of existing losses: 1) for scale sensitivity, we compute a weight for the IoU loss based on target scales to help the detector distinguish targets with different scales: 2) for location sensitivity, we introduce a penalty term based on the center points of targets to help the detector localize targets more precisely. Then, we design a simple Multi-Scale Head to the plain U-Net (MSHNet). By applying SLS loss to each scale of the predictions, our MSHNet outperforms existing state-of-the-art methods by a large margin. In addition, the detection performance of existing detectors can be further improved when trained with our SLS loss, demonstrating the effectiveness and generalization of our SLS loss. The code is available at https://github.com/ying-fu/MSHNet.",
      "paper_authors": [
        "Qiankun Liu",
        "Rui Liu",
        "Bolun Zheng",
        "Hongkui Wang",
        "Ying Fu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-03-28",
      "update_time": "2024-03-28",
      "comments": "Accepted by CVPR 2024",
      "repo_url": "https://github.com/ying-fu/mshnet"
    },
    "2403.08380": {
      "paper_id": "2403.08380v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.08380v1",
      "paper_key": "2403.08380",
      "paper_title": "Mitigate Target-level Insensitivity of Infrared Small Target Detection via Posterior Distribution Modeling",
      "paper_url": "http://arxiv.org/abs/2403.08380v1",
      "paper_abstract": "Infrared Small Target Detection (IRSTD) aims to segment small targets from infrared clutter background. Existing methods mainly focus on discriminative approaches, i.e., a pixel-level front-background binary segmentation. Since infrared small targets are small and low signal-to-clutter ratio, empirical risk has few disturbances when a certain false alarm and missed detection exist, which seriously affect the further improvement of such methods. Motivated by the dense prediction generative methods, in this paper, we propose a diffusion model framework for Infrared Small Target Detection which compensates pixel-level discriminant with mask posterior distribution modeling. Furthermore, we design a Low-frequency Isolation in the wavelet domain to suppress the interference of intrinsic infrared noise on the diffusion noise estimation. This transition from the discriminative paradigm to generative one enables us to bypass the target-level insensitivity. Experiments show that the proposed method achieves competitive performance gains over state-of-the-art methods on NUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets. Code are available at https://github.com/Li-Haoqing/IRSTD-Diff.",
      "paper_authors": [
        "Haoqing Li",
        "Jinfu Yang",
        "Yifei Xu",
        "Runshi Wang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-03-13",
      "update_time": "2024-03-13",
      "comments": null,
      "repo_url": "https://github.com/li-haoqing/irstd-diff"
    },
    "2403.05416": {
      "paper_id": "2403.05416v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.05416v1",
      "paper_key": "2403.05416",
      "paper_title": "SIRST-5K: Exploring Massive Negatives Synthesis with Self-supervised Learning for Robust Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2403.05416v1",
      "paper_abstract": "Single-frame infrared small target (SIRST) detection aims to recognize small targets from clutter backgrounds. Recently, convolutional neural networks have achieved significant advantages in general object detection. With the development of Transformer, the scale of SIRST models is constantly increasing. Due to the limited training samples, performance has not been improved accordingly. The quality, quantity, and diversity of the infrared dataset are critical to the detection of small targets. To highlight this issue, we propose a negative sample augmentation method in this paper. Specifically, a negative augmentation approach is proposed to generate massive negatives for self-supervised learning. Firstly, we perform a sequential noise modeling technology to generate realistic infrared data. Secondly, we fuse the extracted noise with the original data to facilitate diversity and fidelity in the generated data. Lastly, we proposed a negative augmentation strategy to enrich diversity as well as maintain semantic invariance. The proposed algorithm produces a synthetic SIRST-5K dataset, which contains massive pseudo-data and corresponding labels. With a rich diversity of infrared small target data, our algorithm significantly improves the model performance and convergence speed. Compared with other state-of-the-art (SOTA) methods, our method achieves outstanding performance in terms of probability of detection (Pd), false-alarm rate (Fa), and intersection over union (IoU).",
      "paper_authors": [
        "Yahao Lu",
        "Yupei Lin",
        "Han Wu",
        "Xiaoyu Xian",
        "Yukai Shi",
        "Liang Lin"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-03-08",
      "update_time": "2024-03-08",
      "comments": "We address the quality, quantity, and diversity of the infrared data\n  in SIRST, the dataset is available at: https://github.com/luy0222/SIRST-5K",
      "repo_url": "https://github.com/luy0222/sirst-5k"
    },
    "2403.02148": {
      "paper_id": "2403.02148v4",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.02148v4",
      "paper_key": "2403.02148",
      "paper_title": "MiM-ISTD: Mamba-in-Mamba for Efficient Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2403.02148v4",
      "paper_abstract": "Recently, infrared small target detection (ISTD) has made significant progress, thanks to the development of basic models. Specifically, the models combining CNNs with transformers can successfully extract both local and global features. However, the disadvantage of the transformer is also inherited, i.e., the quadratic computational complexity to sequence length. Inspired by the recent basic model with linear complexity for long-distance modeling, Mamba, we explore the potential of this state space model for ISTD task in terms of effectiveness and efficiency in the paper. However, directly applying Mamba achieves suboptimal performances due to the insufficient harnessing of local features, which are imperative for detecting small targets. Instead, we tailor a nested structure, Mamba-in-Mamba (MiM-ISTD), for efficient ISTD. It consists of Outer and Inner Mamba blocks to adeptly capture both global and local features. Specifically, we treat the local patches as \"visual sentences\" and use the Outer Mamba to explore the global information. We then decompose each visual sentence into sub-patches as \"visual words\" and use the Inner Mamba to further explore the local information among words in the visual sentence with negligible computational costs. By aggregating the visual word and visual sentence features, our MiM-ISTD can effectively explore both global and local information. Experiments on NUAA-SIRST and IRSTD-1k show the superior accuracy and efficiency of our method. Specifically, MiM-ISTD is $8 \\times$ faster than the SOTA method and reduces GPU memory usage by 62.2$\\%$ when testing on $2048 \\times 2048$ images, overcoming the computation and memory constraints on high-resolution infrared images.",
      "paper_authors": [
        "Tianxiang Chen",
        "Zi Ye",
        "Zhentao Tan",
        "Tao Gong",
        "Yue Wu",
        "Qi Chu",
        "Bin Liu",
        "Nenghai Yu",
        "Jieping Ye"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-03-04",
      "update_time": "2024-06-24",
      "comments": "The first Mamba-based model for infrared small target detection",
      "repo_url": "https://github.com/txchen-ustc/mim-istd"
    },
    "2402.18003": {
      "paper_id": "2402.18003v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.18003v1",
      "paper_key": "2402.18003",
      "paper_title": "Infrared Small Target Detection via tensor $L_{2,1}$ norm minimization and ASSTV regularization: A Novel Tensor Recovery Approach",
      "paper_url": "http://arxiv.org/abs/2402.18003v1",
      "paper_abstract": "In recent years, there has been a noteworthy focus on infrared small target detection, given its vital importance in processing signals from infrared remote sensing. The considerable computational cost incurred by prior methods, relying excessively on nuclear norm for noise separation, necessitates the exploration of efficient alternatives. The aim of this research is to identify a swift and resilient tensor recovery method for the efficient extraction of infrared small targets from image sequences. Theoretical validation indicates that smaller singular values predominantly contribute to constructing noise information. In the exclusion process, tensor QR decomposition is employed to reasonably reduce the size of the target tensor. Subsequently, we address a tensor $L_{2,1}$ Norm Minimization via T-QR (TLNMTQR) based method to effectively isolate the noise, markedly improving computational speed without compromising accuracy. Concurrently, by integrating the asymmetric spatial-temporal total variation regularization method (ASSTV), our objective is to augment the flexibility and efficacy of our algorithm in handling time series data. Ultimately, our method underwent rigorous testing with real-world data, affirmatively showcasing the superiority of our algorithm in terms of speed, precision, and robustness.",
      "paper_authors": [
        "Jiqian Zhao",
        "An-Bao Xu"
      ],
      "primary_category": "math.NA",
      "publish_time": "2024-02-28",
      "update_time": "2024-02-28",
      "comments": "11 pages, 38 figures",
      "repo_url": "#"
    },
    "2402.05410": {
      "paper_id": "2402.05410v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.05410v1",
      "paper_key": "2402.05410",
      "paper_title": "SpirDet: Towards Efficient, Accurate and Lightweight Infrared Small Target Detector",
      "paper_url": "http://arxiv.org/abs/2402.05410v1",
      "paper_abstract": "In recent years, the detection of infrared small targets using deep learning methods has garnered substantial attention due to notable advancements. To improve the detection capability of small targets, these methods commonly maintain a pathway that preserves high-resolution features of sparse and tiny targets. However, it can result in redundant and expensive computations. To tackle this challenge, we propose SpirDet, a novel approach for efficient detection of infrared small targets. Specifically, to cope with the computational redundancy issue, we employ a new dual-branch sparse decoder to restore the feature map. Firstly, the fast branch directly predicts a sparse map indicating potential small target locations (occupying only 0.5\\% area of the map). Secondly, the slow branch conducts fine-grained adjustments at the positions indicated by the sparse map. Additionally, we design an lightweight DO-RepEncoder based on reparameterization with the Downsampling Orthogonality, which can effectively reduce memory consumption and inference latency. Extensive experiments show that the proposed SpirDet significantly outperforms state-of-the-art models while achieving faster inference speed and fewer parameters. For example, on the IRSTD-1K dataset, SpirDet improves $MIoU$ by 4.7 and has a $7\\times$ $FPS$ acceleration compared to the previous state-of-the-art model. The code will be open to the public.",
      "paper_authors": [
        "Qianchen Mao",
        "Qiang Li",
        "Bingshu Wang",
        "Yongjun Zhang",
        "Tao Dai",
        "C. L. Philip Chen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-02-08",
      "update_time": "2024-02-08",
      "comments": null,
      "repo_url": "#"
    },
    "2402.02288": {
      "paper_id": "2402.02288v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.02288v1",
      "paper_key": "2402.02288",
      "paper_title": "$\\textit{A Contrario}$ Paradigm for YOLO-based Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2402.02288v1",
      "paper_abstract": "Detecting small to tiny targets in infrared images is a challenging task in computer vision, especially when it comes to differentiating these targets from noisy or textured backgrounds. Traditional object detection methods such as YOLO struggle to detect tiny objects compared to segmentation neural networks, resulting in weaker performance when detecting small targets. To reduce the number of false alarms while maintaining a high detection rate, we introduce an $\\textit{a contrario}$ decision criterion into the training of a YOLO detector. The latter takes advantage of the $\\textit{unexpectedness}$ of small targets to discriminate them from complex backgrounds. Adding this statistical criterion to a YOLOv7-tiny bridges the performance gap between state-of-the-art segmentation methods for infrared small target detection and object detection networks. It also significantly increases the robustness of YOLO towards few-shot settings.",
      "paper_authors": [
        "Alina Ciocarlan",
        "Sylvie Le H\u00e9garat-Mascle",
        "Sidonie Lefebvre",
        "Arnaud Woiselle",
        "Clara Barbanson"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-02-03",
      "update_time": "2024-02-03",
      "comments": "Accepted to ICASSP 2024",
      "repo_url": "#"
    },
    "2402.02046": {
      "paper_id": "2402.02046v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.02046v1",
      "paper_key": "2402.02046",
      "paper_title": "TCI-Former: Thermal Conduction-Inspired Transformer for Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2402.02046v1",
      "paper_abstract": "Infrared small target detection (ISTD) is critical to national security and has been extensively applied in military areas. ISTD aims to segment small target pixels from background. Most ISTD networks focus on designing feature extraction blocks or feature fusion modules, but rarely describe the ISTD process from the feature map evolution perspective. In the ISTD process, the network attention gradually shifts towards target areas. We abstract this process as the directional movement of feature map pixels to target areas through convolution, pooling and interactions with surrounding pixels, which can be analogous to the movement of thermal particles constrained by surrounding variables and particles. In light of this analogy, we propose Thermal Conduction-Inspired Transformer (TCI-Former) based on the theoretical principles of thermal conduction. According to thermal conduction differential equation in heat dynamics, we derive the pixel movement differential equation (PMDE) in the image domain and further develop two modules: Thermal Conduction-Inspired Attention (TCIA) and Thermal Conduction Boundary Module (TCBM). TCIA incorporates finite difference method with PMDE to reach a numerical approximation so that target body features can be extracted. To further remove errors in boundary areas, TCBM is designed and supervised by boundary masks to refine target body features with fine boundary details. Experiments on IRSTD-1k and NUAA-SIRST demonstrate the superiority of our method.",
      "paper_authors": [
        "Tianxiang Chen",
        "Zhentao Tan",
        "Qi Chu",
        "Yue Wu",
        "Bin Liu",
        "Nenghai Yu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-02-03",
      "update_time": "2024-02-03",
      "comments": null,
      "repo_url": "#"
    },
    "2401.15583": {
      "paper_id": "2401.15583v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.15583v3",
      "paper_key": "2401.15583",
      "paper_title": "SCTransNet: Spatial-channel Cross Transformer Network for Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2401.15583v3",
      "paper_abstract": "Infrared small target detection (IRSTD) has recently benefitted greatly from U-shaped neural models. However, largely overlooking effective global information modeling, existing techniques struggle when the target has high similarities with the background. We present a Spatial-channel Cross Transformer Network (SCTransNet) that leverages spatial-channel cross transformer blocks (SCTBs) on top of long-range skip connections to address the aforementioned challenge. In the proposed SCTBs, the outputs of all encoders are interacted with cross transformer to generate mixed features, which are redistributed to all decoders to effectively reinforce semantic differences between the target and clutter at full scales. Specifically, SCTB contains the following two key elements: (a) spatial-embedded single-head channel-cross attention (SSCA) for exchanging local spatial features and full-level global channel information to eliminate ambiguity among the encoders and facilitate high-level semantic associations of the images, and (b) a complementary feed-forward network (CFN) for enhancing the feature discriminability via a multi-scale strategy and cross-spatial-channel information interaction to promote beneficial information transfer. Our SCTransNet effectively encodes the semantic differences between targets and backgrounds to boost its internal representation for detecting small infrared targets accurately. Extensive experiments on three public datasets, NUDT-SIRST, NUAA-SIRST, and IRSTD-1k, demonstrate that the proposed SCTransNet outperforms existing IRSTD methods. Our code will be made public at https://github.com/xdFai.",
      "paper_authors": [
        "Shuai Yuan",
        "Hanlin Qin",
        "Xiang Yan",
        "Naveed AKhtar",
        "Ajmal Mian"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-01-28",
      "update_time": "2024-04-30",
      "comments": null,
      "repo_url": "https://github.com/xdfai/sctransnet"
    },
    "2401.15578": {
      "paper_id": "2401.15578v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.15578v2",
      "paper_key": "2401.15578",
      "paper_title": "ASCNet: Asymmetric Sampling Correction Network for Infrared Image Destriping",
      "paper_url": "http://arxiv.org/abs/2401.15578v2",
      "paper_abstract": "In a real-world infrared imaging system, effectively learning a consistent stripe noise removal model is essential. Most existing destriping methods cannot precisely reconstruct images due to cross-level semantic gaps and insufficient characterization of the global column features. To tackle this problem, we propose a novel infrared image destriping method, called Asymmetric Sampling Correction Network (ASCNet), that can effectively capture global column relationships and embed them into a U-shaped framework, providing comprehensive discriminative representation and seamless semantic connectivity. Our ASCNet consists of three core elements: Residual Haar Discrete Wavelet Transform (RHDWT), Pixel Shuffle (PS), and Column Non-uniformity Correction Module (CNCM). Specifically, RHDWT is a novel downsampler that employs double-branch modeling to effectively integrate stripe-directional prior knowledge and data-driven semantic interaction to enrich the feature representation. Observing the semantic patterns crosstalk of stripe noise, PS is introduced as an upsampler to prevent excessive apriori decoding and performing semantic-bias-free image reconstruction. After each sampling, CNCM captures the column relationships in long-range dependencies. By incorporating column, spatial, and self-dependence information, CNCM well establishes a global context to distinguish stripes from the scene's vertical structures. Extensive experiments on synthetic data, real data, and infrared small target detection tasks demonstrate that the proposed method outperforms state-of-the-art single-image destriping methods both visually and quantitatively. Our code will be made publicly available at https://github.com/xdFai/ASCNet.",
      "paper_authors": [
        "Shuai Yuan",
        "Hanlin Qin",
        "Xiang Yan",
        "Shiqi Yang",
        "Shuowen Yang",
        "Naveed Akhtar"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-01-28",
      "update_time": "2024-06-04",
      "comments": null,
      "repo_url": "https://github.com/xdfai/ascnet"
    },
    "2311.08747": {
      "paper_id": "2311.08747v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.08747v3",
      "paper_key": "2311.08747",
      "paper_title": "Improved Dense Nested Attention Network Based on Transformer for Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2311.08747v3",
      "paper_abstract": "Infrared small target detection based on deep learning offers unique advantages in separating small targets from complex and dynamic backgrounds. However, the features of infrared small targets gradually weaken as the depth of convolutional neural network (CNN) increases. To address this issue, we propose a novel method for detecting infrared small targets called improved dense nested attention network (IDNANet), which is based on the transformer architecture. We preserve the dense nested structure of dense nested attention network (DNANet) and introduce the Swin-transformer during feature extraction stage to enhance the continuity of features. Furthermore, we integrate the ACmix attention structure into the dense nested structure to enhance the features of intermediate layers. Additionally, we design a weighted dice binary cross-entropy (WD-BCE) loss function to mitigate the negative impact of foreground-background imbalance in the samples. Moreover, we develop a dataset specifically for infrared small targets, called BIT-SIRST. The dataset comprises a significant amount of real-world targets and manually annotated labels, as well as synthetic data and corresponding labels. We have evaluated the effectiveness of our method through experiments conducted on public datasets. In comparison to other state-of-the-art methods, our approach outperforms in terms of probability of detection ($P_d$), false-alarm rate ($F_a$), and mean intersection of union ($mIoU$). The $mIoU$ reaches 90.89\\% on the NUDT-SIRST dataset and 79.72\\% on the SIRST dataset. The BIT-SIRST dataset and codes are available openly at \\href{https://github.com/EdwardBao1006/bit\\_sirst}{\\color[HTML]{B22222}{https://github.com/EdwardBao1006/bit\\_sirst}}.",
      "paper_authors": [
        "Chun Bao",
        "Jie Cao",
        "Yaqian Ning",
        "Tianhua Zhao",
        "Zhijun Li",
        "Zechen Wang",
        "Li Zhang",
        "Qun Hao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-15",
      "update_time": "2024-01-17",
      "comments": null,
      "repo_url": "https://github.com/edwardbao1006/bit_sirst"
    },
    "2311.00917": {
      "paper_id": "2311.00917v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.00917v1",
      "paper_key": "2311.00917",
      "paper_title": "RPCANet: Deep Unfolding RPCA Based Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2311.00917v1",
      "paper_abstract": "Deep learning (DL) networks have achieved remarkable performance in infrared small target detection (ISTD). However, these structures exhibit a deficiency in interpretability and are widely regarded as black boxes, as they disregard domain knowledge in ISTD. To alleviate this issue, this work proposes an interpretable deep network for detecting infrared dim targets, dubbed RPCANet. Specifically, our approach formulates the ISTD task as sparse target extraction, low-rank background estimation, and image reconstruction in a relaxed Robust Principle Component Analysis (RPCA) model. By unfolding the iterative optimization updating steps into a deep-learning framework, time-consuming and complex matrix calculations are replaced by theory-guided neural networks. RPCANet detects targets with clear interpretability and preserves the intrinsic image feature, instead of directly transforming the detection task into a matrix decomposition problem. Extensive experiments substantiate the effectiveness of our deep unfolding framework and demonstrate its trustworthy results, surpassing baseline methods in both qualitative and quantitative evaluations.",
      "paper_authors": [
        "Fengyi Wu",
        "Tianfang Zhang",
        "Lei Li",
        "Yian Huang",
        "Zhenming Peng"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-02",
      "update_time": "2023-11-02",
      "comments": "WACV2024",
      "repo_url": "https://github.com/fengyiwu98/rpcanet"
    },
    "2310.12562": {
      "paper_id": "2310.12562v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.12562v1",
      "paper_key": "2310.12562",
      "paper_title": "Click on Mask: A Labor-efficient Annotation Framework with Level Set for Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2310.12562v1",
      "paper_abstract": "Infrared Small Target Detection is a challenging task to separate small targets from infrared clutter background. Recently, deep learning paradigms have achieved promising results. However, these data-driven methods need plenty of manual annotation. Due to the small size of infrared targets, manual annotation consumes more resources and restricts the development of this field. This letter proposed a labor-efficient and cursory annotation framework with level set, which obtains a high-quality pseudo mask with only one cursory click. A variational level set formulation with an expectation difference energy functional is designed, in which the zero level contour is intrinsically maintained during the level set evolution. It solves the issue that zero level contour disappearing due to small target size and excessive regularization. Experiments on the NUAA-SIRST and IRSTD-1k datasets reveal that our approach achieves superior performance. Code is available at https://github.com/Li-Haoqing/COM.",
      "paper_authors": [
        "Haoqing Li",
        "Jinfu Yang",
        "Yifei Xu",
        "Runshi Wang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-10-19",
      "update_time": "2023-10-19",
      "comments": "4 pages, 5 figures, references added",
      "repo_url": "https://github.com/li-haoqing/com"
    },
    "2310.05347": {
      "paper_id": "2310.05347v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.05347v1",
      "paper_key": "2310.05347",
      "paper_title": "Infrared Small Target Detection Using Double-Weighted Multi-Granularity Patch Tensor Model With Tensor-Train Decomposition",
      "paper_url": "http://arxiv.org/abs/2310.05347v1",
      "paper_abstract": "Infrared small target detection plays an important role in the remote sensing fields. Therefore, many detection algorithms have been proposed, in which the infrared patch-tensor (IPT) model has become a mainstream tool due to its excellent performance. However, most IPT-based methods face great challenges, such as inaccurate measure of the tensor low-rankness and poor robustness to complex scenes, which will leadto poor detection performance. In order to solve these problems, this paper proposes a novel double-weighted multi-granularity infrared patch tensor (DWMGIPT) model. First, to capture different granularity information of tensor from multiple modes, a multi-granularity infrared patch tensor (MGIPT) model is constructed by collecting nonoverlapping patches and tensor augmentation based on the tensor train (TT) decomposition. Second, to explore the latent structure of tensor more efficiently, we utilize the auto-weighted mechanism to balance the importance of information at different granularity. Then, the steering kernel (SK) is employed to extract local structure prior, which suppresses background interference such as strong edges and noise. Finally, an efficient optimization algorithm based on the alternating direction method of multipliers (ADMM) is presented to solve the model. Extensive experiments in various challenging scenes show that the proposed algorithm is robust to noise and different scenes. Compared with the other eight state-of-the-art methods, different evaluation metrics demonstrate that our method achieves better detection performance in various complex scenes.",
      "paper_authors": [
        "Guiyu Zhang",
        "Qunbo Lv",
        "Zui Tao",
        "Baoyu Zhu",
        "Zheng Tan",
        "Yuan Ma"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-10-09",
      "update_time": "2023-10-09",
      "comments": null,
      "repo_url": "#"
    },
    "2309.13646": {
      "paper_id": "2309.13646v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.13646v1",
      "paper_key": "2309.13646",
      "paper_title": "ILNet: Low-level Matters for Salient Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2309.13646v1",
      "paper_abstract": "Infrared small target detection is a technique for finding small targets from infrared clutter background. Due to the dearth of high-level semantic information, small infrared target features are weakened in the deep layers of the CNN, which underachieves the CNN's representation ability. To address the above problem, in this paper, we propose an infrared low-level network (ILNet) that considers infrared small targets as salient areas with little semantic information. Unlike other SOTA methods, ILNet pays greater attention to low-level information instead of treating them equally. A new lightweight feature fusion module, named Interactive Polarized Orthogonal Fusion module (IPOF), is proposed, which integrates more important low-level features from the shallow layers into the deep layers. A Dynamic One-Dimensional Aggregation layers (DODA) are inserted into the IPOF, to dynamically adjust the aggregation of low dimensional information according to the number of input channels. In addition, the idea of ensemble learning is used to design a Representative Block (RB) to dynamically allocate weights for shallow and deep layers. Experimental results on the challenging NUAA-SIRST (78.22% nIoU and 1.33e-6 Fa) and IRSTD-1K (68.91% nIoU and 3.23e-6 Fa) dataset demonstrate that the proposed ILNet can get better performances than other SOTA methods. Moreover, ILNet can obtain a greater improvement with the increasement of data volume. Training code are available at https://github.com/Li-Haoqing/ILNet.",
      "paper_authors": [
        "Haoqing Li",
        "Jinfu Yang",
        "Runshi Wang",
        "Yifei Xu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-09-24",
      "update_time": "2023-09-24",
      "comments": null,
      "repo_url": "https://github.com/li-haoqing/ilnet"
    },
    "2309.01099": {
      "paper_id": "2309.01099v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.01099v1",
      "paper_key": "2309.01099",
      "paper_title": "Enhancing Infrared Small Target Detection Robustness with Bi-Level Adversarial Framework",
      "paper_url": "http://arxiv.org/abs/2309.01099v1",
      "paper_abstract": "The detection of small infrared targets against blurred and cluttered backgrounds has remained an enduring challenge. In recent years, learning-based schemes have become the mainstream methodology to establish the mapping directly. However, these methods are susceptible to the inherent complexities of changing backgrounds and real-world disturbances, leading to unreliable and compromised target estimations. In this work, we propose a bi-level adversarial framework to promote the robustness of detection in the presence of distinct corruptions. We first propose a bi-level optimization formulation to introduce dynamic adversarial learning. Specifically, it is composited by the learnable generation of corruptions to maximize the losses as the lower-level objective and the robustness promotion of detectors as the upper-level one. We also provide a hierarchical reinforced learning strategy to discover the most detrimental corruptions and balance the performance between robustness and accuracy. To better disentangle the corruptions from salient features, we also propose a spatial-frequency interaction network for target detection. Extensive experiments demonstrate our scheme remarkably improves 21.96% IOU across a wide array of corruptions and notably promotes 4.97% IOU on the general benchmark. The source codes are available at https://github.com/LiuZhu-CV/BALISTD.",
      "paper_authors": [
        "Zhu Liu",
        "Zihang Chen",
        "Jinyuan Liu",
        "Long Ma",
        "Xin Fan",
        "Risheng Liu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-09-03",
      "update_time": "2023-09-03",
      "comments": "9 pages, 6 figures",
      "repo_url": "#"
    },
    "2307.14723": {
      "paper_id": "2307.14723v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.14723v2",
      "paper_key": "2307.14723",
      "paper_title": "EFLNet: Enhancing Feature Learning for Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2307.14723v2",
      "paper_abstract": "Single-frame infrared small target detection is considered to be a challenging task, due to the extreme imbalance between target and background, bounding box regression is extremely sensitive to infrared small target, and target information is easy to lose in the high-level semantic layer. In this article, we propose an enhancing feature learning network (EFLNet) to address these problems. First, we notice that there is an extremely imbalance between the target and the background in the infrared image, which makes the model pay more attention to the background features rather than target features. To address this problem, we propose a new adaptive threshold focal loss (ATFL) function that decouples the target and the background, and utilizes the adaptive mechanism to adjust the loss weight to force the model to allocate more attention to target features. Second, we introduce the normalized Gaussian Wasserstein distance (NWD) to alleviate the difficulty of convergence caused by the extreme sensitivity of the bounding box regression to infrared small target. Finally, we incorporate a dynamic head mechanism into the network to enable adaptive learning of the relative importance of each semantic layer. Experimental results demonstrate our method can achieve better performance in the detection performance of infrared small target compared to the state-of-the-art (SOTA) deep-learning-based methods. The source codes and bounding box annotated datasets are available at https://github.com/YangBo0411/infrared-small-target.",
      "paper_authors": [
        "Bo Yang",
        "Xinyu Zhang",
        "Jian Zhang",
        "Jun Luo",
        "Mingliang Zhou",
        "Yangjun Pi"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-07-27",
      "update_time": "2024-02-27",
      "comments": null,
      "repo_url": "https://github.com/yangbo0411/infrared-small-target"
    },
    "2304.04442": {
      "paper_id": "2304.04442v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2304.04442v1",
      "paper_key": "2304.04442",
      "paper_title": "Monte Carlo Linear Clustering with Single-Point Supervision is Enough for Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2304.04442v1",
      "paper_abstract": "Single-frame infrared small target (SIRST) detection aims at separating small targets from clutter backgrounds on infrared images. Recently, deep learning based methods have achieved promising performance on SIRST detection, but at the cost of a large amount of training data with expensive pixel-level annotations. To reduce the annotation burden, we propose the first method to achieve SIRST detection with single-point supervision. The core idea of this work is to recover the per-pixel mask of each target from the given single point label by using clustering approaches, which looks simple but is indeed challenging since targets are always insalient and accompanied with background clutters. To handle this issue, we introduce randomness to the clustering process by adding noise to the input images, and then obtain much more reliable pseudo masks by averaging the clustered results. Thanks to this \"Monte Carlo\" clustering approach, our method can accurately recover pseudo masks and thus turn arbitrary fully supervised SIRST detection networks into weakly supervised ones with only single point annotation. Experiments on four datasets demonstrate that our method can be applied to existing SIRST detection networks to achieve comparable performance with their fully supervised counterparts, which reveals that single-point supervision is strong enough for SIRST detection. Our code will be available at: https://github.com/YeRen123455/SIRST-Single-Point-Supervision.",
      "paper_authors": [
        "Boyang Li",
        "Yingqian Wang",
        "Longguang Wang",
        "Fei Zhang",
        "Ting Liu",
        "Zaiping Lin",
        "Wei An",
        "Yulan Guo"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-04-10",
      "update_time": "2023-04-10",
      "comments": null,
      "repo_url": "https://github.com/yeren123455/sirst-single-point-supervision"
    },
    "2304.01484": {
      "paper_id": "2304.01484v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2304.01484v2",
      "paper_key": "2304.01484",
      "paper_title": "Mapping Degeneration Meets Label Evolution: Learning Infrared Small Target Detection with Single Point Supervision",
      "paper_url": "http://arxiv.org/abs/2304.01484v2",
      "paper_abstract": "Training a convolutional neural network (CNN) to detect infrared small targets in a fully supervised manner has gained remarkable research interests in recent years, but is highly labor expensive since a large number of per-pixel annotations are required. To handle this problem, in this paper, we make the first attempt to achieve infrared small target detection with point-level supervision. Interestingly, during the training phase supervised by point labels, we discover that CNNs first learn to segment a cluster of pixels near the targets, and then gradually converge to predict groundtruth point labels. Motivated by this \"mapping degeneration\" phenomenon, we propose a label evolution framework named label evolution with single point supervision (LESPS) to progressively expand the point label by leveraging the intermediate predictions of CNNs. In this way, the network predictions can finally approximate the updated pseudo labels, and a pixel-level target mask can be obtained to train CNNs in an end-to-end manner. We conduct extensive experiments with insightful visualizations to validate the effectiveness of our method. Experimental results show that CNNs equipped with LESPS can well recover the target masks from corresponding point labels, {and can achieve over 70% and 95% of their fully supervised performance in terms of pixel-level intersection over union (IoU) and object-level probability of detection (Pd), respectively. Code is available at https://github.com/XinyiYing/LESPS.",
      "paper_authors": [
        "Xinyi Ying",
        "Li Liu",
        "Yingqian Wang",
        "Ruojing Li",
        "Nuo Chen",
        "Zaiping Lin",
        "Weidong Sheng",
        "Shilin Zhou"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-04-04",
      "update_time": "2024-08-23",
      "comments": null,
      "repo_url": "https://github.com/xinyiying/lesps"
    },
    "2303.10321": {
      "paper_id": "2303.10321v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2303.10321v1",
      "paper_key": "2303.10321",
      "paper_title": "ABC: Attention with Bilinear Correlation for Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2303.10321v1",
      "paper_abstract": "Infrared small target detection (ISTD) has a wide range of applications in early warning, rescue, and guidance. However, CNN based deep learning methods are not effective at segmenting infrared small target (IRST) that it lack of clear contour and texture features, and transformer based methods also struggle to achieve significant results due to the absence of convolution induction bias. To address these issues, we propose a new model called attention with bilinear correlation (ABC), which is based on the transformer architecture and includes a convolution linear fusion transformer (CLFT) module with a novel attention mechanism for feature extraction and fusion, which effectively enhances target features and suppresses noise. Additionally, our model includes a u-shaped convolution-dilated convolution (UCDC) module located deeper layers of the network, which takes advantage of the smaller resolution of deeper features to obtain finer semantic information. Experimental results on public datasets demonstrate that our approach achieves state-of-the-art performance. Code is available at https://github.com/PANPEIWEN/ABC",
      "paper_authors": [
        "Peiwen Pan",
        "Huan Wang",
        "Chenyi Wang",
        "Chang Nie"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-03-18",
      "update_time": "2023-03-18",
      "comments": null,
      "repo_url": "https://github.com/panpeiwen/abc"
    },
    "2301.04497": {
      "paper_id": "2301.04497v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2301.04497v2",
      "paper_key": "2301.04497",
      "paper_title": "Dynamic Background Reconstruction via MAE for Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2301.04497v2",
      "paper_abstract": "Infrared small target detection (ISTD) under complex backgrounds is a difficult problem, for the differences between targets and backgrounds are not easy to distinguish. Background reconstruction is one of the methods to deal with this problem. This paper proposes an ISTD method based on background reconstruction called Dynamic Background Reconstruction (DBR). DBR consists of three modules: a dynamic shift window module (DSW), a background reconstruction module (BR), and a detection head (DH). BR takes advantage of Vision Transformers in reconstructing missing patches and adopts a grid masking strategy with a masking ratio of 50\\% to reconstruct clean backgrounds without targets. To avoid dividing one target into two neighboring patches, resulting in reconstructing failure, DSW is performed before input embedding. DSW calculates offsets, according to which infrared images dynamically shift. To reduce False Positive (FP) cases caused by regarding reconstruction errors as targets, DH utilizes a structure of densely connected Transformer to further improve the detection performance. Experimental results show that DBR achieves the best F1-score on the two ISTD datasets, MFIRST (64.10\\%) and SIRST (75.01\\%).",
      "paper_authors": [
        "Jingchao Peng",
        "Haitao Zhao",
        "Kaijie Zhao",
        "Zhongze Wang",
        "Lujian Yao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-01-11",
      "update_time": "2023-04-09",
      "comments": null,
      "repo_url": "#"
    },
    "2301.03796": {
      "paper_id": "2301.03796v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2301.03796v2",
      "paper_key": "2301.03796",
      "paper_title": "Enhancing Evaluation Methods for Infrared Small-Target Detection in Real-world Scenarios",
      "paper_url": "http://arxiv.org/abs/2301.03796v2",
      "paper_abstract": "Infrared small target detection (IRSTD) poses a significant challenge in the field of computer vision. While substantial efforts have been made over the past two decades to improve the detection capabilities of IRSTD algorithms, there has been a lack of extensive investigation into the evaluation metrics used for assessing their performance. In this paper, we employ a systematic approach to address this issue by first evaluating the effectiveness of existing metrics and then proposing new metrics to overcome the limitations of conventional ones. To achieve this, we carefully analyze the necessary conditions for successful detection and identify the shortcomings of current evaluation metrics, including both pre-thresholding and post-thresholding metrics. We then introduce new metrics that are designed to align with the requirements of real-world systems. Furthermore, we utilize these newly proposed metrics to compare and evaluate the performance of five widely recognized small infrared target detection algorithms. The results demonstrate that the new metrics provide consistent and meaningful quantitative assessments, aligning with qualitative observations.",
      "paper_authors": [
        "Saed Moradi",
        "Alireza Memarmoghadam",
        "Payman Moallem",
        "Mohamad Farzan Sabahi"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-01-10",
      "update_time": "2024-08-25",
      "comments": null,
      "repo_url": "#"
    },
    "2212.08472": {
      "paper_id": "2212.08472v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2212.08472v2",
      "paper_key": "2212.08472",
      "paper_title": "One-Stage Cascade Refinement Networks for Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2212.08472v2",
      "paper_abstract": "Single-frame InfraRed Small Target (SIRST) detection has been a challenging task due to a lack of inherent characteristics, imprecise bounding box regression, a scarcity of real-world datasets, and sensitive localization evaluation. In this paper, we propose a comprehensive solution to these challenges. First, we find that the existing anchor-free label assignment method is prone to mislabeling small targets as background, leading to their omission by detectors. To overcome this issue, we propose an all-scale pseudo-box-based label assignment scheme that relaxes the constraints on scale and decouples the spatial assignment from the size of the ground-truth target. Second, motivated by the structured prior of feature pyramids, we introduce the one-stage cascade refinement network (OSCAR), which uses the high-level head as soft proposals for the low-level refinement head. This allows OSCAR to process the same target in a cascade coarse-to-fine manner. Finally, we present a new research benchmark for infrared small target detection, consisting of the SIRST-V2 dataset of real-world, high-resolution single-frame targets, the normalized contrast evaluation metric, and the DeepInfrared toolkit for detection. We conduct extensive ablation studies to evaluate the components of OSCAR and compare its performance to state-of-the-art model-driven and data-driven methods on the SIRST-V2 benchmark. Our results demonstrate that a top-down cascade refinement framework can improve the accuracy of infrared small target detection without sacrificing efficiency. The DeepInfrared toolkit, dataset, and trained models are available at https://github.com/YimianDai/open-deepinfrared to advance further research in this field.",
      "paper_authors": [
        "Yimian Dai",
        "Xiang Li",
        "Fei Zhou",
        "Yulei Qian",
        "Yaohong Chen",
        "Jian Yang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-12-16",
      "update_time": "2022-12-31",
      "comments": "Submitted to TGRS",
      "repo_url": "https://github.com/yimiandai/open-deepinfrared"
    },
    "2210.16561": {
      "paper_id": "2210.16561v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2210.16561v2",
      "paper_key": "2210.16561",
      "paper_title": "iSmallNet: Densely Nested Network with Label Decoupling for Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2210.16561v2",
      "paper_abstract": "Small targets are often submerged in cluttered backgrounds of infrared images. Conventional detectors tend to generate false alarms, while CNN-based detectors lose small targets in deep layers. To this end, we propose iSmallNet, a multi-stream densely nested network with label decoupling for infrared small object detection. On the one hand, to fully exploit the shape information of small targets, we decouple the original labeled ground-truth (GT) map into an interior map and a boundary one. The GT map, in collaboration with the two additional maps, tackles the unbalanced distribution of small object boundaries. On the other hand, two key modules are delicately designed and incorporated into the proposed network to boost the overall performance. First, to maintain small targets in deep layers, we develop a multi-scale nested interaction module to explore a wide range of context information. Second, we develop an interior-boundary fusion module to integrate multi-granularity information. Experiments on NUAA-SIRST and NUDT-SIRST clearly show the superiority of iSmallNet over 11 state-of-the-art detectors.",
      "paper_authors": [
        "Zhiheng Hu",
        "Yongzhen Wang",
        "Peng Li",
        "Jie Qin",
        "Haoran Xie",
        "Mingqiang Wei"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-10-29",
      "update_time": "2023-06-29",
      "comments": null,
      "repo_url": "#"
    },
    "2209.13780": {
      "paper_id": "2209.13780v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2209.13780v2",
      "paper_key": "2209.13780",
      "paper_title": "CourtNet for Infrared Small-Target Detection",
      "paper_url": "http://arxiv.org/abs/2209.13780v2",
      "paper_abstract": "Infrared small-target detection (ISTD) is an important computer vision task. ISTD aims at separating small targets from complex background clutter. The infrared radiation decays over distances, making the targets highly dim and prone to confusion with the background clutter, which makes the detector challenging to balance the precision and recall rate. To deal with this difficulty, this paper proposes a neural-network-based ISTD method called CourtNet, which has three sub-networks: the prosecution network is designed for improving the recall rate; the defendant network is devoted to increasing the precision rate; the jury network weights their results to adaptively balance the precision and recall rate. Furthermore, the prosecution network utilizes a densely connected transformer structure, which can prevent small targets from disappearing in the network forward propagation. In addition, a fine-grained attention module is adopted to accurately locate the small targets. Experimental results show that CourtNet achieves the best F1-score on the two ISTD datasets, MFIRST (0.62) and SIRST (0.73).",
      "paper_authors": [
        "Jingchao Peng",
        "Haitao Zhao",
        "Kaijie Zhao",
        "Zhongze Wang",
        "Lujian Yao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-09-28",
      "update_time": "2023-04-15",
      "comments": null,
      "repo_url": "#"
    },
    "2206.06923": {
      "paper_id": "2206.06923v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2206.06923v1",
      "paper_key": "2206.06923",
      "paper_title": "A Multi-task Framework for Infrared Small Target Detection and Segmentation",
      "paper_url": "http://arxiv.org/abs/2206.06923v1",
      "paper_abstract": "Due to the complicated background and noise of infrared images, infrared small target detection is one of the most difficult problems in the field of computer vision. In most existing studies, semantic segmentation methods are typically used to achieve better results. The centroid of each target is calculated from the segmentation map as the detection result. In contrast, we propose a novel end-to-end framework for infrared small target detection and segmentation in this paper. First, with the use of UNet as the backbone to maintain resolution and semantic information, our model can achieve a higher detection accuracy than other state-of-the-art methods by attaching a simple anchor-free head. Then, a pyramid pool module is used to further extract features and improve the precision of target segmentation. Next, we use semantic segmentation tasks that pay more attention to pixel-level features to assist in the training process of object detection, which increases the average precision and allows the model to detect some targets that were previously not detectable. Furthermore, we develop a multi-task framework for infrared small target detection and segmentation. Our multi-task learning model reduces complexity by nearly half and speeds up inference by nearly twice compared to the composite single-task model, while maintaining accuracy. The code and models are publicly available at https://github.com/Chenastron/MTUNet.",
      "paper_authors": [
        "Yuhang Chen",
        "Liyuan Li",
        "Xin Liu",
        "Xiaofeng Su",
        "Fansheng Chen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-06-14",
      "update_time": "2022-06-14",
      "comments": null,
      "repo_url": "https://github.com/chenastron/mtunet"
    },
    "2206.02120": {
      "paper_id": "2206.02120v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2206.02120v1",
      "paper_key": "2206.02120",
      "paper_title": "MPANet: Multi-Patch Attention For Infrared Small Target object Detection",
      "paper_url": "http://arxiv.org/abs/2206.02120v1",
      "paper_abstract": "Infrared small target detection (ISTD) has attracted widespread attention and been applied in various fields. Due to the small size of infrared targets and the noise interference from complex backgrounds, the performance of ISTD using convolutional neural networks (CNNs) is restricted. Moreover, the constriant that long-distance dependent features can not be encoded by the vanilla CNNs also impairs the robustness of capturing targets' shapes and locations in complex scenarios. To this end, a multi-patch attention network (MPANet) based on the axial-attention encoder and the multi-scale patch branch (MSPB) structure is proposed. Specially, an axial-attention-improved encoder architecture is designed to highlight the effective features of small targets and suppress background noises. Furthermore, the developed MSPB structure fuses the coarse-grained and fine-grained features from different semantic scales. Extensive experiments on the SIRST dataset show the superiority performance and effectiveness of the proposed MPANet compared to the state-of-the-art methods.",
      "paper_authors": [
        "Ao Wang",
        "Wei Li",
        "Xin Wu",
        "Zhanchao Huang",
        "Ran Tao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-06-05",
      "update_time": "2022-06-05",
      "comments": "4 pages 3 figures",
      "repo_url": "#"
    },
    "2201.01014": {
      "paper_id": "2201.01014v5",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2201.01014v5",
      "paper_key": "2201.01014",
      "paper_title": "Local Motion and Contrast Priors Driven Deep Network for Infrared Small Target Super-Resolution",
      "paper_url": "http://arxiv.org/abs/2201.01014v5",
      "paper_abstract": "Infrared small target super-resolution (SR) aims to recover reliable and detailed high-resolution image with high-contrast targets from its low-resolution counterparts. Since the infrared small target lacks color and fine structure information, it is significant to exploit the supplementary information among sequence images to enhance the target. In this paper, we propose the first infrared small target SR method named local motion and contrast prior driven deep network (MoCoPnet) to integrate the domain knowledge of infrared small target into deep network, which can mitigate the intrinsic feature scarcity of infrared small targets. Specifically, motivated by the local motion prior in the spatio-temporal dimension, we propose a local spatio-temporal attention module to perform implicit frame alignment and incorporate the local spatio-temporal information to enhance the local features (especially for small targets). Motivated by the local contrast prior in the spatial dimension, we propose a central difference residual group to incorporate the central difference convolution into the feature extraction backbone, which can achieve center-oriented gradient-aware feature extraction to further improve the target contrast. Extensive experiments have demonstrated that our method can recover accurate spatial dependency and improve the target contrast. Comparative results show that MoCoPnet can outperform the state-of-the-art video SR and single image SR methods in terms of both SR performance and target enhancement. Based on the SR results, we further investigate the influence of SR on infrared small target detection and the experimental results demonstrate that MoCoPnet promotes the detection performance. The code is available at https://github.com/XinyiYing/MoCoPnet.",
      "paper_authors": [
        "Xinyi Ying",
        "Yingqian Wang",
        "Longguang Wang",
        "Weidong Sheng",
        "Li Liu",
        "Zaiping Lin",
        "Shilin Zhou"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2022-01-04",
      "update_time": "2023-04-04",
      "comments": null,
      "repo_url": "https://github.com/xinyiying/mocopnet"
    },
    "2111.03580": {
      "paper_id": "2111.03580v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2111.03580v1",
      "paper_key": "2111.03580",
      "paper_title": "AGPCNet: Attention-Guided Pyramid Context Networks for Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2111.03580v1",
      "paper_abstract": "Infrared small target detection is an important problem in many fields such as earth observation, military reconnaissance, disaster relief, and has received widespread attention recently. This paper presents the Attention-Guided Pyramid Context Network (AGPCNet) algorithm. Its main components are an Attention-Guided Context Block (AGCB), a Context Pyramid Module (CPM), and an Asymmetric Fusion Module (AFM). AGCB divides the feature map into patches to compute local associations and uses Global Context Attention (GCA) to compute global associations between semantics, CPM integrates features from multi-scale AGCBs, and AFM integrates low-level and deep-level semantics from a feature-fusion perspective to enhance the utilization of features. The experimental results illustrate that AGPCNet has achieved new state-of-the-art performance on two available infrared small target datasets. The source codes are available at https://github.com/Tianfang-Zhang/AGPCNet.",
      "paper_authors": [
        "Tianfang Zhang",
        "Siying Cao",
        "Tian Pu",
        "Zhenming Peng"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-11-05",
      "update_time": "2021-11-05",
      "comments": "12 pages, 13 figures, 8 tables",
      "repo_url": "https://github.com/tianfang-zhang/agpcnet"
    },
    "2108.06054": {
      "paper_id": "2108.06054v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2108.06054v3",
      "paper_key": "2108.06054",
      "paper_title": "Local Patch Network with Global Attention for Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2108.06054v3",
      "paper_abstract": "Infrared small target detection plays an important role in the infrared search and tracking applications. In recent years, deep learning techniques were introduced to this task and achieved noteworthy effects. Following general object segmentation methods, existing deep learning methods usually processed the image from the global view. However, the imaging locality of small targets and extreme class-imbalance between the target and background pixels were not well-considered by these deep learning methods, which causes the low-efficiency on training and high-dependence on numerous data. A local patch network (LPNet) with global attention is proposed in this paper to detect small targets by jointly considering the global and local properties of infrared small target images. From the global view, a supervised attention module trained by the small target spread map is proposed to suppress most background pixels irrelevant with small target features. From the local view, local patches are split from global features and share the same convolution weights with each other in a patch net. By leveraging both the global and local properties, the data-driven framework proposed in this paper has fused multi-scale features for small target detection. Extensive synthetic and real data experiments show that the proposed method achieves the state-of-the-art performance compared with existing both conventional and deep learning methods.",
      "paper_authors": [
        "Fang Chen",
        "Chenqiang Gao",
        "Fangcen Liu",
        "Yue Zhao",
        "Yuxi Zhou",
        "Deyu Meng",
        "Wangmeng Zuo"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2021-08-13",
      "update_time": "2021-09-29",
      "comments": "11 pages, 7 figures",
      "repo_url": "https://github.com/cquptimg/Local-Patch-Network-with-Global-Attention"
    },
    "2106.00487": {
      "paper_id": "2106.00487v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2106.00487v3",
      "paper_key": "2106.00487",
      "paper_title": "Dense Nested Attention Network for Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2106.00487v3",
      "paper_abstract": "Single-frame infrared small target (SIRST) detection aims at separating small targets from clutter backgrounds. With the advances of deep learning, CNN-based methods have yielded promising results in generic object detection due to their powerful modeling capability. However, existing CNN-based methods cannot be directly applied for infrared small targets since pooling layers in their networks could lead to the loss of targets in deep layers. To handle this problem, we propose a dense nested attention network (DNANet) in this paper. Specifically, we design a dense nested interactive module (DNIM) to achieve progressive interaction among high-level and low-level features. With the repeated interaction in DNIM, infrared small targets in deep layers can be maintained. Based on DNIM, we further propose a cascaded channel and spatial attention module (CSAM) to adaptively enhance multi-level features. With our DNANet, contextual information of small targets can be well incorporated and fully exploited by repeated fusion and enhancement. Moreover, we develop an infrared small target dataset (namely, NUDT-SIRST) and propose a set of evaluation metrics to conduct comprehensive performance evaluation. Experiments on both public and our self-developed datasets demonstrate the effectiveness of our method. Compared to other state-of-the-art methods, our method achieves better performance in terms of probability of detection (Pd), false-alarm rate (Fa), and intersection of union (IoU).",
      "paper_authors": [
        "Boyang Li",
        "Chao Xiao",
        "Longguang Wang",
        "Yingqian Wang",
        "Zaiping Lin",
        "Miao Li",
        "Wei An",
        "Yulan Guo"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-06-01",
      "update_time": "2022-08-16",
      "comments": "Accepted by IEEE Transactions on Image Processing (TIP)",
      "repo_url": "https://github.com/YeRen123455/Infrared-Small-Target-Detection"
    },
    "2105.14974": {
      "paper_id": "2105.14974v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2105.14974v2",
      "paper_key": "2105.14974",
      "paper_title": "Non-Convex Tensor Low-Rank Approximation for Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2105.14974v2",
      "paper_abstract": "Infrared small target detection is an important fundamental task in the infrared system. Therefore, many infrared small target detection methods have been proposed, in which the low-rank model has been used as a powerful tool. However, most low-rank-based methods assign the same weights for different singular values, which will lead to inaccurate background estimation. Considering that different singular values have different importance and should be treated discriminatively, in this paper, we propose a non-convex tensor low-rank approximation (NTLA) method for infrared small target detection. In our method, NTLA regularization adaptively assigns different weights to different singular values for accurate background estimation. Based on the proposed NTLA, we propose asymmetric spatial-temporal total variation (ASTTV) regularization to achieve more accurate background estimation in complex scenes. Compared with the traditional total variation approach, ASTTV exploits different smoothness intensities for spatial and temporal regularization. We design an efficient algorithm to find the optimal solution of our method. Compared with some state-of-the-art methods, the proposed method achieves an improvement in terms of various evaluation metrics. Extensive experimental results in various complex scenes demonstrate that our method has strong robustness and low false-alarm rate. Code is available at https://github.com/LiuTing20a/ASTTV-NTLA.",
      "paper_authors": [
        "Ting Liu",
        "Jungang Yang",
        "Boyang Li",
        "Chao Xiao",
        "Yang Sun",
        "Yingqian Wang",
        "Wei An"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-05-31",
      "update_time": "2021-11-21",
      "comments": "This paper is accepted by IEEE Transactions on Geoscience and Remote\n  Sensing",
      "repo_url": "https://github.com/LiuTing20a/ASTTV-NTLA"
    },
    "2012.08573": {
      "paper_id": "2012.08573v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2012.08573v1",
      "paper_key": "2012.08573",
      "paper_title": "Attentional Local Contrast Networks for Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2012.08573v1",
      "paper_abstract": "To mitigate the issue of minimal intrinsic features for pure data-driven methods, in this paper, we propose a novel model-driven deep network for infrared small target detection, which combines discriminative networks and conventional model-driven methods to make use of both labeled data and the domain knowledge. By designing a feature map cyclic shift scheme, we modularize a conventional local contrast measure method as a depth-wise parameterless nonlinear feature refinement layer in an end-to-end network, which encodes relatively long-range contextual interactions with clear physical interpretability. To highlight and preserve the small target features, we also exploit a bottom-up attentional modulation integrating the smaller scale subtle details of low-level features into high-level features of deeper layers. We conduct detailed ablation studies with varying network depths to empirically verify the effectiveness and efficiency of the design of each component in our network architecture. We also compare the performance of our network against other model-driven methods and deep networks on the open SIRST dataset as well. The results suggest that our network yields a performance boost over its competitors. Our code, trained models, and results are available online.",
      "paper_authors": [
        "Yimian Dai",
        "Yiquan Wu",
        "Fei Zhou",
        "Kobus Barnard"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2020-12-15",
      "update_time": "2020-12-15",
      "comments": "Accepted by IEEE TGRS",
      "repo_url": "https://github.com/YimianDai/open-alcnet"
    },
    "2011.12059": {
      "paper_id": "2011.12059v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2011.12059v1",
      "paper_key": "2011.12059",
      "paper_title": "Infrared small target detection based on isotropic constraint under complex background",
      "paper_url": "http://arxiv.org/abs/2011.12059v1",
      "paper_abstract": "Infrared search and tracking (IRST) system has been widely concerned and applied in the area of national defence. Small target detection under complex background is a very challenging task in the development of system algorithm. Low signal-to-clutter ratio (SCR) of target and the interference caused by irregular background clutter make it difficult to get an accurate result. In this paper, small targets are considered to have two characteristics of high contrast and isotropy, and we propose a multilayer gray difference (MGD) method constrained by isotropy. Firstly, the suspected regions are obtained through MGD, and then the eigenvalues of the original image's Hessian matrix are calculated to obtain the isotropy parameter of each region. Finally, those regions do not meet the isotropic constraint condition are suppressed. Experiments show that the proposed method is effective and superior to several common methods in terms of signal-to-clutter ratio gain (SCRG) and receiver operating characteristic (ROC) curve.",
      "paper_authors": [
        "Fan Wang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2020-11-24",
      "update_time": "2020-11-24",
      "comments": null,
      "repo_url": "#"
    },
    "2010.00923": {
      "paper_id": "2010.00923v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2010.00923v1",
      "paper_key": "2010.00923",
      "paper_title": "Multiple Infrared Small Targets Detection based on Hierarchical Maximal Entropy Random Walk",
      "paper_url": "http://arxiv.org/abs/2010.00923v1",
      "paper_abstract": "The technique of detecting multiple dim and small targets with low signal-to-clutter ratios (SCR) is very important for infrared search and tracking systems. In this paper, we establish a detection method derived from maximal entropy random walk (MERW) to robustly detect multiple small targets. Initially, we introduce the primal MERW and analyze the feasibility of applying it to small target detection. However, the original weight matrix of the MERW is sensitive to interferences. Therefore, a specific weight matrix is designed for the MERW in principle of enhancing characteristics of small targets and suppressing strong clutters. Moreover, the primal MERW has a critical limitation of strong bias to the most salient small target. To achieve multiple small targets detection, we develop a hierarchical version of the MERW method. Based on the hierarchical MERW (HMERW), we propose a small target detection method as follows. First, filtering technique is used to smooth the infrared image. Second, an output map is obtained by importing the filtered image into the HMERW. Then, a coefficient map is constructed to fuse the stationary dirtribution map of the HMERW. Finally, an adaptive threshold is used to segment multiple small targets from the fusion map. Extensive experiments on practical data sets demonstrate that the proposed method is superior to the state-of-the-art methods in terms of target enhancement, background suppression and multiple small targets detection.",
      "paper_authors": [
        "Chaoqun Xia",
        "Xiaorun Li",
        "Liaoying Zhao",
        "Shuhan Chen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2020-10-02",
      "update_time": "2020-10-02",
      "comments": null,
      "repo_url": "#"
    },
    "2009.14530": {
      "paper_id": "2009.14530v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2009.14530v1",
      "paper_key": "2009.14530",
      "paper_title": "Asymmetric Contextual Modulation for Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2009.14530v1",
      "paper_abstract": "Single-frame infrared small target detection remains a challenge not only due to the scarcity of intrinsic target characteristics but also because of lacking a public dataset. In this paper, we first contribute an open dataset with high-quality annotations to advance the research in this field. We also propose an asymmetric contextual modulation module specially designed for detecting infrared small targets. To better highlight small targets, besides a top-down global contextual feedback, we supplement a bottom-up modulation pathway based on point-wise channel attention for exchanging high-level semantics and subtle low-level details. We report ablation studies and comparisons to state-of-the-art methods, where we find that our approach performs significantly better. Our dataset and code are available online.",
      "paper_authors": [
        "Yimian Dai",
        "Yiquan Wu",
        "Fei Zhou",
        "Kobus Barnard"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2020-09-30",
      "update_time": "2020-09-30",
      "comments": null,
      "repo_url": "https://github.com/YimianDai/open-acm"
    },
    "2006.08162": {
      "paper_id": "2006.08162v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2006.08162v1",
      "paper_key": "2006.08162",
      "paper_title": "Filter design for small target detection on infrared imagery using normalized-cross-correlation layer",
      "paper_url": "http://arxiv.org/abs/2006.08162v1",
      "paper_abstract": "In this paper, we introduce a machine learning approach to the problem of infrared small target detection filter design. For this purpose, similarly to a convolutional layer of a neural network, the normalized-cross-correlational (NCC) layer, which we utilize for designing a target detection/recognition filter bank, is proposed. By employing the NCC layer in a neural network structure, we introduce a framework, in which supervised training is used to calculate the optimal filter shape and the optimum number of filters required for a specific target detection/recognition task on infrared images. We also propose the mean-absolute-deviation NCC (MAD-NCC) layer, an efficient implementation of the proposed NCC layer, designed especially for FPGA systems, in which square root operations are avoided for real-time computation. As a case study we work on dim-target detection on mid-wave infrared imagery and obtain the filters that can discriminate a dim target from various types of background clutter, specific to our operational concept.",
      "paper_authors": [
        "H. Se\u00e7kin Demir",
        "Erdem Akagunduz"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2020-06-15",
      "update_time": "2020-06-15",
      "comments": null,
      "repo_url": "#"
    },
    "2001.05852": {
      "paper_id": "2001.05852v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2001.05852v1",
      "paper_key": "2001.05852",
      "paper_title": "TBC-Net: A real-time detector for infrared small target detection using semantic constraint",
      "paper_url": "http://arxiv.org/abs/2001.05852v1",
      "paper_abstract": "Infrared small target detection is a key technique in infrared search and tracking (IRST) systems. Although deep learning has been widely used in the vision tasks of visible light images recently, it is rarely used in infrared small target detection due to the difficulty in learning small target features. In this paper, we propose a novel lightweight convolutional neural network TBC-Net for infrared small target detection. The TBCNet consists of a target extraction module (TEM) and a semantic constraint module (SCM), which are used to extract small targets from infrared images and to classify the extracted target images during the training, respectively. Meanwhile, we propose a joint loss function and a training method. The SCM imposes a semantic constraint on TEM by combining the high-level classification task and solve the problem of the difficulty to learn features caused by class imbalance problem. During the training, the targets are extracted from the input image and then be classified by SCM. During the inference, only the TEM is used to detect the small targets. We also propose a data synthesis method to generate training data. The experimental results show that compared with the traditional methods, TBC-Net can better reduce the false alarm caused by complicated background, the proposed network structure and joint loss have a significant improvement on small target feature learning. Besides, TBC-Net can achieve real-time detection on the NVIDIA Jetson AGX Xavier development board, which is suitable for applications such as field research with drones equipped with infrared sensors.",
      "paper_authors": [
        "Mingxin Zhao",
        "Li Cheng",
        "Xu Yang",
        "Peng Feng",
        "Liyuan Liu",
        "Nanjian Wu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2019-12-27",
      "update_time": "2019-12-27",
      "comments": null,
      "repo_url": "#"
    },
    "1810.03173": {
      "paper_id": "1810.03173v4",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/1810.03173v4",
      "paper_key": "1810.03173",
      "paper_title": "Fast and Robust Small Infrared Target Detection Using Absolute Directional Mean Difference Algorithm",
      "paper_url": "http://arxiv.org/abs/1810.03173v4",
      "paper_abstract": "Infrared small target detection in an infrared search and track (IRST) system is a challenging task. This situation becomes more complicated when high gray-intensity structural backgrounds appear in the field of view (FoV) of the infrared seeker. While the majority of the infrared small target detection algorithms neglect directional information, in this paper, a directional approach is presented to suppress structural backgrounds and develop a more effective detection algorithm. To this end, a similar concept to the average absolute gray difference (AAGD) is utilized to construct a novel directional small target detection algorithm called absolute directional mean difference (ADMD). Also, an efficient implementation procedure is presented for the proposed algorithm. The proposed algorithm effectively enhances the target area and eliminates background clutter. Simulation results on real infrared images prove the significant effectiveness of the proposed algorithm.",
      "paper_authors": [
        "Saed Moradi",
        "Payman Moallem",
        "Mohamad Farzan Sabahi"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2018-10-07",
      "update_time": "2020-07-29",
      "comments": "The Final version (Accepted in Signal Processing journal)",
      "repo_url": "https://github.com/moradisaed/ADMD"
    },
    "1703.09157": {
      "paper_id": "1703.09157v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/1703.09157v1",
      "paper_key": "1703.09157",
      "paper_title": "Reweighted Infrared Patch-Tensor Model With Both Non-Local and Local Priors for Single-Frame Small Target Detection",
      "paper_url": "http://arxiv.org/abs/1703.09157v1",
      "paper_abstract": "Many state-of-the-art methods have been proposed for infrared small target detection. They work well on the images with homogeneous backgrounds and high-contrast targets. However, when facing highly heterogeneous backgrounds, they would not perform very well, mainly due to: 1) the existence of strong edges and other interfering components, 2) not utilizing the priors fully. Inspired by this, we propose a novel method to exploit both local and non-local priors simultaneously. Firstly, we employ a new infrared patch-tensor (IPT) model to represent the image and preserve its spatial correlations. Exploiting the target sparse prior and background non-local self-correlation prior, the target-background separation is modeled as a robust low-rank tensor recovery problem. Moreover, with the help of the structure tensor and reweighted idea, we design an entry-wise local-structure-adaptive and sparsity enhancing weight to replace the globally constant weighting parameter. The decomposition could be achieved via the element-wise reweighted higher-order robust principal component analysis with an additional convergence condition according to the practical situation of target detection. Extensive experiments demonstrate that our model outperforms the other state-of-the-arts, in particular for the images with very dim targets and heavy clutters.",
      "paper_authors": [
        "Yimian Dai",
        "Yiquan Wu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2017-03-27",
      "update_time": "2017-03-27",
      "comments": "Submitted to IEEE Journal of Selected Topics in Applied Earth\n  Observations and Remote Sensing, 16 pages, 16 figures",
      "repo_url": "https://github.com/YimianDai/DENTIST"
    },
    "2409.04011": {
      "paper_id": "2409.04011v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.04011v1",
      "paper_key": "2409.04011",
      "paper_title": "Hybrid Mask Generation for Infrared Small Target Detection with Single-Point Supervision",
      "paper_url": "http://arxiv.org/abs/2409.04011v1",
      "paper_abstract": "Single-frame infrared small target (SIRST) detection poses a significant challenge due to the requirement to discern minute targets amidst complex infrared background clutter. Recently, deep learning approaches have shown promising results in this domain. However, these methods heavily rely on extensive manual annotations, which are particularly cumbersome and resource-intensive for infrared small targets owing to their minute sizes. To address this limitation, we introduce a Hybrid Mask Generation (HMG) approach that recovers high-quality masks for each target from only a single-point label for network training. Specifically, our HMG approach consists of a handcrafted Points-to-Mask Generation strategy coupled with a pseudo mask updating strategy to recover and refine pseudo masks from point labels. The Points-to-Mask Generation strategy divides two distinct stages: Points-to-Box conversion, where individual point labels are transformed into bounding boxes, and subsequently, Box-to-Mask prediction, where these bounding boxes are elaborated into precise masks. The mask updating strategy integrates the complementary strengths of handcrafted and deep-learning algorithms to iteratively refine the initial pseudo masks. Experimental results across three datasets demonstrate that our method outperforms the existing methods for infrared small target detection with single-point supervision.",
      "paper_authors": [
        "Weijie He",
        "Mushui Liu",
        "Yunlong Yu",
        "Zheming Lu",
        "Xi Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-06",
      "update_time": "2024-09-06",
      "comments": "9 pages, 5 figures",
      "repo_url": "#"
    },
    "2409.04714": {
      "paper_id": "2409.04714v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.04714v1",
      "paper_key": "2409.04714",
      "paper_title": "Unleashing the Power of Generic Segmentation Models: A Simple Baseline for Infrared Small Target Detection",
      "paper_url": "http://arxiv.org/abs/2409.04714v1",
      "paper_abstract": "Recent advancements in deep learning have greatly advanced the field of infrared small object detection (IRSTD). Despite their remarkable success, a notable gap persists between these IRSTD methods and generic segmentation approaches in natural image domains. This gap primarily arises from the significant modality differences and the limited availability of infrared data. In this study, we aim to bridge this divergence by investigating the adaptation of generic segmentation models, such as the Segment Anything Model (SAM), to IRSTD tasks. Our investigation reveals that many generic segmentation models can achieve comparable performance to state-of-the-art IRSTD methods. However, their full potential in IRSTD remains untapped. To address this, we propose a simple, lightweight, yet effective baseline model for segmenting small infrared objects. Through appropriate distillation strategies, we empower smaller student models to outperform state-of-the-art methods, even surpassing fine-tuned teacher results. Furthermore, we enhance the model's performance by introducing a novel query design comprising dense and sparse queries to effectively encode multi-scale features. Through extensive experimentation across four popular IRSTD datasets, our model demonstrates significantly improved performance in both accuracy and throughput compared to existing approaches, surpassing SAM and Semantic-SAM by over 14 IoU on NUDT and 4 IoU on IRSTD1k. The source code and models will be released at https://github.com/O937-blip/SimIR.",
      "paper_authors": [
        "Mingjin Zhang",
        "Chi Zhang",
        "Qiming Zhang",
        "Yunsong Li",
        "Xinbo Gao",
        "Jing Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-07",
      "update_time": "2024-09-07",
      "comments": "ACM MM'24",
      "repo_url": "#"
    }
  },
  "Salient Object Detection": {
    "2408.16645": {
      "paper_id": "2408.16645v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.16645v1",
      "paper_key": "2408.16645",
      "paper_title": "SODAWideNet++: Combining Attention and Convolutions for Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2408.16645v1",
      "paper_abstract": "Salient Object Detection (SOD) has traditionally relied on feature refinement modules that utilize the features of an ImageNet pre-trained backbone. However, this approach limits the possibility of pre-training the entire network because of the distinct nature of SOD and image classification. Additionally, the architecture of these backbones originally built for Image classification is sub-optimal for a dense prediction task like SOD. To address these issues, we propose a novel encoder-decoder-style neural network called SODAWideNet++ that is designed explicitly for SOD. Inspired by the vision transformers ability to attain a global receptive field from the initial stages, we introduce the Attention Guided Long Range Feature Extraction (AGLRFE) module, which combines large dilated convolutions and self-attention. Specifically, we use attention features to guide long-range information extracted by multiple dilated convolutions, thus taking advantage of the inductive biases of a convolution operation and the input dependency brought by self-attention. In contrast to the current paradigm of ImageNet pre-training, we modify 118K annotated images from the COCO semantic segmentation dataset by binarizing the annotations to pre-train the proposed model end-to-end. Further, we supervise the background predictions along with the foreground to push our model to generate accurate saliency predictions. SODAWideNet++ performs competitively on five different datasets while only containing 35% of the trainable parameters compared to the state-of-the-art models. The code and pre-computed saliency maps are provided at https://github.com/VimsLab/SODAWideNetPlusPlus.",
      "paper_authors": [
        "Rohit Venkata Sai Dulam",
        "Chandra Kambhamettu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-29",
      "update_time": "2024-08-29",
      "comments": "Accepted at ICPR 2024",
      "repo_url": "#"
    },
    "2408.15063": {
      "paper_id": "2408.15063v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.15063v3",
      "paper_key": "2408.15063",
      "paper_title": "Adapting Segment Anything Model to Multi-modal Salient Object Detection with Semantic Feature Fusion Guidance",
      "paper_url": "http://arxiv.org/abs/2408.15063v3",
      "paper_abstract": "Although most existing multi-modal salient object detection (SOD) methods demonstrate effectiveness through training models from scratch, the limited multi-modal data hinders these methods from reaching optimality. In this paper, we propose a novel framework to explore and exploit the powerful feature representation and zero-shot generalization ability of the pre-trained Segment Anything Model (SAM) for multi-modal SOD. Despite serving as a recent vision fundamental model, driving the class-agnostic SAM to comprehend and detect salient objects accurately is non-trivial, especially in challenging scenes. To this end, we develop \\underline{SAM} with se\\underline{m}antic f\\underline{e}ature fu\\underline{s}ion guidanc\\underline{e} (Sammese), which incorporates multi-modal saliency-specific knowledge into SAM to adapt SAM to multi-modal SOD tasks. However, it is difficult for SAM trained on single-modal data to directly mine the complementary benefits of multi-modal inputs and comprehensively utilize them to achieve accurate saliency prediction. To address these issues, we first design a multi-modal complementary fusion module to extract robust multi-modal semantic features by integrating information from visible and thermal or depth image pairs. Then, we feed the extracted multi-modal semantic features into both the SAM image encoder and mask decoder for fine-tuning and prompting, respectively. Specifically, in the image encoder, a multi-modal adapter is proposed to adapt the single-modal SAM to multi-modal information. In the mask decoder, a semantic-geometric prompt generation strategy is proposed to produce corresponding embeddings with various saliency cues. Extensive experiments on both RGB-D and RGB-T SOD benchmarks show the effectiveness of the proposed framework. The code will be available at \\url{https://github.com/Angknpng/Sammese}.",
      "paper_authors": [
        "Kunpeng Wang",
        "Danying Lin",
        "Chenglong Li",
        "Zhengzheng Tu",
        "Bin Luo"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-27",
      "update_time": "2024-09-02",
      "comments": "10 pages, 9 figures",
      "repo_url": "https://github.com/angknpng/sammese"
    },
    "2408.09097": {
      "paper_id": "2408.09097v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.09097v1",
      "paper_key": "2408.09097",
      "paper_title": "Depth-guided Texture Diffusion for Image Semantic Segmentation",
      "paper_url": "http://arxiv.org/abs/2408.09097v1",
      "paper_abstract": "Depth information provides valuable insights into the 3D structure especially the outline of objects, which can be utilized to improve the semantic segmentation tasks. However, a naive fusion of depth information can disrupt feature and compromise accuracy due to the modality gap between the depth and the vision. In this work, we introduce a Depth-guided Texture Diffusion approach that effectively tackles the outlined challenge. Our method extracts low-level features from edges and textures to create a texture image. This image is then selectively diffused across the depth map, enhancing structural information vital for precisely extracting object outlines. By integrating this enriched depth map with the original RGB image into a joint feature embedding, our method effectively bridges the disparity between the depth map and the image, enabling more accurate semantic segmentation. We conduct comprehensive experiments across diverse, commonly-used datasets spanning a wide range of semantic segmentation tasks, including Camouflaged Object Detection (COD), Salient Object Detection (SOD), and indoor semantic segmentation. With source-free estimated depth or depth captured by depth cameras, our method consistently outperforms existing baselines and achieves new state-of-theart results, demonstrating the effectiveness of our Depth-guided Texture Diffusion for image semantic segmentation.",
      "paper_authors": [
        "Wei Sun",
        "Yuan Li",
        "Qixiang Ye",
        "Jianbin Jiao",
        "Yanzhao Zhou"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-17",
      "update_time": "2024-08-17",
      "comments": null,
      "repo_url": "#"
    },
    "2408.08870": {
      "paper_id": "2408.08870v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.08870v1",
      "paper_key": "2408.08870",
      "paper_title": "SAM2-UNet: Segment Anything 2 Makes Strong Encoder for Natural and Medical Image Segmentation",
      "paper_url": "http://arxiv.org/abs/2408.08870v1",
      "paper_abstract": "Image segmentation plays an important role in vision understanding. Recently, the emerging vision foundation models continuously achieved superior performance on various tasks. Following such success, in this paper, we prove that the Segment Anything Model 2 (SAM2) can be a strong encoder for U-shaped segmentation models. We propose a simple but effective framework, termed SAM2-UNet, for versatile image segmentation. Specifically, SAM2-UNet adopts the Hiera backbone of SAM2 as the encoder, while the decoder uses the classic U-shaped design. Additionally, adapters are inserted into the encoder to allow parameter-efficient fine-tuning. Preliminary experiments on various downstream tasks, such as camouflaged object detection, salient object detection, marine animal segmentation, mirror detection, and polyp segmentation, demonstrate that our SAM2-UNet can simply beat existing specialized state-of-the-art methods without bells and whistles. Project page: \\url{https://github.com/WZH0120/SAM2-UNet}.",
      "paper_authors": [
        "Xinyu Xiong",
        "Zihuang Wu",
        "Shuangyi Tan",
        "Wenxue Li",
        "Feilong Tang",
        "Ying Chen",
        "Siying Li",
        "Jie Ma",
        "Guanbin Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-16",
      "update_time": "2024-08-16",
      "comments": "Technical Report",
      "repo_url": "https://github.com/wzh0120/sam2-unet"
    },
    "2408.04326": {
      "paper_id": "2408.04326v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.04326v1",
      "paper_key": "2408.04326",
      "paper_title": "Multi-Scale and Detail-Enhanced Segment Anything Model for Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2408.04326v1",
      "paper_abstract": "Salient Object Detection (SOD) aims to identify and segment the most prominent objects in images. Advanced SOD methods often utilize various Convolutional Neural Networks (CNN) or Transformers for deep feature extraction. However, these methods still deliver low performance and poor generalization in complex cases. Recently, Segment Anything Model (SAM) has been proposed as a visual fundamental model, which gives strong segmentation and generalization capabilities. Nonetheless, SAM requires accurate prompts of target objects, which are unavailable in SOD. Additionally, SAM lacks the utilization of multi-scale and multi-level information, as well as the incorporation of fine-grained details. To address these shortcomings, we propose a Multi-scale and Detail-enhanced SAM (MDSAM) for SOD. Specifically, we first introduce a Lightweight Multi-Scale Adapter (LMSA), which allows SAM to learn multi-scale information with very few trainable parameters. Then, we propose a Multi-Level Fusion Module (MLFM) to comprehensively utilize the multi-level information from the SAM's encoder. Finally, we propose a Detail Enhancement Module (DEM) to incorporate SAM with fine-grained details. Experimental results demonstrate the superior performance of our model on multiple SOD datasets and its strong generalization on other segmentation tasks. The source code is released at https://github.com/BellyBeauty/MDSAM.",
      "paper_authors": [
        "Shixuan Gao",
        "Pingping Zhang",
        "Tianyu Yan",
        "Huchuan Lu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-08",
      "update_time": "2024-08-08",
      "comments": "This work is accepted by ACM MM2024",
      "repo_url": "https://github.com/bellybeauty/mdsam"
    },
    "2408.01137": {
      "paper_id": "2408.01137v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.01137v1",
      "paper_key": "2408.01137",
      "paper_title": "PGNeXt: High-Resolution Salient Object Detection via Pyramid Grafting Network",
      "paper_url": "http://arxiv.org/abs/2408.01137v1",
      "paper_abstract": "We present an advanced study on more challenging high-resolution salient object detection (HRSOD) from both dataset and network framework perspectives. To compensate for the lack of HRSOD dataset, we thoughtfully collect a large-scale high resolution salient object detection dataset, called UHRSD, containing 5,920 images from real-world complex scenarios at 4K-8K resolutions. All the images are finely annotated in pixel-level, far exceeding previous low-resolution SOD datasets. Aiming at overcoming the contradiction between the sampling depth and the receptive field size in the past methods, we propose a novel one-stage framework for HR-SOD task using pyramid grafting mechanism. In general, transformer-based and CNN-based backbones are adopted to extract features from different resolution images independently and then these features are grafted from transformer branch to CNN branch. An attention-based Cross-Model Grafting Module (CMGM) is proposed to enable CNN branch to combine broken detailed information more holistically, guided by different source feature during decoding process. Moreover, we design an Attention Guided Loss (AGL) to explicitly supervise the attention matrix generated by CMGM to help the network better interact with the attention from different branches. Comprehensive experiments on UHRSD and widely-used SOD datasets demonstrate that our method can simultaneously locate salient object and preserve rich details, outperforming state-of-the-art methods. To verify the generalization ability of the proposed framework, we apply it to the camouflaged object detection (COD) task. Notably, our method performs superior to most state-of-the-art COD methods without bells and whistles.",
      "paper_authors": [
        "Changqun Xia",
        "Chenxi Xie",
        "Zhentao He",
        "Tianshu Yu",
        "Jia Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-02",
      "update_time": "2024-08-02",
      "comments": null,
      "repo_url": "#"
    },
    "2407.17628": {
      "paper_id": "2407.17628v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.17628v1",
      "paper_key": "2407.17628",
      "paper_title": "PEEKABOO: Hiding parts of an image for unsupervised object localization",
      "paper_url": "http://arxiv.org/abs/2407.17628v1",
      "paper_abstract": "Localizing objects in an unsupervised manner poses significant challenges due to the absence of key visual information such as the appearance, type and number of objects, as well as the lack of labeled object classes typically available in supervised settings. While recent approaches to unsupervised object localization have demonstrated significant progress by leveraging self-supervised visual representations, they often require computationally intensive training processes, resulting in high resource demands in terms of computation, learnable parameters, and data. They also lack explicit modeling of visual context, potentially limiting their accuracy in object localization. To tackle these challenges, we propose a single-stage learning framework, dubbed PEEKABOO, for unsupervised object localization by learning context-based representations at both the pixel- and shape-level of the localized objects through image masking. The key idea is to selectively hide parts of an image and leverage the remaining image information to infer the location of objects without explicit supervision. The experimental results, both quantitative and qualitative, across various benchmark datasets, demonstrate the simplicity, effectiveness and competitive performance of our approach compared to state-of-the-art methods in both single object discovery and unsupervised salient object detection tasks. Code and pre-trained models are available at: https://github.com/hasibzunair/peekaboo",
      "paper_authors": [
        "Hasib Zunair",
        "A. Ben Hamza"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-24",
      "update_time": "2024-07-24",
      "comments": null,
      "repo_url": "https://github.com/hasibzunair/peekaboo"
    },
    "2407.11714": {
      "paper_id": "2407.11714v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.11714v1",
      "paper_key": "2407.11714",
      "paper_title": "Improving Unsupervised Video Object Segmentation via Fake Flow Generation",
      "paper_url": "http://arxiv.org/abs/2407.11714v1",
      "paper_abstract": "Unsupervised video object segmentation (VOS), also known as video salient object detection, aims to detect the most prominent object in a video at the pixel level. Recently, two-stream approaches that leverage both RGB images and optical flow maps have gained significant attention. However, the limited amount of training data remains a substantial challenge. In this study, we propose a novel data generation method that simulates fake optical flows from single images, thereby creating large-scale training data for stable network learning. Inspired by the observation that optical flow maps are highly dependent on depth maps, we generate fake optical flows by refining and augmenting the estimated depth maps of each image. By incorporating our simulated image-flow pairs, we achieve new state-of-the-art performance on all public benchmark datasets without relying on complex modules. We believe that our data generation method represents a potential breakthrough for future VOS research.",
      "paper_authors": [
        "Suhwan Cho",
        "Minhyeok Lee",
        "Jungho Lee",
        "Donghyeong Kim",
        "Seunghoon Lee",
        "Sungmin Woo",
        "Sangyoun Lee"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-16",
      "update_time": "2024-07-16",
      "comments": null,
      "repo_url": "https://github.com/suhwan-cho/FakeFlow"
    },
    "2407.06780": {
      "paper_id": "2407.06780v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.06780v1",
      "paper_key": "2407.06780",
      "paper_title": "CoLA: Conditional Dropout and Language-driven Robust Dual-modal Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2407.06780v1",
      "paper_abstract": "The depth/thermal information is beneficial for detecting salient object with conventional RGB images. However, in dual-modal salient object detection (SOD) model, the robustness against noisy inputs and modality missing is crucial but rarely studied. To tackle this problem, we introduce \\textbf{Co}nditional Dropout and \\textbf{LA}nguage-driven(\\textbf{CoLA}) framework comprising two core components. 1) Language-driven Quality Assessment (LQA): Leveraging a pretrained vision-language model with a prompt learner, the LQA recalibrates image contributions without requiring additional quality annotations. This approach effectively mitigates the impact of noisy inputs. 2) Conditional Dropout (CD): A learning method to strengthen the model's adaptability in scenarios with missing modalities, while preserving its performance under complete modalities. The CD serves as a plug-in training scheme that treats modality-missing as conditions, strengthening the overall robustness of various dual-modal SOD models. Extensive experiments demonstrate that the proposed method outperforms state-of-the-art dual-modal SOD models, under both modality-complete and modality-missing conditions. We will release source code upon acceptance.",
      "paper_authors": [
        "Shuang Hao",
        "Chunlin Zhong",
        "He Tang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-09",
      "update_time": "2024-07-09",
      "comments": null,
      "repo_url": "https://github.com/ssecv/CoLA"
    },
    "2407.04085": {
      "paper_id": "2407.04085v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.04085v1",
      "paper_key": "2407.04085",
      "paper_title": "FIPGNet:Pyramid grafting network with feature interaction strategies",
      "paper_url": "http://arxiv.org/abs/2407.04085v1",
      "paper_abstract": "Salient object detection is designed to identify the objects in an image that attract the most visual attention.Currently, the most advanced method of significance object detection adopts pyramid grafting network architecture.However, pyramid-graft network architecture still has the problem of failing to accurately locate significant targets.We observe that this is mainly due to the fact that current salient object detection methods simply aggregate different scale features, ignoring the correlation between different scale features.To overcome these problems, we propose a new salience object detection framework(FIPGNet),which is a pyramid graft network with feature interaction strategies.Specifically, we propose an attention-mechanism based feature interaction strategy (FIA) that innovatively introduces spatial agent Cross Attention (SACA) to achieve multi-level feature interaction, highlighting important spatial regions from a spatial perspective, thereby enhancing salient regions.And the channel proxy Cross Attention Module (CCM), which is used to effectively connect the features extracted by the backbone network and the features processed using the spatial proxy cross attention module, eliminating inconsistencies.Finally, under the action of these two modules, the prominent target location problem in the current pyramid grafting network model is solved.Experimental results on six challenging datasets show that the proposed method outperforms the current 12 salient object detection methods on four indicators.",
      "paper_authors": [
        "Ziyi Ding",
        "Like Xin"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-04",
      "update_time": "2024-07-04",
      "comments": "arXiv admin note: text overlap with arXiv:2309.08365 by other authors",
      "repo_url": "#"
    },
    "2406.12536": {
      "paper_id": "2406.12536v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.12536v2",
      "paper_key": "2406.12536",
      "paper_title": "ViDSOD-100: A New Dataset and a Baseline Model for RGB-D Video Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2406.12536v2",
      "paper_abstract": "With the rapid development of depth sensor, more and more RGB-D videos could be obtained. Identifying the foreground in RGB-D videos is a fundamental and important task. However, the existing salient object detection (SOD) works only focus on either static RGB-D images or RGB videos, ignoring the collaborating of RGB-D and video information. In this paper, we first collect a new annotated RGB-D video SOD (ViDSOD-100) dataset, which contains 100 videos within a total of 9,362 frames, acquired from diverse natural scenes. All the frames in each video are manually annotated to a high-quality saliency annotation. Moreover, we propose a new baseline model, named attentive triple-fusion network (ATF-Net), for RGB-D video salient object detection. Our method aggregates the appearance information from an input RGB image, spatio-temporal information from an estimated motion map, and the geometry information from the depth map by devising three modality-specific branches and a multi-modality integration branch. The modality-specific branches extract the representation of different inputs, while the multi-modality integration branch combines the multi-level modality-specific features by introducing the encoder feature aggregation (MEA) modules and decoder feature aggregation (MDA) modules. The experimental findings conducted on both our newly introduced ViDSOD-100 dataset and the well-established DAVSOD dataset highlight the superior performance of the proposed ATF-Net. This performance enhancement is demonstrated both quantitatively and qualitatively, surpassing the capabilities of current state-of-the-art techniques across various domains, including RGB-D saliency detection, video saliency detection, and video object segmentation. Our data and our code are available at github.com/jhl-Det/RGBD_Video_SOD.",
      "paper_authors": [
        "Junhao Lin",
        "Lei Zhu",
        "Jiaxing Shen",
        "Huazhu Fu",
        "Qing Zhang",
        "Liansheng Wang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-18",
      "update_time": "2024-09-19",
      "comments": null,
      "repo_url": "https://github.com/jhl-det/rgbd_video_sod"
    },
    "2406.08249": {
      "paper_id": "2406.08249v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.08249v1",
      "paper_key": "2406.08249",
      "paper_title": "Dataset Enhancement with Instance-Level Augmentations",
      "paper_url": "http://arxiv.org/abs/2406.08249v1",
      "paper_abstract": "We present a method for expanding a dataset by incorporating knowledge from the wide distribution of pre-trained latent diffusion models. Data augmentations typically incorporate inductive biases about the image formation process into the training (e.g. translation, scaling, colour changes, etc.). Here, we go beyond simple pixel transformations and introduce the concept of instance-level data augmentation by repainting parts of the image at the level of object instances. The method combines a conditional diffusion model with depth and edge maps control conditioning to seamlessly repaint individual objects inside the scene, being applicable to any segmentation or detection dataset. Used as a data augmentation method, it improves the performance and generalization of the state-of-the-art salient object detection, semantic segmentation and object detection models. By redrawing all privacy-sensitive instances (people, license plates, etc.), the method is also applicable for data anonymization. We also release fully synthetic and anonymized expansions for popular datasets: COCO, Pascal VOC and DUTS.",
      "paper_authors": [
        "Orest Kupyn",
        "Christian Rupprecht"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-12",
      "update_time": "2024-06-12",
      "comments": null,
      "repo_url": "https://github.com/KupynOrest/instance_augmentation"
    },
    "2406.01127": {
      "paper_id": "2406.01127v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.01127v1",
      "paper_key": "2406.01127",
      "paper_title": "Learning Adaptive Fusion Bank for Multi-modal Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2406.01127v1",
      "paper_abstract": "Multi-modal salient object detection (MSOD) aims to boost saliency detection performance by integrating visible sources with depth or thermal infrared ones. Existing methods generally design different fusion schemes to handle certain issues or challenges. Although these fusion schemes are effective at addressing specific issues or challenges, they may struggle to handle multiple complex challenges simultaneously. To solve this problem, we propose a novel adaptive fusion bank that makes full use of the complementary benefits from a set of basic fusion schemes to handle different challenges simultaneously for robust MSOD. We focus on handling five major challenges in MSOD, namely center bias, scale variation, image clutter, low illumination, and thermal crossover or depth ambiguity. The fusion bank proposed consists of five representative fusion schemes, which are specifically designed based on the characteristics of each challenge, respectively. The bank is scalable, and more fusion schemes could be incorporated into the bank for more challenges. To adaptively select the appropriate fusion scheme for multi-modal input, we introduce an adaptive ensemble module that forms the adaptive fusion bank, which is embedded into hierarchical layers for sufficient fusion of different source data. Moreover, we design an indirect interactive guidance module to accurately detect salient hollow objects via the skip integration of high-level semantic information and low-level spatial details. Extensive experiments on three RGBT datasets and seven RGBD datasets demonstrate that the proposed method achieves the outstanding performance compared to the state-of-the-art methods. The code and results are available at https://github.com/Angknpng/LAFB.",
      "paper_authors": [
        "Kunpeng Wang",
        "Zhengzheng Tu",
        "Chenglong Li",
        "Cheng Zhang",
        "Bin Luo"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-03",
      "update_time": "2024-06-03",
      "comments": "Accepted by TCSVT 2024",
      "repo_url": "https://github.com/angknpng/lafb"
    },
    "2406.00917": {
      "paper_id": "2406.00917v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.00917v1",
      "paper_key": "2406.00917",
      "paper_title": "Alignment-Free RGBT Salient Object Detection: Semantics-guided Asymmetric Correlation Network and A Unified Benchmark",
      "paper_url": "http://arxiv.org/abs/2406.00917v1",
      "paper_abstract": "RGB and Thermal (RGBT) Salient Object Detection (SOD) aims to achieve high-quality saliency prediction by exploiting the complementary information of visible and thermal image pairs, which are initially captured in an unaligned manner. However, existing methods are tailored for manually aligned image pairs, which are labor-intensive, and directly applying these methods to original unaligned image pairs could significantly degrade their performance. In this paper, we make the first attempt to address RGBT SOD for initially captured RGB and thermal image pairs without manual alignment. Specifically, we propose a Semantics-guided Asymmetric Correlation Network (SACNet) that consists of two novel components: 1) an asymmetric correlation module utilizing semantics-guided attention to model cross-modal correlations specific to unaligned salient regions; 2) an associated feature sampling module to sample relevant thermal features according to the corresponding RGB features for multi-modal feature integration. In addition, we construct a unified benchmark dataset called UVT2000, containing 2000 RGB and thermal image pairs directly captured from various real-world scenes without any alignment, to facilitate research on alignment-free RGBT SOD. Extensive experiments on both aligned and unaligned datasets demonstrate the effectiveness and superior performance of our method. The dataset and code are available at https://github.com/Angknpng/SACNet.",
      "paper_authors": [
        "Kunpeng Wang",
        "Danying Lin",
        "Chenglong Li",
        "Zhengzheng Tu",
        "Bin Luo"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-03",
      "update_time": "2024-06-03",
      "comments": "Accepted by TMM 2024",
      "repo_url": "https://github.com/angknpng/sacnet"
    },
    "2405.17916": {
      "paper_id": "2405.17916v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.17916v1",
      "paper_key": "2405.17916",
      "paper_title": "Boosting General Trimap-free Matting in the Real-World Image",
      "paper_url": "http://arxiv.org/abs/2405.17916v1",
      "paper_abstract": "Image matting aims to obtain an alpha matte that separates foreground objects from the background accurately. Recently, trimap-free matting has been well studied because it requires only the original image without any extra input. Such methods usually extract a rough foreground by itself to take place trimap as further guidance. However, the definition of 'foreground' lacks a unified standard and thus ambiguities arise. Besides, the extracted foreground is sometimes incomplete due to inadequate network design. Most importantly, there is not a large-scale real-world matting dataset, and current trimap-free methods trained with synthetic images suffer from large domain shift problems in practice. In this paper, we define the salient object as foreground, which is consistent with human cognition and annotations of the current matting dataset. Meanwhile, data and technologies in salient object detection can be transferred to matting in a breeze. To obtain a more accurate and complete alpha matte, we propose a network called \\textbf{M}ulti-\\textbf{F}eature fusion-based \\textbf{C}oarse-to-fine Network \\textbf{(MFC-Net)}, which fully integrates multiple features for an accurate and complete alpha matte. Furthermore, we introduce image harmony in data composition to bridge the gap between synthetic and real images. More importantly, we establish the largest general matting dataset \\textbf{(Real-19k)} in the real world to date. Experiments show that our method is significantly effective on both synthetic and real-world images, and the performance in the real-world dataset is far better than existing matting-free methods. Our code and data will be released soon.",
      "paper_authors": [
        "Leo Shan Wenzhang Zhou Grace Zhao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-28",
      "update_time": "2024-05-28",
      "comments": "13 pages, 8 figures",
      "repo_url": "#"
    },
    "2405.17776": {
      "paper_id": "2405.17776v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.17776v1",
      "paper_key": "2405.17776",
      "paper_title": "The Binary Quantized Neural Network for Dense Prediction via Specially Designed Upsampling and Attention",
      "paper_url": "http://arxiv.org/abs/2405.17776v1",
      "paper_abstract": "Deep learning-based information processing consumes long time and requires huge computing resources, especially for dense prediction tasks which require an output for each pixel, like semantic segmentation and salient object detection. There are mainly two challenges for quantization of dense prediction tasks. Firstly, directly applying the upsampling operation that dense prediction tasks require is extremely crude and causes unacceptable accuracy reduction. Secondly, the complex structure of dense prediction networks means it is difficult to maintain a fast speed as well as a high accuracy when performing quantization. In this paper, we propose an effective upsampling method and an efficient attention computation strategy to transfer the success of the binary neural networks (BNN) from single prediction tasks to dense prediction tasks. Firstly, we design a simple and robust multi-branch parallel upsampling structure to achieve the high accuracy. Then we further optimize the attention method which plays an important role in segmentation but has huge computation complexity. Our attention method can reduce the computational complexity by a factor of one hundred times but retain the original effect. Experiments on Cityscapes, KITTI road, and ECSSD fully show the effectiveness of our work.",
      "paper_authors": [
        "Xingyu Ding",
        "Lianlei Shan",
        "Guiqin Zhao",
        "Meiqi Wu",
        "Wenzhang Zhou",
        "Wei Li"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-05-28",
      "update_time": "2024-05-28",
      "comments": "30 pages, 6 figures",
      "repo_url": "#"
    },
    "2405.16096": {
      "paper_id": "2405.16096v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.16096v1",
      "paper_key": "2405.16096",
      "paper_title": "MINet: Multi-scale Interactive Network for Real-time Salient Object Detection of Strip Steel Surface Defects",
      "paper_url": "http://arxiv.org/abs/2405.16096v1",
      "paper_abstract": "The automated surface defect detection is a fundamental task in industrial production, and the existing saliencybased works overcome the challenging scenes and give promising detection results. However, the cutting-edge efforts often suffer from large parameter size, heavy computational cost, and slow inference speed, which heavily limits the practical applications. To this end, we devise a multi-scale interactive (MI) module, which employs depthwise convolution (DWConv) and pointwise convolution (PWConv) to independently extract and interactively fuse features of different scales, respectively. Particularly, the MI module can provide satisfactory characterization for defect regions with fewer parameters. Embarking on this module, we propose a lightweight Multi-scale Interactive Network (MINet) to conduct real-time salient object detection of strip steel surface defects. Comprehensive experimental results on SD-Saliency-900 dataset, which contains three kinds of strip steel surface defect detection images (i.e., inclusion, patches, and scratches), demonstrate that the proposed MINet presents comparable detection accuracy with the state-of-the-art methods while running at a GPU speed of 721FPS and a CPU speed of 6.3FPS for 368*368 images with only 0.28M parameters. The code is available at https://github.com/Kunye-Shen/MINet.",
      "paper_authors": [
        "Kunye Shen",
        "Xiaofei Zhou",
        "Zhi Liu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-25",
      "update_time": "2024-05-25",
      "comments": "accepted by IEEE Transactions on Industrial Informatics",
      "repo_url": "https://github.com/kunye-shen/minet"
    },
    "2405.11855": {
      "paper_id": "2405.11855v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.11855v1",
      "paper_key": "2405.11855",
      "paper_title": "Salience-guided Ground Factor for Robust Localization of Delivery Robots in Complex Urban Environments",
      "paper_url": "http://arxiv.org/abs/2405.11855v1",
      "paper_abstract": "In urban environments for delivery robots, particularly in areas such as campuses and towns, many custom features defy standard road semantic categorizations. Addressing this challenge, our paper introduces a method leveraging Salient Object Detection (SOD) to extract these unique features, employing them as pivotal factors for enhanced robot loop closure and localization. Traditional geometric feature-based localization is hampered by fluctuating illumination and appearance changes. Our preference for SOD over semantic segmentation sidesteps the intricacies of classifying a myriad of non-standardized urban features. To achieve consistent ground features, the Motion Compensate IPM (MC-IPM) technique is implemented, capitalizing on motion for distortion compensation and subsequently selecting the most pertinent salient ground features through moment computations. For thorough evaluation, we validated the saliency detection and localization performances to the real urban scenarios. Project page: https://sites.google.com/view/salient-ground-feature/home.",
      "paper_authors": [
        "Jooyong Park",
        "Jungwoo Lee",
        "Euncheol Choi",
        "Younggun Cho"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-05-20",
      "update_time": "2024-05-20",
      "comments": "8 pages, 9 figures, 2024 IEEE International Conference on Robotics\n  and Automation (ICRA 2024)",
      "repo_url": "#"
    },
    "2405.09782": {
      "paper_id": "2405.09782v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.09782v2",
      "paper_key": "2405.09782",
      "paper_title": "Size-invariance Matters: Rethinking Metrics and Losses for Imbalanced Multi-object Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2405.09782v2",
      "paper_abstract": "This paper explores the size-invariance of evaluation metrics in Salient Object Detection (SOD), especially when multiple targets of diverse sizes co-exist in the same image. We observe that current metrics are size-sensitive, where larger objects are focused, and smaller ones tend to be ignored. We argue that the evaluation should be size-invariant because bias based on size is unjustified without additional semantic information. In pursuit of this, we propose a generic approach that evaluates each salient object separately and then combines the results, effectively alleviating the imbalance. We further develop an optimization framework tailored to this goal, achieving considerable improvements in detecting objects of different sizes. Theoretically, we provide evidence supporting the validity of our new metrics and present the generalization analysis of SOD. Extensive experiments demonstrate the effectiveness of our method. The code is available at https://github.com/Ferry-Li/SI-SOD.",
      "paper_authors": [
        "Feiran Li",
        "Qianqian Xu",
        "Shilong Bao",
        "Zhiyong Yang",
        "Runmin Cong",
        "Xiaochun Cao",
        "Qingming Huang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-16",
      "update_time": "2024-05-27",
      "comments": "This paper has been accepted by ICML2024",
      "repo_url": "https://github.com/ferry-li/si-sod"
    },
    "2405.07655": {
      "paper_id": "2405.07655v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.07655v1",
      "paper_key": "2405.07655",
      "paper_title": "Quality-aware Selective Fusion Network for V-D-T Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2405.07655v1",
      "paper_abstract": "Depth images and thermal images contain the spatial geometry information and surface temperature information, which can act as complementary information for the RGB modality. However, the quality of the depth and thermal images is often unreliable in some challenging scenarios, which will result in the performance degradation of the two-modal based salient object detection (SOD). Meanwhile, some researchers pay attention to the triple-modal SOD task, where they attempt to explore the complementarity of the RGB image, the depth image, and the thermal image. However, existing triple-modal SOD methods fail to perceive the quality of depth maps and thermal images, which leads to performance degradation when dealing with scenes with low-quality depth and thermal images. Therefore, we propose a quality-aware selective fusion network (QSF-Net) to conduct VDT salient object detection, which contains three subnets including the initial feature extraction subnet, the quality-aware region selection subnet, and the region-guided selective fusion subnet. Firstly, except for extracting features, the initial feature extraction subnet can generate a preliminary prediction map from each modality via a shrinkage pyramid architecture. Then, we design the weakly-supervised quality-aware region selection subnet to generate the quality-aware maps. Concretely, we first find the high-quality and low-quality regions by using the preliminary predictions, which further constitute the pseudo label that can be used to train this subnet. Finally, the region-guided selective fusion subnet purifies the initial features under the guidance of the quality-aware maps, and then fuses the triple-modal features and refines the edge details of prediction maps through the intra-modality and inter-modality attention (IIA) module and the edge refinement (ER) module, respectively. Extensive experiments are performed on VDT-2048",
      "paper_authors": [
        "Liuxin Bao",
        "Xiaofei Zhou",
        "Xiankai Lu",
        "Yaoqi Sun",
        "Haibing Yin",
        "Zhenghui Hu",
        "Jiyong Zhang",
        "Chenggang Yan"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-13",
      "update_time": "2024-05-13",
      "comments": "Accepted by IEEE Transactions on Image Processing (TIP)",
      "repo_url": "https://github.com/Lx-Bao/QSFNet"
    },
    "2405.03352": {
      "paper_id": "2405.03352v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.03352v2",
      "paper_key": "2405.03352",
      "paper_title": "Salient Object Detection From Arbitrary Modalities",
      "paper_url": "http://arxiv.org/abs/2405.03352v2",
      "paper_abstract": "Toward desirable saliency prediction, the types and numbers of inputs for a salient object detection (SOD) algorithm may dynamically change in many real-life applications. However, existing SOD algorithms are mainly designed or trained for one particular type of inputs, failing to be generalized to other types of inputs. Consequentially, more types of SOD algorithms need to be prepared in advance for handling different types of inputs, raising huge hardware and research costs. Differently, in this paper, we propose a new type of SOD task, termed Arbitrary Modality SOD (AM SOD). The most prominent characteristics of AM SOD are that the modality types and modality numbers will be arbitrary or dynamically changed. The former means that the inputs to the AM SOD algorithm may be arbitrary modalities such as RGB, depths, or even any combination of them. While, the latter indicates that the inputs may have arbitrary modality numbers as the input type is changed, e.g. single-modality RGB image, dual-modality RGB-Depth (RGB-D) images or triple-modality RGB-Depth-Thermal (RGB-D-T) images. Accordingly, a preliminary solution to the above challenges, \\i.e. a modality switch network (MSN), is proposed in this paper. In particular, a modality switch feature extractor (MSFE) is first designed to extract discriminative features from each modality effectively by introducing some modality indicators, which will generate some weights for modality switching. Subsequently, a dynamic fusion module (DFM) is proposed to adaptively fuse features from a variable number of modalities based on a novel Transformer structure. Finally, a new dataset, named AM-XD, is constructed to facilitate research on AM SOD. Extensive experiments demonstrate that our AM SOD method can effectively cope with changes in the type and number of input modalities for robust salient object detection.",
      "paper_authors": [
        "Nianchang Huang",
        "Yang Yang",
        "Ruida Xi",
        "Qiang Zhang",
        "Jungong Han",
        "Jin Huang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-06",
      "update_time": "2024-05-09",
      "comments": "15 Pages, 7 Figures, 8 Tables",
      "repo_url": "https://github.com/nexiakele/AMSODFirst"
    },
    "2405.03351": {
      "paper_id": "2405.03351v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.03351v1",
      "paper_key": "2405.03351",
      "paper_title": "Modality Prompts for Arbitrary Modality Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2405.03351v1",
      "paper_abstract": "This paper delves into the task of arbitrary modality salient object detection (AM SOD), aiming to detect salient objects from arbitrary modalities, eg RGB images, RGB-D images, and RGB-D-T images. A novel modality-adaptive Transformer (MAT) will be proposed to investigate two fundamental challenges of AM SOD, ie more diverse modality discrepancies caused by varying modality types that need to be processed, and dynamic fusion design caused by an uncertain number of modalities present in the inputs of multimodal fusion strategy. Specifically, inspired by prompt learning's ability of aligning the distributions of pre-trained models to the characteristic of downstream tasks by learning some prompts, MAT will first present a modality-adaptive feature extractor (MAFE) to tackle the diverse modality discrepancies by introducing a modality prompt for each modality. In the training stage, a new modality translation contractive (MTC) loss will be further designed to assist MAFE in learning those modality-distinguishable modality prompts. Accordingly, in the testing stage, MAFE can employ those learned modality prompts to adaptively adjust its feature space according to the characteristics of the input modalities, thus being able to extract discriminative unimodal features. Then, MAFE will present a channel-wise and spatial-wise fusion hybrid (CSFH) strategy to meet the demand for dynamic fusion. For that, CSFH dedicates a channel-wise dynamic fusion module (CDFM) and a novel spatial-wise dynamic fusion module (SDFM) to fuse the unimodal features from varying numbers of modalities and meanwhile effectively capture cross-modal complementary semantic and detail information, respectively. Moreover, CSFH will carefully align CDFM and SDFM to different levels of unimodal features based on their characteristics for more effective complementary information exploitation.",
      "paper_authors": [
        "Nianchang Huang",
        "Yang Yang",
        "Qiang Zhang",
        "Jungong Han",
        "Jin Huang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-06",
      "update_time": "2024-05-06",
      "comments": "13 pages, 7 Figures, 3 Tables",
      "repo_url": "#"
    },
    "2405.02906": {
      "paper_id": "2405.02906v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.02906v1",
      "paper_key": "2405.02906",
      "paper_title": "SalFAU-Net: Saliency Fusion Attention U-Net for Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2405.02906v1",
      "paper_abstract": "Salient object detection (SOD) remains an important task in computer vision, with applications ranging from image segmentation to autonomous driving. Fully convolutional network (FCN)-based methods have made remarkable progress in visual saliency detection over the last few decades. However, these methods have limitations in accurately detecting salient objects, particularly in challenging scenes with multiple objects, small objects, or objects with low resolutions. To address this issue, we proposed a Saliency Fusion Attention U-Net (SalFAU-Net) model, which incorporates a saliency fusion module into each decoder block of the attention U-net model to generate saliency probability maps from each decoder block. SalFAU-Net employs an attention mechanism to selectively focus on the most informative regions of an image and suppress non-salient regions. We train SalFAU-Net on the DUTS dataset using a binary cross-entropy loss function. We conducted experiments on six popular SOD evaluation datasets to evaluate the effectiveness of the proposed method. The experimental results demonstrate that our method, SalFAU-Net, achieves competitive performance compared to other methods in terms of mean absolute error (MAE), F-measure, s-measure, and e-measure.",
      "paper_authors": [
        "Kassaw Abraham Mulat",
        "Zhengyong Feng",
        "Tegegne Solomon Eshetie",
        "Ahmed Endris Hasen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-05",
      "update_time": "2024-05-05",
      "comments": "9 pages, 5 figures",
      "repo_url": "#"
    },
    "2404.15008": {
      "paper_id": "2404.15008v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.15008v2",
      "paper_key": "2404.15008",
      "paper_title": "External Prompt Features Enhanced Parameter-efficient Fine-tuning for Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2404.15008v2",
      "paper_abstract": "Salient object detection (SOD) aims at finding the most salient objects in images and outputs pixel-level binary masks. Transformer-based methods achieve promising performance due to their global semantic understanding, crucial for identifying salient objects. However, these models tend to be large and require numerous training parameters. To better harness the potential of transformers for SOD, we propose a novel parameter-efficient fine-tuning method aimed at reducing the number of training parameters while enhancing the salient object detection capability. Our model, termed EXternal Prompt features Enhanced adapteR Tuning (ExPert), features an encoder-decoder structure with adapters and injectors interspersed between the layers of a frozen transformer encoder. The adapter modules adapt the pretrained backbone to SOD while the injector modules incorporate external prompt features to enhance the awareness of salient objects. Comprehensive experiments demonstrate the superiority of our method. Surpassing former state-of-the-art (SOTA) models across five SOD datasets, ExPert achieves 0.215 mean absolute error (MAE) in the ECSSD dataset with 80.2M trained parameters, 21% better than SelfReformer and 47% better than EGNet.",
      "paper_authors": [
        "Wen Liang",
        "Peipei Ran",
        "Mengchao Bai",
        "Xiao Liu",
        "P. Bilha Githinji",
        "Wei Zhao",
        "Peiwu Qin"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-04-23",
      "update_time": "2024-08-24",
      "comments": "ICPR24 accepted",
      "repo_url": "#"
    },
    "2404.14759": {
      "paper_id": "2404.14759v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.14759v2",
      "paper_key": "2404.14759",
      "paper_title": "Unified Unsupervised Salient Object Detection via Knowledge Transfer",
      "paper_url": "http://arxiv.org/abs/2404.14759v2",
      "paper_abstract": "Recently, unsupervised salient object detection (USOD) has gained increasing attention due to its annotation-free nature. However, current methods mainly focus on specific tasks such as RGB and RGB-D, neglecting the potential for task migration. In this paper, we propose a unified USOD framework for generic USOD tasks. Firstly, we propose a Progressive Curriculum Learning-based Saliency Distilling (PCL-SD) mechanism to extract saliency cues from a pre-trained deep network. This mechanism starts with easy samples and progressively moves towards harder ones, to avoid initial interference caused by hard samples. Afterwards, the obtained saliency cues are utilized to train a saliency detector, and we employ a Self-rectify Pseudo-label Refinement (SPR) mechanism to improve the quality of pseudo-labels. Finally, an adapter-tuning method is devised to transfer the acquired saliency knowledge, leveraging shared knowledge to attain superior transferring performance on the target tasks. Extensive experiments on five representative SOD tasks confirm the effectiveness and feasibility of our proposed method. Code and supplement materials are available at https://github.com/I2-Multimedia-Lab/A2S-v3.",
      "paper_authors": [
        "Yao Yuan",
        "Wutao Liu",
        "Pan Gao",
        "Qun Dai",
        "Jie Qin"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-04-23",
      "update_time": "2024-07-13",
      "comments": "Accepted by IJCAI 2024",
      "repo_url": "https://github.com/I2-Multimedia-Lab/A2S-v3"
    },
    "2404.00918": {
      "paper_id": "2404.00918v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.00918v2",
      "paper_key": "2404.00918",
      "paper_title": "Rethinking Saliency-Guided Weakly-Supervised Semantic Segmentation",
      "paper_url": "http://arxiv.org/abs/2404.00918v2",
      "paper_abstract": "This paper presents a fresh perspective on the role of saliency maps in weakly-supervised semantic segmentation (WSSS) and offers new insights and research directions based on our empirical findings. We conduct comprehensive experiments and observe that the quality of the saliency map is a critical factor in saliency-guided WSSS approaches. Nonetheless, we find that the saliency maps used in previous works are often arbitrarily chosen, despite their significant impact on WSSS. Additionally, we observe that the choice of the threshold, which has received less attention before, is non-trivial in WSSS. To facilitate more meaningful and rigorous research for saliency-guided WSSS, we introduce \\texttt{WSSS-BED}, a standardized framework for conducting research under unified conditions. \\texttt{WSSS-BED} provides various saliency maps and activation maps for seven WSSS methods, as well as saliency maps from unsupervised salient object detection models.",
      "paper_authors": [
        "Beomyoung Kim",
        "Donghyun Kim",
        "Sung Ju Hwang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-04-01",
      "update_time": "2024-04-02",
      "comments": "Preprint, 17 pages, 7 figures",
      "repo_url": "https://github.com/clovaai/wsss-bed"
    },
    "2404.00694": {
      "paper_id": "2404.00694v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.00694v1",
      "paper_key": "2404.00694",
      "paper_title": "DMSSN: Distilled Mixed Spectral-Spatial Network for Hyperspectral Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2404.00694v1",
      "paper_abstract": "Hyperspectral salient object detection (HSOD) has exhibited remarkable promise across various applications, particularly in intricate scenarios where conventional RGB-based approaches fall short. Despite the considerable progress in HSOD method advancements, two critical challenges require immediate attention. Firstly, existing hyperspectral data dimension reduction techniques incur a loss of spectral information, which adversely affects detection accuracy. Secondly, previous methods insufficiently harness the inherent distinctive attributes of hyperspectral images (HSIs) during the feature extraction process. To address these challenges, we propose a novel approach termed the Distilled Mixed Spectral-Spatial Network (DMSSN), comprising a Distilled Spectral Encoding process and a Mixed Spectral-Spatial Transformer (MSST) feature extraction network. The encoding process utilizes knowledge distillation to construct a lightweight autoencoder for dimension reduction, striking a balance between robust encoding capabilities and low computational costs. The MSST extracts spectral-spatial features through multiple attention head groups, collaboratively enhancing its resistance to intricate scenarios. Moreover, we have created a large-scale HSOD dataset, HSOD-BIT, to tackle the issue of data scarcity in this field and meet the fundamental data requirements of deep network training. Extensive experiments demonstrate that our proposed DMSSN achieves state-of-the-art performance on multiple datasets. We will soon make the code and dataset publicly available on https://github.com/anonymous0519/HSOD-BIT.",
      "paper_authors": [
        "Haolin Qin",
        "Tingfa Xu",
        "Peifu Liu",
        "Jingxuan Xu",
        "Jianan Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-03-31",
      "update_time": "2024-03-31",
      "comments": null,
      "repo_url": "https://github.com/anonymous0519/hsod-bit"
    },
    "2403.18554": {
      "paper_id": "2403.18554v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.18554v2",
      "paper_key": "2403.18554",
      "paper_title": "CosalPure: Learning Concept from Group Images for Robust Co-Saliency Detection",
      "paper_url": "http://arxiv.org/abs/2403.18554v2",
      "paper_abstract": "Co-salient object detection (CoSOD) aims to identify the common and salient (usually in the foreground) regions across a given group of images. Although achieving significant progress, state-of-the-art CoSODs could be easily affected by some adversarial perturbations, leading to substantial accuracy reduction. The adversarial perturbations can mislead CoSODs but do not change the high-level semantic information (e.g., concept) of the co-salient objects. In this paper, we propose a novel robustness enhancement framework by first learning the concept of the co-salient objects based on the input group images and then leveraging this concept to purify adversarial perturbations, which are subsequently fed to CoSODs for robustness enhancement. Specifically, we propose CosalPure containing two modules, i.e., group-image concept learning and concept-guided diffusion purification. For the first module, we adopt a pre-trained text-to-image diffusion model to learn the concept of co-salient objects within group images where the learned concept is robust to adversarial examples. For the second module, we map the adversarial image to the latent space and then perform diffusion generation by embedding the learned concept into the noise prediction function as an extra condition. Our method can effectively alleviate the influence of the SOTA adversarial attack containing different adversarial patterns, including exposure and noise. The extensive results demonstrate that our method could enhance the robustness of CoSODs significantly.",
      "paper_authors": [
        "Jiayi Zhu",
        "Qing Guo",
        "Felix Juefei-Xu",
        "Yihao Huang",
        "Yang Liu",
        "Geguang Pu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-03-27",
      "update_time": "2024-04-12",
      "comments": "This paper is accepted by CVPR 2024",
      "repo_url": "#"
    },
    "2403.11107": {
      "paper_id": "2403.11107v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.11107v3",
      "paper_key": "2403.11107",
      "paper_title": "Self-supervised co-salient object detection via feature correspondence at multiple scales",
      "paper_url": "http://arxiv.org/abs/2403.11107v3",
      "paper_abstract": "Our paper introduces a novel two-stage self-supervised approach for detecting co-occurring salient objects (CoSOD) in image groups without requiring segmentation annotations. Unlike existing unsupervised methods that rely solely on patch-level information (e.g. clustering patch descriptors) or on computation heavy off-the-shelf components for CoSOD, our lightweight model leverages feature correspondences at both patch and region levels, significantly improving prediction performance. In the first stage, we train a self-supervised network that detects co-salient regions by computing local patch-level feature correspondences across images. We obtain the segmentation predictions using confidence-based adaptive thresholding. In the next stage, we refine these intermediate segmentations by eliminating the detected regions (within each image) whose averaged feature representations are dissimilar to the foreground feature representation averaged across all the cross-attention maps (from the previous stage). Extensive experiments on three CoSOD benchmark datasets show that our self-supervised model outperforms the corresponding state-of-the-art models by a huge margin (e.g. on the CoCA dataset, our model has a 13.7% F-measure gain over the SOTA unsupervised CoSOD model). Notably, our self-supervised model also outperforms several recent fully supervised CoSOD models on the three test datasets (e.g., on the CoCA dataset, our model has a 4.6% F-measure gain over a recent supervised CoSOD model).",
      "paper_authors": [
        "Souradeep Chakraborty",
        "Dimitris Samaras"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-03-17",
      "update_time": "2024-07-03",
      "comments": "Accepted to ECCV 2024",
      "repo_url": "https://github.com/sourachakra/scosparc"
    },
    "2403.10104": {
      "paper_id": "2403.10104v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.10104v1",
      "paper_key": "2403.10104",
      "paper_title": "CSDNet: Detect Salient Object in Depth-Thermal via A Lightweight Cross Shallow and Deep Perception Network",
      "paper_url": "http://arxiv.org/abs/2403.10104v1",
      "paper_abstract": "While we enjoy the richness and informativeness of multimodal data, it also introduces interference and redundancy of information. To achieve optimal domain interpretation with limited resources, we propose CSDNet, a lightweight \\textbf{C}ross \\textbf{S}hallow and \\textbf{D}eep Perception \\textbf{Net}work designed to integrate two modalities with less coherence, thereby discarding redundant information or even modality. We implement our CSDNet for Salient Object Detection (SOD) task in robotic perception. The proposed method capitalises on spatial information prescreening and implicit coherence navigation across shallow and deep layers of the depth-thermal (D-T) modality, prioritising integration over fusion to maximise the scene interpretation. To further refine the descriptive capabilities of the encoder for the less-known D-T modalities, we also propose SAMAEP to guide an effective feature mapping to the generalised feature space. Our approach is tested on the VDT-2048 dataset, leveraging the D-T modality outperforms those of SOTA methods using RGB-T or RGB-D modalities for the first time, achieves comparable performance with the RGB-D-T triple-modality benchmark method with 5.97 times faster at runtime and demanding 0.0036 times fewer FLOPs. Demonstrates the proposed CSDNet effectively integrates the information from the D-T modality. The code will be released upon acceptance.",
      "paper_authors": [
        "Xiaotong Yu",
        "Ruihan Xie",
        "Zhihe Zhao",
        "Chang-Wen Chen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-03-15",
      "update_time": "2024-03-15",
      "comments": null,
      "repo_url": "#"
    },
    "2403.06070": {
      "paper_id": "2403.06070v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.06070v1",
      "paper_key": "2403.06070",
      "paper_title": "Reframe Anything: LLM Agent for Open World Video Reframing",
      "paper_url": "http://arxiv.org/abs/2403.06070v1",
      "paper_abstract": "The proliferation of mobile devices and social media has revolutionized content dissemination, with short-form video becoming increasingly prevalent. This shift has introduced the challenge of video reframing to fit various screen aspect ratios, a process that highlights the most compelling parts of a video. Traditionally, video reframing is a manual, time-consuming task requiring professional expertise, which incurs high production costs. A potential solution is to adopt some machine learning models, such as video salient object detection, to automate the process. However, these methods often lack generalizability due to their reliance on specific training data. The advent of powerful large language models (LLMs) open new avenues for AI capabilities. Building on this, we introduce Reframe Any Video Agent (RAVA), a LLM-based agent that leverages visual foundation models and human instructions to restructure visual content for video reframing. RAVA operates in three stages: perception, where it interprets user instructions and video content; planning, where it determines aspect ratios and reframing strategies; and execution, where it invokes the editing tools to produce the final video. Our experiments validate the effectiveness of RAVA in video salient object detection and real-world reframing tasks, demonstrating its potential as a tool for AI-powered video editing.",
      "paper_authors": [
        "Jiawang Cao",
        "Yongliang Wu",
        "Weiheng Chi",
        "Wenbo Zhu",
        "Ziyue Su",
        "Jay Wu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-03-10",
      "update_time": "2024-03-10",
      "comments": "14 pages, 6 figures",
      "repo_url": "#"
    },
    "2402.18922": {
      "paper_id": "2402.18922v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.18922v1",
      "paper_key": "2402.18922",
      "paper_title": "A Simple yet Effective Network based on Vision Transformer for Camouflaged Object and Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2402.18922v1",
      "paper_abstract": "Camouflaged object detection (COD) and salient object detection (SOD) are two distinct yet closely-related computer vision tasks widely studied during the past decades. Though sharing the same purpose of segmenting an image into binary foreground and background regions, their distinction lies in the fact that COD focuses on concealed objects hidden in the image, while SOD concentrates on the most prominent objects in the image. Previous works achieved good performance by stacking various hand-designed modules and multi-scale features. However, these carefully-designed complex networks often performed well on one task but not on another. In this work, we propose a simple yet effective network (SENet) based on vision Transformer (ViT), by employing a simple design of an asymmetric ViT-based encoder-decoder structure, we yield competitive results on both tasks, exhibiting greater versatility than meticulously crafted ones. Furthermore, to enhance the Transformer's ability to model local information, which is important for pixel-level binary segmentation tasks, we propose a local information capture module (LICM). We also propose a dynamic weighted loss (DW loss) based on Binary Cross-Entropy (BCE) and Intersection over Union (IoU) loss, which guides the network to pay more attention to those smaller and more difficult-to-find target objects according to their size. Moreover, we explore the issue of joint training of SOD and COD, and propose a preliminary solution to the conflict in joint training, further improving the performance of SOD. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our method. The code is available at https://github.com/linuxsino/SENet.",
      "paper_authors": [
        "Chao Hao",
        "Zitong Yu",
        "Xin Liu",
        "Jun Xu",
        "Huanjing Yue",
        "Jingyu Yang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-02-29",
      "update_time": "2024-02-29",
      "comments": "submitted to IEEE TIP",
      "repo_url": "#"
    },
    "2402.02096": {
      "paper_id": "2402.02096v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.02096v1",
      "paper_key": "2402.02096",
      "paper_title": "Decomposition-based and Interference Perception for Infrared and Visible Image Fusion in Complex Scenes",
      "paper_url": "http://arxiv.org/abs/2402.02096v1",
      "paper_abstract": "Infrared and visible image fusion has emerged as a prominent research in computer vision. However, little attention has been paid on complex scenes fusion, causing existing techniques to produce sub-optimal results when suffers from real interferences. To fill this gap, we propose a decomposition-based and interference perception image fusion method. Specifically, we classify the pixels of visible image from the degree of scattering of light transmission, based on which we then separate the detail and energy information of the image. This refined decomposition facilitates the proposed model in identifying more interfering pixels that are in complex scenes. To strike a balance between denoising and detail preservation, we propose an adaptive denoising scheme for fusing detail components. Meanwhile, we propose a new weighted fusion rule by considering the distribution of image energy information from the perspective of multiple directions. Extensive experiments in complex scenes fusions cover adverse weathers, noise, blur, overexposure, fire, as well as downstream tasks including semantic segmentation, object detection, salient object detection and depth estimation, consistently indicate the effectiveness and superiority of the proposed method compared with the recent representative methods.",
      "paper_authors": [
        "Xilai Li",
        "Xiaosong Li",
        "Haishu Tan"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-02-03",
      "update_time": "2024-02-03",
      "comments": null,
      "repo_url": "#"
    },
    "2401.16712": {
      "paper_id": "2401.16712v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.16712v2",
      "paper_key": "2401.16712",
      "paper_title": "LF Tracy: A Unified Single-Pipeline Approach for Salient Object Detection in Light Field Cameras",
      "paper_url": "http://arxiv.org/abs/2401.16712v2",
      "paper_abstract": "Leveraging rich information is crucial for dense prediction tasks. Light field (LF) cameras are instrumental in this regard, as they allow data to be sampled from various perspectives. This capability provides valuable spatial, depth, and angular information, enhancing scene-parsing tasks. However, we have identified two overlooked issues for the LF salient object detection (SOD) task. (1): Previous approaches predominantly employ a customized two-stream design to discover the spatial and depth features within light field images. The network struggles to learn the implicit angular information between different images due to a lack of intra-network data connectivity. (2): Little research has been directed towards the data augmentation strategy for LF SOD. Research on inter-network data connectivity is scant. In this study, we propose an efficient paradigm (LF Tracy) to address those issues. This comprises a single-pipeline encoder paired with a highly efficient information aggregation (IA) module (around 8M parameters) to establish an intra-network connection. Then, a simple yet effective data augmentation strategy called MixLD is designed to bridge the inter-network connections. Owing to this innovative paradigm, our model surpasses the existing state-of-the-art method through extensive experiments. Especially, LF Tracy demonstrates a 23% improvement over previous results on the latest large-scale PKU dataset. The source code is publicly available at: https://github.com/FeiBryantkit/LF-Tracy.",
      "paper_authors": [
        "Fei Teng",
        "Jiaming Zhang",
        "Jiawei Liu",
        "Kunyu Peng",
        "Xina Cheng",
        "Zhiyong Li",
        "Kailun Yang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-01-30",
      "update_time": "2024-08-26",
      "comments": "Accepted to ICPR 2024. The source code is publicly available at:\n  https://github.com/FeiBryantkit/LF-Tracy",
      "repo_url": "https://github.com/feibryantkit/lf-tracy"
    },
    "2401.11914": {
      "paper_id": "2401.11914v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.11914v1",
      "paper_key": "2401.11914",
      "paper_title": "A Saliency Enhanced Feature Fusion based multiscale RGB-D Salient Object Detection Network",
      "paper_url": "http://arxiv.org/abs/2401.11914v1",
      "paper_abstract": "Multiscale convolutional neural network (CNN) has demonstrated remarkable capabilities in solving various vision problems. However, fusing features of different scales alwaysresults in large model sizes, impeding the application of multiscale CNNs in RGB-D saliency detection. In this paper, we propose a customized feature fusion module, called Saliency Enhanced Feature Fusion (SEFF), for RGB-D saliency detection. SEFF utilizes saliency maps of the neighboring scales to enhance the necessary features for fusing, resulting in more representative fused features. Our multiscale RGB-D saliency detector uses SEFF and processes images with three different scales. SEFF is used to fuse the features of RGB and depth images, as well as the features of decoders at different scales. Extensive experiments on five benchmark datasets have demonstrated the superiority of our method over ten SOTA saliency detectors.",
      "paper_authors": [
        "Rui Huang",
        "Qingyi Zhao",
        "Yan Xing",
        "Sihua Gao",
        "Weifeng Xu",
        "Yuxiang Zhang",
        "Wei Fan"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-01-22",
      "update_time": "2024-01-22",
      "comments": "Accpeted by 2024 IEEE International Conference on Acoustics, Speech,\n  and Signal Processing (ICASSP 2024)",
      "repo_url": "#"
    },
    "2403.12057": {
      "paper_id": "2403.12057v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.12057v1",
      "paper_key": "2403.12057",
      "paper_title": "Discriminative Consensus Mining with A Thousand Groups for More Accurate Co-Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2403.12057v1",
      "paper_abstract": "Co-Salient Object Detection (CoSOD) is a rapidly growing task, extended from Salient Object Detection (SOD) and Common Object Segmentation (Co-Segmentation). It is aimed at detecting the co-occurring salient object in the given image group. Many effective approaches have been proposed on the basis of existing datasets. However, there is still no standard and efficient training set in CoSOD, which makes it chaotic to choose training sets in the recently proposed CoSOD methods. First, the drawbacks of existing training sets in CoSOD are analyzed in a comprehensive way, and potential improvements are provided to solve existing problems to some extent. In particular, in this thesis, a new CoSOD training set is introduced, named Co-Saliency of ImageNet (CoSINe) dataset. The proposed CoSINe is the largest number of groups among all existing CoSOD datasets. The images obtained here span a wide variety in terms of categories, object sizes, etc. In experiments, models trained on CoSINe can achieve significantly better performance with fewer images compared to all existing datasets. Second, to make the most of the proposed CoSINe, a novel CoSOD approach named Hierarchical Instance-aware COnsensus MinEr (HICOME) is proposed, which efficiently mines the consensus feature from different feature levels and discriminates objects of different classes in an object-aware contrastive way. As extensive experiments show, the proposed HICOME achieves SoTA performance on all the existing CoSOD test sets. Several useful training tricks suitable for training CoSOD models are also provided. Third, practical applications are given using the CoSOD technique to show the effectiveness. Finally, the remaining challenges and potential improvements of CoSOD are discussed to inspire related work in the future. The source code, the dataset, and the online demo will be publicly available at github.com/ZhengPeng7/CoSINe.",
      "paper_authors": [
        "Peng Zheng"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-01-15",
      "update_time": "2024-01-15",
      "comments": "Master's thesis of the M.S. program at Aalto University and\n  University of Trento, v1. The source codes, the dataset, and the demos will\n  not be released until the conference/journal version is finished and made\n  public",
      "repo_url": "#"
    },
    "2312.06736": {
      "paper_id": "2312.06736v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.06736v3",
      "paper_key": "2312.06736",
      "paper_title": "SqueezeSAM: User friendly mobile interactive segmentation",
      "paper_url": "http://arxiv.org/abs/2312.06736v3",
      "paper_abstract": "The Segment Anything Model (SAM) has been a cornerstone in the field of interactive segmentation, propelling significant progress in generative AI, computational photography, and medical imaging. Despite its ability to process arbitrary user input and generate corresponding segmentation masks, SAM's 600 million parameter architecture, based on ViT-H, is not compatible with current mobile hardware due to its high computational demands and large model size. Our research aims to adapt SAM for use in mobile photography applications. To this end, we have developed a fully convolutional SqueezeSAM model architecture, which is 62.5 times faster and 31.6 times smaller than the original SAM, making it a viable solution for mobile applications. Furthermore, our tiny model achieves an mIOU within 1% of the original VIT-H architecture.   Automated segmentation holds significant value in the creation flow for photography applications, as evidenced by its adoption by leading industry players like apple and capcut. To facilitate this automation, we employ salient object detection and simulate potential user clicks for foreground object selection, generating an initial segmentation mask that users can subsequently edit interactively. A common user expectation is that a click on a specific part of an object will result in the segmentation of the entire object. For example, a click on a person's t-shirt in a photo should ideally segment the entire person, not just the t-shirt. However, SAM typically only segments the clicked area. We address this limitation through a novel data augmentation scheme. Consequently, if a user clicks on a person holding a basketball, both the person and the basketball are segmented together, aligning with user expectations and enhancing the overall user experience.",
      "paper_authors": [
        "Balakrishnan Varadarajan",
        "Bilge Soran",
        "Forrest Iandola",
        "Xiaoyu Xiang",
        "Yunyang Xiong",
        "Lemeng Wu",
        "Chenchen Zhu",
        "Raghuraman Krishnamoorthi",
        "Vikas Chandra"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-12-11",
      "update_time": "2024-05-20",
      "comments": null,
      "repo_url": "#"
    },
    "2312.03548": {
      "paper_id": "2312.03548v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.03548v1",
      "paper_key": "2312.03548",
      "paper_title": "Texture-Semantic Collaboration Network for ORSI Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2312.03548v1",
      "paper_abstract": "Salient object detection (SOD) in optical remote sensing images (ORSIs) has become increasingly popular recently. Due to the characteristics of ORSIs, ORSI-SOD is full of challenges, such as multiple objects, small objects, low illuminations, and irregular shapes. To address these challenges, we propose a concise yet effective Texture-Semantic Collaboration Network (TSCNet) to explore the collaboration of texture cues and semantic cues for ORSI-SOD. Specifically, TSCNet is based on the generic encoder-decoder structure. In addition to the encoder and decoder, TSCNet includes a vital Texture-Semantic Collaboration Module (TSCM), which performs valuable feature modulation and interaction on basic features extracted from the encoder. The main idea of our TSCM is to make full use of the texture features at the lowest level and the semantic features at the highest level to achieve the expression enhancement of salient regions on features. In the TSCM, we first enhance the position of potential salient regions using semantic features. Then, we render and restore the object details using the texture features. Meanwhile, we also perceive regions of various scales, and construct interactions between different regions. Thanks to the perfect combination of TSCM and generic structure, our TSCNet can take care of both the position and details of salient objects, effectively handling various scenes. Extensive experiments on three datasets demonstrate that our TSCNet achieves competitive performance compared to 14 state-of-the-art methods. The code and results of our method are available at https://github.com/MathLee/TSCNet.",
      "paper_authors": [
        "Gongyang Li",
        "Zhen Bai",
        "Zhi Liu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-12-06",
      "update_time": "2023-12-06",
      "comments": "5 pages, 3 figures, Accepted by IEEE Transactions on Circuits and\n  Systems II: Express Briefs 2023",
      "repo_url": "https://github.com/mathlee/tscnet"
    },
    "2312.03226": {
      "paper_id": "2312.03226v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.03226v1",
      "paper_key": "2312.03226",
      "paper_title": "Rethinking Object Saliency Ranking: A Novel Whole-flow Processing Paradigm",
      "paper_url": "http://arxiv.org/abs/2312.03226v1",
      "paper_abstract": "Existing salient object detection methods are capable of predicting binary maps that highlight visually salient regions. However, these methods are limited in their ability to differentiate the relative importance of multiple objects and the relationships among them, which can lead to errors and reduced accuracy in downstream tasks that depend on the relative importance of multiple objects. To conquer, this paper proposes a new paradigm for saliency ranking, which aims to completely focus on ranking salient objects by their \"importance order\". While previous works have shown promising performance, they still face ill-posed problems. First, the saliency ranking ground truth (GT) orders generation methods are unreasonable since determining the correct ranking order is not well-defined, resulting in false alarms. Second, training a ranking model remains challenging because most saliency ranking methods follow the multi-task paradigm, leading to conflicts and trade-offs among different tasks. Third, existing regression-based saliency ranking methods are complex for saliency ranking models due to their reliance on instance mask-based saliency ranking orders. These methods require a significant amount of data to perform accurately and can be challenging to implement effectively. To solve these problems, this paper conducts an in-depth analysis of the causes and proposes a whole-flow processing paradigm of saliency ranking task from the perspective of \"GT data generation\", \"network structure design\" and \"training protocol\". The proposed approach outperforms existing state-of-the-art methods on the widely-used SALICON set, as demonstrated by extensive experiments with fair and reasonable comparisons. The saliency ranking task is still in its infancy, and our proposed unified framework can serve as a fundamental strategy to guide future work.",
      "paper_authors": [
        "Mengke Song",
        "Linfeng Li",
        "Dunquan Wu",
        "Wenfeng Song",
        "Chenglizhao Chen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-12-06",
      "update_time": "2023-12-06",
      "comments": "16 pages, 14 figures, accepted by IEEE Transactions on Image\n  Processing",
      "repo_url": "https://github.com/mengkesong/saliency-ranking-paradigm"
    },
    "2312.01060": {
      "paper_id": "2312.01060v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.01060v1",
      "paper_key": "2312.01060",
      "paper_title": "Spectrum-driven Mixed-frequency Network for Hyperspectral Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2312.01060v1",
      "paper_abstract": "Hyperspectral salient object detection (HSOD) aims to detect spectrally salient objects in hyperspectral images (HSIs). However, existing methods inadequately utilize spectral information by either converting HSIs into false-color images or converging neural networks with clustering. We propose a novel approach that fully leverages the spectral characteristics by extracting two distinct frequency components from the spectrum: low-frequency Spectral Saliency and high-frequency Spectral Edge. The Spectral Saliency approximates the region of salient objects, while the Spectral Edge captures edge information of salient objects. These two complementary components, crucial for HSOD, are derived by computing from the inter-layer spectral angular distance of the Gaussian pyramid and the intra-neighborhood spectral angular gradients, respectively. To effectively utilize this dual-frequency information, we introduce a novel lightweight Spectrum-driven Mixed-frequency Network (SMN). SMN incorporates two parameter-free plug-and-play operators, namely Spectral Saliency Generator and Spectral Edge Operator, to extract the Spectral Saliency and Spectral Edge components from the input HSI independently. Subsequently, the Mixed-frequency Attention module, comprised of two frequency-dependent heads, intelligently combines the embedded features of edge and saliency information, resulting in a mixed-frequency feature representation. Furthermore, a saliency-edge-aware decoder progressively scales up the mixed-frequency feature while preserving rich detail and saliency information for accurate salient object prediction. Extensive experiments conducted on the HS-SOD benchmark and our custom dataset HSOD-BIT demonstrate that our SMN outperforms state-of-the-art methods regarding HSOD performance. Code and dataset will be available at https://github.com/laprf/SMN.",
      "paper_authors": [
        "Peifu Liu",
        "Tingfa Xu",
        "Huan Chen",
        "Shiyun Zhou",
        "Haolin Qin",
        "Jianan Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-12-02",
      "update_time": "2023-12-02",
      "comments": "Accepted by IEEE Transactions on Multimedia, to be published",
      "repo_url": "https://github.com/laprf/smn"
    },
    "2312.00360": {
      "paper_id": "2312.00360v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.00360v2",
      "paper_key": "2312.00360",
      "paper_title": "Efficient Multimodal Semantic Segmentation via Dual-Prompt Learning",
      "paper_url": "http://arxiv.org/abs/2312.00360v2",
      "paper_abstract": "Multimodal (e.g., RGB-Depth/RGB-Thermal) fusion has shown great potential for improving semantic segmentation in complex scenes (e.g., indoor/low-light conditions). Existing approaches often fully fine-tune a dual-branch encoder-decoder framework with a complicated feature fusion strategy for achieving multimodal semantic segmentation, which is training-costly due to the massive parameter updates in feature extraction and fusion. To address this issue, we propose a surprisingly simple yet effective dual-prompt learning network (dubbed DPLNet) for training-efficient multimodal (e.g., RGB-D/T) semantic segmentation. The core of DPLNet is to directly adapt a frozen pre-trained RGB model to multimodal semantic segmentation, reducing parameter updates. For this purpose, we present two prompt learning modules, comprising multimodal prompt generator (MPG) and multimodal feature adapter (MFA). MPG works to fuse the features from different modalities in a compact manner and is inserted from shadow to deep stages to generate the multi-level multimodal prompts that are injected into the frozen backbone, while MPG adapts prompted multimodal features in the frozen backbone for better multimodal semantic segmentation. Since both the MPG and MFA are lightweight, only a few trainable parameters (3.88M, 4.4% of the pre-trained backbone parameters) are introduced for multimodal feature fusion and learning. Using a simple decoder (3.27M parameters), DPLNet achieves new state-of-the-art performance or is on a par with other complex approaches on four RGB-D/T semantic segmentation datasets while satisfying parameter efficiency. Moreover, we show that DPLNet is general and applicable to other multimodal tasks such as salient object detection and video semantic segmentation. Without special design, DPLNet outperforms many complicated models. Our code will be available at github.com/ShaohuaDong2021/DPLNet.",
      "paper_authors": [
        "Shaohua Dong",
        "Yunhe Feng",
        "Qing Yang",
        "Yan Huang",
        "Dongfang Liu",
        "Heng Fan"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-12-01",
      "update_time": "2023-12-04",
      "comments": "11 pages, 4 figures, 9 tables",
      "repo_url": "https://github.com/shaohuadong2021/dplnet"
    },
    "2311.18675": {
      "paper_id": "2311.18675v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.18675v1",
      "paper_key": "2311.18675",
      "paper_title": "Cascaded Interaction with Eroded Deep Supervision for Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2311.18675v1",
      "paper_abstract": "Deep convolutional neural networks have been widely applied in salient object detection and have achieved remarkable results in this field. However, existing models suffer from information distortion caused by interpolation during up-sampling and down-sampling. In response to this drawback, this article starts from two directions in the network: feature and label. On the one hand, a novel cascaded interaction network with a guidance module named global-local aligned attention (GAA) is designed to reduce the negative impact of interpolation on the feature side. On the other hand, a deep supervision strategy based on edge erosion is proposed to reduce the negative guidance of label interpolation on lateral output. Extensive experiments on five popular datasets demonstrate the superiority of our method.",
      "paper_authors": [
        "Hewen Xiao",
        "Jie Mei",
        "Guangfu Ma",
        "Weiren Wu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-30",
      "update_time": "2023-11-30",
      "comments": null,
      "repo_url": "#"
    },
    "2311.18286": {
      "paper_id": "2311.18286v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.18286v1",
      "paper_key": "2311.18286",
      "paper_title": "SimulFlow: Simultaneously Extracting Feature and Identifying Target for Unsupervised Video Object Segmentation",
      "paper_url": "http://arxiv.org/abs/2311.18286v1",
      "paper_abstract": "Unsupervised video object segmentation (UVOS) aims at detecting the primary objects in a given video sequence without any human interposing. Most existing methods rely on two-stream architectures that separately encode the appearance and motion information before fusing them to identify the target and generate object masks. However, this pipeline is computationally expensive and can lead to suboptimal performance due to the difficulty of fusing the two modalities properly. In this paper, we propose a novel UVOS model called SimulFlow that simultaneously performs feature extraction and target identification, enabling efficient and effective unsupervised video object segmentation. Concretely, we design a novel SimulFlow Attention mechanism to bridege the image and motion by utilizing the flexibility of attention operation, where coarse masks predicted from fused feature at each stage are used to constrain the attention operation within the mask area and exclude the impact of noise. Because of the bidirectional information flow between visual and optical flow features in SimulFlow Attention, no extra hand-designed fusing module is required and we only adopt a light decoder to obtain the final prediction. We evaluate our method on several benchmark datasets and achieve state-of-the-art results. Our proposed approach not only outperforms existing methods but also addresses the computational complexity and fusion difficulties caused by two-stream architectures. Our models achieve 87.4% J & F on DAVIS-16 with the highest speed (63.7 FPS on a 3090) and the lowest parameters (13.7 M). Our SimulFlow also obtains competitive results on video salient object detection datasets.",
      "paper_authors": [
        "Lingyi Hong",
        "Wei Zhang",
        "Shuyong Gao",
        "Hong Lu",
        "WenQiang Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-30",
      "update_time": "2023-11-30",
      "comments": "Accepted to ACM MM 2023",
      "repo_url": "#"
    },
    "2311.16835": {
      "paper_id": "2311.16835v5",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.16835v5",
      "paper_key": "2311.16835",
      "paper_title": "Unified-modal Salient Object Detection via Adaptive Prompt Learning",
      "paper_url": "http://arxiv.org/abs/2311.16835v5",
      "paper_abstract": "Existing single-modal and multi-modal salient object detection (SOD) methods focus on designing specific architectures tailored for their respective tasks. However, developing completely different models for different tasks leads to labor and time consumption, as well as high computational and practical deployment costs. In this paper, we attempt to address both single-modal and multi-modal SOD in a unified framework called UniSOD, which fully exploits the overlapping prior knowledge between different tasks. Nevertheless, assigning appropriate strategies to modality variable inputs is challenging. To this end, UniSOD learns modality-aware prompts with task-specific hints through adaptive prompt learning, which are plugged into the proposed pre-trained baseline SOD model to handle corresponding tasks, while only requiring few learnable parameters compared to training the entire model. Each modality-aware prompt is generated from a switchable prompt generation block, which adaptively performs structural switching based on single-modal and multi-modal inputs without human intervention. Through end-to-end joint training, UniSOD achieves overall performance improvement on 14 benchmark datasets for RGB, RGB-D, and RGB-T SOD, which demonstrates that our method effectively and efficiently unifies single-modal and multi-modal SOD tasks.The code and results are available at https://github.com/Angknpng/UniSOD.",
      "paper_authors": [
        "Kunpeng Wang",
        "Chenglong Li",
        "Zhengzheng Tu",
        "Zhengyi Liu",
        "Bin Luo"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-28",
      "update_time": "2024-06-05",
      "comments": "13 pages, 11 figures",
      "repo_url": "https://github.com/angknpng/unisod"
    },
    "2311.15011": {
      "paper_id": "2311.15011v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.15011v3",
      "paper_key": "2311.15011",
      "paper_title": "VSCode: General Visual Salient and Camouflaged Object Detection with 2D Prompt Learning",
      "paper_url": "http://arxiv.org/abs/2311.15011v3",
      "paper_abstract": "Salient object detection (SOD) and camouflaged object detection (COD) are related yet distinct binary mapping tasks. These tasks involve multiple modalities, sharing commonalities and unique cues. Existing research often employs intricate task-specific specialist models, potentially leading to redundancy and suboptimal results. We introduce VSCode, a generalist model with novel 2D prompt learning, to jointly address four SOD tasks and three COD tasks. We utilize VST as the foundation model and introduce 2D prompts within the encoder-decoder architecture to learn domain and task-specific knowledge on two separate dimensions. A prompt discrimination loss helps disentangle peculiarities to benefit model optimization. VSCode outperforms state-of-the-art methods across six tasks on 26 datasets and exhibits zero-shot generalization to unseen tasks by combining 2D prompts, such as RGB-D COD. Source code has been available at https://github.com/Sssssuperior/VSCode.",
      "paper_authors": [
        "Ziyang Luo",
        "Nian Liu",
        "Wangbo Zhao",
        "Xuguang Yang",
        "Dingwen Zhang",
        "Deng-Ping Fan",
        "Fahad Khan",
        "Junwei Han"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-25",
      "update_time": "2024-04-11",
      "comments": "Accepted by CVPR2024",
      "repo_url": "https://github.com/sssssuperior/vscode"
    },
    "2311.14746": {
      "paper_id": "2311.14746v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.14746v1",
      "paper_key": "2311.14746",
      "paper_title": "All in One: RGB, RGB-D, and RGB-T Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2311.14746v1",
      "paper_abstract": "Salient object detection (SOD) aims to identify the most attractive objects within an image. Depending on the type of data being detected, SOD can be categorized into various forms, including RGB, RGB-D (Depth), RGB-T (Thermal) and light field SOD. Previous researches have focused on saliency detection with individual data type. If the RGB-D SOD model is forced to detect RGB-T data it will perform poorly. We propose an innovative model framework that provides a unified solution for the salient object detection task of three types of data (RGB, RGB-D, and RGB-T). The three types of data can be handled in one model (all in one) with the same weight parameters. In this framework, the three types of data are concatenated in an ordered manner within a single input batch, and features are extracted using a transformer network. Based on this framework, we propose an efficient lightweight SOD model, namely AiOSOD, which can detect any RGB, RGB-D, and RGB-T data with high speed (780FPS for RGB data, 485FPS for RGB-D or RGB-T data). Notably, with only 6.25M parameters, AiOSOD achieves excellent performance on RGB, RGB-D, and RGB-T datasets.",
      "paper_authors": [
        "Xingzhao Jia",
        "Zhongqiu Zhao",
        "Changlei Dongye",
        "Zhao Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-23",
      "update_time": "2023-11-23",
      "comments": null,
      "repo_url": "#"
    },
    "2311.06654": {
      "paper_id": "2311.06654v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.06654v1",
      "paper_key": "2311.06654",
      "paper_title": "Unsupervised and semi-supervised co-salient object detection via segmentation frequency statistics",
      "paper_url": "http://arxiv.org/abs/2311.06654v1",
      "paper_abstract": "In this paper, we address the detection of co-occurring salient objects (CoSOD) in an image group using frequency statistics in an unsupervised manner, which further enable us to develop a semi-supervised method. While previous works have mostly focused on fully supervised CoSOD, less attention has been allocated to detecting co-salient objects when limited segmentation annotations are available for training. Our simple yet effective unsupervised method US-CoSOD combines the object co-occurrence frequency statistics of unsupervised single-image semantic segmentations with salient foreground detections using self-supervised feature learning. For the first time, we show that a large unlabeled dataset e.g. ImageNet-1k can be effectively leveraged to significantly improve unsupervised CoSOD performance. Our unsupervised model is a great pre-training initialization for our semi-supervised model SS-CoSOD, especially when very limited labeled data is available for training. To avoid propagating erroneous signals from predictions on unlabeled data, we propose a confidence estimation module to guide our semi-supervised training. Extensive experiments on three CoSOD benchmark datasets show that both of our unsupervised and semi-supervised models outperform the corresponding state-of-the-art models by a significant margin (e.g., on the Cosal2015 dataset, our US-CoSOD model has an 8.8% F-measure gain over a SOTA unsupervised co-segmentation model and our SS-CoSOD model has an 11.81% F-measure gain over a SOTA semi-supervised CoSOD model).",
      "paper_authors": [
        "Souradeep Chakraborty",
        "Shujon Naha",
        "Muhammet Bastan",
        "Amit Kumar K C",
        "Dimitris Samaras"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-11",
      "update_time": "2023-11-11",
      "comments": "Accepted at IEEE WACV 2024",
      "repo_url": "#"
    },
    "2311.04828": {
      "paper_id": "2311.04828v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.04828v2",
      "paper_key": "2311.04828",
      "paper_title": "SODAWideNet -- Salient Object Detection with an Attention augmented Wide Encoder Decoder network without ImageNet pre-training",
      "paper_url": "http://arxiv.org/abs/2311.04828v2",
      "paper_abstract": "Developing a new Salient Object Detection (SOD) model involves selecting an ImageNet pre-trained backbone and creating novel feature refinement modules to use backbone features. However, adding new components to a pre-trained backbone needs retraining the whole network on the ImageNet dataset, which requires significant time. Hence, we explore developing a neural network from scratch directly trained on SOD without ImageNet pre-training. Such a formulation offers full autonomy to design task-specific components. To that end, we propose SODAWideNet, an encoder-decoder-style network for Salient Object Detection. We deviate from the commonly practiced paradigm of narrow and deep convolutional models to a wide and shallow architecture, resulting in a parameter-efficient deep neural network. To achieve a shallower network, we increase the receptive field from the beginning of the network using a combination of dilated convolutions and self-attention. Therefore, we propose Multi Receptive Field Feature Aggregation Module (MRFFAM) that efficiently obtains discriminative features from farther regions at higher resolutions using dilated convolutions. Next, we propose Multi-Scale Attention (MSA), which creates a feature pyramid and efficiently computes attention across multiple resolutions to extract global features from larger feature maps. Finally, we propose two variants, SODAWideNet-S (3.03M) and SODAWideNet (9.03M), that achieve competitive performance against state-of-the-art models on five datasets.",
      "paper_authors": [
        "Rohit Venkata Sai Dulam",
        "Chandra Kambhamettu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-08",
      "update_time": "2023-11-09",
      "comments": "Accepted at ISVC'23",
      "repo_url": "https://github.com/VimsLab/SODAWideNet"
    },
    "2310.15482": {
      "paper_id": "2310.15482v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.15482v2",
      "paper_key": "2310.15482",
      "paper_title": "Salient Object Detection in RGB-D Videos",
      "paper_url": "http://arxiv.org/abs/2310.15482v2",
      "paper_abstract": "Given the widespread adoption of depth-sensing acquisition devices, RGB-D videos and related data/media have gained considerable traction in various aspects of daily life. Consequently, conducting salient object detection (SOD) in RGB-D videos presents a highly promising and evolving avenue. Despite the potential of this area, SOD in RGB-D videos remains somewhat under-explored, with RGB-D SOD and video SOD (VSOD) traditionally studied in isolation. To explore this emerging field, this paper makes two primary contributions: the dataset and the model. On one front, we construct the RDVS dataset, a new RGB-D VSOD dataset with realistic depth and characterized by its diversity of scenes and rigorous frame-by-frame annotations. We validate the dataset through comprehensive attribute and object-oriented analyses, and provide training and testing splits. Moreover, we introduce DCTNet+, a three-stream network tailored for RGB-D VSOD, with an emphasis on RGB modality and treats depth and optical flow as auxiliary modalities. In pursuit of effective feature enhancement, refinement, and fusion for precise final prediction, we propose two modules: the multi-modal attention module (MAM) and the refinement fusion module (RFM). To enhance interaction and fusion within RFM, we design a universal interaction module (UIM) and then integrate holistic multi-modal attentive paths (HMAPs) for refining multi-modal low-level features before reaching RFMs. Comprehensive experiments, conducted on pseudo RGB-D video datasets alongside our RDVS, highlight the superiority of DCTNet+ over 17 VSOD models and 14 RGB-D SOD models. Ablation experiments were performed on both pseudo and realistic RGB-D video datasets to demonstrate the advantages of individual modules as well as the necessity of introducing realistic depth. Our code together with RDVS dataset will be available at https://github.com/kerenfu/RDVS/.",
      "paper_authors": [
        "Ao Mou",
        "Yukang Lu",
        "Jiahao He",
        "Dingyao Min",
        "Keren Fu",
        "Qijun Zhao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-10-24",
      "update_time": "2024-05-21",
      "comments": "IEEE TIP (under major revision)",
      "repo_url": "https://github.com/kerenfu/rdvs"
    },
    "2310.11725": {
      "paper_id": "2310.11725v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.11725v2",
      "paper_key": "2310.11725",
      "paper_title": "VST++: Efficient and Stronger Visual Saliency Transformer",
      "paper_url": "http://arxiv.org/abs/2310.11725v2",
      "paper_abstract": "While previous CNN-based models have exhibited promising results for salient object detection (SOD), their ability to explore global long-range dependencies is restricted. Our previous work, the Visual Saliency Transformer (VST), addressed this constraint from a transformer-based sequence-to-sequence perspective, to unify RGB and RGB-D SOD. In VST, we developed a multi-task transformer decoder that concurrently predicts saliency and boundary outcomes in a pure transformer architecture. Moreover, we introduced a novel token upsampling method called reverse T2T for predicting a high-resolution saliency map effortlessly within transformer-based structures. Building upon the VST model, we further propose an efficient and stronger VST version in this work, i.e. VST++. To mitigate the computational costs of the VST model, we propose a Select-Integrate Attention (SIA) module, partitioning foreground into fine-grained segments and aggregating background information into a single coarse-grained token. To incorporate 3D depth information with low cost, we design a novel depth position encoding method tailored for depth maps. Furthermore, we introduce a token-supervised prediction loss to provide straightforward guidance for the task-related tokens. We evaluate our VST++ model across various transformer-based backbones on RGB, RGB-D, and RGB-T SOD benchmark datasets. Experimental results show that our model outperforms existing methods while achieving a 25% reduction in computational costs without significant performance compromise. The demonstrated strong ability for generalization, enhanced performance, and heightened efficiency of our VST++ model highlight its potential.",
      "paper_authors": [
        "Nian Liu",
        "Ziyang Luo",
        "Ni Zhang",
        "Junwei Han"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-10-18",
      "update_time": "2024-04-11",
      "comments": null,
      "repo_url": "#"
    },
    "2310.10264": {
      "paper_id": "2310.10264v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.10264v1",
      "paper_key": "2310.10264",
      "paper_title": "Towards Open-World Co-Salient Object Detection with Generative Uncertainty-aware Group Selective Exchange-Masking",
      "paper_url": "http://arxiv.org/abs/2310.10264v1",
      "paper_abstract": "The traditional definition of co-salient object detection (CoSOD) task is to segment the common salient objects in a group of relevant images. This definition is based on an assumption of group consensus consistency that is not always reasonable in the open-world setting, which results in robustness issue in the model when dealing with irrelevant images in the inputting image group under the open-word scenarios. To tackle this problem, we introduce a group selective exchange-masking (GSEM) approach for enhancing the robustness of the CoSOD model. GSEM takes two groups of images as input, each containing different types of salient objects. Based on the mixed metric we designed, GSEM selects a subset of images from each group using a novel learning-based strategy, then the selected images are exchanged. To simultaneously consider the uncertainty introduced by irrelevant images and the consensus features of the remaining relevant images in the group, we designed a latent variable generator branch and CoSOD transformer branch. The former is composed of a vector quantised-variational autoencoder to generate stochastic global variables that model uncertainty. The latter is designed to capture correlation-based local features that include group consensus. Finally, the outputs of the two branches are merged and passed to a transformer-based decoder to generate robust predictions. Taking into account that there are currently no benchmark datasets specifically designed for open-world scenarios, we constructed three open-world benchmark datasets, namely OWCoSal, OWCoSOD, and OWCoCA, based on existing datasets. By breaking the group-consistency assumption, these datasets provide effective simulations of real-world scenarios and can better evaluate the robustness and practicality of models.",
      "paper_authors": [
        "Yang Wu",
        "Shenglong Hu",
        "Huihui Song",
        "Kaihua Zhang",
        "Bo Liu",
        "Dong Liu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-10-16",
      "update_time": "2023-10-16",
      "comments": null,
      "repo_url": "https://github.com/wuyang98/CoSOD"
    },
    "2310.09533": {
      "paper_id": "2310.09533v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.09533v1",
      "paper_key": "2310.09533",
      "paper_title": "Towards End-to-End Unsupervised Saliency Detection with Self-Supervised Top-Down Context",
      "paper_url": "http://arxiv.org/abs/2310.09533v1",
      "paper_abstract": "Unsupervised salient object detection aims to detect salient objects without using supervision signals eliminating the tedious task of manually labeling salient objects. To improve training efficiency, end-to-end methods for USOD have been proposed as a promising alternative. However, current solutions rely heavily on noisy handcraft labels and fail to mine rich semantic information from deep features. In this paper, we propose a self-supervised end-to-end salient object detection framework via top-down context. Specifically, motivated by contrastive learning, we exploit the self-localization from the deepest feature to construct the location maps which are then leveraged to learn the most instructive segmentation guidance. Further considering the lack of detailed information in deepest features, we exploit the detail-boosting refiner module to enrich the location labels with details. Moreover, we observe that due to lack of supervision, current unsupervised saliency models tend to detect non-salient objects that are salient in some other samples of corresponding scenarios. To address this widespread issue, we design a novel Unsupervised Non-Salient Suppression (UNSS) method developing the ability to ignore non-salient objects. Extensive experiments on benchmark datasets demonstrate that our method achieves leading performance among the recent end-to-end methods and most of the multi-stage solutions. The code is available.",
      "paper_authors": [
        "Yicheng Song",
        "Shuyong Gao",
        "Haozhe Xing",
        "Yiting Cheng",
        "Yan Wang",
        "Wenqiang Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-10-14",
      "update_time": "2023-10-14",
      "comments": "accepted by ACM MM 2023",
      "repo_url": "#"
    },
    "2310.09016": {
      "paper_id": "2310.09016v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.09016v1",
      "paper_key": "2310.09016",
      "paper_title": "A Spatial-Temporal Dual-Mode Mixed Flow Network for Panoramic Video Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2310.09016v1",
      "paper_abstract": "Salient object detection (SOD) in panoramic video is still in the initial exploration stage. The indirect application of 2D video SOD method to the detection of salient objects in panoramic video has many unmet challenges, such as low detection accuracy, high model complexity, and poor generalization performance. To overcome these hurdles, we design an Inter-Layer Attention (ILA) module, an Inter-Layer weight (ILW) module, and a Bi-Modal Attention (BMA) module. Based on these modules, we propose a Spatial-Temporal Dual-Mode Mixed Flow Network (STDMMF-Net) that exploits the spatial flow of panoramic video and the corresponding optical flow for SOD. First, the ILA module calculates the attention between adjacent level features of consecutive frames of panoramic video to improve the accuracy of extracting salient object features from the spatial flow. Then, the ILW module quantifies the salient object information contained in the features of each level to improve the fusion efficiency of the features of each level in the mixed flow. Finally, the BMA module improves the detection accuracy of STDMMF-Net. A large number of subjective and objective experimental results testify that the proposed method demonstrates better detection accuracy than the state-of-the-art (SOTA) methods. Moreover, the comprehensive performance of the proposed method is better in terms of memory required for model inference, testing time, complexity, and generalization performance.",
      "paper_authors": [
        "Xiaolei Chen",
        "Pengcheng Zhang",
        "Zelong Du",
        "Ishfaq Ahmad"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-10-13",
      "update_time": "2023-10-13",
      "comments": null,
      "repo_url": "#"
    },
    "2309.10972": {
      "paper_id": "2309.10972v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.10972v1",
      "paper_key": "2309.10972",
      "paper_title": "SEMPART: Self-supervised Multi-resolution Partitioning of Image Semantics",
      "paper_url": "http://arxiv.org/abs/2309.10972v1",
      "paper_abstract": "Accurately determining salient regions of an image is challenging when labeled data is scarce. DINO-based self-supervised approaches have recently leveraged meaningful image semantics captured by patch-wise features for locating foreground objects. Recent methods have also incorporated intuitive priors and demonstrated value in unsupervised methods for object partitioning. In this paper, we propose SEMPART, which jointly infers coarse and fine bi-partitions over an image's DINO-based semantic graph. Furthermore, SEMPART preserves fine boundary details using graph-driven regularization and successfully distills the coarse mask semantics into the fine mask. Our salient object detection and single object localization findings suggest that SEMPART produces high-quality masks rapidly without additional post-processing and benefits from co-optimizing the coarse and fine branches.",
      "paper_authors": [
        "Sriram Ravindran",
        "Debraj Basu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-09-20",
      "update_time": "2023-09-20",
      "comments": null,
      "repo_url": "#"
    },
    "2309.09668": {
      "paper_id": "2309.09668v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.09668v2",
      "paper_key": "2309.09668",
      "paper_title": "DFormer: Rethinking RGBD Representation Learning for Semantic Segmentation",
      "paper_url": "http://arxiv.org/abs/2309.09668v2",
      "paper_abstract": "We present DFormer, a novel RGB-D pretraining framework to learn transferable representations for RGB-D segmentation tasks. DFormer has two new key innovations: 1) Unlike previous works that encode RGB-D information with RGB pretrained backbone, we pretrain the backbone using image-depth pairs from ImageNet-1K, and hence the DFormer is endowed with the capacity to encode RGB-D representations; 2) DFormer comprises a sequence of RGB-D blocks, which are tailored for encoding both RGB and depth information through a novel building block design. DFormer avoids the mismatched encoding of the 3D geometry relationships in depth maps by RGB pretrained backbones, which widely lies in existing methods but has not been resolved. We finetune the pretrained DFormer on two popular RGB-D tasks, i.e., RGB-D semantic segmentation and RGB-D salient object detection, with a lightweight decoder head. Experimental results show that our DFormer achieves new state-of-the-art performance on these two tasks with less than half of the computational cost of the current best methods on two RGB-D semantic segmentation datasets and five RGB-D salient object detection datasets. Our code is available at: https://github.com/VCIP-RGBD/DFormer.",
      "paper_authors": [
        "Bowen Yin",
        "Xuying Zhang",
        "Zhongyu Li",
        "Li Liu",
        "Ming-Ming Cheng",
        "Qibin Hou"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-09-18",
      "update_time": "2024-02-07",
      "comments": "Accepted by ICLR 2024",
      "repo_url": "#"
    },
    "2309.08365": {
      "paper_id": "2309.08365v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.08365v1",
      "paper_key": "2309.08365",
      "paper_title": "M$^3$Net: Multilevel, Mixed and Multistage Attention Network for Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2309.08365v1",
      "paper_abstract": "Most existing salient object detection methods mostly use U-Net or feature pyramid structure, which simply aggregates feature maps of different scales, ignoring the uniqueness and interdependence of them and their respective contributions to the final prediction. To overcome these, we propose the M$^3$Net, i.e., the Multilevel, Mixed and Multistage attention network for Salient Object Detection (SOD). Firstly, we propose Multiscale Interaction Block which innovatively introduces the cross-attention approach to achieve the interaction between multilevel features, allowing high-level features to guide low-level feature learning and thus enhancing salient regions. Secondly, considering the fact that previous Transformer based SOD methods locate salient regions only using global self-attention while inevitably overlooking the details of complex objects, we propose the Mixed Attention Block. This block combines global self-attention and window self-attention, aiming at modeling context at both global and local levels to further improve the accuracy of the prediction map. Finally, we proposed a multilevel supervision strategy to optimize the aggregated feature stage-by-stage. Experiments on six challenging datasets demonstrate that the proposed M$^3$Net surpasses recent CNN and Transformer-based SOD arts in terms of four metrics. Codes are available at https://github.com/I2-Multimedia-Lab/M3Net.",
      "paper_authors": [
        "Yao Yuan",
        "Pan Gao",
        "XiaoYang Tan"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-09-15",
      "update_time": "2023-09-15",
      "comments": null,
      "repo_url": "https://github.com/I2-Multimedia-Lab/M3Net"
    },
    "2309.08220": {
      "paper_id": "2309.08220v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.08220v1",
      "paper_key": "2309.08220",
      "paper_title": "UniST: Towards Unifying Saliency Transformer for Video Saliency Prediction and Detection",
      "paper_url": "http://arxiv.org/abs/2309.08220v1",
      "paper_abstract": "Video saliency prediction and detection are thriving research domains that enable computers to simulate the distribution of visual attention akin to how humans perceiving dynamic scenes. While many approaches have crafted task-specific training paradigms for either video saliency prediction or video salient object detection tasks, few attention has been devoted to devising a generalized saliency modeling framework that seamlessly bridges both these distinct tasks. In this study, we introduce the Unified Saliency Transformer (UniST) framework, which comprehensively utilizes the essential attributes of video saliency prediction and video salient object detection. In addition to extracting representations of frame sequences, a saliency-aware transformer is designed to learn the spatio-temporal representations at progressively increased resolutions, while incorporating effective cross-scale saliency information to produce a robust representation. Furthermore, a task-specific decoder is proposed to perform the final prediction for each task. To the best of our knowledge, this is the first work that explores designing a transformer structure for both saliency modeling tasks. Convincible experiments demonstrate that the proposed UniST achieves superior performance across seven challenging benchmarks for two tasks, and significantly outperforms the other state-of-the-art methods.",
      "paper_authors": [
        "Junwen Xiong",
        "Peng Zhang",
        "Chuanyue Li",
        "Wei Huang",
        "Yufei Zha",
        "Tao You"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-09-15",
      "update_time": "2023-09-15",
      "comments": "11 pages, 7 figures",
      "repo_url": "#"
    },
    "2309.08206": {
      "paper_id": "2309.08206v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.08206v1",
      "paper_key": "2309.08206",
      "paper_title": "Salient Object Detection in Optical Remote Sensing Images Driven by Transformer",
      "paper_url": "http://arxiv.org/abs/2309.08206v1",
      "paper_abstract": "Existing methods for Salient Object Detection in Optical Remote Sensing Images (ORSI-SOD) mainly adopt Convolutional Neural Networks (CNNs) as the backbone, such as VGG and ResNet. Since CNNs can only extract features within certain receptive fields, most ORSI-SOD methods generally follow the local-to-contextual paradigm. In this paper, we propose a novel Global Extraction Local Exploration Network (GeleNet) for ORSI-SOD following the global-to-local paradigm. Specifically, GeleNet first adopts a transformer backbone to generate four-level feature embeddings with global long-range dependencies. Then, GeleNet employs a Direction-aware Shuffle Weighted Spatial Attention Module (D-SWSAM) and its simplified version (SWSAM) to enhance local interactions, and a Knowledge Transfer Module (KTM) to further enhance cross-level contextual interactions. D-SWSAM comprehensively perceives the orientation information in the lowest-level features through directional convolutions to adapt to various orientations of salient objects in ORSIs, and effectively enhances the details of salient objects with an improved attention mechanism. SWSAM discards the direction-aware part of D-SWSAM to focus on localizing salient objects in the highest-level features. KTM models the contextual correlation knowledge of two middle-level features of different scales based on the self-attention mechanism, and transfers the knowledge to the raw features to generate more discriminative features. Finally, a saliency predictor is used to generate the saliency map based on the outputs of the above three modules. Extensive experiments on three public datasets demonstrate that the proposed GeleNet outperforms relevant state-of-the-art methods. The code and results of our method are available at https://github.com/MathLee/GeleNet.",
      "paper_authors": [
        "Gongyang Li",
        "Zhen Bai",
        "Zhi Liu",
        "Xinpeng Zhang",
        "Haibin Ling"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-09-15",
      "update_time": "2023-09-15",
      "comments": "13 pages, 6 figures, Accepted by IEEE Transactions on Image\n  Processing 2023",
      "repo_url": "https://github.com/mathlee/gelenet"
    },
    "2309.07753": {
      "paper_id": "2309.07753v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.07753v1",
      "paper_key": "2309.07753",
      "paper_title": "Co-Salient Object Detection with Semantic-Level Consensus Extraction and Dispersion",
      "paper_url": "http://arxiv.org/abs/2309.07753v1",
      "paper_abstract": "Given a group of images, co-salient object detection (CoSOD) aims to highlight the common salient object in each image. There are two factors closely related to the success of this task, namely consensus extraction, and the dispersion of consensus to each image. Most previous works represent the group consensus using local features, while we instead utilize a hierarchical Transformer module for extracting semantic-level consensus. Therefore, it can obtain a more comprehensive representation of the common object category, and exclude interference from other objects that share local similarities with the target object. In addition, we propose a Transformer-based dispersion module that takes into account the variation of the co-salient object in different scenes. It distributes the consensus to the image feature maps in an image-specific way while making full use of interactions within the group. These two modules are integrated with a ViT encoder and an FPN-like decoder to form an end-to-end trainable network, without additional branch and auxiliary loss. The proposed method is evaluated on three commonly used CoSOD datasets and achieves state-of-the-art performance.",
      "paper_authors": [
        "Peiran Xu",
        "Yadong Mu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-09-14",
      "update_time": "2023-09-14",
      "comments": "Accepted by ACM MM 2023",
      "repo_url": "#"
    },
    "2309.05900": {
      "paper_id": "2309.05900v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.05900v1",
      "paper_key": "2309.05900",
      "paper_title": "Adversarial Attacks Assessment of Salient Object Detection via Symbolic Learning",
      "paper_url": "http://arxiv.org/abs/2309.05900v1",
      "paper_abstract": "Machine learning is at the center of mainstream technology and outperforms classical approaches to handcrafted feature design. Aside from its learning process for artificial feature extraction, it has an end-to-end paradigm from input to output, reaching outstandingly accurate results. However, security concerns about its robustness to malicious and imperceptible perturbations have drawn attention since its prediction can be changed entirely. Salient object detection is a research area where deep convolutional neural networks have proven effective but whose trustworthiness represents a significant issue requiring analysis and solutions to hackers' attacks. Brain programming is a kind of symbolic learning in the vein of good old-fashioned artificial intelligence. This work provides evidence that symbolic learning robustness is crucial in designing reliable visual attention systems since it can withstand even the most intense perturbations. We test this evolutionary computation methodology against several adversarial attacks and noise perturbations using standard databases and a real-world problem of a shorebird called the Snowy Plover portraying a visual attention task. We compare our methodology with five different deep learning approaches, proving that they do not match the symbolic paradigm regarding robustness. All neural networks suffer significant performance losses, while brain programming stands its ground and remains unaffected. Also, by studying the Snowy Plover, we remark on the importance of security in surveillance activities regarding wildlife protection and conservation.",
      "paper_authors": [
        "Gustavo Olague",
        "Roberto Pineda",
        "Gerardo Ibarra-Vazquez",
        "Matthieu Olague",
        "Axel Martinez",
        "Sambit Bakshi",
        "Jonathan Vargas",
        "Isnardo Reducindo"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-09-12",
      "update_time": "2023-09-12",
      "comments": "14 pages, 8 figures, 6 tables, IEEE Transactions on Emerging Topics\n  in Computing, Accepted for publication",
      "repo_url": "#"
    },
    "2309.05499": {
      "paper_id": "2309.05499v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.05499v3",
      "paper_key": "2309.05499",
      "paper_title": "Zero-Shot Co-salient Object Detection Framework",
      "paper_url": "http://arxiv.org/abs/2309.05499v3",
      "paper_abstract": "Co-salient Object Detection (CoSOD) endeavors to replicate the human visual system's capacity to recognize common and salient objects within a collection of images. Despite recent advancements in deep learning models, these models still rely on training with well-annotated CoSOD datasets. The exploration of training-free zero-shot CoSOD frameworks has been limited. In this paper, taking inspiration from the zero-shot transfer capabilities of foundational computer vision models, we introduce the first zero-shot CoSOD framework that harnesses these models without any training process. To achieve this, we introduce two novel components in our proposed framework: the group prompt generation (GPG) module and the co-saliency map generation (CMP) module. We evaluate the framework's performance on widely-used datasets and observe impressive results. Our approach surpasses existing unsupervised methods and even outperforms fully supervised methods developed before 2020, while remaining competitive with some fully supervised methods developed before 2022.",
      "paper_authors": [
        "Haoke Xiao",
        "Lv Tang",
        "Bo Li",
        "Zhiming Luo",
        "Shaozi Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-09-11",
      "update_time": "2024-01-12",
      "comments": null,
      "repo_url": "https://github.com/hkxiao/zs-cosod"
    },
    "2309.02043": {
      "paper_id": "2309.02043v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.02043v1",
      "paper_key": "2309.02043",
      "paper_title": "Decomposed Guided Dynamic Filters for Efficient RGB-Guided Depth Completion",
      "paper_url": "http://arxiv.org/abs/2309.02043v1",
      "paper_abstract": "RGB-guided depth completion aims at predicting dense depth maps from sparse depth measurements and corresponding RGB images, where how to effectively and efficiently exploit the multi-modal information is a key issue. Guided dynamic filters, which generate spatially-variant depth-wise separable convolutional filters from RGB features to guide depth features, have been proven to be effective in this task. However, the dynamically generated filters require massive model parameters, computational costs and memory footprints when the number of feature channels is large. In this paper, we propose to decompose the guided dynamic filters into a spatially-shared component multiplied by content-adaptive adaptors at each spatial location. Based on the proposed idea, we introduce two decomposition schemes A and B, which decompose the filters by splitting the filter structure and using spatial-wise attention, respectively. The decomposed filters not only maintain the favorable properties of guided dynamic filters as being content-dependent and spatially-variant, but also reduce model parameters and hardware costs, as the learned adaptors are decoupled with the number of feature channels. Extensive experimental results demonstrate that the methods using our schemes outperform state-of-the-art methods on the KITTI dataset, and rank 1st and 2nd on the KITTI benchmark at the time of submission. Meanwhile, they also achieve comparable performance on the NYUv2 dataset. In addition, our proposed methods are general and could be employed as plug-and-play feature fusion blocks in other multi-modal fusion tasks such as RGB-D salient object detection.",
      "paper_authors": [
        "Yufei Wang",
        "Yuxin Mao",
        "Qi Liu",
        "Yuchao Dai"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-09-05",
      "update_time": "2023-09-05",
      "comments": null,
      "repo_url": "#"
    },
    "2308.09764": {
      "paper_id": "2308.09764v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.09764v2",
      "paper_key": "2308.09764",
      "paper_title": "The Impact of Background Removal on Performance of Neural Networks for Fashion Image Classification and Segmentation",
      "paper_url": "http://arxiv.org/abs/2308.09764v2",
      "paper_abstract": "Fashion understanding is a hot topic in computer vision, with many applications having great business value in the market. Fashion understanding remains a difficult challenge for computer vision due to the immense diversity of garments and various scenes and backgrounds. In this work, we try removing the background from fashion images to boost data quality and increase model performance. Having fashion images of evident persons in fully visible garments, we can utilize Salient Object Detection to achieve the background removal of fashion data to our expectations. A fashion image with the background removed is claimed as the \"rembg\" image, contrasting with the original one in the fashion dataset. We conducted extensive comparative experiments with these two types of images on multiple aspects of model training, including model architectures, model initialization, compatibility with other training tricks and data augmentations, and target task types. Our experiments show that background removal can effectively work for fashion data in simple and shallow networks that are not susceptible to overfitting. It can improve model accuracy by up to 5% in the classification on the FashionStyle14 dataset when training models from scratch. However, background removal does not perform well in deep neural networks due to incompatibility with other regularization techniques like batch normalization, pre-trained initialization, and data augmentations introducing randomness. The loss of background pixels invalidates many existing training tricks in the model training, adding the risk of overfitting for deep models.",
      "paper_authors": [
        "Junhui Liang",
        "Ying Liu",
        "Vladimir Vlassov"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-08-18",
      "update_time": "2024-05-07",
      "comments": "9 pages, 9 figures",
      "repo_url": "#"
    },
    "2308.08930": {
      "paper_id": "2308.08930v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.08930v1",
      "paper_key": "2308.08930",
      "paper_title": "Point-aware Interaction and CNN-induced Refinement Network for RGB-D Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2308.08930v1",
      "paper_abstract": "By integrating complementary information from RGB image and depth map, the ability of salient object detection (SOD) for complex and challenging scenes can be improved. In recent years, the important role of Convolutional Neural Networks (CNNs) in feature extraction and cross-modality interaction has been fully explored, but it is still insufficient in modeling global long-range dependencies of self-modality and cross-modality. To this end, we introduce CNNs-assisted Transformer architecture and propose a novel RGB-D SOD network with Point-aware Interaction and CNN-induced Refinement (PICR-Net). On the one hand, considering the prior correlation between RGB modality and depth modality, an attention-triggered cross-modality point-aware interaction (CmPI) module is designed to explore the feature interaction of different modalities with positional constraints. On the other hand, in order to alleviate the block effect and detail destruction problems brought by the Transformer naturally, we design a CNN-induced refinement (CNNR) unit for content refinement and supplementation. Extensive experiments on five RGB-D SOD datasets show that the proposed network achieves competitive results in both quantitative and qualitative comparisons.",
      "paper_authors": [
        "Runmin Cong",
        "Hongyu Liu",
        "Chen Zhang",
        "Wei Zhang",
        "Feng Zheng",
        "Ran Song",
        "Sam Kwong"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-08-17",
      "update_time": "2023-08-17",
      "comments": "Accepted by ACM MM 2023",
      "repo_url": "https://github.com/big-feather/PICR-Net_ACMMM23_MS"
    },
    "2308.05426": {
      "paper_id": "2308.05426v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.05426v1",
      "paper_key": "2308.05426",
      "paper_title": "Adaptive Low Rank Adaptation of Segment Anything to Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2308.05426v1",
      "paper_abstract": "Foundation models, such as OpenAI's GPT-3 and GPT-4, Meta's LLaMA, and Google's PaLM2, have revolutionized the field of artificial intelligence. A notable paradigm shift has been the advent of the Segment Anything Model (SAM), which has exhibited a remarkable capability to segment real-world objects, trained on 1 billion masks and 11 million images. Although SAM excels in general object segmentation, it lacks the intrinsic ability to detect salient objects, resulting in suboptimal performance in this domain. To address this challenge, we present the Segment Salient Object Model (SSOM), an innovative approach that adaptively fine-tunes SAM for salient object detection by harnessing the low-rank structure inherent in deep learning. Comprehensive qualitative and quantitative evaluations across five challenging RGB benchmark datasets demonstrate the superior performance of our approach, surpassing state-of-the-art methods.",
      "paper_authors": [
        "Ruikai Cui",
        "Siyuan He",
        "Shi Qiu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-08-10",
      "update_time": "2023-08-10",
      "comments": "13 pages, 0 figures",
      "repo_url": "https://github.com/CuiRuikai/SAM-SOD"
    },
    "2308.03826": {
      "paper_id": "2308.03826v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.03826v2",
      "paper_key": "2308.03826",
      "paper_title": "Recurrent Multi-scale Transformer for High-Resolution Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2308.03826v2",
      "paper_abstract": "Salient Object Detection (SOD) aims to identify and segment the most conspicuous objects in an image or video. As an important pre-processing step, it has many potential applications in multimedia and vision tasks. With the advance of imaging devices, SOD with high-resolution images is of great demand, recently. However, traditional SOD methods are largely limited to low-resolution images, making them difficult to adapt to the development of High-Resolution SOD (HRSOD). Although some HRSOD methods emerge, there are no large enough datasets for training and evaluating. Besides, current HRSOD methods generally produce incomplete object regions and irregular object boundaries. To address above issues, in this work, we first propose a new HRS10K dataset, which contains 10,500 high-quality annotated images at 2K-8K resolution. As far as we know, it is the largest dataset for the HRSOD task, which will significantly help future works in training and evaluating models. Furthermore, to improve the HRSOD performance, we propose a novel Recurrent Multi-scale Transformer (RMFormer), which recurrently utilizes shared Transformers and multi-scale refinement architectures. Thus, high-resolution saliency maps can be generated with the guidance of lower-resolution predictions. Extensive experiments on both high-resolution and low-resolution benchmarks show the effectiveness and superiority of the proposed framework. The source code and dataset are released at: https://github.com/DrowsyMon/RMFormer.",
      "paper_authors": [
        "Xinhao Deng",
        "Pingping Zhang",
        "Wei Liu",
        "Huchuan Lu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-08-07",
      "update_time": "2023-09-04",
      "comments": "This work is the camera-ready version of ACM MM2023",
      "repo_url": "https://github.com/drowsymon/rmformer"
    },
    "2308.03359": {
      "paper_id": "2308.03359v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.03359v1",
      "paper_key": "2308.03359",
      "paper_title": "Distortion-aware Transformer in 360\u00b0 Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2308.03359v1",
      "paper_abstract": "With the emergence of VR and AR, 360{\\deg} data attracts increasing attention from the computer vision and multimedia communities. Typically, 360{\\deg} data is projected into 2D ERP (equirectangular projection) images for feature extraction. However, existing methods cannot handle the distortions that result from the projection, hindering the development of 360-data-based tasks. Therefore, in this paper, we propose a Transformer-based model called DATFormer to address the distortion problem. We tackle this issue from two perspectives. Firstly, we introduce two distortion-adaptive modules. The first is a Distortion Mapping Module, which guides the model to pre-adapt to distorted features globally. The second module is a Distortion-Adaptive Attention Block that reduces local distortions on multi-scale features. Secondly, to exploit the unique characteristics of 360{\\deg} data, we present a learnable relation matrix and use it as part of the positional embedding to further improve performance. Extensive experiments are conducted on three public datasets, and the results show that our model outperforms existing 2D SOD (salient object detection) and 360 SOD methods.",
      "paper_authors": [
        "Yinjie Zhao",
        "Lichen Zhao",
        "Qian Yu",
        "Jing Zhang",
        "Lu Sheng",
        "Dong Xu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-08-07",
      "update_time": "2023-08-07",
      "comments": "10 pages, 5 figures",
      "repo_url": "https://github.com/yjzhao19981027/datformer"
    },
    "2307.12349": {
      "paper_id": "2307.12349v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.12349v1",
      "paper_key": "2307.12349",
      "paper_title": "ComPtr: Towards Diverse Bi-source Dense Prediction Tasks via A Simple yet General Complementary Transformer",
      "paper_url": "http://arxiv.org/abs/2307.12349v1",
      "paper_abstract": "Deep learning (DL) has advanced the field of dense prediction, while gradually dissolving the inherent barriers between different tasks. However, most existing works focus on designing architectures and constructing visual cues only for the specific task, which ignores the potential uniformity introduced by the DL paradigm. In this paper, we attempt to construct a novel \\underline{ComP}lementary \\underline{tr}ansformer, \\textbf{ComPtr}, for diverse bi-source dense prediction tasks. Specifically, unlike existing methods that over-specialize in a single task or a subset of tasks, ComPtr starts from the more general concept of bi-source dense prediction. Based on the basic dependence on information complementarity, we propose consistency enhancement and difference awareness components with which ComPtr can evacuate and collect important visual semantic cues from different image sources for diverse tasks, respectively. ComPtr treats different inputs equally and builds an efficient dense interaction model in the form of sequence-to-sequence on top of the transformer. This task-generic design provides a smooth foundation for constructing the unified model that can simultaneously deal with various bi-source information. In extensive experiments across several representative vision tasks, i.e. remote sensing change detection, RGB-T crowd counting, RGB-D/T salient object detection, and RGB-D semantic segmentation, the proposed method consistently obtains favorable performance. The code will be available at \\url{https://github.com/lartpang/ComPtr}.",
      "paper_authors": [
        "Youwei Pang",
        "Xiaoqi Zhao",
        "Lihe Zhang",
        "Huchuan Lu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-07-23",
      "update_time": "2023-07-23",
      "comments": null,
      "repo_url": "https://github.com/lartpang/comptr"
    },
    "2307.04651": {
      "paper_id": "2307.04651v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.04651v1",
      "paper_key": "2307.04651",
      "paper_title": "Joint Salient Object Detection and Camouflaged Object Detection via Uncertainty-aware Learning",
      "paper_url": "http://arxiv.org/abs/2307.04651v1",
      "paper_abstract": "Salient objects attract human attention and usually stand out clearly from their surroundings. In contrast, camouflaged objects share similar colors or textures with the environment. In this case, salient objects are typically non-camouflaged, and camouflaged objects are usually not salient. Due to this inherent contradictory attribute, we introduce an uncertainty-aware learning pipeline to extensively explore the contradictory information of salient object detection (SOD) and camouflaged object detection (COD) via data-level and task-wise contradiction modeling. We first exploit the dataset correlation of these two tasks and claim that the easy samples in the COD dataset can serve as hard samples for SOD to improve the robustness of the SOD model. Based on the assumption that these two models should lead to activation maps highlighting different regions of the same input image, we further introduce a contrastive module with a joint-task contrastive learning framework to explicitly model the contradictory attributes of these two tasks. Different from conventional intra-task contrastive learning for unsupervised representation learning, our contrastive module is designed to model the task-wise correlation, leading to cross-task representation learning. To better understand the two tasks from the perspective of uncertainty, we extensively investigate the uncertainty estimation techniques for modeling the main uncertainties of the two tasks, namely task uncertainty (for SOD) and data uncertainty (for COD), and aiming to effectively estimate the challenging regions for each task to achieve difficulty-aware learning. Experimental results on benchmark datasets demonstrate that our solution leads to both state-of-the-art performance and informative uncertainty estimation.",
      "paper_authors": [
        "Aixuan Li",
        "Jing Zhang",
        "Yunqiu Lv",
        "Tong Zhang",
        "Yiran Zhong",
        "Mingyi He",
        "Yuchao Dai"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-07-10",
      "update_time": "2023-07-10",
      "comments": null,
      "repo_url": "#"
    },
    "2307.00954": {
      "paper_id": "2307.00954v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.00954v1",
      "paper_key": "2307.00954",
      "paper_title": "HODINet: High-Order Discrepant Interaction Network for RGB-D Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2307.00954v1",
      "paper_abstract": "RGB-D salient object detection (SOD) aims to detect the prominent regions by jointly modeling RGB and depth information. Most RGB-D SOD methods apply the same type of backbones and fusion modules to identically learn the multimodality and multistage features. However, these features contribute differently to the final saliency results, which raises two issues: 1) how to model discrepant characteristics of RGB images and depth maps; 2) how to fuse these cross-modality features in different stages. In this paper, we propose a high-order discrepant interaction network (HODINet) for RGB-D SOD. Concretely, we first employ transformer-based and CNN-based architectures as backbones to encode RGB and depth features, respectively. Then, the high-order representations are delicately extracted and embedded into spatial and channel attentions for cross-modality feature fusion in different stages. Specifically, we design a high-order spatial fusion (HOSF) module and a high-order channel fusion (HOCF) module to fuse features of the first two and the last two stages, respectively. Besides, a cascaded pyramid reconstruction network is adopted to progressively decode the fused features in a top-down pathway. Extensive experiments are conducted on seven widely used datasets to demonstrate the effectiveness of the proposed approach. We achieve competitive performance against 24 state-of-the-art methods under four evaluation metrics.",
      "paper_authors": [
        "Kang Yi",
        "Jing Xu",
        "Xiao Jin",
        "Fu Guo",
        "Yan-Feng Wu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-07-03",
      "update_time": "2023-07-03",
      "comments": null,
      "repo_url": "#"
    },
    "2306.17431": {
      "paper_id": "2306.17431v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2306.17431v2",
      "paper_key": "2306.17431",
      "paper_title": "Defense against Adversarial Cloud Attack on Remote Sensing Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2306.17431v2",
      "paper_abstract": "Detecting the salient objects in a remote sensing image has wide applications for the interdisciplinary research. Many existing deep learning methods have been proposed for Salient Object Detection (SOD) in remote sensing images and get remarkable results. However, the recent adversarial attack examples, generated by changing a few pixel values on the original remote sensing image, could result in a collapse for the well-trained deep learning based SOD model. Different with existing methods adding perturbation to original images, we propose to jointly tune adversarial exposure and additive perturbation for attack and constrain image close to cloudy image as Adversarial Cloud. Cloud is natural and common in remote sensing images, however, camouflaging cloud based adversarial attack and defense for remote sensing images are not well studied before. Furthermore, we design DefenseNet as a learn-able pre-processing to the adversarial cloudy images so as to preserve the performance of the deep learning based remote sensing SOD model, without tuning the already deployed deep SOD model. By considering both regular and generalized adversarial examples, the proposed DefenseNet can defend the proposed Adversarial Cloud in white-box setting and other attack methods in black-box setting. Experimental results on a synthesized benchmark from the public remote sensing SOD dataset (EORSSD) show the promising defense against adversarial cloud attacks.",
      "paper_authors": [
        "Huiming Sun",
        "Lan Fu",
        "Jinlong Li",
        "Qing Guo",
        "Zibo Meng",
        "Tianyun Zhang",
        "Yuewei Lin",
        "Hongkai Yu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-06-30",
      "update_time": "2023-07-05",
      "comments": null,
      "repo_url": "#"
    },
    "2306.12621": {
      "paper_id": "2306.12621v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2306.12621v1",
      "paper_key": "2306.12621",
      "paper_title": "RXFOOD: Plug-in RGB-X Fusion for Object of Interest Detection",
      "paper_url": "http://arxiv.org/abs/2306.12621v1",
      "paper_abstract": "The emergence of different sensors (Near-Infrared, Depth, etc.) is a remedy for the limited application scenarios of traditional RGB camera. The RGB-X tasks, which rely on RGB input and another type of data input to resolve specific problems, have become a popular research topic in multimedia. A crucial part in two-branch RGB-X deep neural networks is how to fuse information across modalities. Given the tremendous information inside RGB-X networks, previous works typically apply naive fusion (e.g., average or max fusion) or only focus on the feature fusion at the same scale(s). While in this paper, we propose a novel method called RXFOOD for the fusion of features across different scales within the same modality branch and from different modality branches simultaneously in a unified attention mechanism. An Energy Exchange Module is designed for the interaction of each feature map's energy matrix, who reflects the inter-relationship of different positions and different channels inside a feature map. The RXFOOD method can be easily incorporated to any dual-branch encoder-decoder network as a plug-in module, and help the original backbone network better focus on important positions and channels for object of interest detection. Experimental results on RGB-NIR salient object detection, RGB-D salient object detection, and RGBFrequency image manipulation detection demonstrate the clear effectiveness of the proposed RXFOOD.",
      "paper_authors": [
        "Jin Ma",
        "Jinlong Li",
        "Qing Guo",
        "Tianyun Zhang",
        "Yuewei Lin",
        "Hongkai Yu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-06-22",
      "update_time": "2023-06-22",
      "comments": "10 pages",
      "repo_url": "#"
    },
    "2306.03630": {
      "paper_id": "2306.03630v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2306.03630v1",
      "paper_key": "2306.03630",
      "paper_title": "Mutual Information Regularization for Weakly-supervised RGB-D Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2306.03630v1",
      "paper_abstract": "In this paper, we present a weakly-supervised RGB-D salient object detection model via scribble supervision. Specifically, as a multimodal learning task, we focus on effective multimodal representation learning via inter-modal mutual information regularization. In particular, following the principle of disentangled representation learning, we introduce a mutual information upper bound with a mutual information minimization regularizer to encourage the disentangled representation of each modality for salient object detection. Based on our multimodal representation learning framework, we introduce an asymmetric feature extractor for our multimodal data, which is proven more effective than the conventional symmetric backbone setting. We also introduce multimodal variational auto-encoder as stochastic prediction refinement techniques, which takes pseudo labels from the first training stage as supervision and generates refined prediction. Experimental results on benchmark RGB-D salient object detection datasets verify both effectiveness of our explicit multimodal disentangled representation learning method and the stochastic prediction refinement strategy, achieving comparable performance with the state-of-the-art fully supervised models. Our code and data are available at: https://github.com/baneitixiaomai/MIRV.",
      "paper_authors": [
        "Aixuan Li",
        "Yuxin Mao",
        "Jing Zhang",
        "Yuchao Dai"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-06-06",
      "update_time": "2023-06-06",
      "comments": "IEEE Transactions on Circuits and Systems for Video Technology 2023",
      "repo_url": "https://github.com/baneitixiaomai/mirv"
    },
    "2306.02351": {
      "paper_id": "2306.02351v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2306.02351v1",
      "paper_key": "2306.02351",
      "paper_title": "RSSOD-Bench: A large-scale benchmark dataset for Salient Object Detection in Optical Remote Sensing Imagery",
      "paper_url": "http://arxiv.org/abs/2306.02351v1",
      "paper_abstract": "We present the RSSOD-Bench dataset for salient object detection (SOD) in optical remote sensing imagery. While SOD has achieved success in natural scene images with deep learning, research in SOD for remote sensing imagery (RSSOD) is still in its early stages. Existing RSSOD datasets have limitations in terms of scale, and scene categories, which make them misaligned with real-world applications. To address these shortcomings, we construct the RSSOD-Bench dataset, which contains images from four different cities in the USA. The dataset provides annotations for various salient object categories, such as buildings, lakes, rivers, highways, bridges, aircraft, ships, athletic fields, and more. The salient objects in RSSOD-Bench exhibit large-scale variations, cluttered backgrounds, and different seasons. Unlike existing datasets, RSSOD-Bench offers uniform distribution across scene categories. We benchmark 23 different state-of-the-art approaches from both the computer vision and remote sensing communities. Experimental results demonstrate that more research efforts are required for the RSSOD task.",
      "paper_authors": [
        "Zhitong Xiong",
        "Yanfeng Liu",
        "Qi Wang",
        "Xiao Xiang Zhu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-06-04",
      "update_time": "2023-06-04",
      "comments": "IGARSS 2023, 4 pages",
      "repo_url": "#"
    },
    "2305.18476": {
      "paper_id": "2305.18476v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2305.18476v1",
      "paper_key": "2305.18476",
      "paper_title": "Explicit Visual Prompting for Universal Foreground Segmentations",
      "paper_url": "http://arxiv.org/abs/2305.18476v1",
      "paper_abstract": "Foreground segmentation is a fundamental problem in computer vision, which includes salient object detection, forgery detection, defocus blur detection, shadow detection, and camouflage object detection. Previous works have typically relied on domain-specific solutions to address accuracy and robustness issues in those applications. In this paper, we present a unified framework for a number of foreground segmentation tasks without any task-specific designs. We take inspiration from the widely-used pre-training and then prompt tuning protocols in NLP and propose a new visual prompting model, named Explicit Visual Prompting (EVP). Different from the previous visual prompting which is typically a dataset-level implicit embedding, our key insight is to enforce the tunable parameters focusing on the explicit visual content from each individual image, i.e., the features from frozen patch embeddings and high-frequency components. Our method freezes a pre-trained model and then learns task-specific knowledge using a few extra parameters. Despite introducing only a small number of tunable parameters, EVP achieves superior performance than full fine-tuning and other parameter-efficient fine-tuning methods. Experiments in fourteen datasets across five tasks show the proposed method outperforms other task-specific methods while being considerably simple. The proposed method demonstrates the scalability in different architectures, pre-trained weights, and tasks. The code is available at: https://github.com/NiFangBaAGe/Explicit-Visual-Prompt.",
      "paper_authors": [
        "Weihuang Liu",
        "Xi Shen",
        "Chi-Man Pun",
        "Xiaodong Cun"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-05-29",
      "update_time": "2023-05-29",
      "comments": "arXiv admin note: substantial text overlap with arXiv:2303.10883",
      "repo_url": "https://github.com/nifangbaage/explicit-visual-prompt"
    },
    "2305.14955": {
      "paper_id": "2305.14955v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2305.14955v3",
      "paper_key": "2305.14955",
      "paper_title": "DC-Net: Divide-and-Conquer for Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2305.14955v3",
      "paper_abstract": "In this paper, we introduce Divide-and-Conquer into the salient object detection (SOD) task to enable the model to learn prior knowledge that is for predicting the saliency map. We design a novel network, Divide-and-Conquer Network (DC-Net) which uses two encoders to solve different subtasks that are conducive to predicting the final saliency map, here is to predict the edge maps with width 4 and location maps of salient objects and then aggregate the feature maps with different semantic information into the decoder to predict the final saliency map. The decoder of DC-Net consists of our newly designed two-level Residual nested-ASPP (ResASPP$^{2}$) modules, which have the ability to capture a large number of different scale features with a small number of convolution operations and have the advantages of maintaining high resolution all the time and being able to obtain a large and compact effective receptive field (ERF). Based on the advantage of Divide-and-Conquer's parallel computing, we use Parallel Acceleration to speed up DC-Net, allowing it to achieve competitive performance on six LR-SOD and five HR-SOD datasets under high efficiency (60 FPS and 55 FPS). Codes and results are available: https://github.com/PiggyJerry/DC-Net.",
      "paper_authors": [
        "Jiayi Zhu",
        "Xuebin Qin",
        "Abdulmotaleb Elsaddik"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-05-24",
      "update_time": "2024-01-10",
      "comments": null,
      "repo_url": "https://github.com/piggyjerry/dc-net"
    },
    "2305.12452": {
      "paper_id": "2305.12452v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2305.12452v1",
      "paper_key": "2305.12452",
      "paper_title": "Advancing Referring Expression Segmentation Beyond Single Image",
      "paper_url": "http://arxiv.org/abs/2305.12452v1",
      "paper_abstract": "Referring Expression Segmentation (RES) is a widely explored multi-modal task, which endeavors to segment the pre-existing object within a single image with a given linguistic expression. However, in broader real-world scenarios, it is not always possible to determine if the described object exists in a specific image. Typically, we have a collection of images, some of which may contain the described objects. The current RES setting curbs its practicality in such situations. To overcome this limitation, we propose a more realistic and general setting, named Group-wise Referring Expression Segmentation (GRES), which expands RES to a collection of related images, allowing the described objects to be present in a subset of input images. To support this new setting, we introduce an elaborately compiled dataset named Grouped Referring Dataset (GRD), containing complete group-wise annotations of target objects described by given expressions. We also present a baseline method named Grouped Referring Segmenter (GRSer), which explicitly captures the language-vision and intra-group vision-vision interactions to achieve state-of-the-art results on the proposed GRES and related tasks, such as Co-Salient Object Detection and RES. Our dataset and codes will be publicly released in https://github.com/yixuan730/group-res.",
      "paper_authors": [
        "Yixuan Wu",
        "Zhao Zhang",
        "Xie Chi",
        "Feng Zhu",
        "Rui Zhao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-05-21",
      "update_time": "2023-05-21",
      "comments": null,
      "repo_url": "#"
    },
    "2305.11729": {
      "paper_id": "2305.11729v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2305.11729v1",
      "paper_key": "2305.11729",
      "paper_title": "ViDaS Video Depth-aware Saliency Network",
      "paper_url": "http://arxiv.org/abs/2305.11729v1",
      "paper_abstract": "We introduce ViDaS, a two-stream, fully convolutional Video, Depth-Aware Saliency network to address the problem of attention modeling ``in-the-wild\", via saliency prediction in videos. Contrary to existing visual saliency approaches using only RGB frames as input, our network employs also depth as an additional modality. The network consists of two visual streams, one for the RGB frames, and one for the depth frames. Both streams follow an encoder-decoder approach and are fused to obtain a final saliency map. The network is trained end-to-end and is evaluated in a variety of different databases with eye-tracking data, containing a wide range of video content. Although the publicly available datasets do not contain depth, we estimate it using three different state-of-the-art methods, to enable comparisons and a deeper insight. Our method outperforms in most cases state-of-the-art models and our RGB-only variant, which indicates that depth can be beneficial to accurately estimating saliency in videos displayed on a 2D screen. Depth has been widely used to assist salient object detection problems, where it has been proven to be very beneficial. Our problem though differs significantly from salient object detection, since it is not restricted to specific salient objects, but predicts human attention in a more general aspect. These two problems not only have different objectives, but also different ground truth data and evaluation metrics. To our best knowledge, this is the first competitive deep learning video saliency estimation approach that combines both RGB and Depth features to address the general problem of saliency estimation ``in-the-wild\". The code will be publicly released.",
      "paper_authors": [
        "Ioanna Diamanti",
        "Antigoni Tsiami",
        "Petros Koutras",
        "Petros Maragos"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-05-19",
      "update_time": "2023-05-19",
      "comments": null,
      "repo_url": "#"
    },
    "2305.09999": {
      "paper_id": "2305.09999v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2305.09999v1",
      "paper_key": "2305.09999",
      "paper_title": "An Interactively Reinforced Paradigm for Joint Infrared-Visible Image Fusion and Saliency Object Detection",
      "paper_url": "http://arxiv.org/abs/2305.09999v1",
      "paper_abstract": "This research focuses on the discovery and localization of hidden objects in the wild and serves unmanned systems. Through empirical analysis, infrared and visible image fusion (IVIF) enables hard-to-find objects apparent, whereas multimodal salient object detection (SOD) accurately delineates the precise spatial location of objects within the picture. Their common characteristic of seeking complementary cues from different source images motivates us to explore the collaborative relationship between Fusion and Salient object detection tasks on infrared and visible images via an Interactively Reinforced multi-task paradigm for the first time, termed IRFS. To the seamless bridge of multimodal image fusion and SOD tasks, we specifically develop a Feature Screening-based Fusion subnetwork (FSFNet) to screen out interfering features from source images, thereby preserving saliency-related features. After generating the fused image through FSFNet, it is then fed into the subsequent Fusion-Guided Cross-Complementary SOD subnetwork (FC$^2$Net) as the third modality to drive the precise prediction of the saliency map by leveraging the complementary information derived from the fused image. In addition, we develop an interactive loop learning strategy to achieve the mutual reinforcement of IVIF and SOD tasks with a shorter training period and fewer network parameters. Comprehensive experiment results demonstrate that the seamless bridge of IVIF and SOD mutually enhances their performance, and highlights their superiority.",
      "paper_authors": [
        "Di Wang",
        "Jinyuan Liu",
        "Risheng Liu",
        "Xin Fan"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-05-17",
      "update_time": "2023-05-17",
      "comments": null,
      "repo_url": "https://github.com/wdhudiekou/irfs"
    },
    "2305.05260": {
      "paper_id": "2305.05260v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2305.05260v1",
      "paper_key": "2305.05260",
      "paper_title": "Guided Focal Stack Refinement Network for Light Field Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2305.05260v1",
      "paper_abstract": "Light field salient object detection (SOD) is an emerging research direction attributed to the richness of light field data. However, most existing methods lack effective handling of focal stacks, therefore making the latter involved in a lot of interfering information and degrade the performance of SOD. To address this limitation, we propose to utilize multi-modal features to refine focal stacks in a guided manner, resulting in a novel guided focal stack refinement network called GFRNet. To this end, we propose a guided refinement and fusion module (GRFM) to refine focal stacks and aggregate multi-modal features. In GRFM, all-in-focus (AiF) and depth modalities are utilized to refine focal stacks separately, leading to two novel sub-modules for different modalities, namely AiF-based refinement module (ARM) and depth-based refinement module (DRM). Such refinement modules enhance structural and positional information of salient objects in focal stacks, and are able to improve SOD accuracy. Experimental results on four benchmark datasets demonstrate the superiority of our GFRNet model against 12 state-of-the-art models.",
      "paper_authors": [
        "Bo Yuan",
        "Yao Jiang",
        "Keren Fu",
        "Qijun Zhao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-05-09",
      "update_time": "2023-05-09",
      "comments": "Accepted by ICME 2023",
      "repo_url": "#"
    },
    "2305.04396": {
      "paper_id": "2305.04396v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2305.04396v1",
      "paper_key": "2305.04396",
      "paper_title": "SegGPT Meets Co-Saliency Scene",
      "paper_url": "http://arxiv.org/abs/2305.04396v1",
      "paper_abstract": "Co-salient object detection targets at detecting co-existed salient objects among a group of images. Recently, a generalist model for segmenting everything in context, called SegGPT, is gaining public attention. In view of its breakthrough for segmentation, we can hardly wait to probe into its contribution to the task of co-salient object detection. In this report, we first design a framework to enable SegGPT for the problem of co-salient object detection. Proceed to the next step, we evaluate the performance of SegGPT on the problem of co-salient object detection on three available datasets. We achieve a finding that co-saliency scenes challenges SegGPT due to context discrepancy within a group of co-saliency images.",
      "paper_authors": [
        "Yi Liu",
        "Shoukun Xu",
        "Dingwen Zhang",
        "Jungong Han"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-05-08",
      "update_time": "2023-05-08",
      "comments": null,
      "repo_url": "#"
    },
    "2305.00514": {
      "paper_id": "2305.00514v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2305.00514v2",
      "paper_key": "2305.00514",
      "paper_title": "Discriminative Co-Saliency and Background Mining Transformer for Co-Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2305.00514v2",
      "paper_abstract": "Most previous co-salient object detection works mainly focus on extracting co-salient cues via mining the consistency relations across images while ignoring explicit exploration of background regions. In this paper, we propose a Discriminative co-saliency and background Mining Transformer framework (DMT) based on several economical multi-grained correlation modules to explicitly mine both co-saliency and background information and effectively model their discrimination. Specifically, we first propose a region-to-region correlation module for introducing inter-image relations to pixel-wise segmentation features while maintaining computational efficiency. Then, we use two types of pre-defined tokens to mine co-saliency and background information via our proposed contrast-induced pixel-to-token correlation and co-saliency token-to-token correlation modules. We also design a token-guided feature refinement module to enhance the discriminability of the segmentation features under the guidance of the learned tokens. We perform iterative mutual promotion for the segmentation feature extraction and token construction. Experimental results on three benchmark datasets demonstrate the effectiveness of our proposed method. The source code is available at: https://github.com/dragonlee258079/DMT.",
      "paper_authors": [
        "Long Li",
        "Junwei Han",
        "Ni Zhang",
        "Nian Liu",
        "Salman Khan",
        "Hisham Cholakkal",
        "Rao Muhammad Anwer",
        "Fahad Shahbaz Khan"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-04-30",
      "update_time": "2023-05-06",
      "comments": "Accepted by CVPR 2023",
      "repo_url": "https://github.com/dragonlee258079/DMT"
    },
    "2304.14619": {
      "paper_id": "2304.14619v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2304.14619v1",
      "paper_key": "2304.14619",
      "paper_title": "A positive feedback method based on F-measure value for Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2304.14619v1",
      "paper_abstract": "The majority of current salient object detection (SOD) models are focused on designing a series of decoders based on fully convolutional networks (FCNs) or Transformer architectures and integrating them in a skillful manner. These models have achieved remarkable high performance and made significant contributions to the development of SOD. Their primary research objective is to develop novel algorithms that can outperform state-of-the-art models, a task that is extremely difficult and time-consuming. In contrast, this paper proposes a positive feedback method based on F-measure value for SOD, aiming to improve the accuracy of saliency prediction using existing methods. Specifically, our proposed method takes an image to be detected and inputs it into several existing models to obtain their respective prediction maps. These prediction maps are then fed into our positive feedback method to generate the final prediction result, without the need for careful decoder design or model training. Moreover, our method is adaptive and can be implemented based on existing models without any restrictions. Experimental results on five publicly available datasets show that our proposed positive feedback method outperforms the latest 12 methods in five evaluation metrics for saliency map prediction. Additionally, we conducted a robustness experiment, which shows that when at least one good prediction result exists in the selected existing model, our proposed approach can ensure that the prediction result is not worse. Our approach achieves a prediction speed of 20 frames per second (FPS) when evaluated on a low configuration host and after removing the prediction time overhead of inserted models. These results highlight the effectiveness, efficiency, and robustness of our proposed approach for salient object detection.",
      "paper_authors": [
        "Ailing Pan",
        "Chao Dai",
        "Chen Pan",
        "Dongping Zhang",
        "Yunchao Xu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-04-28",
      "update_time": "2023-04-28",
      "comments": "13 pages, 4 figures, 3 table",
      "repo_url": "https://github.com/dc3234/pf"
    },
    "2303.15710": {
      "paper_id": "2303.15710v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2303.15710v1",
      "paper_key": "2303.15710",
      "paper_title": "Explicit Attention-Enhanced Fusion for RGB-Thermal Perception Tasks",
      "paper_url": "http://arxiv.org/abs/2303.15710v1",
      "paper_abstract": "Recently, RGB-Thermal based perception has shown significant advances. Thermal information provides useful clues when visual cameras suffer from poor lighting conditions, such as low light and fog. However, how to effectively fuse RGB images and thermal data remains an open challenge. Previous works involve naive fusion strategies such as merging them at the input, concatenating multi-modality features inside models, or applying attention to each data modality. These fusion strategies are straightforward yet insufficient. In this paper, we propose a novel fusion method named Explicit Attention-Enhanced Fusion (EAEF) that fully takes advantage of each type of data. Specifically, we consider the following cases: i) both RGB data and thermal data, ii) only one of the types of data, and iii) none of them generate discriminative features. EAEF uses one branch to enhance feature extraction for i) and iii) and the other branch to remedy insufficient representations for ii). The outputs of two branches are fused to form complementary features. As a result, the proposed fusion method outperforms state-of-the-art by 1.6\\% in mIoU on semantic segmentation, 3.1\\% in MAE on salient object detection, 2.3\\% in mAP on object detection, and 8.1\\% in MAE on crowd counting. The code is available at https://github.com/FreeformRobotics/EAEFNet.",
      "paper_authors": [
        "Mingjian Liang",
        "Junjie Hu",
        "Chenyu Bao",
        "Hua Feng",
        "Fuqin Deng",
        "Tin Lun Lam"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-03-28",
      "update_time": "2023-03-28",
      "comments": null,
      "repo_url": "https://github.com/freeformrobotics/eaefnet"
    },
    "2303.09801": {
      "paper_id": "2303.09801v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2303.09801v1",
      "paper_key": "2303.09801",
      "paper_title": "Adaptive Graph Convolution Module for Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2303.09801v1",
      "paper_abstract": "Salient object detection (SOD) is a task that involves identifying and segmenting the most visually prominent object in an image. Existing solutions can accomplish this use a multi-scale feature fusion mechanism to detect the global context of an image. However, as there is no consideration of the structures in the image nor the relations between distant pixels, conventional methods cannot deal with complex scenes effectively. In this paper, we propose an adaptive graph convolution module (AGCM) to overcome these limitations. Prototype features are initially extracted from the input image using a learnable region generation layer that spatially groups features in the image. The prototype features are then refined by propagating information between them based on a graph architecture, where each feature is regarded as a node. Experimental results show that the proposed AGCM dramatically improves the SOD performance both quantitatively and quantitatively.",
      "paper_authors": [
        "Yongwoo Lee",
        "Minhyeok Lee",
        "Suhwan Cho",
        "Sangyoun Lee"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-03-17",
      "update_time": "2023-03-17",
      "comments": "4 pages, 3 figures",
      "repo_url": "#"
    },
    "2303.09733": {
      "paper_id": "2303.09733v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2303.09733v1",
      "paper_key": "2303.09733",
      "paper_title": "Scribble-Supervised RGB-T Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2303.09733v1",
      "paper_abstract": "Salient object detection segments attractive objects in scenes. RGB and thermal modalities provide complementary information and scribble annotations alleviate large amounts of human labor. Based on the above facts, we propose a scribble-supervised RGB-T salient object detection model. By a four-step solution (expansion, prediction, aggregation, and supervision), label-sparse challenge of scribble-supervised method is solved. To expand scribble annotations, we collect the superpixels that foreground scribbles pass through in RGB and thermal images, respectively. The expanded multi-modal labels provide the coarse object boundary. To further polish the expanded labels, we propose a prediction module to alleviate the sharpness of boundary. To play the complementary roles of two modalities, we combine the two into aggregated pseudo labels. Supervised by scribble annotations and pseudo labels, our model achieves the state-of-the-art performance on the relabeled RGBT-S dataset. Furthermore, the model is applied to RGB-D and video scribble-supervised applications, achieving consistently excellent performance.",
      "paper_authors": [
        "Zhengyi Liu",
        "Xiaoshen Huang",
        "Guanghui Zhang",
        "Xianyong Fang",
        "Linbo Wang",
        "Bin Tang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-03-17",
      "update_time": "2023-03-17",
      "comments": "ICME2023",
      "repo_url": "https://github.com/liuzywen/rgbtscribble-icme2023"
    },
    "2303.07670": {
      "paper_id": "2303.07670v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2303.07670v1",
      "paper_key": "2303.07670",
      "paper_title": "Co-Salient Object Detection with Co-Representation Purification",
      "paper_url": "http://arxiv.org/abs/2303.07670v1",
      "paper_abstract": "Co-salient object detection (Co-SOD) aims at discovering the common objects in a group of relevant images. Mining a co-representation is essential for locating co-salient objects. Unfortunately, the current Co-SOD method does not pay enough attention that the information not related to the co-salient object is included in the co-representation. Such irrelevant information in the co-representation interferes with its locating of co-salient objects. In this paper, we propose a Co-Representation Purification (CoRP) method aiming at searching noise-free co-representation. We search a few pixel-wise embeddings probably belonging to co-salient regions. These embeddings constitute our co-representation and guide our prediction. For obtaining purer co-representation, we use the prediction to iteratively reduce irrelevant embeddings in our co-representation. Experiments on three datasets demonstrate that our CoRP achieves state-of-the-art performances on the benchmark datasets. Our source code is available at https://github.com/ZZY816/CoRP.",
      "paper_authors": [
        "Ziyue Zhu",
        "Zhao Zhang",
        "Zheng Lin",
        "Xing Sun",
        "Ming-Ming Cheng"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-03-14",
      "update_time": "2023-03-14",
      "comments": "Accepted by TPAMI 2023",
      "repo_url": "https://github.com/zzy816/corp"
    },
    "2303.02867": {
      "paper_id": "2303.02867v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2303.02867v3",
      "paper_key": "2303.02867",
      "paper_title": "Boundary-semantic collaborative guidance network with dual-stream feedback mechanism for salient object detection in optical remote sensing imagery",
      "paper_url": "http://arxiv.org/abs/2303.02867v3",
      "paper_abstract": "With the increasing application of deep learning in various domains, salient object detection in optical remote sensing images (ORSI-SOD) has attracted significant attention. However, most existing ORSI-SOD methods predominantly rely on local information from low-level features to infer salient boundary cues and supervise them using boundary ground truth, but fail to sufficiently optimize and protect the local information, and almost all approaches ignore the potential advantages offered by the last layer of the decoder to maintain the integrity of saliency maps. To address these issues, we propose a novel method named boundary-semantic collaborative guidance network (BSCGNet) with dual-stream feedback mechanism. First, we propose a boundary protection calibration (BPC) module, which effectively reduces the loss of edge position information during forward propagation and suppresses noise in low-level features without relying on boundary ground truth. Second, based on the BPC module, a dual feature feedback complementary (DFFC) module is proposed, which aggregates boundary-semantic dual features and provides effective feedback to coordinate features across different layers, thereby enhancing cross-scale knowledge communication. Finally, to obtain more complete saliency maps, we consider the uniqueness of the last layer of the decoder for the first time and propose the adaptive feedback refinement (AFR) module, which further refines feature representation and eliminates differences between features through a unique feedback mechanism. Extensive experiments on three benchmark datasets demonstrate that BSCGNet exhibits distinct advantages in challenging scenarios and outperforms the 17 state-of-the-art (SOTA) approaches proposed in recent years. Codes and results have been released on GitHub: https://github.com/YUHsss/BSCGNet.",
      "paper_authors": [
        "Dejun Feng",
        "Hongyu Chen",
        "Suning Liu",
        "Ziyang Liao",
        "Xingyu Shen",
        "Yakun Xie",
        "Jun Zhu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-03-06",
      "update_time": "2023-11-14",
      "comments": "Accepted by TGRS",
      "repo_url": "https://github.com/yuhsss/bscgnet"
    },
    "2302.14485": {
      "paper_id": "2302.14485v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2302.14485v2",
      "paper_key": "2302.14485",
      "paper_title": "Memory-aided Contrastive Consensus Learning for Co-salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2302.14485v2",
      "paper_abstract": "Co-Salient Object Detection (CoSOD) aims at detecting common salient objects within a group of relevant source images. Most of the latest works employ the attention mechanism for finding common objects. To achieve accurate CoSOD results with high-quality maps and high efficiency, we propose a novel Memory-aided Contrastive Consensus Learning (MCCL) framework, which is capable of effectively detecting co-salient objects in real time (~150 fps). To learn better group consensus, we propose the Group Consensus Aggregation Module (GCAM) to abstract the common features of each image group; meanwhile, to make the consensus representation more discriminative, we introduce the Memory-based Contrastive Module (MCM), which saves and updates the consensus of images from different groups in a queue of memories. Finally, to improve the quality and integrity of the predicted maps, we develop an Adversarial Integrity Learning (AIL) strategy to make the segmented regions more likely composed of complete objects with less surrounding noise. Extensive experiments on all the latest CoSOD benchmarks demonstrate that our lite MCCL outperforms 13 cutting-edge models, achieving the new state of the art (~5.9% and ~6.2% improvement in S-measure on CoSOD3k and CoSal2015, respectively). Our source codes, saliency maps, and online demos are publicly available at https://github.com/ZhengPeng7/MCCL.",
      "paper_authors": [
        "Peng Zheng",
        "Jie Qin",
        "Shuo Wang",
        "Tian-Zhu Xiang",
        "Huan Xiong"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-02-28",
      "update_time": "2023-03-11",
      "comments": "AAAI 2023",
      "repo_url": "https://github.com/zhengpeng7/mccl"
    },
    "2302.11361": {
      "paper_id": "2302.11361v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2302.11361v2",
      "paper_key": "2302.11361",
      "paper_title": "HDR image watermarking using saliency detection and quantization index modulation",
      "paper_url": "http://arxiv.org/abs/2302.11361v2",
      "paper_abstract": "High-dynamic range (HDR) images are circulated rapidly over the internet with risks of being exploited for unauthorized usage. To protect these images, some HDR image based watermarking (HDR-IW) methods were put forward. However, they inherited the same problem faced by conventional IW methods for standard dynamic range (SDR) images, where only trade-offs among conflicting requirements are managed instead of simultaneous improvement. In this paper, a novel saliency (eye-catching object) detection based trade-off independent HDR-IW is proposed, to simultaneously improve robustness, imperceptibility and payload. First, the host image goes through our proposed salient object detection model to produce a saliency map, which is, in turn, exploited to segment the foreground and background of the host image. Next, the binary watermark is partitioned into the foregrounds and backgrounds using the same mask and scrambled using a random permutation algorithm. Finally, the watermark segments are embedded into selected bit-plane of the corresponding host segments using quantized indexed modulation. Experimental results suggest that the proposed work outperforms state-of-the-art methods in terms of improving the conflicting requirements.",
      "paper_authors": [
        "Ahmed Khan",
        "Minoru Kuribayashi",
        "KokSheik Wong",
        "Vishnu Monn Baskaran"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-02-22",
      "update_time": "2023-02-23",
      "comments": null,
      "repo_url": "#"
    },
    "2302.10697": {
      "paper_id": "2302.10697v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2302.10697v2",
      "paper_key": "2302.10697",
      "paper_title": "A Visual Representation-guided Framework with Global Affinity for Weakly Supervised Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2302.10697v2",
      "paper_abstract": "Fully supervised salient object detection (SOD) methods have made considerable progress in performance, yet these models rely heavily on expensive pixel-wise labels. Recently, to achieve a trade-off between labeling burden and performance, scribble-based SOD methods have attracted increasing attention. Previous scribble-based models directly implement the SOD task only based on SOD training data with limited information, it is extremely difficult for them to understand the image and further achieve a superior SOD task. In this paper, we propose a simple yet effective framework guided by general visual representations with rich contextual semantic knowledge for scribble-based SOD. These general visual representations are generated by self-supervised learning based on large-scale unlabeled datasets. Our framework consists of a task-related encoder, a general visual module, and an information integration module to efficiently combine the general visual representations with task-related features to perform the SOD task based on understanding the contextual connections of images. Meanwhile, we propose a novel global semantic affinity loss to guide the model to perceive the global structure of the salient objects. Experimental results on five public benchmark datasets demonstrate that our method, which only utilizes scribble annotations without introducing any extra label, outperforms the state-of-the-art weakly supervised SOD methods. Specifically, it outperforms the previous best scribble-based method on all datasets with an average gain of 5.5% for max f-measure, 5.8% for mean f-measure, 24% for MAE, and 3.1% for E-measure. Moreover, our method achieves comparable or even superior performance to the state-of-the-art fully supervised models.",
      "paper_authors": [
        "Binwei Xu",
        "Haoran Liang",
        "Weihua Gong",
        "Ronghua Liang",
        "Peng Chen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-02-21",
      "update_time": "2023-06-09",
      "comments": null,
      "repo_url": "#"
    },
    "2302.08052": {
      "paper_id": "2302.08052v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2302.08052v1",
      "paper_key": "2302.08052",
      "paper_title": "Hierarchical Cross-modal Transformer for RGB-D Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2302.08052v1",
      "paper_abstract": "Most of existing RGB-D salient object detection (SOD) methods follow the CNN-based paradigm, which is unable to model long-range dependencies across space and modalities due to the natural locality of CNNs. Here we propose the Hierarchical Cross-modal Transformer (HCT), a new multi-modal transformer, to tackle this problem. Unlike previous multi-modal transformers that directly connecting all patches from two modalities, we explore the cross-modal complementarity hierarchically to respect the modality gap and spatial discrepancy in unaligned regions. Specifically, we propose to use intra-modal self-attention to explore complementary global contexts, and measure spatial-aligned inter-modal attention locally to capture cross-modal correlations. In addition, we present a Feature Pyramid module for Transformer (FPT) to boost informative cross-scale integration as well as a consistency-complementarity module to disentangle the multi-modal integration path and improve the fusion adaptivity. Comprehensive experiments on a large variety of public datasets verify the efficacy of our designs and the consistent improvement over state-of-the-art models.",
      "paper_authors": [
        "Hao Chen",
        "Feihong Shen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-02-16",
      "update_time": "2023-02-16",
      "comments": "10 pages, 10 figures",
      "repo_url": "#"
    },
    "2301.07405": {
      "paper_id": "2301.07405v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2301.07405v1",
      "paper_key": "2301.07405",
      "paper_title": "HiDAnet: RGB-D Salient Object Detection via Hierarchical Depth Awareness",
      "paper_url": "http://arxiv.org/abs/2301.07405v1",
      "paper_abstract": "RGB-D saliency detection aims to fuse multi-modal cues to accurately localize salient regions. Existing works often adopt attention modules for feature modeling, with few methods explicitly leveraging fine-grained details to merge with semantic cues. Thus, despite the auxiliary depth information, it is still challenging for existing models to distinguish objects with similar appearances but at distinct camera distances. In this paper, from a new perspective, we propose a novel Hierarchical Depth Awareness network (HiDAnet) for RGB-D saliency detection. Our motivation comes from the observation that the multi-granularity properties of geometric priors correlate well with the neural network hierarchies. To realize multi-modal and multi-level fusion, we first use a granularity-based attention scheme to strengthen the discriminatory power of RGB and depth features separately. Then we introduce a unified cross dual-attention module for multi-modal and multi-level fusion in a coarse-to-fine manner. The encoded multi-modal features are gradually aggregated into a shared decoder. Further, we exploit a multi-scale loss to take full advantage of the hierarchical information. Extensive experiments on challenging benchmark datasets demonstrate that our HiDAnet performs favorably over the state-of-the-art methods by large margins.",
      "paper_authors": [
        "Zongwei Wu",
        "Guillaume Allibert",
        "Fabrice Meriaudeau",
        "Chao Ma",
        "C\u00e9dric Demonceaux"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-01-18",
      "update_time": "2023-01-18",
      "comments": null,
      "repo_url": "#"
    },
    "2301.06679": {
      "paper_id": "2301.06679v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2301.06679v1",
      "paper_key": "2301.06679",
      "paper_title": "Rethinking Lightweight Salient Object Detection via Network Depth-Width Tradeoff",
      "paper_url": "http://arxiv.org/abs/2301.06679v1",
      "paper_abstract": "Existing salient object detection methods often adopt deeper and wider networks for better performance, resulting in heavy computational burden and slow inference speed. This inspires us to rethink saliency detection to achieve a favorable balance between efficiency and accuracy. To this end, we design a lightweight framework while maintaining satisfying competitive accuracy. Specifically, we propose a novel trilateral decoder framework by decoupling the U-shape structure into three complementary branches, which are devised to confront the dilution of semantic context, loss of spatial structure and absence of boundary detail, respectively. Along with the fusion of three branches, the coarse segmentation results are gradually refined in structure details and boundary quality. Without adding additional learnable parameters, we further propose Scale-Adaptive Pooling Module to obtain multi-scale receptive filed. In particular, on the premise of inheriting this framework, we rethink the relationship among accuracy, parameters and speed via network depth-width tradeoff. With these insightful considerations, we comprehensively design shallower and narrower models to explore the maximum potential of lightweight SOD. Our models are purposed for different application environments: 1) a tiny version CTD-S (1.7M, 125FPS) for resource constrained devices, 2) a fast version CTD-M (12.6M, 158FPS) for speed-demanding scenarios, 3) a standard version CTD-L (26.5M, 84FPS) for high-performance platforms. Extensive experiments validate the superiority of our method, which achieves better efficiency-accuracy balance across five benchmarks.",
      "paper_authors": [
        "Jia Li",
        "Shengye Qiao",
        "Zhirui Zhao",
        "Chenxi Xie",
        "Xiaowu Chen",
        "Changqun Xia"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-01-17",
      "update_time": "2023-01-17",
      "comments": null,
      "repo_url": "#"
    },
    "2301.05323": {
      "paper_id": "2301.05323v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2301.05323v2",
      "paper_key": "2301.05323",
      "paper_title": "Salient Object Detection for Images Taken by People With Vision Impairments",
      "paper_url": "http://arxiv.org/abs/2301.05323v2",
      "paper_abstract": "Salient object detection is the task of producing a binary mask for an image that deciphers which pixels belong to the foreground object versus background. We introduce a new salient object detection dataset using images taken by people who are visually impaired who were seeking to better understand their surroundings, which we call VizWiz-SalientObject. Compared to seven existing datasets, VizWiz-SalientObject is the largest (i.e., 32,000 human-annotated images) and contains unique characteristics including a higher prevalence of text in the salient objects (i.e., in 68\\% of images) and salient objects that occupy a larger ratio of the images (i.e., on average, $\\sim$50\\% coverage). We benchmarked seven modern salient object detection methods on our dataset and found they struggle most with images featuring salient objects that are large, have less complex boundaries, and lack text as well as for lower quality images. We invite the broader community to work on our new dataset challenge by publicly sharing the dataset at https://vizwiz.org/tasks-and-datasets/salient-object .",
      "paper_authors": [
        "Jarek Reynolds",
        "Chandra Kanth Nagesh",
        "Danna Gurari"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-01-12",
      "update_time": "2023-09-05",
      "comments": "Computer Vision and Pattern Recognition",
      "repo_url": "#"
    },
    "2301.03036": {
      "paper_id": "2301.03036v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2301.03036v1",
      "paper_key": "2301.03036",
      "paper_title": "HRTransNet: HRFormer-Driven Two-Modality Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2301.03036v1",
      "paper_abstract": "The High-Resolution Transformer (HRFormer) can maintain high-resolution representation and share global receptive fields. It is friendly towards salient object detection (SOD) in which the input and output have the same resolution. However, two critical problems need to be solved for two-modality SOD. One problem is two-modality fusion. The other problem is the HRFormer output's fusion. To address the first problem, a supplementary modality is injected into the primary modality by using global optimization and an attention mechanism to select and purify the modality at the input level. To solve the second problem, a dual-direction short connection fusion module is used to optimize the output features of HRFormer, thereby enhancing the detailed representation of objects at the output level. The proposed model, named HRTransNet, first introduces an auxiliary stream for feature extraction of supplementary modality. Then, features are injected into the primary modality at the beginning of each multi-resolution branch. Next, HRFormer is applied to achieve forwarding propagation. Finally, all the output features with different resolutions are aggregated by intra-feature and inter-feature interactive transformers. Application of the proposed model results in impressive improvement for driving two-modality SOD tasks, e.g., RGB-D, RGB-T, and light field SOD.https://github.com/liuzywen/HRTransNet",
      "paper_authors": [
        "Bin Tang",
        "Zhengyi Liu",
        "Yacheng Tan",
        "Qian He"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-01-08",
      "update_time": "2023-01-08",
      "comments": null,
      "repo_url": "https://github.com/liuzywen/hrtransnet"
    },
    "2301.02778": {
      "paper_id": "2301.02778v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2301.02778v2",
      "paper_key": "2301.02778",
      "paper_title": "Lightweight Salient Object Detection in Optical Remote-Sensing Images via Semantic Matching and Edge Alignment",
      "paper_url": "http://arxiv.org/abs/2301.02778v2",
      "paper_abstract": "Recently, relying on convolutional neural networks (CNNs), many methods for salient object detection in optical remote sensing images (ORSI-SOD) are proposed. However, most methods ignore the huge parameters and computational cost brought by CNNs, and only a few pay attention to the portability and mobility. To facilitate practical applications, in this paper, we propose a novel lightweight network for ORSI-SOD based on semantic matching and edge alignment, termed SeaNet. Specifically, SeaNet includes a lightweight MobileNet-V2 for feature extraction, a dynamic semantic matching module (DSMM) for high-level features, an edge self-alignment module (ESAM) for low-level features, and a portable decoder for inference. First, the high-level features are compressed into semantic kernels. Then, semantic kernels are used to activate salient object locations in two groups of high-level features through dynamic convolution operations in DSMM. Meanwhile, in ESAM, cross-scale edge information extracted from two groups of low-level features is self-aligned through L2 loss and used for detail enhancement. Finally, starting from the highest-level features, the decoder infers salient objects based on the accurate locations and fine details contained in the outputs of the two modules. Extensive experiments on two public datasets demonstrate that our lightweight SeaNet not only outperforms most state-of-the-art lightweight methods but also yields comparable accuracy with state-of-the-art conventional methods, while having only 2.76M parameters and running with 1.7G FLOPs for 288x288 inputs. Our code and results are available at https://github.com/MathLee/SeaNet.",
      "paper_authors": [
        "Gongyang Li",
        "Zhi Liu",
        "Xinpeng Zhang",
        "Weisi Lin"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-01-07",
      "update_time": "2023-04-03",
      "comments": "11 pages, 4 figures, Accepted by IEEE Transactions on Geoscience and\n  Remote Sensing 2023",
      "repo_url": "https://github.com/mathlee/seanet"
    },
    "2212.12378": {
      "paper_id": "2212.12378v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2212.12378v1",
      "paper_key": "2212.12378",
      "paper_title": "Multi-Projection Fusion and Refinement Network for Salient Object Detection in 360\u00b0 Omnidirectional Image",
      "paper_url": "http://arxiv.org/abs/2212.12378v1",
      "paper_abstract": "Salient object detection (SOD) aims to determine the most visually attractive objects in an image. With the development of virtual reality technology, 360{\\deg} omnidirectional image has been widely used, but the SOD task in 360{\\deg} omnidirectional image is seldom studied due to its severe distortions and complex scenes. In this paper, we propose a Multi-Projection Fusion and Refinement Network (MPFR-Net) to detect the salient objects in 360{\\deg} omnidirectional image. Different from the existing methods, the equirectangular projection image and four corresponding cube-unfolding images are embedded into the network simultaneously as inputs, where the cube-unfolding images not only provide supplementary information for equirectangular projection image, but also ensure the object integrity of the cube-map projection. In order to make full use of these two projection modes, a Dynamic Weighting Fusion (DWF) module is designed to adaptively integrate the features of different projections in a complementary and dynamic manner from the perspective of inter and intra features. Furthermore, in order to fully explore the way of interaction between encoder and decoder features, a Filtration and Refinement (FR) module is designed to suppress the redundant information between the feature itself and the feature. Experimental results on two omnidirectional datasets demonstrate that the proposed approach outperforms the state-of-the-art methods both qualitatively and quantitatively.",
      "paper_authors": [
        "Runmin Cong",
        "Ke Huang",
        "Jianjun Lei",
        "Yao Zhao",
        "Qingming Huang",
        "Sam Kwong"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-12-23",
      "update_time": "2022-12-23",
      "comments": "Accepted by IEEE Transactions on Neural Networks and Learning Systems\n  2022",
      "repo_url": "#"
    },
    "2212.06493": {
      "paper_id": "2212.06493v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2212.06493v1",
      "paper_key": "2212.06493",
      "paper_title": "Pixel is All You Need: Adversarial Trajectory-Ensemble Active Learning for Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2212.06493v1",
      "paper_abstract": "Although weakly-supervised techniques can reduce the labeling effort, it is unclear whether a saliency model trained with weakly-supervised data (e.g., point annotation) can achieve the equivalent performance of its fully-supervised version. This paper attempts to answer this unexplored question by proving a hypothesis: there is a point-labeled dataset where saliency models trained on it can achieve equivalent performance when trained on the densely annotated dataset. To prove this conjecture, we proposed a novel yet effective adversarial trajectory-ensemble active learning (ATAL). Our contributions are three-fold: 1) Our proposed adversarial attack triggering uncertainty can conquer the overconfidence of existing active learning methods and accurately locate these uncertain pixels. {2)} Our proposed trajectory-ensemble uncertainty estimation method maintains the advantages of the ensemble networks while significantly reducing the computational cost. {3)} Our proposed relationship-aware diversity sampling algorithm can conquer oversampling while boosting performance. Experimental results show that our ATAL can find such a point-labeled dataset, where a saliency model trained on it obtained $97\\%$ -- $99\\%$ performance of its fully-supervised version with only ten annotated points per image.",
      "paper_authors": [
        "Zhenyu Wu",
        "Lin Wang",
        "Wei Wang",
        "Qing Xia",
        "Chenglizhao Chen",
        "Aimin Hao",
        "Shuo Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-12-13",
      "update_time": "2022-12-13",
      "comments": "9 pages, 8 figures",
      "repo_url": "#"
    },
    "2212.05370": {
      "paper_id": "2212.05370v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2212.05370v3",
      "paper_key": "2212.05370",
      "paper_title": "Source-free Depth for Object Pop-out",
      "paper_url": "http://arxiv.org/abs/2212.05370v3",
      "paper_abstract": "Depth cues are known to be useful for visual perception. However, direct measurement of depth is often impracticable. Fortunately, though, modern learning-based methods offer promising depth maps by inference in the wild. In this work, we adapt such depth inference models for object segmentation using the objects' \"pop-out\" prior in 3D. The \"pop-out\" is a simple composition prior that assumes objects reside on the background surface. Such compositional prior allows us to reason about objects in the 3D space. More specifically, we adapt the inferred depth maps such that objects can be localized using only 3D information. Such separation, however, requires knowledge about contact surface which we learn using the weak supervision of the segmentation mask. Our intermediate representation of contact surface, and thereby reasoning about objects purely in 3D, allows us to better transfer the depth knowledge into semantics. The proposed adaptation method uses only the depth model without needing the source data used for training, making the learning process efficient and practical. Our experiments on eight datasets of two challenging tasks, namely camouflaged object detection and salient object detection, consistently demonstrate the benefit of our method in terms of both performance and generalizability.",
      "paper_authors": [
        "Zongwei Wu",
        "Danda Pani Paudel",
        "Deng-Ping Fan",
        "Jingjing Wang",
        "Shuo Wang",
        "C\u00e9dric Demonceaux",
        "Radu Timofte",
        "Luc Van Gool"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-12-10",
      "update_time": "2023-09-25",
      "comments": "Accepted to ICCV 2023",
      "repo_url": "https://github.com/zongwei97/popnet"
    },
    "2212.01764": {
      "paper_id": "2212.01764v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2212.01764v1",
      "paper_key": "2212.01764",
      "paper_title": "Synthesize Boundaries: A Boundary-aware Self-consistent Framework for Weakly Supervised Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2212.01764v1",
      "paper_abstract": "Fully supervised salient object detection (SOD) has made considerable progress based on expensive and time-consuming data with pixel-wise annotations. Recently, to relieve the labeling burden while maintaining performance, some scribble-based SOD methods have been proposed. However, learning precise boundary details from scribble annotations that lack edge information is still difficult. In this paper, we propose to learn precise boundaries from our designed synthetic images and labels without introducing any extra auxiliary data. The synthetic image creates boundary information by inserting synthetic concave regions that simulate the real concave regions of salient objects. Furthermore, we propose a novel self-consistent framework that consists of a global integral branch (GIB) and a boundary-aware branch (BAB) to train a saliency detector. GIB aims to identify integral salient objects, whose input is the original image. BAB aims to help predict accurate boundaries, whose input is the synthetic image. These two branches are connected through a self-consistent loss to guide the saliency detector to predict precise boundaries while identifying salient objects. Experimental results on five benchmarks demonstrate that our method outperforms the state-of-the-art weakly supervised SOD methods and further narrows the gap with the fully supervised methods.",
      "paper_authors": [
        "Binwei Xu",
        "Haoran Liang",
        "Ronghua Liang",
        "Peng Chen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-12-04",
      "update_time": "2022-12-04",
      "comments": null,
      "repo_url": "#"
    },
    "2211.14419": {
      "paper_id": "2211.14419v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2211.14419v1",
      "paper_key": "2211.14419",
      "paper_title": "Panoramic Video Salient Object Detection with Ambisonic Audio Guidance",
      "paper_url": "http://arxiv.org/abs/2211.14419v1",
      "paper_abstract": "Video salient object detection (VSOD), as a fundamental computer vision problem, has been extensively discussed in the last decade. However, all existing works focus on addressing the VSOD problem in 2D scenarios. With the rapid development of VR devices, panoramic videos have been a promising alternative to 2D videos to provide immersive feelings of the real world. In this paper, we aim to tackle the video salient object detection problem for panoramic videos, with their corresponding ambisonic audios. A multimodal fusion module equipped with two pseudo-siamese audio-visual context fusion (ACF) blocks is proposed to effectively conduct audio-visual interaction. The ACF block equipped with spherical positional encoding enables the fusion in the 3D context to capture the spatial correspondence between pixels and sound sources from the equirectangular frames and ambisonic audios. Experimental results verify the effectiveness of our proposed components and demonstrate that our method achieves state-of-the-art performance on the ASOD60K dataset.",
      "paper_authors": [
        "Xiang Li",
        "Haoyuan Cao",
        "Shijie Zhao",
        "Junlin Li",
        "Li Zhang",
        "Bhiksha Raj"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-11-26",
      "update_time": "2022-11-26",
      "comments": null,
      "repo_url": "#"
    },
    "2211.10608": {
      "paper_id": "2211.10608v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2211.10608v1",
      "paper_key": "2211.10608",
      "paper_title": "Semantic-aware Texture-Structure Feature Collaboration for Underwater Image Enhancement",
      "paper_url": "http://arxiv.org/abs/2211.10608v1",
      "paper_abstract": "Underwater image enhancement has become an attractive topic as a significant technology in marine engineering and aquatic robotics. However, the limited number of datasets and imperfect hand-crafted ground truth weaken its robustness to unseen scenarios, and hamper the application to high-level vision tasks. To address the above limitations, we develop an efficient and compact enhancement network in collaboration with a high-level semantic-aware pretrained model, aiming to exploit its hierarchical feature representation as an auxiliary for the low-level underwater image enhancement. Specifically, we tend to characterize the shallow layer features as textures while the deep layer features as structures in the semantic-aware model, and propose a multi-path Contextual Feature Refinement Module (CFRM) to refine features in multiple scales and model the correlation between different features. In addition, a feature dominative network is devised to perform channel-wise modulation on the aggregated texture and structure features for the adaptation to different feature patterns of the enhancement network. Extensive experiments on benchmarks demonstrate that the proposed algorithm achieves more appealing results and outperforms state-of-the-art methods by large margins. We also apply the proposed algorithm to the underwater salient object detection task to reveal the favorable semantic-aware ability for high-level vision tasks. The code is available at STSC.",
      "paper_authors": [
        "Di Wang",
        "Long Ma",
        "Risheng Liu",
        "Xin Fan"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-11-19",
      "update_time": "2022-11-19",
      "comments": "Accepted by ICRA2022",
      "repo_url": "https://github.com/wdhudiekou/stsc"
    },
    "2211.08724": {
      "paper_id": "2211.08724v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2211.08724v1",
      "paper_key": "2211.08724",
      "paper_title": "PAANet:Visual Perception based Four-stage Framework for Salient Object Detection using High-order Contrast Operator",
      "paper_url": "http://arxiv.org/abs/2211.08724v1",
      "paper_abstract": "It is believed that human vision system (HVS) consists of pre-attentive process and attention process when performing salient object detection (SOD). Based on this fact, we propose a four-stage framework for SOD, in which the first two stages match the \\textbf{P}re-\\textbf{A}ttentive process consisting of general feature extraction (GFE) and feature preprocessing (FP), and the last two stages are corresponding to \\textbf{A}ttention process containing saliency feature extraction (SFE) and the feature aggregation (FA), namely \\textbf{PAANet}. According to the pre-attentive process, the GFE stage applies the fully-trained backbone and needs no further finetuning for different datasets. This modification can greatly increase the training speed. The FP stage plays the role of finetuning but works more efficiently because of its simpler structure and fewer parameters. Moreover, in SFE stage we design for saliency feature extraction a novel contrast operator, which works more semantically in contrast with the traditional convolution operator when extracting the interactive information between the foreground and its surroundings. Interestingly, this contrast operator can be cascaded to form a deeper structure and extract higher-order saliency more effective for complex scene. Comparative experiments with the state-of-the-art methods on 5 datasets demonstrate the effectiveness of our framework.",
      "paper_authors": [
        "Yanbo Yuan",
        "Hua Zhong",
        "Haixiong Li",
        "Xiao cheng",
        "Linmei Xia"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-11-16",
      "update_time": "2022-11-16",
      "comments": null,
      "repo_url": "#"
    },
    "2211.06697": {
      "paper_id": "2211.06697v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2211.06697v1",
      "paper_key": "2211.06697",
      "paper_title": "Multistep feature aggregation framework for salient object detection",
      "paper_url": "http://arxiv.org/abs/2211.06697v1",
      "paper_abstract": "Recent works on salient object detection have made use of multi-scale features in a way such that high-level features and low-level features can collaborate in locating salient objects. Many of the previous methods have achieved great performance in salient object detection. By merging the high-level and low-level features, a large number of feature information can be extracted. Generally, they are doing these in a one-way framework, and interweaving the variable features all the way to the final feature output. Which may cause some blurring or inaccurate localization of saliency maps. To overcome these difficulties, we introduce a multistep feature aggregation (MSFA) framework for salient object detection, which is composed of three modules, including the Diverse Reception (DR) module, multiscale interaction (MSI) module and Feature Enhancement (FE) module to accomplish better multi-level feature fusion. Experimental results on six benchmark datasets demonstrate that MSFA achieves state-of-the-art performance.",
      "paper_authors": [
        "Xiaogang Liu Shuang Song"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-11-12",
      "update_time": "2022-11-12",
      "comments": null,
      "repo_url": "#"
    },
    "2211.06097": {
      "paper_id": "2211.06097v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2211.06097v1",
      "paper_key": "2211.06097",
      "paper_title": "Interactive Context-Aware Network for RGB-T Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2211.06097v1",
      "paper_abstract": "Salient object detection (SOD) focuses on distinguishing the most conspicuous objects in the scene. However, most related works are based on RGB images, which lose massive useful information. Accordingly, with the maturity of thermal technology, RGB-T (RGB-Thermal) multi-modality tasks attain more and more attention. Thermal infrared images carry important information which can be used to improve the accuracy of SOD prediction. To accomplish it, the methods to integrate multi-modal information and suppress noises are critical. In this paper, we propose a novel network called Interactive Context-Aware Network (ICANet). It contains three modules that can effectively perform the cross-modal and cross-scale fusions. We design a Hybrid Feature Fusion (HFF) module to integrate the features of two modalities, which utilizes two types of feature extraction. The Multi-Scale Attention Reinforcement (MSAR) and Upper Fusion (UF) blocks are responsible for the cross-scale fusion that converges different levels of features and generate the prediction maps. We also raise a novel Context-Aware Multi-Supervised Network (CAMSNet) to calculate the content loss between the prediction and the ground truth (GT). Experiments prove that our network performs favorably against the state-of-the-art RGB-T SOD methods.",
      "paper_authors": [
        "Yuxuan Wang",
        "Feng Dong",
        "Jinchao Zhu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-11-11",
      "update_time": "2022-11-11",
      "comments": "17 pages, 7 figures",
      "repo_url": "#"
    },
    "2210.15933": {
      "paper_id": "2210.15933v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2210.15933v1",
      "paper_key": "2210.15933",
      "paper_title": "PSFormer: Point Transformer for 3D Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2210.15933v1",
      "paper_abstract": "We propose PSFormer, an effective point transformer model for 3D salient object detection. PSFormer is an encoder-decoder network that takes full advantage of transformers to model the contextual information in both multi-scale point- and scene-wise manners. In the encoder, we develop a Point Context Transformer (PCT) module to capture region contextual features at the point level; PCT contains two different transformers to excavate the relationship among points. In the decoder, we develop a Scene Context Transformer (SCT) module to learn context representations at the scene level; SCT contains both Upsampling-and-Transformer blocks and Multi-context Aggregation units to integrate the global semantic and multi-level features from the encoder into the global scene context. Experiments show clear improvements of PSFormer over its competitors and validate that PSFormer is more robust to challenging cases such as small objects, multiple objects, and objects with complex structures.",
      "paper_authors": [
        "Baian Chen",
        "Lipeng Gu",
        "Xin Zhuang",
        "Yiyang Shen",
        "Weiming Wang",
        "Mingqiang Wei"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-10-28",
      "update_time": "2022-10-28",
      "comments": null,
      "repo_url": "#"
    },
    "2210.15392": {
      "paper_id": "2210.15392v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2210.15392v2",
      "paper_key": "2210.15392",
      "paper_title": "LeNo: Adversarial Robust Salient Object Detection Networks with Learnable Noise",
      "paper_url": "http://arxiv.org/abs/2210.15392v2",
      "paper_abstract": "Pixel-wise prediction with deep neural network has become an effective paradigm for salient object detection (SOD) and achieved remarkable performance. However, very few SOD models are robust against adversarial attacks which are visually imperceptible for human visual attention. The previous work robust saliency (ROSA) shuffles the pre-segmented superpixels and then refines the coarse saliency map by the densely connected conditional random field (CRF). Different from ROSA that relies on various pre- and post-processings, this paper proposes a light-weight Learnable Noise (LeNo) to defend adversarial attacks for SOD models. LeNo preserves accuracy of SOD models on both adversarial and clean images, as well as inference speed. In general, LeNo consists of a simple shallow noise and noise estimation that embedded in the encoder and decoder of arbitrary SOD networks respectively. Inspired by the center prior of human visual attention mechanism, we initialize the shallow noise with a cross-shaped gaussian distribution for better defense against adversarial attacks. Instead of adding additional network components for post-processing, the proposed noise estimation modifies only one channel of the decoder. With the deeply-supervised noise-decoupled training on state-of-the-art RGB and RGB-D SOD networks, LeNo outperforms previous works not only on adversarial images but also on clean images, which contributes stronger robustness for SOD. Our code is available at https://github.com/ssecv/LeNo.",
      "paper_authors": [
        "He Wang",
        "Lin Wan",
        "He Tang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-10-27",
      "update_time": "2022-12-07",
      "comments": "8 pages, 6 figures, accepted by AAAI 2023",
      "repo_url": "https://github.com/ssecv/leno"
    },
    "2210.13835": {
      "paper_id": "2210.13835v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2210.13835v1",
      "paper_key": "2210.13835",
      "paper_title": "Synthetic Data Supervised Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2210.13835v1",
      "paper_abstract": "Although deep salient object detection (SOD) has achieved remarkable progress, deep SOD models are extremely data-hungry, requiring large-scale pixel-wise annotations to deliver such promising results. In this paper, we propose a novel yet effective method for SOD, coined SODGAN, which can generate infinite high-quality image-mask pairs requiring only a few labeled data, and these synthesized pairs can replace the human-labeled DUTS-TR to train any off-the-shelf SOD model. Its contribution is three-fold. 1) Our proposed diffusion embedding network can address the manifold mismatch and is tractable for the latent code generation, better matching with the ImageNet latent space. 2) For the first time, our proposed few-shot saliency mask generator can synthesize infinite accurate image synchronized saliency masks with a few labeled data. 3) Our proposed quality-aware discriminator can select highquality synthesized image-mask pairs from noisy synthetic data pool, improving the quality of synthetic data. For the first time, our SODGAN tackles SOD with synthetic data directly generated from the generative model, which opens up a new research paradigm for SOD. Extensive experimental results show that the saliency model trained on synthetic data can achieve $98.4\\%$ F-measure of the saliency model trained on the DUTS-TR. Moreover, our approach achieves a new SOTA performance in semi/weakly-supervised methods, and even outperforms several fully-supervised SOTA methods. Code is available at https://github.com/wuzhenyubuaa/SODGAN",
      "paper_authors": [
        "Zhenyu Wu",
        "Lin Wang",
        "Wei Wang",
        "Tengfei Shi",
        "Chenglizhao Chen",
        "Aimin Hao",
        "Shuo Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-10-25",
      "update_time": "2022-10-25",
      "comments": "9 pages, 8 figures",
      "repo_url": "https://github.com/wuzhenyubuaa/sodgan"
    },
    "2210.13821": {
      "paper_id": "2210.13821v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2210.13821v1",
      "paper_key": "2210.13821",
      "paper_title": "Salient Object Detection via Dynamic Scale Routing",
      "paper_url": "http://arxiv.org/abs/2210.13821v1",
      "paper_abstract": "Recent research advances in salient object detection (SOD) could largely be attributed to ever-stronger multi-scale feature representation empowered by the deep learning technologies. The existing SOD deep models extract multi-scale features via the off-the-shelf encoders and combine them smartly via various delicate decoders. However, the kernel sizes in this commonly-used thread are usually \"fixed\". In our new experiments, we have observed that kernels of small size are preferable in scenarios containing tiny salient objects. In contrast, large kernel sizes could perform better for images with large salient objects. Inspired by this observation, we advocate the \"dynamic\" scale routing (as a brand-new idea) in this paper. It will result in a generic plug-in that could directly fit the existing feature backbone. This paper's key technical innovations are two-fold. First, instead of using the vanilla convolution with fixed kernel sizes for the encoder design, we propose the dynamic pyramid convolution (DPConv), which dynamically selects the best-suited kernel sizes w.r.t. the given input. Second, we provide a self-adaptive bidirectional decoder design to accommodate the DPConv-based encoder best. The most significant highlight is its capability of routing between feature scales and their dynamic collection, making the inference process scale-aware. As a result, this paper continues to enhance the current SOTA performance. Both the code and dataset are publicly available at https://github.com/wuzhenyubuaa/DPNet.",
      "paper_authors": [
        "Zhenyu Wu",
        "Shuai Li",
        "Chenglizhao Chen",
        "Hong Qin",
        "Aimin Hao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-10-25",
      "update_time": "2022-10-25",
      "comments": "15 pages, 15 figures",
      "repo_url": "https://github.com/wuzhenyubuaa/dpnet"
    },
    "2210.08472": {
      "paper_id": "2210.08472v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2210.08472v1",
      "paper_key": "2210.08472",
      "paper_title": "Object-Attentional Untargeted Adversarial Attack",
      "paper_url": "http://arxiv.org/abs/2210.08472v1",
      "paper_abstract": "Deep neural networks are facing severe threats from adversarial attacks. Most existing black-box attacks fool target model by generating either global perturbations or local patches. However, both global perturbations and local patches easily cause annoying visual artifacts in adversarial example. Compared with some smooth regions of an image, the object region generally has more edges and a more complex texture. Thus small perturbations on it will be more imperceptible. On the other hand, the object region is undoubtfully the decisive part of an image to classification tasks. Motivated by these two facts, we propose an object-attentional adversarial attack method for untargeted attack. Specifically, we first generate an object region by intersecting the object detection region from YOLOv4 with the salient object detection (SOD) region from HVPNet. Furthermore, we design an activation strategy to avoid the reaction caused by the incomplete SOD. Then, we perform an adversarial attack only on the detected object region by leveraging Simple Black-box Adversarial Attack (SimBA). To verify the proposed method, we create a unique dataset by extracting all the images containing the object defined by COCO from ImageNet-1K, named COCO-Reduced-ImageNet in this paper. Experimental results on ImageNet-1K and COCO-Reduced-ImageNet show that under various system settings, our method yields the adversarial example with better perceptual quality meanwhile saving the query budget up to 24.16\\% compared to the state-of-the-art approaches including SimBA.",
      "paper_authors": [
        "Chao Zhou",
        "Yuan-Gen Wang",
        "Guopu Zhu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-10-16",
      "update_time": "2022-10-16",
      "comments": null,
      "repo_url": "#"
    },
    "2210.07920": {
      "paper_id": "2210.07920v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2210.07920v2",
      "paper_key": "2210.07920",
      "paper_title": "MOVE: Unsupervised Movable Object Segmentation and Detection",
      "paper_url": "http://arxiv.org/abs/2210.07920v2",
      "paper_abstract": "We introduce MOVE, a novel method to segment objects without any form of supervision. MOVE exploits the fact that foreground objects can be shifted locally relative to their initial position and result in realistic (undistorted) new images. This property allows us to train a segmentation model on a dataset of images without annotation and to achieve state of the art (SotA) performance on several evaluation datasets for unsupervised salient object detection and segmentation. In unsupervised single object discovery, MOVE gives an average CorLoc improvement of 7.2% over the SotA, and in unsupervised class-agnostic object detection it gives a relative AP improvement of 53% on average. Our approach is built on top of self-supervised features (e.g. from DINO or MAE), an inpainting network (based on the Masked AutoEncoder) and adversarial training.",
      "paper_authors": [
        "Adam Bielski",
        "Paolo Favaro"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-10-14",
      "update_time": "2022-10-20",
      "comments": "36th Conference on Neural Information Processing Systems (NeurIPS\n  2022)",
      "repo_url": "https://github.com/adambielski/move-seg"
    },
    "2210.05912": {
      "paper_id": "2210.05912v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2210.05912v1",
      "paper_key": "2210.05912",
      "paper_title": "PSNet: Parallel Symmetric Network for Video Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2210.05912v1",
      "paper_abstract": "For the video salient object detection (VSOD) task, how to excavate the information from the appearance modality and the motion modality has always been a topic of great concern. The two-stream structure, including an RGB appearance stream and an optical flow motion stream, has been widely used as a typical pipeline for VSOD tasks, but the existing methods usually only use motion features to unidirectionally guide appearance features or adaptively but blindly fuse two modality features. However, these methods underperform in diverse scenarios due to the uncomprehensive and unspecific learning schemes. In this paper, following a more secure modeling philosophy, we deeply investigate the importance of appearance modality and motion modality in a more comprehensive way and propose a VSOD network with up and down parallel symmetry, named PSNet. Two parallel branches with different dominant modalities are set to achieve complete video saliency decoding with the cooperation of the Gather Diffusion Reinforcement (GDR) module and Cross-modality Refinement and Complement (CRC) module. Finally, we use the Importance Perception Fusion (IPF) module to fuse the features from two parallel branches according to their different importance in different scenarios. Experiments on four dataset benchmarks demonstrate that our method achieves desirable and competitive performance.",
      "paper_authors": [
        "Runmin Cong",
        "Weiyu Song",
        "Jianjun Lei",
        "Guanghui Yue",
        "Yao Zhao",
        "Sam Kwong"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-10-12",
      "update_time": "2022-10-12",
      "comments": "Accepted by IEEE Transactions on Emerging Topics in Computational\n  Intelligence 2022, 13 pages, 8 figures",
      "repo_url": "#"
    },
    "2210.04266": {
      "paper_id": "2210.04266v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2210.04266v1",
      "paper_key": "2210.04266",
      "paper_title": "Does Thermal Really Always Matter for RGB-T Salient Object Detection?",
      "paper_url": "http://arxiv.org/abs/2210.04266v1",
      "paper_abstract": "In recent years, RGB-T salient object detection (SOD) has attracted continuous attention, which makes it possible to identify salient objects in environments such as low light by introducing thermal image. However, most of the existing RGB-T SOD models focus on how to perform cross-modality feature fusion, ignoring whether thermal image is really always matter in SOD task. Starting from the definition and nature of this task, this paper rethinks the connotation of thermal modality, and proposes a network named TNet to solve the RGB-T SOD task. In this paper, we introduce a global illumination estimation module to predict the global illuminance score of the image, so as to regulate the role played by the two modalities. In addition, considering the role of thermal modality, we set up different cross-modality interaction mechanisms in the encoding phase and the decoding phase. On the one hand, we introduce a semantic constraint provider to enrich the semantics of thermal images in the encoding phase, which makes thermal modality more suitable for the SOD task. On the other hand, we introduce a two-stage localization and complementation module in the decoding phase to transfer object localization cue and internal integrity cue in thermal features to the RGB modality. Extensive experiments on three datasets show that the proposed TNet achieves competitive performance compared with 20 state-of-the-art methods.",
      "paper_authors": [
        "Runmin Cong",
        "Kepu Zhang",
        "Chen Zhang",
        "Feng Zheng",
        "Yao Zhao",
        "Qingming Huang",
        "Sam Kwong"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-10-09",
      "update_time": "2022-10-09",
      "comments": "Accepted by IEEE Trans. Multimedia 2022, 13 pages, 9 figures",
      "repo_url": "#"
    },
    "2210.02843": {
      "paper_id": "2210.02843v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2210.02843v1",
      "paper_key": "2210.02843",
      "paper_title": "CIR-Net: Cross-modality Interaction and Refinement for RGB-D Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2210.02843v1",
      "paper_abstract": "Focusing on the issue of how to effectively capture and utilize cross-modality information in RGB-D salient object detection (SOD) task, we present a convolutional neural network (CNN) model, named CIR-Net, based on the novel cross-modality interaction and refinement. For the cross-modality interaction, 1) a progressive attention guided integration unit is proposed to sufficiently integrate RGB-D feature representations in the encoder stage, and 2) a convergence aggregation structure is proposed, which flows the RGB and depth decoding features into the corresponding RGB-D decoding streams via an importance gated fusion unit in the decoder stage. For the cross-modality refinement, we insert a refinement middleware structure between the encoder and the decoder, in which the RGB, depth, and RGB-D encoder features are further refined by successively using a self-modality attention refinement unit and a cross-modality weighting refinement unit. At last, with the gradually refined features, we predict the saliency map in the decoder stage. Extensive experiments on six popular RGB-D SOD benchmarks demonstrate that our network outperforms the state-of-the-art saliency detectors both qualitatively and quantitatively.",
      "paper_authors": [
        "Runmin Cong",
        "Qinwei Lin",
        "Chen Zhang",
        "Chongyi Li",
        "Xiaochun Cao",
        "Qingming Huang",
        "Yao Zhao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-10-06",
      "update_time": "2022-10-06",
      "comments": "Accepted by IEEE Transactions on Image Processing 2022, 16 pages, 11\n  figures",
      "repo_url": "https://gitee.com/Lin-Qinwei/CIR-Net-MindSpore.git"
    },
    "2209.13801": {
      "paper_id": "2209.13801v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2209.13801v1",
      "paper_key": "2209.13801",
      "paper_title": "Translation, Scale and Rotation: Cross-Modal Alignment Meets RGB-Infrared Vehicle Detection",
      "paper_url": "http://arxiv.org/abs/2209.13801v1",
      "paper_abstract": "Integrating multispectral data in object detection, especially visible and infrared images, has received great attention in recent years. Since visible (RGB) and infrared (IR) images can provide complementary information to handle light variations, the paired images are used in many fields, such as multispectral pedestrian detection, RGB-IR crowd counting and RGB-IR salient object detection. Compared with natural RGB-IR images, we find detection in aerial RGB-IR images suffers from cross-modal weakly misalignment problems, which are manifested in the position, size and angle deviations of the same object. In this paper, we mainly address the challenge of cross-modal weakly misalignment in aerial RGB-IR images. Specifically, we firstly explain and analyze the cause of the weakly misalignment problem. Then, we propose a Translation-Scale-Rotation Alignment (TSRA) module to address the problem by calibrating the feature maps from these two modalities. The module predicts the deviation between two modality objects through an alignment process and utilizes Modality-Selection (MS) strategy to improve the performance of alignment. Finally, a two-stream feature alignment detector (TSFADet) based on the TSRA module is constructed for RGB-IR object detection in aerial images. With comprehensive experiments on the public DroneVehicle datasets, we verify that our method reduces the effect of the cross-modal misalignment and achieve robust detection results.",
      "paper_authors": [
        "Maoxun Yuan",
        "Yinyan Wang",
        "Xingxing Wei"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-09-28",
      "update_time": "2022-09-28",
      "comments": null,
      "repo_url": "#"
    },
    "2209.13222": {
      "paper_id": "2209.13222v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2209.13222v1",
      "paper_key": "2209.13222",
      "paper_title": "View-aware Salient Object Detection for 360\u00b0 Omnidirectional Image",
      "paper_url": "http://arxiv.org/abs/2209.13222v1",
      "paper_abstract": "Image-based salient object detection (ISOD) in 360{\\deg} scenarios is significant for understanding and applying panoramic information. However, research on 360{\\deg} ISOD has not been widely explored due to the lack of large, complex, high-resolution, and well-labeled datasets. Towards this end, we construct a large scale 360{\\deg} ISOD dataset with object-level pixel-wise annotation on equirectangular projection (ERP), which contains rich panoramic scenes with not less than 2K resolution and is the largest dataset for 360{\\deg} ISOD by far to our best knowledge. By observing the data, we find current methods face three significant challenges in panoramic scenarios: diverse distortion degrees, discontinuous edge effects and changeable object scales. Inspired by humans' observing process, we propose a view-aware salient object detection method based on a Sample Adaptive View Transformer (SAVT) module with two sub-modules to mitigate these issues. Specifically, the sub-module View Transformer (VT) contains three transform branches based on different kinds of transformations to learn various features under different views and heighten the model's feature toleration of distortion, edge effects and object scales. Moreover, the sub-module Sample Adaptive Fusion (SAF) is to adjust the weights of different transform branches based on various sample features and make transformed enhanced features fuse more appropriately. The benchmark results of 20 state-of-the-art ISOD methods reveal the constructed dataset is very challenging. Moreover, exhaustive experiments verify the proposed approach is practical and outperforms the state-of-the-art methods.",
      "paper_authors": [
        "Junjie Wu",
        "Changqun Xia",
        "Tianshu Yu",
        "Jia Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-09-27",
      "update_time": "2022-09-27",
      "comments": "This paper has been accepted by IEEE Transactions on Multimedia",
      "repo_url": "#"
    },
    "2209.10158": {
      "paper_id": "2209.10158v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2209.10158v1",
      "paper_key": "2209.10158",
      "paper_title": "Position-Aware Relation Learning for RGB-Thermal Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2209.10158v1",
      "paper_abstract": "RGB-Thermal salient object detection (SOD) combines two spectra to segment visually conspicuous regions in images. Most existing methods use boundary maps to learn the sharp boundary. These methods ignore the interactions between isolated boundary pixels and other confident pixels, leading to sub-optimal performance. To address this problem,we propose a position-aware relation learning network (PRLNet) for RGB-T SOD based on swin transformer. PRLNet explores the distance and direction relationships between pixels to strengthen intra-class compactness and inter-class separation, generating salient object masks with clear boundaries and homogeneous regions. Specifically, we develop a novel signed distance map auxiliary module (SDMAM) to improve encoder feature representation, which takes into account the distance relation of different pixels in boundary neighborhoods. Then, we design a feature refinement approach with directional field (FRDF), which rectifies features of boundary neighborhood by exploiting the features inside salient objects. FRDF utilizes the directional information between object pixels to effectively enhance the intra-class compactness of salient regions. In addition, we constitute a pure transformer encoder-decoder network to enhance multispectral feature representation for RGB-T SOD. Finally, we conduct quantitative and qualitative experiments on three public benchmark datasets.The results demonstrate that our proposed method outperforms the state-of-the-art methods.",
      "paper_authors": [
        "Heng Zhou",
        "Chunna Tian",
        "Zhenxi Zhang",
        "Chengyang Li",
        "Yuxuan Ding",
        "Yongqiang Xie",
        "Zhongbo Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-09-21",
      "update_time": "2022-09-21",
      "comments": null,
      "repo_url": "#"
    },
    "2209.09475": {
      "paper_id": "2209.09475v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2209.09475v3",
      "paper_key": "2209.09475",
      "paper_title": "Revisiting Image Pyramid Structure for High Resolution Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2209.09475v3",
      "paper_abstract": "Salient object detection (SOD) has been in the spotlight recently, yet has been studied less for high-resolution (HR) images. Unfortunately, HR images and their pixel-level annotations are certainly more labor-intensive and time-consuming compared to low-resolution (LR) images and annotations. Therefore, we propose an image pyramid-based SOD framework, Inverse Saliency Pyramid Reconstruction Network (InSPyReNet), for HR prediction without any of HR datasets. We design InSPyReNet to produce a strict image pyramid structure of saliency map, which enables to ensemble multiple results with pyramid-based image blending. For HR prediction, we design a pyramid blending method which synthesizes two different image pyramids from a pair of LR and HR scale from the same image to overcome effective receptive field (ERF) discrepancy. Our extensive evaluations on public LR and HR SOD benchmarks demonstrate that InSPyReNet surpasses the State-of-the-Art (SotA) methods on various SOD metrics and boundary accuracy.",
      "paper_authors": [
        "Taehun Kim",
        "Kunhee Kim",
        "Joonyeong Lee",
        "Dongmin Cha",
        "Jiho Lee",
        "Daijin Kim"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-09-20",
      "update_time": "2022-11-16",
      "comments": "27 pages, 15 figures, 7 tables. To appear in the 16th Asian\n  Conference on Computer Vision (ACCV2022), December 4-8, 2022, Macau SAR,\n  China. DOI will be added soon. Results on DIS5K are added in appendices which\n  will not be in the published version",
      "repo_url": "https://github.com/plemeri/inspyrenet"
    },
    "2209.04639": {
      "paper_id": "2209.04639v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2209.04639v1",
      "paper_key": "2209.04639",
      "paper_title": "Large-Field Contextual Feature Learning for Glass Detection",
      "paper_url": "http://arxiv.org/abs/2209.04639v1",
      "paper_abstract": "Glass is very common in our daily life. Existing computer vision systems neglect it and thus may have severe consequences, e.g., a robot may crash into a glass wall. However, sensing the presence of glass is not straightforward. The key challenge is that arbitrary objects/scenes can appear behind the glass. In this paper, we propose an important problem of detecting glass surfaces from a single RGB image. To address this problem, we construct the first large-scale glass detection dataset (GDD) and propose a novel glass detection network, called GDNet-B, which explores abundant contextual cues in a large field-of-view via a novel large-field contextual feature integration (LCFI) module and integrates both high-level and low-level boundary features with a boundary feature enhancement (BFE) module. Extensive experiments demonstrate that our GDNet-B achieves satisfying glass detection results on the images within and beyond the GDD testing set. We further validate the effectiveness and generalization capability of our proposed GDNet-B by applying it to other vision tasks, including mirror segmentation and salient object detection. Finally, we show the potential applications of glass detection and discuss possible future research directions.",
      "paper_authors": [
        "Haiyang Mei",
        "Xin Yang",
        "Letian Yu",
        "Qiang Zhang",
        "Xiaopeng Wei",
        "Rynson W. H. Lau"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-09-10",
      "update_time": "2022-09-10",
      "comments": null,
      "repo_url": "#"
    },
    "2209.02957": {
      "paper_id": "2209.02957v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2209.02957v1",
      "paper_key": "2209.02957",
      "paper_title": "A Weakly Supervised Learning Framework for Salient Object Detection via Hybrid Labels",
      "paper_url": "http://arxiv.org/abs/2209.02957v1",
      "paper_abstract": "Fully-supervised salient object detection (SOD) methods have made great progress, but such methods often rely on a large number of pixel-level annotations, which are time-consuming and labour-intensive. In this paper, we focus on a new weakly-supervised SOD task under hybrid labels, where the supervision labels include a large number of coarse labels generated by the traditional unsupervised method and a small number of real labels. To address the issues of label noise and quantity imbalance in this task, we design a new pipeline framework with three sophisticated training strategies. In terms of model framework, we decouple the task into label refinement sub-task and salient object detection sub-task, which cooperate with each other and train alternately. Specifically, the R-Net is designed as a two-stream encoder-decoder model equipped with Blender with Guidance and Aggregation Mechanisms (BGA), aiming to rectify the coarse labels for more reliable pseudo-labels, while the S-Net is a replaceable SOD network supervised by the pseudo labels generated by the current R-Net. Note that, we only need to use the trained S-Net for testing. Moreover, in order to guarantee the effectiveness and efficiency of network training, we design three training strategies, including alternate iteration mechanism, group-wise incremental mechanism, and credibility verification mechanism. Experiments on five SOD benchmarks show that our method achieves competitive performance against weakly-supervised/unsupervised methods both qualitatively and quantitatively.",
      "paper_authors": [
        "Runmin Cong",
        "Qi Qin",
        "Chen Zhang",
        "Qiuping Jiang",
        "Shiqi Wang",
        "Yao Zhao",
        "Sam Kwong"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-09-07",
      "update_time": "2022-09-07",
      "comments": "Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology 2022",
      "repo_url": "https://github.com/rmcong/Hybrid-Label-SOD_TCSVT2022"
    },
    "2208.09668": {
      "paper_id": "2208.09668v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2208.09668v3",
      "paper_key": "2208.09668",
      "paper_title": "Generalised Co-Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2208.09668v3",
      "paper_abstract": "We propose a new setting that relaxes an assumption in the conventional Co-Salient Object Detection (CoSOD) setting by allowing the presence of \"noisy images\" which do not show the shared co-salient object. We call this new setting Generalised Co-Salient Object Detection (GCoSOD). We propose a novel random sampling based Generalised CoSOD Training (GCT) strategy to distill the awareness of inter-image absence of co-salient objects into CoSOD models. It employs a Diverse Sampling Self-Supervised Learning (DS3L) that, in addition to the provided supervised co-salient label, introduces additional self-supervised labels for noisy images (being null, that no co-salient object is present). Further, the random sampling process inherent in GCT enables the generation of a high-quality uncertainty map highlighting potential false-positive predictions at instance level. To evaluate the performance of CoSOD models under the GCoSOD setting, we propose two new testing datasets, namely CoCA-Common and CoCA-Zero, where a common salient object is partially present in the former and completely absent in the latter. Extensive experiments demonstrate that our proposed method significantly improves the performance of CoSOD models in terms of the performance under the GCoSOD setting as well as the model calibration degrees.",
      "paper_authors": [
        "Jiawei Liu",
        "Jing Zhang",
        "Ruikai Cui",
        "Kaihao Zhang",
        "Weihao Li",
        "Nick Barnes"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-08-20",
      "update_time": "2023-08-11",
      "comments": null,
      "repo_url": "#"
    },
    "2208.08145": {
      "paper_id": "2208.08145v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2208.08145v1",
      "paper_key": "2208.08145",
      "paper_title": "Stereo Superpixel Segmentation Via Decoupled Dynamic Spatial-Embedding Fusion Network",
      "paper_url": "http://arxiv.org/abs/2208.08145v1",
      "paper_abstract": "Stereo superpixel segmentation aims at grouping the discretizing pixels into perceptual regions through left and right views more collaboratively and efficiently. Existing superpixel segmentation algorithms mostly utilize color and spatial features as input, which may impose strong constraints on spatial information while utilizing the disparity information in terms of stereo image pairs. To alleviate this issue, we propose a stereo superpixel segmentation method with a decoupling mechanism of spatial information in this work. To decouple stereo disparity information and spatial information, the spatial information is temporarily removed before fusing the features of stereo image pairs, and a decoupled stereo fusion module (DSFM) is proposed to handle the stereo features alignment as well as occlusion problems. Moreover, since the spatial information is vital to superpixel segmentation, we further design a dynamic spatiality embedding module (DSEM) to re-add spatial information, and the weights of spatial information will be adaptively adjusted through the dynamic fusion (DF) mechanism in DSEM for achieving a finer segmentation. Comprehensive experimental results demonstrate that our method can achieve the state-of-the-art performance on the KITTI2015 and Cityscapes datasets, and also verify the efficiency when applied in salient object detection on NJU2K dataset. The source code will be available publicly after paper is accepted.",
      "paper_authors": [
        "Hua Li",
        "Junyan Liang",
        "Ruiqi Wu",
        "Runmin Cong",
        "Junhui Wu",
        "Sam Tak Wu Kwong"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-08-17",
      "update_time": "2022-08-17",
      "comments": "11 pages, 13 figures",
      "repo_url": "#"
    },
    "2208.04361": {
      "paper_id": "2208.04361v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2208.04361v1",
      "paper_key": "2208.04361",
      "paper_title": "Semi-Supervised Cross-Modal Salient Object Detection with U-Structure Networks",
      "paper_url": "http://arxiv.org/abs/2208.04361v1",
      "paper_abstract": "Salient Object Detection (SOD) is a popular and important topic aimed at precise detection and segmentation of the interesting regions in the images. We integrate the linguistic information into the vision-based U-Structure networks designed for salient object detection tasks. The experiments are based on the newly created DUTS Cross Modal (DUTS-CM) dataset, which contains both visual and linguistic labels. We propose a new module called efficient Cross-Modal Self-Attention (eCMSA) to combine visual and linguistic features and improve the performance of the original U-structure networks. Meanwhile, to reduce the heavy burden of labeling, we employ a semi-supervised learning method by training an image caption model based on the DUTS-CM dataset, which can automatically label other datasets like DUT-OMRON and HKU-IS. The comprehensive experiments show that the performance of SOD can be improved with the natural language input and is competitive compared with other SOD methods.",
      "paper_authors": [
        "Yunqing Bao",
        "Hang Dai",
        "Abdulmotaleb Elsaddik"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-08-08",
      "update_time": "2022-08-08",
      "comments": null,
      "repo_url": "#"
    },
    "2208.03918": {
      "paper_id": "2208.03918v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2208.03918v2",
      "paper_key": "2208.03918",
      "paper_title": "Depth Quality-Inspired Feature Manipulation for Efficient RGB-D and Video Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2208.03918v2",
      "paper_abstract": "Recently CNN-based RGB-D salient object detection (SOD) has obtained significant improvement on detection accuracy. However, existing models often fail to perform well in terms of efficiency and accuracy simultaneously. This hinders their potential applications on mobile devices as well as many real-world problems. To bridge the accuracy gap between lightweight and large models for RGB-D SOD, in this paper, an efficient module that can greatly improve the accuracy but adds little computation is proposed. Inspired by the fact that depth quality is a key factor influencing the accuracy, we propose an efficient depth quality-inspired feature manipulation (DQFM) process, which can dynamically filter depth features according to depth quality. The proposed DQFM resorts to the alignment of low-level RGB and depth features, as well as holistic attention of the depth stream to explicitly control and enhance cross-modal fusion. We embed DQFM to obtain an efficient lightweight RGB-D SOD model called DFM-Net, where we in addition design a tailored depth backbone and a two-stage decoder as basic parts. Extensive experimental results on nine RGB-D datasets demonstrate that our DFM-Net outperforms recent efficient models, running at about 20 FPS on CPU with only 8.5Mb model size, and meanwhile being 2.9/2.4 times faster and 6.7/3.1 times smaller than the latest best models A2dele and MobileSal. It also maintains state-of-the-art accuracy when even compared to non-efficient models. Interestingly, further statistics and analyses verify the ability of DQFM in distinguishing depth maps of various qualities without any quality labels. Last but not least, we further apply DFM-Net to deal with video SOD (VSOD), achieving comparable performance against recent efficient models while being 3/2.3 times faster/smaller than the prior best in this field. Our code is available at https://github.com/zwbx/DFM-Net.",
      "paper_authors": [
        "Wenbo Zhang",
        "Keren Fu",
        "Zhuo Wang",
        "Ge-Peng Ji",
        "Qijun Zhao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-08-08",
      "update_time": "2022-12-13",
      "comments": "arXiv admin note: substantial text overlap with arXiv:2107.01779",
      "repo_url": "https://github.com/zwbx/DFM-Net"
    },
    "2208.02178": {
      "paper_id": "2208.02178v6",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2208.02178v6",
      "paper_key": "2208.02178",
      "paper_title": "KD-SCFNet: Towards More Accurate and Efficient Salient Object Detection via Knowledge Distillation",
      "paper_url": "http://arxiv.org/abs/2208.02178v6",
      "paper_abstract": "Most existing salient object detection (SOD) models are difficult to apply due to the complex and huge model structures. Although some lightweight models are proposed, the accuracy is barely satisfactory. In this paper, we design a novel semantics-guided contextual fusion network (SCFNet) that focuses on the interactive fusion of multi-level features for accurate and efficient salient object detection. Furthermore, we apply knowledge distillation to SOD task and provide a sizeable dataset KD-SOD80K. In detail, we transfer the rich knowledge from a seasoned teacher to the untrained SCFNet through unlabeled images, enabling SCFNet to learn a strong generalization ability to detect salient objects more accurately. The knowledge distillation based SCFNet (KDSCFNet) achieves comparable accuracy to the state-of-the-art heavyweight methods with less than 1M parameters and 174 FPS real-time detection speed. Extensive experiments demonstrate the robustness and effectiveness of the proposed distillation method and SOD framework. Code and data: https://github.com/zhangjinCV/KD-SCFNet.",
      "paper_authors": [
        "Jin Zhang",
        "Qiuwei Liang",
        "Yanjiao Shi"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-08-03",
      "update_time": "2022-11-21",
      "comments": "There are some important mistakes in the article that need to be\n  modified",
      "repo_url": "#"
    },
    "2208.00946": {
      "paper_id": "2208.00946v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2208.00946v2",
      "paper_key": "2208.00946",
      "paper_title": "Motion-aware Memory Network for Fast Video Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2208.00946v2",
      "paper_abstract": "Previous methods based on 3DCNN, convLSTM, or optical flow have achieved great success in video salient object detection (VSOD). However, they still suffer from high computational costs or poor quality of the generated saliency maps. To solve these problems, we design a space-time memory (STM)-based network, which extracts useful temporal information of the current frame from adjacent frames as the temporal branch of VSOD. Furthermore, previous methods only considered single-frame prediction without temporal association. As a result, the model may not focus on the temporal information sufficiently. Thus, we initially introduce object motion prediction between inter-frame into VSOD. Our model follows standard encoder--decoder architecture. In the encoding stage, we generate high-level temporal features by using high-level features from the current and its adjacent frames. This approach is more efficient than the optical flow-based methods. In the decoding stage, we propose an effective fusion strategy for spatial and temporal branches. The semantic information of the high-level features is used to fuse the object details in the low-level features, and then the spatiotemporal features are obtained step by step to reconstruct the saliency maps. Moreover, inspired by the boundary supervision commonly used in image salient object detection (ISOD), we design a motion-aware loss for predicting object boundary motion and simultaneously perform multitask learning for VSOD and object motion prediction, which can further facilitate the model to extract spatiotemporal features accurately and maintain the object integrity. Extensive experiments on several datasets demonstrated the effectiveness of our method and can achieve state-of-the-art metrics on some datasets. The proposed model does not require optical flow or other preprocessing, and can reach a speed of nearly 100 FPS during inference.",
      "paper_authors": [
        "Xing Zhao",
        "Haoran Liang",
        "Peipei Li",
        "Guodao Sun",
        "Dongdong Zhao",
        "Ronghua Liang",
        "Xiaofei He"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-08-01",
      "update_time": "2023-12-31",
      "comments": "13 pages, 10 figures",
      "repo_url": "https://github.com/zhaoxing2022/mmn-vsod"
    },
    "2207.11889": {
      "paper_id": "2207.11889v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2207.11889v1",
      "paper_key": "2207.11889",
      "paper_title": "Salient Object Detection for Point Clouds",
      "paper_url": "http://arxiv.org/abs/2207.11889v1",
      "paper_abstract": "This paper researches the unexplored task-point cloud salient object detection (SOD). Differing from SOD for images, we find the attention shift of point clouds may provoke saliency conflict, i.e., an object paradoxically belongs to salient and non-salient categories. To eschew this issue, we present a novel view-dependent perspective of salient objects, reasonably reflecting the most eye-catching objects in point cloud scenarios. Following this formulation, we introduce PCSOD, the first dataset proposed for point cloud SOD consisting of 2,872 in-/out-door 3D views. The samples in our dataset are labeled with hierarchical annotations, e.g., super-/sub-class, bounding box, and segmentation map, which endows the brilliant generalizability and broad applicability of our dataset verifying various conjectures. To evidence the feasibility of our solution, we further contribute a baseline model and benchmark five representative models for a comprehensive comparison. The proposed model can effectively analyze irregular and unordered points for detecting salient objects. Thanks to incorporating the task-tailored designs, our method shows visible superiority over other baselines, producing more satisfactory results. Extensive experiments and discussions reveal the promising potential of this research field, paving the way for further study.",
      "paper_authors": [
        "Songlin Fan",
        "Wei Gao",
        "Ge Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-07-25",
      "update_time": "2022-07-25",
      "comments": "Accepted to ECCV 2022. Project Page:\n  https://git.openi.org.cn/OpenPointCloud/PCSOD",
      "repo_url": "https://git.openi.org.cn/OpenPointCloud/PCSOD"
    },
    "2207.07898": {
      "paper_id": "2207.07898v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2207.07898v1",
      "paper_key": "2207.07898",
      "paper_title": "SPSN: Superpixel Prototype Sampling Network for RGB-D Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2207.07898v1",
      "paper_abstract": "RGB-D salient object detection (SOD) has been in the spotlight recently because it is an important preprocessing operation for various vision tasks. However, despite advances in deep learning-based methods, RGB-D SOD is still challenging due to the large domain gap between an RGB image and the depth map and low-quality depth maps. To solve this problem, we propose a novel superpixel prototype sampling network (SPSN) architecture. The proposed model splits the input RGB image and depth map into component superpixels to generate component prototypes. We design a prototype sampling network so that the network only samples prototypes corresponding to salient objects. In addition, we propose a reliance selection module to recognize the quality of each RGB and depth feature map and adaptively weight them in proportion to their reliability. The proposed method makes the model robust to inconsistencies between RGB images and depth maps and eliminates the influence of non-salient objects. Our method is evaluated on five popular datasets, achieving state-of-the-art performance. We prove the effectiveness of the proposed method through comparative experiments.",
      "paper_authors": [
        "Minhyeok Lee",
        "Chaewon Park",
        "Suhwan Cho",
        "Sangyoun Lee"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-07-16",
      "update_time": "2022-07-16",
      "comments": "Accepted to European Conference on Computer Vision (ECCV) 2022",
      "repo_url": "https://github.com/Hydragon516/SPSN"
    },
    "2207.07269": {
      "paper_id": "2207.07269v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2207.07269v1",
      "paper_key": "2207.07269",
      "paper_title": "Weakly Supervised Video Salient Object Detection via Point Supervision",
      "paper_url": "http://arxiv.org/abs/2207.07269v1",
      "paper_abstract": "Video salient object detection models trained on pixel-wise dense annotation have achieved excellent performance, yet obtaining pixel-by-pixel annotated datasets is laborious. Several works attempt to use scribble annotations to mitigate this problem, but point supervision as a more labor-saving annotation method (even the most labor-saving method among manual annotation methods for dense prediction), has not been explored. In this paper, we propose a strong baseline model based on point supervision. To infer saliency maps with temporal information, we mine inter-frame complementary information from short-term and long-term perspectives, respectively. Specifically, we propose a hybrid token attention module, which mixes optical flow and image information from orthogonal directions, adaptively highlighting critical optical flow information (channel dimension) and critical token information (spatial dimension). To exploit long-term cues, we develop the Long-term Cross-Frame Attention module (LCFA), which assists the current frame in inferring salient objects based on multi-frame tokens. Furthermore, we label two point-supervised datasets, P-DAVIS and P-DAVSOD, by relabeling the DAVIS and the DAVSOD dataset. Experiments on the six benchmark datasets illustrate our method outperforms the previous state-of-the-art weakly supervised methods and even is comparable with some fully supervised approaches. Source code and datasets are available.",
      "paper_authors": [
        "Shuyong Gao",
        "Haozhe Xing",
        "Wei Zhang",
        "Yan Wang",
        "Qianyu Guo",
        "Wenqiang Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-07-15",
      "update_time": "2022-07-15",
      "comments": "accepted by ACM MM 2022",
      "repo_url": "#"
    },
    "2207.05921": {
      "paper_id": "2207.05921v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2207.05921v3",
      "paper_key": "2207.05921",
      "paper_title": "Texture-guided Saliency Distilling for Unsupervised Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2207.05921v3",
      "paper_abstract": "Deep Learning-based Unsupervised Salient Object Detection (USOD) mainly relies on the noisy saliency pseudo labels that have been generated from traditional handcraft methods or pre-trained networks. To cope with the noisy labels problem, a class of methods focus on only easy samples with reliable labels but ignore valuable knowledge in hard samples. In this paper, we propose a novel USOD method to mine rich and accurate saliency knowledge from both easy and hard samples. First, we propose a Confidence-aware Saliency Distilling (CSD) strategy that scores samples conditioned on samples' confidences, which guides the model to distill saliency knowledge from easy samples to hard samples progressively. Second, we propose a Boundary-aware Texture Matching (BTM) strategy to refine the boundaries of noisy labels by matching the textures around the predicted boundary. Extensive experiments on RGB, RGB-D, RGB-T, and video SOD benchmarks prove that our method achieves state-of-the-art USOD performance.",
      "paper_authors": [
        "Huajun Zhou",
        "Bo Qiao",
        "Lingxiao Yang",
        "Jianhuang Lai",
        "Xiaohua Xie"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-07-13",
      "update_time": "2023-05-09",
      "comments": "8 pages, accepted to CVPR 2023",
      "repo_url": "https://github.com/moothes/a2s-v2"
    },
    "2207.04224": {
      "paper_id": "2207.04224v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2207.04224v1",
      "paper_key": "2207.04224",
      "paper_title": "SiaTrans: Siamese Transformer Network for RGB-D Salient Object Detection with Depth Image Classification",
      "paper_url": "http://arxiv.org/abs/2207.04224v1",
      "paper_abstract": "RGB-D SOD uses depth information to handle challenging scenes and obtain high-quality saliency maps. Existing state-of-the-art RGB-D saliency detection methods overwhelmingly rely on the strategy of directly fusing depth information. Although these methods improve the accuracy of saliency prediction through various cross-modality fusion strategies, misinformation provided by some poor-quality depth images can affect the saliency prediction result. To address this issue, a novel RGB-D salient object detection model (SiaTrans) is proposed in this paper, which allows training on depth image quality classification at the same time as training on SOD. In light of the common information between RGB and depth images on salient objects, SiaTrans uses a Siamese transformer network with shared weight parameters as the encoder and extracts RGB and depth features concatenated on the batch dimension, saving space resources without compromising performance. SiaTrans uses the Class token in the backbone network (T2T-ViT) to classify the quality of depth images without preventing the token sequence from going on with the saliency detection task. Transformer-based cross-modality fusion module (CMF) can effectively fuse RGB and depth information. And in the testing process, CMF can choose to fuse cross-modality information or enhance RGB information according to the quality classification signal of the depth image. The greatest benefit of our designed CMF and decoder is that they maintain the consistency of RGB and RGB-D information decoding: SiaTrans decodes RGB-D or RGB information under the same model parameters according to the classification signal during testing. Comprehensive experiments on nine RGB-D SOD benchmark datasets show that SiaTrans has the best overall performance and the least computation compared with recent state-of-the-art methods.",
      "paper_authors": [
        "Xingzhao Jia",
        "Dongye Changlei",
        "Yanjun Peng"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-07-09",
      "update_time": "2022-07-09",
      "comments": "21 pages, 7 figures",
      "repo_url": "#"
    },
    "2207.03558": {
      "paper_id": "2207.03558v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2207.03558v1",
      "paper_key": "2207.03558",
      "paper_title": "Mirror Complementary Transformer Network for RGB-thermal Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2207.03558v1",
      "paper_abstract": "RGB-thermal salient object detection (RGB-T SOD) aims to locate the common prominent objects of an aligned visible and thermal infrared image pair and accurately segment all the pixels belonging to those objects. It is promising in challenging scenes such as nighttime and complex backgrounds due to the insensitivity to lighting conditions of thermal images. Thus, the key problem of RGB-T SOD is to make the features from the two modalities complement and adjust each other flexibly, since it is inevitable that any modalities of RGB-T image pairs failure due to challenging scenes such as extreme light conditions and thermal crossover. In this paper, we propose a novel mirror complementary Transformer network (MCNet) for RGB-T SOD. Specifically, we introduce a Transformer-based feature extraction module to effective extract hierarchical features of RGB and thermal images. Then, through the attention-based feature interaction and serial multiscale dilated convolution (SDC) based feature fusion modules, the proposed model achieves the complementary interaction of low-level features and the semantic fusion of deep features. Finally, based on the mirror complementary structure, the salient regions of the two modalities can be accurately extracted even one modality is invalid. To demonstrate the robustness of the proposed model under challenging scenes in real world, we build a novel RGB-T SOD dataset VT723 based on a large public semantic segmentation RGB-T dataset used in the autonomous driving domain. Expensive experiments on benchmark and VT723 datasets show that the proposed method outperforms state-of-the-art approaches, including CNN-based and Transformer-based methods. The code and dataset will be released later at https://github.com/jxr326/SwinMCNet.",
      "paper_authors": [
        "Xiurong Jiang",
        "Lin Zhu",
        "Yifan Hou",
        "Hui Tian"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-07-07",
      "update_time": "2022-07-07",
      "comments": null,
      "repo_url": "https://github.com/jxr326/swinmcnet"
    },
    "2207.01755": {
      "paper_id": "2207.01755v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2207.01755v1",
      "paper_key": "2207.01755",
      "paper_title": "Attention Guided Network for Salient Object Detection in Optical Remote Sensing Images",
      "paper_url": "http://arxiv.org/abs/2207.01755v1",
      "paper_abstract": "Due to the extreme complexity of scale and shape as well as the uncertainty of the predicted location, salient object detection in optical remote sensing images (RSI-SOD) is a very difficult task. The existing SOD methods can satisfy the detection performance for natural scene images, but they are not well adapted to RSI-SOD due to the above-mentioned image characteristics in remote sensing images. In this paper, we propose a novel Attention Guided Network (AGNet) for SOD in optical RSIs, including position enhancement stage and detail refinement stage. Specifically, the position enhancement stage consists of a semantic attention module and a contextual attention module to accurately describe the approximate location of salient objects. The detail refinement stage uses the proposed self-refinement module to progressively refine the predicted results under the guidance of attention and reverse attention. In addition, the hybrid loss is applied to supervise the training of the network, which can improve the performance of the model from three perspectives of pixel, region and statistics. Extensive experiments on two popular benchmarks demonstrate that AGNet achieves competitive performance compared to other state-of-the-art methods. The code will be available at https://github.com/NuaaYH/AGNet.",
      "paper_authors": [
        "Yuhan Lin",
        "Han Sun",
        "Ningzhong Liu",
        "Yetong Bian",
        "Jun Cen",
        "Huiyu Zhou"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-07-05",
      "update_time": "2022-07-05",
      "comments": "accepted by ICANN2022, The code is available at\n  https://github.com/NuaaYH/AGNet",
      "repo_url": "https://github.com/nuaayh/agnet"
    },
    "2207.01172": {
      "paper_id": "2207.01172v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2207.01172v1",
      "paper_key": "2207.01172",
      "paper_title": "TANet: Transformer-based Asymmetric Network for RGB-D Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2207.01172v1",
      "paper_abstract": "Existing RGB-D SOD methods mainly rely on a symmetric two-stream CNN-based network to extract RGB and depth channel features separately. However, there are two problems with the symmetric conventional network structure: first, the ability of CNN in learning global contexts is limited; second, the symmetric two-stream structure ignores the inherent differences between modalities. In this paper, we propose a Transformer-based asymmetric network (TANet) to tackle the issues mentioned above. We employ the powerful feature extraction capability of Transformer (PVTv2) to extract global semantic information from RGB data and design a lightweight CNN backbone (LWDepthNet) to extract spatial structure information from depth data without pre-training. The asymmetric hybrid encoder (AHE) effectively reduces the number of parameters in the model while increasing speed without sacrificing performance. Then, we design a cross-modal feature fusion module (CMFFM), which enhances and fuses RGB and depth features with each other. Finally, we add edge prediction as an auxiliary task and propose an edge enhancement module (EEM) to generate sharper contours. Extensive experiments demonstrate that our method achieves superior performance over 14 state-of-the-art RGB-D methods on six public datasets. Our code will be released at https://github.com/lc012463/TANet.",
      "paper_authors": [
        "Chang Liu",
        "Gang Yang",
        "Shuo Wang",
        "Hangxu Wang",
        "Yunhua Zhang",
        "Yutao Wang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-07-04",
      "update_time": "2022-07-04",
      "comments": null,
      "repo_url": "https://github.com/lc012463/tanet"
    },
    "2206.09564": {
      "paper_id": "2206.09564v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2206.09564v1",
      "paper_key": "2206.09564",
      "paper_title": "A Novel Long-term Iterative Mining Scheme for Video Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2206.09564v1",
      "paper_abstract": "The existing state-of-the-art (SOTA) video salient object detection (VSOD) models have widely followed short-term methodology, which dynamically determines the balance between spatial and temporal saliency fusion by solely considering the current consecutive limited frames. However, the short-term methodology has one critical limitation, which conflicts with the real mechanism of our visual system -- a typical long-term methodology. As a result, failure cases keep showing up in the results of the current SOTA models, and the short-term methodology becomes the major technical bottleneck. To solve this problem, this paper proposes a novel VSOD approach, which performs VSOD in a complete long-term way. Our approach converts the sequential VSOD, a sequential task, to a data mining problem, i.e., decomposing the input video sequence to object proposals in advance and then mining salient object proposals as much as possible in an easy-to-hard way. Since all object proposals are simultaneously available, the proposed approach is a complete long-term approach, which can alleviate some difficulties rooted in conventional short-term approaches. In addition, we devised an online updating scheme that can grasp the most representative and trustworthy pattern profile of the salient objects, outputting framewise saliency maps with rich details and smoothing both spatially and temporally. The proposed approach outperforms almost all SOTA models on five widely used benchmark datasets.",
      "paper_authors": [
        "Chenglizhao Chen",
        "Hengsen Wang",
        "Yuming Fang",
        "Chong Peng"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-06-20",
      "update_time": "2022-06-20",
      "comments": null,
      "repo_url": "#"
    },
    "2206.09552": {
      "paper_id": "2206.09552v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2206.09552v1",
      "paper_key": "2206.09552",
      "paper_title": "Dynamic Message Propagation Network for RGB-D Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2206.09552v1",
      "paper_abstract": "This paper presents a novel deep neural network framework for RGB-D salient object detection by controlling the message passing between the RGB images and depth maps on the feature level and exploring the long-range semantic contexts and geometric information on both RGB and depth features to infer salient objects. To achieve this, we formulate a dynamic message propagation (DMP) module with the graph neural networks and deformable convolutions to dynamically learn the context information and to automatically predict filter weights and affinity matrices for message propagation control. We further embed this module into a Siamese-based network to process the RGB image and depth map respectively and design a multi-level feature fusion (MFF) module to explore the cross-level information between the refined RGB and depth features. Compared with 17 state-of-the-art methods on six benchmark datasets for RGB-D salient object detection, experimental results show that our method outperforms all the others, both quantitatively and visually.",
      "paper_authors": [
        "Baian Chen",
        "Zhilei Chen",
        "Xiaowei Hu",
        "Jun Xu",
        "Haoran Xie",
        "Mingqiang Wei",
        "Jing Qin"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-06-20",
      "update_time": "2022-06-20",
      "comments": "12 pages, 8 figures",
      "repo_url": "#"
    },
    "2206.03105": {
      "paper_id": "2206.03105v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2206.03105v1",
      "paper_key": "2206.03105",
      "paper_title": "Dual Swin-Transformer based Mutual Interactive Network for RGB-D Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2206.03105v1",
      "paper_abstract": "Salient Object Detection is the task of predicting the human attended region in a given scene. Fusing depth information has been proven effective in this task. The main challenge of this problem is how to aggregate the complementary information from RGB modality and depth modality. However, conventional deep models heavily rely on CNN feature extractors, and the long-range contextual dependencies are usually ignored. In this work, we propose Dual Swin-Transformer based Mutual Interactive Network. We adopt Swin-Transformer as the feature extractor for both RGB and depth modality to model the long-range dependencies in visual inputs. Before fusing the two branches of features into one, attention-based modules are applied to enhance features from each modality. We design a self-attention-based cross-modality interaction module and a gated modality attention module to leverage the complementary information between the two modalities. For the saliency decoding, we create different stages enhanced with dense connections and keep a decoding memory while the multi-level encoding features are considered simultaneously. Considering the inaccurate depth map issue, we collect the RGB features of early stages into a skip convolution module to give more guidance from RGB modality to the final saliency prediction. In addition, we add edge supervision to regularize the feature learning process. Comprehensive experiments on five standard RGB-D SOD benchmark datasets over four evaluation metrics demonstrate the superiority of the proposed DTMINet method.",
      "paper_authors": [
        "Chao Zeng",
        "Sam Kwong"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-06-07",
      "update_time": "2022-06-07",
      "comments": null,
      "repo_url": "#"
    },
    "2205.15469": {
      "paper_id": "2205.15469v4",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2205.15469v4",
      "paper_key": "2205.15469",
      "paper_title": "GCoNet+: A Stronger Group Collaborative Co-Salient Object Detector",
      "paper_url": "http://arxiv.org/abs/2205.15469v4",
      "paper_abstract": "In this paper, we present a novel end-to-end group collaborative learning network, termed GCoNet+, which can effectively and efficiently (250 fps) identify co-salient objects in natural scenes. The proposed GCoNet+ achieves the new state-of-the-art performance for co-salient object detection (CoSOD) through mining consensus representations based on the following two essential criteria: 1) intra-group compactness to better formulate the consistency among co-salient objects by capturing their inherent shared attributes using our novel group affinity module (GAM); 2) inter-group separability to effectively suppress the influence of noisy objects on the output by introducing our new group collaborating module (GCM) conditioning on the inconsistent consensus. To further improve the accuracy, we design a series of simple yet effective components as follows: i) a recurrent auxiliary classification module (RACM) promoting model learning at the semantic level; ii) a confidence enhancement module (CEM) assisting the model in improving the quality of the final predictions; and iii) a group-based symmetric triplet (GST) loss guiding the model to learn more discriminative features. Extensive experiments on three challenging benchmarks, i.e., CoCA, CoSOD3k, and CoSal2015, demonstrate that our GCoNet+ outperforms the existing 12 cutting-edge models. Code has been released at https://github.com/ZhengPeng7/GCoNet_plus.",
      "paper_authors": [
        "Peng Zheng",
        "Huazhu Fu",
        "Deng-Ping Fan",
        "Qi Fan",
        "Jie Qin",
        "Yu-Wing Tai",
        "Chi-Keung Tang",
        "Luc Van Gool"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-05-30",
      "update_time": "2023-04-10",
      "comments": "T-PAMI 2023",
      "repo_url": "https://github.com/zhengpeng7/cosod_fps_collection"
    },
    "2205.11283": {
      "paper_id": "2205.11283v4",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2205.11283v4",
      "paper_key": "2205.11283",
      "paper_title": "SelfReformer: Self-Refined Network with Transformer for Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2205.11283v4",
      "paper_abstract": "The global and local contexts significantly contribute to the integrity of predictions in Salient Object Detection (SOD). Unfortunately, existing methods still struggle to generate complete predictions with fine details. There are two major problems in conventional approaches: first, for global context, high-level CNN-based encoder features cannot effectively catch long-range dependencies, resulting in incomplete predictions. Second, downsampling the ground truth to fit the size of predictions will introduce inaccuracy as the ground truth details are lost during interpolation or pooling. Thus, in this work, we developed a Transformer-based network and framed a supervised task for a branch to learn the global context information explicitly. Besides, we adopt Pixel Shuffle from Super-Resolution (SR) to reshape the predictions back to the size of ground truth instead of the reverse. Thus details in the ground truth are untouched. In addition, we developed a two-stage Context Refinement Module (CRM) to fuse global context and automatically locate and refine the local details in the predictions. The proposed network can guide and correct itself based on the global and local context generated, thus is named, Self-Refined Transformer (SelfReformer). Extensive experiments and evaluation results on five benchmark datasets demonstrate the outstanding performance of the network, and we achieved the state-of-the-art.",
      "paper_authors": [
        "Yi Ke Yun",
        "Weisi Lin"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-05-23",
      "update_time": "2022-07-18",
      "comments": null,
      "repo_url": "https://github.com/BarCodeReader/SelfReformer"
    },
    "2205.08959": {
      "paper_id": "2205.08959v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2205.08959v1",
      "paper_key": "2205.08959",
      "paper_title": "A lightweight multi-scale context network for salient object detection in optical remote sensing images",
      "paper_url": "http://arxiv.org/abs/2205.08959v1",
      "paper_abstract": "Due to the more dramatic multi-scale variations and more complicated foregrounds and backgrounds in optical remote sensing images (RSIs), the salient object detection (SOD) for optical RSIs becomes a huge challenge. However, different from natural scene images (NSIs), the discussion on the optical RSI SOD task still remains scarce. In this paper, we propose a multi-scale context network, namely MSCNet, for SOD in optical RSIs. Specifically, a multi-scale context extraction module is adopted to address the scale variation of salient objects by effectively learning multi-scale contextual information. Meanwhile, in order to accurately detect complete salient objects in complex backgrounds, we design an attention-based pyramid feature aggregation mechanism for gradually aggregating and refining the salient regions from the multi-scale context extraction module. Extensive experiments on two benchmarks demonstrate that MSCNet achieves competitive performance with only 3.26M parameters. The code will be available at https://github.com/NuaaYH/MSCNet.",
      "paper_authors": [
        "Yuhan Lin",
        "Han Sun",
        "Ningzhong Liu",
        "Yetong Bian",
        "Jun Cen",
        "Huiyu Zhou"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-05-18",
      "update_time": "2022-05-18",
      "comments": "accepted by ICPR2022, source code, see\n  https://github.com/NuaaYH/MSCNet",
      "repo_url": "https://github.com/nuaayh/mscnet"
    },
    "2205.07179": {
      "paper_id": "2205.07179v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2205.07179v1",
      "paper_key": "2205.07179",
      "paper_title": "Promoting Saliency From Depth: Deep Unsupervised RGB-D Saliency Detection",
      "paper_url": "http://arxiv.org/abs/2205.07179v1",
      "paper_abstract": "Growing interests in RGB-D salient object detection (RGB-D SOD) have been witnessed in recent years, owing partly to the popularity of depth sensors and the rapid progress of deep learning techniques. Unfortunately, existing RGB-D SOD methods typically demand large quantity of training images being thoroughly annotated at pixel-level. The laborious and time-consuming manual annotation has become a real bottleneck in various practical scenarios. On the other hand, current unsupervised RGB-D SOD methods still heavily rely on handcrafted feature representations. This inspires us to propose in this paper a deep unsupervised RGB-D saliency detection approach, which requires no manual pixel-level annotation during training. It is realized by two key ingredients in our training pipeline. First, a depth-disentangled saliency update (DSU) framework is designed to automatically produce pseudo-labels with iterative follow-up refinements, which provides more trustworthy supervision signals for training the saliency network. Second, an attentive training strategy is introduced to tackle the issue of noisy pseudo-labels, by properly re-weighting to highlight the more reliable pseudo-labels. Extensive experiments demonstrate the superior efficiency and effectiveness of our approach in tackling the challenging unsupervised RGB-D SOD scenarios. Moreover, our approach can also be adapted to work in fully-supervised situation. Empirical studies show the incorporation of our approach gives rise to notably performance improvement in existing supervised RGB-D SOD models.",
      "paper_authors": [
        "Wei Ji",
        "Jingjing Li",
        "Qi Bi",
        "Chuan Guo",
        "Jie Liu",
        "Li Cheng"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-05-15",
      "update_time": "2022-05-15",
      "comments": "This paper appeared at ICLR 2022",
      "repo_url": "https://github.com/jiwei0921/dsu"
    },
    "2205.06934": {
      "paper_id": "2205.06934v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2205.06934v2",
      "paper_key": "2205.06934",
      "paper_title": "A Saliency-Guided Street View Image Inpainting Framework for Efficient Last-Meters Wayfinding",
      "paper_url": "http://arxiv.org/abs/2205.06934v2",
      "paper_abstract": "Global Positioning Systems (GPS) have played a crucial role in various navigation applications. Nevertheless, localizing the perfect destination within the last few meters remains an important but unresolved problem. Limited by the GPS positioning accuracy, navigation systems always show users a vicinity of a destination, but not its exact location. Street view images (SVI) in maps as an immersive media technology have served as an aid to provide the physical environment for human last-meters wayfinding. However, due to the large diversity of geographic context and acquisition conditions, the captured SVI always contains various distracting objects (e.g., pedestrians and vehicles), which will distract human visual attention from efficiently finding the destination in the last few meters. To address this problem, we highlight the importance of reducing visual distraction in image-based wayfinding by proposing a saliency-guided image inpainting framework. It aims at redirecting human visual attention from distracting objects to destination-related objects for more efficient and accurate wayfinding in the last meters. Specifically, a context-aware distracting object detection method driven by deep salient object detection has been designed to extract distracting objects from three semantic levels in SVI. Then we employ a large-mask inpainting method with fast Fourier convolutions to remove the detected distracting objects. Experimental results with both qualitative and quantitative analysis show that our saliency-guided inpainting method can not only achieve great perceptual quality in street view images but also redirect the human's visual attention to focus more on static location-related objects than distracting ones. The human-based evaluation also justified the effectiveness of our method in improving the efficiency of locating the target destination.",
      "paper_authors": [
        "Chuanbo Hu",
        "Shan Jia",
        "Fan Zhang",
        "Xin Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-05-14",
      "update_time": "2022-11-17",
      "comments": null,
      "repo_url": "https://github.com/cbhu523/saliency_last_way_finding"
    },
    "2205.05245": {
      "paper_id": "2205.05245v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2205.05245v1",
      "paper_key": "2205.05245",
      "paper_title": "Salient Object Detection via Bounding-box Supervision",
      "paper_url": "http://arxiv.org/abs/2205.05245v1",
      "paper_abstract": "The success of fully supervised saliency detection models depends on a large number of pixel-wise labeling. In this paper, we work on bounding-box based weakly-supervised saliency detection to relieve the labeling effort. Given the bounding box annotation, we observe that pixels inside the bounding box may contain extensive labeling noise. However, as a large amount of background is excluded, the foreground bounding box region contains a less complex background, making it possible to perform handcrafted features-based saliency detection with only the cropped foreground region. As the conventional handcrafted features are not representative enough, leading to noisy saliency maps, we further introduce structure-aware self-supervised loss to regularize the structure of the prediction. Further, we claim that pixels outside the bounding box should be background, thus partial cross-entropy loss function can be used to accurately localize the accurate background region. Experimental results on six benchmark RGB saliency datasets illustrate the effectiveness of our model.",
      "paper_authors": [
        "Mengqi He",
        "Jing Zhang",
        "Wenxin Yu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-05-11",
      "update_time": "2022-05-11",
      "comments": "5 pages,4 figures,submitted to ICIP 2022",
      "repo_url": "#"
    },
    "2204.08917": {
      "paper_id": "2204.08917v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2204.08917v1",
      "paper_key": "2204.08917",
      "paper_title": "Global-and-Local Collaborative Learning for Co-Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2204.08917v1",
      "paper_abstract": "The goal of co-salient object detection (CoSOD) is to discover salient objects that commonly appear in a query group containing two or more relevant images. Therefore, how to effectively extract inter-image correspondence is crucial for the CoSOD task. In this paper, we propose a global-and-local collaborative learning architecture, which includes a global correspondence modeling (GCM) and a local correspondence modeling (LCM) to capture comprehensive inter-image corresponding relationship among different images from the global and local perspectives. Firstly, we treat different images as different time slices and use 3D convolution to integrate all intra features intuitively, which can more fully extract the global group semantics. Secondly, we design a pairwise correlation transformation (PCT) to explore similarity correspondence between pairwise images and combine the multiple local pairwise correspondences to generate the local inter-image relationship. Thirdly, the inter-image relationships of the GCM and LCM are integrated through a global-and-local correspondence aggregation (GLA) module to explore more comprehensive inter-image collaboration cues. Finally, the intra- and inter-features are adaptively integrated by an intra-and-inter weighting fusion (AEWF) module to learn co-saliency features and predict the co-saliency map. The proposed GLNet is evaluated on three prevailing CoSOD benchmark datasets, demonstrating that our model trained on a small dataset (about 3k images) still outperforms eleven state-of-the-art competitors trained on some large datasets (about 8k-200k images).",
      "paper_authors": [
        "Runmin Cong",
        "Ning Yang",
        "Chongyi Li",
        "Huazhu Fu",
        "Yao Zhao",
        "Qingming Huang",
        "Sam Kwong"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-04-19",
      "update_time": "2022-04-19",
      "comments": "Accepted by IEEE Transactions on Cybernetics 2022, project page:\n  https://rmcong.github.io/proj_GLNet.html",
      "repo_url": "https://gitee.com/HarveyYeung/GLNet-TCYB2022-MindSpore.git"
    },
    "2204.08803": {
      "paper_id": "2204.08803v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2204.08803v3",
      "paper_key": "2204.08803",
      "paper_title": "An Energy-Based Prior for Generative Saliency",
      "paper_url": "http://arxiv.org/abs/2204.08803v3",
      "paper_abstract": "We propose a novel generative saliency prediction framework that adopts an informative energy-based model as a prior distribution. The energy-based prior model is defined on the latent space of a saliency generator network that generates the saliency map based on a continuous latent variables and an observed image. Both the parameters of saliency generator and the energy-based prior are jointly trained via Markov chain Monte Carlo-based maximum likelihood estimation, in which the sampling from the intractable posterior and prior distributions of the latent variables are performed by Langevin dynamics. With the generative saliency model, we can obtain a pixel-wise uncertainty map from an image, indicating model confidence in the saliency prediction. Different from existing generative models, which define the prior distribution of the latent variables as a simple isotropic Gaussian distribution, our model uses an energy-based informative prior which can be more expressive in capturing the latent space of the data. With the informative energy-based prior, we extend the Gaussian distribution assumption of generative models to achieve a more representative distribution of the latent space, leading to more reliable uncertainty estimation. We apply the proposed frameworks to both RGB and RGB-D salient object detection tasks with both transformer and convolutional neural network backbones. We further propose an adversarial learning algorithm and a variational inference algorithm as alternatives to train the proposed generative framework. Experimental results show that our generative saliency model with an energy-based prior can achieve not only accurate saliency predictions but also reliable uncertainty maps that are consistent with human perception. Results and code are available at \\url{https://github.com/JingZhang617/EBMGSOD}.",
      "paper_authors": [
        "Jing Zhang",
        "Jianwen Xie",
        "Nick Barnes",
        "Ping Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-04-19",
      "update_time": "2023-06-27",
      "comments": "Accepted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence 2023. arXiv admin note: text overlap with arXiv:2112.13528",
      "repo_url": "https://github.com/jingzhang617/ebmgsod"
    },
    "2204.06788": {
      "paper_id": "2204.06788v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2204.06788v1",
      "paper_key": "2204.06788",
      "paper_title": "Pyramidal Attention for Saliency Detection",
      "paper_url": "http://arxiv.org/abs/2204.06788v1",
      "paper_abstract": "Salient object detection (SOD) extracts meaningful contents from an input image. RGB-based SOD methods lack the complementary depth clues; hence, providing limited performance for complex scenarios. Similarly, RGB-D models process RGB and depth inputs, but the depth data availability during testing may hinder the model's practical applicability. This paper exploits only RGB images, estimates depth from RGB, and leverages the intermediate depth features. We employ a pyramidal attention structure to extract multi-level convolutional-transformer features to process initial stage representations and further enhance the subsequent ones. At each stage, the backbone transformer model produces global receptive fields and computing in parallel to attain fine-grained global predictions refined by our residual convolutional attention decoder for optimal saliency prediction. We report significantly improved performance against 21 and 40 state-of-the-art SOD methods on eight RGB and RGB-D datasets, respectively. Consequently, we present a new SOD perspective of generating RGB-D SOD without acquiring depth data during training and testing and assist RGB methods with depth clues for improved performance. The code and trained models are available at https://github.com/tanveer-hussain/EfficientSOD2",
      "paper_authors": [
        "Tanveer Hussain",
        "Abbas Anwar",
        "Saeed Anwar",
        "Lars Petersson",
        "Sung Wook Baik"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-04-14",
      "update_time": "2022-04-14",
      "comments": "Accepted at CVPRW 2022. (2022 IEEE CVPR Workshop on Fair, Data\n  Efficient and Trusted Computer Vision)",
      "repo_url": "https://github.com/tanveer-hussain/efficientsod2"
    },
    "2204.05585": {
      "paper_id": "2204.05585v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2204.05585v1",
      "paper_key": "2204.05585",
      "paper_title": "SwinNet: Swin Transformer drives edge-aware RGB-D and RGB-T salient object detection",
      "paper_url": "http://arxiv.org/abs/2204.05585v1",
      "paper_abstract": "Convolutional neural networks (CNNs) are good at extracting contexture features within certain receptive fields, while transformers can model the global long-range dependency features. By absorbing the advantage of transformer and the merit of CNN, Swin Transformer shows strong feature representation ability. Based on it, we propose a cross-modality fusion model SwinNet for RGB-D and RGB-T salient object detection. It is driven by Swin Transformer to extract the hierarchical features, boosted by attention mechanism to bridge the gap between two modalities, and guided by edge information to sharp the contour of salient object. To be specific, two-stream Swin Transformer encoder first extracts multi-modality features, and then spatial alignment and channel re-calibration module is presented to optimize intra-level cross-modality features. To clarify the fuzzy boundary, edge-guided decoder achieves inter-level cross-modality fusion under the guidance of edge features. The proposed model outperforms the state-of-the-art models on RGB-D and RGB-T datasets, showing that it provides more insight into the cross-modality complementarity task.",
      "paper_authors": [
        "Zhengyi Liu",
        "Yacheng Tan",
        "Qian He",
        "Yun Xiao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-04-12",
      "update_time": "2022-04-12",
      "comments": "Online published in TCSVT",
      "repo_url": "https://github.com/liuzywen/swinnet"
    },
    "2204.05041": {
      "paper_id": "2204.05041v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2204.05041v2",
      "paper_key": "2204.05041",
      "paper_title": "Pyramid Grafting Network for One-Stage High Resolution Saliency Detection",
      "paper_url": "http://arxiv.org/abs/2204.05041v2",
      "paper_abstract": "Recent salient object detection (SOD) methods based on deep neural network have achieved remarkable performance. However, most of existing SOD models designed for low-resolution input perform poorly on high-resolution images due to the contradiction between the sampling depth and the receptive field size. Aiming at resolving this contradiction, we propose a novel one-stage framework called Pyramid Grafting Network (PGNet), using transformer and CNN backbone to extract features from different resolution images independently and then graft the features from transformer branch to CNN branch. An attention-based Cross-Model Grafting Module (CMGM) is proposed to enable CNN branch to combine broken detailed information more holistically, guided by different source feature during decoding process. Moreover, we design an Attention Guided Loss (AGL) to explicitly supervise the attention matrix generated by CMGM to help the network better interact with the attention from different models. We contribute a new Ultra-High-Resolution Saliency Detection dataset UHRSD, containing 5,920 images at 4K-8K resolutions. To our knowledge, it is the largest dataset in both quantity and resolution for high-resolution SOD task, which can be used for training and testing in future research. Sufficient experiments on UHRSD and widely-used SOD datasets demonstrate that our method achieves superior performance compared to the state-of-the-art methods.",
      "paper_authors": [
        "Chenxi Xie",
        "Changqun Xia",
        "Mingcan Ma",
        "Zhirui Zhao",
        "Xiaowu Chen",
        "Jia Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-04-11",
      "update_time": "2022-04-12",
      "comments": "Camera-Ready, CVPR 2022. Code: https://github.com/iCVTEAM/PGNet",
      "repo_url": "https://github.com/icvteam/pgnet"
    },
    "2204.03722": {
      "paper_id": "2204.03722v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2204.03722v1",
      "paper_key": "2204.03722",
      "paper_title": "Automated Design of Salient Object Detection Algorithms with Brain Programming",
      "paper_url": "http://arxiv.org/abs/2204.03722v1",
      "paper_abstract": "Despite recent improvements in computer vision, artificial visual systems' design is still daunting since an explanation of visual computing algorithms remains elusive. Salient object detection is one problem that is still open due to the difficulty of understanding the brain's inner workings. Progress on this research area follows the traditional path of hand-made designs using neuroscience knowledge. In recent years two different approaches based on genetic programming appear to enhance their technique. One follows the idea of combining previous hand-made methods through genetic programming and fuzzy logic. The other approach consists of improving the inner computational structures of basic hand-made models through artificial evolution. This research work proposes expanding the artificial dorsal stream using a recent proposal to solve salient object detection problems. This approach uses the benefits of the two main aspects of this research area: fixation prediction and detection of salient objects. We decided to apply the fusion of visual saliency and image segmentation algorithms as a template. The proposed methodology discovers several critical structures in the template through artificial evolution. We present results on a benchmark designed by experts with outstanding results in comparison with the state-of-the-art.",
      "paper_authors": [
        "Gustavo Olague",
        "Jose Armando Menendez-Clavijo",
        "Matthieu Olague",
        "Arturo Ocampo",
        "Gerardo Ibarra-Vazquez",
        "Rocio Ochoa",
        "Roberto Pineda"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-04-07",
      "update_time": "2022-04-07",
      "comments": "35 pages, 5 figures",
      "repo_url": "#"
    },
    "2204.02008": {
      "paper_id": "2204.02008v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2204.02008v1",
      "paper_key": "2204.02008",
      "paper_title": "Learning Video Salient Object Detection Progressively from Unlabeled Videos",
      "paper_url": "http://arxiv.org/abs/2204.02008v1",
      "paper_abstract": "Recent deep learning-based video salient object detection (VSOD) has achieved some breakthrough, but these methods rely on expensive annotated videos with pixel-wise annotations, weak annotations, or part of the pixel-wise annotations. In this paper, based on the similarities and the differences between VSOD and image salient object detection (SOD), we propose a novel VSOD method via a progressive framework that locates and segments salient objects in sequence without utilizing any video annotation. To use the knowledge learned in the SOD dataset for VSOD efficiently, we introduce dynamic saliency to compensate for the lack of motion information of SOD during the locating process but retain the same fine segmenting process. Specifically, an algorithm for generating spatiotemporal location labels, which consists of generating high-saliency location labels and tracking salient objects in adjacent frames, is proposed. Based on these location labels, a two-stream locating network that introduces an optical flow branch for video salient object locating is presented. Although our method does not require labeled video at all, the experimental results on five public benchmarks of DAVIS, FBMS, ViSal, VOS, and DAVSOD demonstrate that our proposed method is competitive with fully supervised methods and outperforms the state-of-the-art weakly and unsupervised methods.",
      "paper_authors": [
        "Binwei Xu",
        "Haoran Liang",
        "Wentian Ni",
        "Weihua Gong",
        "Ronghua Liang",
        "Peng Chen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-04-05",
      "update_time": "2022-04-05",
      "comments": null,
      "repo_url": "https://github.com/bradleybin/locate-globally-segment-locally-a-progressive-architecture-with-knowledge-review-network-for-sod"
    },
    "2203.13664": {
      "paper_id": "2203.13664v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2203.13664v1",
      "paper_key": "2203.13664",
      "paper_title": "Adjacent Context Coordination Network for Salient Object Detection in Optical Remote Sensing Images",
      "paper_url": "http://arxiv.org/abs/2203.13664v1",
      "paper_abstract": "Salient object detection (SOD) in optical remote sensing images (RSIs), or RSI-SOD, is an emerging topic in understanding optical RSIs. However, due to the difference between optical RSIs and natural scene images (NSIs), directly applying NSI-SOD methods to optical RSIs fails to achieve satisfactory results. In this paper, we propose a novel Adjacent Context Coordination Network (ACCoNet) to explore the coordination of adjacent features in an encoder-decoder architecture for RSI-SOD. Specifically, ACCoNet consists of three parts: an encoder, Adjacent Context Coordination Modules (ACCoMs), and a decoder. As the key component of ACCoNet, ACCoM activates the salient regions of output features of the encoder and transmits them to the decoder. ACCoM contains a local branch and two adjacent branches to coordinate the multi-level features simultaneously. The local branch highlights the salient regions in an adaptive way, while the adjacent branches introduce global information of adjacent levels to enhance salient regions. Additionally, to extend the capabilities of the classic decoder block (i.e., several cascaded convolutional layers), we extend it with two bifurcations and propose a Bifurcation-Aggregation Block to capture the contextual information in the decoder. Extensive experiments on two benchmark datasets demonstrate that the proposed ACCoNet outperforms 22 state-of-the-art methods under nine evaluation metrics, and runs up to 81 fps on a single NVIDIA Titan X GPU. The code and results of our method are available at https://github.com/MathLee/ACCoNet.",
      "paper_authors": [
        "Gongyang Li",
        "Zhi Liu",
        "Dan Zeng",
        "Weisi Lin",
        "Haibin Ling"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-03-25",
      "update_time": "2022-03-25",
      "comments": "13 pages, 7 figures, Accepted by IEEE Transactions on Cybernetics\n  2022",
      "repo_url": "https://github.com/mathlee/acconet"
    },
    "2203.12614": {
      "paper_id": "2203.12614v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2203.12614v1",
      "paper_key": "2203.12614",
      "paper_title": "Unsupervised Salient Object Detection with Spectral Cluster Voting",
      "paper_url": "http://arxiv.org/abs/2203.12614v1",
      "paper_abstract": "In this paper, we tackle the challenging task of unsupervised salient object detection (SOD) by leveraging spectral clustering on self-supervised features. We make the following contributions: (i) We revisit spectral clustering and demonstrate its potential to group the pixels of salient objects; (ii) Given mask proposals from multiple applications of spectral clustering on image features computed from various self-supervised models, e.g., MoCov2, SwAV, DINO, we propose a simple but effective winner-takes-all voting mechanism for selecting the salient masks, leveraging object priors based on framing and distinctiveness; (iii) Using the selected object segmentation as pseudo groundtruth masks, we train a salient object detector, dubbed SelfMask, which outperforms prior approaches on three unsupervised SOD benchmarks. Code is publicly available at https://github.com/NoelShin/selfmask.",
      "paper_authors": [
        "Gyungin Shin",
        "Samuel Albanie",
        "Weidi Xie"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-03-23",
      "update_time": "2022-03-23",
      "comments": "14 pages, 5 figures",
      "repo_url": "https://github.com/noelshin/selfmask"
    },
    "2203.11652": {
      "paper_id": "2203.11652v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2203.11652v2",
      "paper_key": "2203.11652",
      "paper_title": "Weakly-Supervised Salient Object Detection Using Point Supervision",
      "paper_url": "http://arxiv.org/abs/2203.11652v2",
      "paper_abstract": "Current state-of-the-art saliency detection models rely heavily on large datasets of accurate pixel-wise annotations, but manually labeling pixels is time-consuming and labor-intensive. There are some weakly supervised methods developed for alleviating the problem, such as image label, bounding box label, and scribble label, while point label still has not been explored in this field. In this paper, we propose a novel weakly-supervised salient object detection method using point supervision. To infer the saliency map, we first design an adaptive masked flood filling algorithm to generate pseudo labels. Then we develop a transformer-based point-supervised saliency detection model to produce the first round of saliency maps. However, due to the sparseness of the label, the weakly supervised model tends to degenerate into a general foreground detection model. To address this issue, we propose a Non-Salient Suppression (NSS) method to optimize the erroneous saliency maps generated in the first round and leverage them for the second round of training. Moreover, we build a new point-supervised dataset (P-DUTS) by relabeling the DUTS dataset. In P-DUTS, there is only one labeled point for each salient object. Comprehensive experiments on five largest benchmark datasets demonstrate our method outperforms the previous state-of-the-art methods trained with the stronger supervision and even surpass several fully supervised state-of-the-art models. The code is available at: https://github.com/shuyonggao/PSOD.",
      "paper_authors": [
        "Shuyong Gao",
        "Wei Zhang",
        "Yan Wang",
        "Qianyu Guo",
        "Chenglong Zhang",
        "Yangji He",
        "Wenqiang Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-03-22",
      "update_time": "2022-07-12",
      "comments": "accepted by AAAI2022",
      "repo_url": "https://github.com/shuyonggao/psod"
    },
    "2203.10785": {
      "paper_id": "2203.10785v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2203.10785v1",
      "paper_key": "2203.10785",
      "paper_title": "GroupTransNet: Group Transformer Network for RGB-D Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2203.10785v1",
      "paper_abstract": "Salient object detection on RGB-D images is an active topic in computer vision. Although the existing methods have achieved appreciable performance, there are still some challenges. The locality of convolutional neural network requires that the model has a sufficiently deep global receptive field, which always leads to the loss of local details. To address the challenge, we propose a novel Group Transformer Network (GroupTransNet) for RGB-D salient object detection. This method is good at learning the long-range dependencies of cross layer features to promote more perfect feature expression. At the beginning, the features of the slightly higher classes of the middle three levels and the latter three levels are soft grouped to absorb the advantages of the high-level features. The input features are repeatedly purified and enhanced by the attention mechanism to purify the cross modal features of color modal and depth modal. The features of the intermediate process are first fused by the features of different layers, and then processed by several transformers in multiple groups, which not only makes the size of the features of each scale unified and interrelated, but also achieves the effect of sharing the weight of the features within the group. The output features in different groups complete the clustering staggered by two owing to the level difference, and combine with the low-level features. Extensive experiments demonstrate that GroupTransNet outperforms the comparison models and achieves the new state-of-the-art performance.",
      "paper_authors": [
        "Xian Fang",
        "Jinshao Zhu",
        "Xiuli Shao",
        "Hongpeng Wang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-03-21",
      "update_time": "2022-03-21",
      "comments": null,
      "repo_url": "#"
    },
    "2203.10433": {
      "paper_id": "2203.10433v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2203.10433v2",
      "paper_key": "2203.10433",
      "paper_title": "End-to-End Human-Gaze-Target Detection with Transformers",
      "paper_url": "http://arxiv.org/abs/2203.10433v2",
      "paper_abstract": "In this paper, we propose an effective and efficient method for Human-Gaze-Target (HGT) detection, i.e., gaze following. Current approaches decouple the HGT detection task into separate branches of salient object detection and human gaze prediction, employing a two-stage framework where human head locations must first be detected and then be fed into the next gaze target prediction sub-network. In contrast, we redefine the HGT detection task as detecting human head locations and their gaze targets, simultaneously. By this way, our method, named Human-Gaze-Target detection TRansformer or HGTTR, streamlines the HGT detection pipeline by eliminating all other additional components. HGTTR reasons about the relations of salient objects and human gaze from the global image context. Moreover, unlike existing two-stage methods that require human head locations as input and can predict only one human's gaze target at a time, HGTTR can directly predict the locations of all people and their gaze targets at one time in an end-to-end manner. The effectiveness and robustness of our proposed method are verified with extensive experiments on the two standard benchmark datasets, GazeFollowing and VideoAttentionTarget. Without bells and whistles, HGTTR outperforms existing state-of-the-art methods by large margins (6.4 mAP gain on GazeFollowing and 10.3 mAP gain on VideoAttentionTarget) with a much simpler architecture.",
      "paper_authors": [
        "Danyang Tu",
        "Xiongkuo Min",
        "Huiyu Duan",
        "Guodong Guo",
        "Guangtao Zhai",
        "Wei Shen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-03-20",
      "update_time": "2022-03-24",
      "comments": "Accepted to CVPR 2022",
      "repo_url": "#"
    },
    "2203.06429": {
      "paper_id": "2203.06429v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2203.06429v2",
      "paper_key": "2203.06429",
      "paper_title": "DFTR: Depth-supervised Fusion Transformer for Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2203.06429v2",
      "paper_abstract": "Automated salient object detection (SOD) plays an increasingly crucial role in many computer vision applications. By reformulating the depth information as supervision rather than as input, depth-supervised convolutional neural networks (CNN) have achieved promising results on both RGB and RGB-D SOD scenarios with the merits of no requirements for extra depth networks and depth inputs in the inference stage. This paper, for the first time, seeks to expand the applicability of depth supervision to the Transformer architecture. Specifically, we develop a Depth-supervised Fusion TRansformer (DFTR), to further improve the accuracy of both RGB and RGB-D SOD. The proposed DFTR involves three primary features: 1) DFTR, to the best of our knowledge, is the first pure Transformer-based model for depth-supervised SOD; 2) A multi-scale feature aggregation (MFA) module is proposed to fully exploit the multi-scale features encoded by the Swin Transformer in a coarse-to-fine manner; 3) To enable bidirectional information flow across different streams of features, a novel multi-stage feature fusion (MFF) module is further integrated into our DFTR with the emphasis on salient regions at different network learning stages. We extensively evaluate the proposed DFTR on ten benchmarking datasets. Experimental results show that our DFTR consistently outperforms the existing state-of-the-art methods for both RGB and RGB-D SOD tasks. The code and model will be made publicly available.",
      "paper_authors": [
        "Heqin Zhu",
        "Xu Sun",
        "Yuexiang Li",
        "Kai Ma",
        "S. Kevin Zhou",
        "Yefeng Zheng"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-03-12",
      "update_time": "2022-04-11",
      "comments": "15 pages, 5 figures, 4 tables",
      "repo_url": "#"
    },
    "2203.05787": {
      "paper_id": "2203.05787v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2203.05787v1",
      "paper_key": "2203.05787",
      "paper_title": "Democracy Does Matter: Comprehensive Feature Mining for Co-Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2203.05787v1",
      "paper_abstract": "Co-salient object detection, with the target of detecting co-existed salient objects among a group of images, is gaining popularity. Recent works use the attention mechanism or extra information to aggregate common co-salient features, leading to incomplete even incorrect responses for target objects. In this paper, we aim to mine comprehensive co-salient features with democracy and reduce background interference without introducing any extra information. To achieve this, we design a democratic prototype generation module to generate democratic response maps, covering sufficient co-salient regions and thereby involving more shared attributes of co-salient objects. Then a comprehensive prototype based on the response maps can be generated as a guide for final prediction. To suppress the noisy background information in the prototype, we propose a self-contrastive learning module, where both positive and negative pairs are formed without relying on additional classification information. Besides, we also design a democratic feature enhancement module to further strengthen the co-salient features by readjusting attention values. Extensive experiments show that our model obtains better performance than previous state-of-the-art methods, especially on challenging real-world cases (e.g., for CoCA, we obtain a gain of 2.0% for MAE, 5.4% for maximum F-measure, 2.3% for maximum E-measure, and 3.7% for S-measure) under the same settings. Code will be released soon.",
      "paper_authors": [
        "Siyue Yu",
        "Jimin Xiao",
        "Bingfeng Zhang",
        "Eng Gee Lim"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-03-11",
      "update_time": "2022-03-11",
      "comments": "accepted by cvpr2022",
      "repo_url": "https://github.com/siyueyu/dcfm"
    },
    "2203.04895": {
      "paper_id": "2203.04895v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2203.04895v2",
      "paper_key": "2203.04895",
      "paper_title": "Joint Learning of Salient Object Detection, Depth Estimation and Contour Extraction",
      "paper_url": "http://arxiv.org/abs/2203.04895v2",
      "paper_abstract": "Benefiting from color independence, illumination invariance and location discrimination attributed by the depth map, it can provide important supplemental information for extracting salient objects in complex environments. However, high-quality depth sensors are expensive and can not be widely applied. While general depth sensors produce the noisy and sparse depth information, which brings the depth-based networks with irreversible interference. In this paper, we propose a novel multi-task and multi-modal filtered transformer (MMFT) network for RGB-D salient object detection (SOD). Specifically, we unify three complementary tasks: depth estimation, salient object detection and contour estimation. The multi-task mechanism promotes the model to learn the task-aware features from the auxiliary tasks. In this way, the depth information can be completed and purified. Moreover, we introduce a multi-modal filtered transformer (MFT) module, which equips with three modality-specific filters to generate the transformer-enhanced feature for each modality. The proposed model works in a depth-free style during the testing phase. Experiments show that it not only significantly surpasses the depth-based RGB-D SOD methods on multiple datasets, but also precisely predicts a high-quality depth map and salient contour at the same time. And, the resulted depth map can help existing RGB-D SOD methods obtain significant performance gain. The source code will be publicly available at https://github.com/Xiaoqi-Zhao-DLUT/MMFT.",
      "paper_authors": [
        "Xiaoqi Zhao",
        "Youwei Pang",
        "Lihe Zhang",
        "Huchuan Lu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-03-09",
      "update_time": "2022-11-08",
      "comments": "Accepted by IEEE TIP",
      "repo_url": "https://github.com/xiaoqi-zhao-dlut/mmft"
    },
    "2203.04708": {
      "paper_id": "2203.04708v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2203.04708v2",
      "paper_key": "2203.04708",
      "paper_title": "A Unified Transformer Framework for Group-based Segmentation: Co-Segmentation, Co-Saliency Detection and Video Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2203.04708v2",
      "paper_abstract": "Humans tend to mine objects by learning from a group of images or several frames of video since we live in a dynamic world. In the computer vision area, many researches focus on co-segmentation (CoS), co-saliency detection (CoSD) and video salient object detection (VSOD) to discover the co-occurrent objects. However, previous approaches design different networks on these similar tasks separately, and they are difficult to apply to each other, which lowers the upper bound of the transferability of deep learning frameworks. Besides, they fail to take full advantage of the cues among inter- and intra-feature within a group of images. In this paper, we introduce a unified framework to tackle these issues, term as UFO (Unified Framework for Co-Object Segmentation). Specifically, we first introduce a transformer block, which views the image feature as a patch token and then captures their long-range dependencies through the self-attention mechanism. This can help the network to excavate the patch structured similarities among the relevant objects. Furthermore, we propose an intra-MLP learning module to produce self-mask to enhance the network to avoid partial activation. Extensive experiments on four CoS benchmarks (PASCAL, iCoseg, Internet and MSRC), three CoSD benchmarks (Cosal2015, CoSOD3k, and CocA) and four VSOD benchmarks (DAVIS16, FBMS, ViSal and SegV2) show that our method outperforms other state-of-the-arts on three different tasks in both accuracy and speed by using the same network architecture , which can reach 140 FPS in real-time.",
      "paper_authors": [
        "Yukun Su",
        "Jingliang Deng",
        "Ruizhou Sun",
        "Guosheng Lin",
        "Qingyao Wu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-03-09",
      "update_time": "2022-03-11",
      "comments": "Code: https://github.com/suyukun666/UFO",
      "repo_url": "https://github.com/suyukun666/UFO"
    },
    "2203.04478": {
      "paper_id": "2203.04478v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2203.04478v1",
      "paper_key": "2203.04478",
      "paper_title": "3SD: Self-Supervised Saliency Detection With No Labels",
      "paper_url": "http://arxiv.org/abs/2203.04478v1",
      "paper_abstract": "We present a conceptually simple self-supervised method for saliency detection. Our method generates and uses pseudo-ground truth labels for training. The generated pseudo-GT labels don't require any kind of human annotations (e.g., pixel-wise labels or weak labels like scribbles). Recent works show that features extracted from classification tasks provide important saliency cues like structure and semantic information of salient objects in the image. Our method, called 3SD, exploits this idea by adding a branch for a self-supervised classification task in parallel with salient object detection, to obtain class activation maps (CAM maps). These CAM maps along with the edges of the input image are used to generate the pseudo-GT saliency maps to train our 3SD network. Specifically, we propose a contrastive learning-based training on multiple image patches for the classification task. We show the multi-patch classification with contrastive loss improves the quality of the CAM maps compared to naive classification on the entire image. Experiments on six benchmark datasets demonstrate that without any labels, our 3SD method outperforms all existing weakly supervised and unsupervised methods, and its performance is on par with the fully-supervised methods. Code is available at :https://github.com/rajeevyasarla/3SD",
      "paper_authors": [
        "Rajeev Yasarla",
        "Renliang Weng",
        "Wongun Choi",
        "Vishal Patel",
        "Amir Sadeghian"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-03-09",
      "update_time": "2022-03-09",
      "comments": null,
      "repo_url": "https://github.com/rajeevyasarla/3sd"
    },
    "2203.04076": {
      "paper_id": "2203.04076v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2203.04076v1",
      "paper_key": "2203.04076",
      "paper_title": "Semantic Distillation Guided Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2203.04076v1",
      "paper_abstract": "Most existing CNN-based salient object detection methods can identify local segmentation details like hair and animal fur, but often misinterpret the real saliency due to the lack of global contextual information caused by the subjectiveness of the SOD task and the locality of convolution layers. Moreover, due to the unrealistically expensive labeling costs, the current existing SOD datasets are insufficient to cover the real data distribution. The limitation and bias of the training data add additional difficulty to fully exploring the semantic association between object-to-object and object-to-environment in a given image. In this paper, we propose a semantic distillation guided SOD (SDG-SOD) method that produces accurate results by fusing semantically distilled knowledge from generated image captioning into the Vision-Transformer-based SOD framework. SDG-SOD can better uncover inter-objects and object-to-environment saliency and cover the gap between the subjective nature of SOD and its expensive labeling. Comprehensive experiments on five benchmark datasets demonstrate that the SDG-SOD outperforms the state-of-the-art approaches on four evaluation metrics, and largely improves the model performance on DUTS, ECSSD, DUT, HKU-IS, and PASCAL-S datasets.",
      "paper_authors": [
        "Bo Xu",
        "Guanze Liu",
        "Han Huang",
        "Cheng Lu",
        "Yandong Guo"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-03-08",
      "update_time": "2022-03-08",
      "comments": "14 pages, 10 figures",
      "repo_url": "#"
    },
    "2202.13170": {
      "paper_id": "2202.13170v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2202.13170v1",
      "paper_key": "2202.13170",
      "paper_title": "Unsupervised Domain Adaptive Salient Object Detection Through Uncertainty-Aware Pseudo-Label Learning",
      "paper_url": "http://arxiv.org/abs/2202.13170v1",
      "paper_abstract": "Recent advances in deep learning significantly boost the performance of salient object detection (SOD) at the expense of labeling larger-scale per-pixel annotations. To relieve the burden of labor-intensive labeling, deep unsupervised SOD methods have been proposed to exploit noisy labels generated by handcrafted saliency methods. However, it is still difficult to learn accurate saliency details from rough noisy labels. In this paper, we propose to learn saliency from synthetic but clean labels, which naturally has higher pixel-labeling quality without the effort of manual annotations. Specifically, we first construct a novel synthetic SOD dataset by a simple copy-paste strategy. Considering the large appearance differences between the synthetic and real-world scenarios, directly training with synthetic data will lead to performance degradation on real-world scenarios. To mitigate this problem, we propose a novel unsupervised domain adaptive SOD method to adapt between these two domains by uncertainty-aware self-training. Experimental results show that our proposed method outperforms the existing state-of-the-art deep unsupervised SOD methods on several benchmark datasets, and is even comparable to fully-supervised ones.",
      "paper_authors": [
        "Pengxiang Yan",
        "Ziyi Wu",
        "Mengmeng Liu",
        "Kun Zeng",
        "Liang Lin",
        "Guanbin Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-02-26",
      "update_time": "2022-02-26",
      "comments": "Accepted by AAAI2022, code is available at\n  https://github.com/Kinpzz/UDASOD-UPL",
      "repo_url": "https://github.com/kinpzz/udasod-upl"
    },
    "2202.06060": {
      "paper_id": "2202.06060v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2202.06060v2",
      "paper_key": "2202.06060",
      "paper_title": "Depth-Cooperated Trimodal Network for Video Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2202.06060v2",
      "paper_abstract": "Depth can provide useful geographical cues for salient object detection (SOD), and has been proven helpful in recent RGB-D SOD methods. However, existing video salient object detection (VSOD) methods only utilize spatiotemporal information and seldom exploit depth information for detection. In this paper, we propose a depth-cooperated trimodal network, called DCTNet for VSOD, which is a pioneering work to incorporate depth information to assist VSOD. To this end, we first generate depth from RGB frames, and then propose an approach to treat the three modalities unequally. Specifically, a multi-modal attention module (MAM) is designed to model multi-modal long-range dependencies between the main modality (RGB) and the two auxiliary modalities (depth, optical flow). We also introduce a refinement fusion module (RFM) to suppress noises in each modality and select useful information dynamically for further feature refinement. Lastly, a progressive fusion strategy is adopted after the refined features to achieve final cross-modal fusion. Experiments on five benchmark datasets demonstrate the superiority of our depth-cooperated model against 12 state-of-the-art methods, and the necessity of depth is also validated.",
      "paper_authors": [
        "Yukang Lu",
        "Dingyao Min",
        "Keren Fu",
        "Qijun Zhao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-02-12",
      "update_time": "2022-07-11",
      "comments": "5 pages, 3 figures, Accepted at ICIP-2022",
      "repo_url": "https://github.com/luyukang/DCTNet"
    },
    "2202.04112": {
      "paper_id": "2202.04112v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2202.04112v1",
      "paper_key": "2202.04112",
      "paper_title": "Disentangle Saliency Detection into Cascaded Detail Modeling and Body Filling",
      "paper_url": "http://arxiv.org/abs/2202.04112v1",
      "paper_abstract": "Salient object detection has been long studied to identify the most visually attractive objects in images/videos. Recently, a growing amount of approaches have been proposed all of which rely on the contour/edge information to improve detection performance. The edge labels are either put into the loss directly or used as extra supervision. The edge and body can also be learned separately and then fused afterward. Both methods either lead to high prediction errors near the edge or cannot be trained in an end-to-end manner. Another problem is that existing methods may fail to detect objects of various sizes due to the lack of efficient and effective feature fusion mechanisms. In this work, we propose to decompose the saliency detection task into two cascaded sub-tasks, \\emph{i.e.}, detail modeling and body filling. Specifically, the detail modeling focuses on capturing the object edges by supervision of explicitly decomposed detail label that consists of the pixels that are nested on the edge and near the edge. Then the body filling learns the body part which will be filled into the detail map to generate more accurate saliency map. To effectively fuse the features and handle objects at different scales, we have also proposed two novel multi-scale detail attention and body attention blocks for precise detail and body modeling. Experimental results show that our method achieves state-of-the-art performances on six public datasets.",
      "paper_authors": [
        "Yue Song",
        "Hao Tang",
        "Nicu Sebe",
        "Wei Wang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-02-08",
      "update_time": "2022-02-08",
      "comments": "Accepted by TOMM; the first two authors contribute equally to this\n  work",
      "repo_url": "#"
    },
    "2202.03501": {
      "paper_id": "2202.03501v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2202.03501v1",
      "paper_key": "2202.03501",
      "paper_title": "Scribble-based Boundary-aware Network for Weakly Supervised Salient Object Detection in Remote Sensing Images",
      "paper_url": "http://arxiv.org/abs/2202.03501v1",
      "paper_abstract": "Existing CNNs-based salient object detection (SOD) heavily depends on the large-scale pixel-level annotations, which is labor-intensive, time-consuming, and expensive. By contrast, the sparse annotations become appealing to the salient object detection community. However, few efforts are devoted to learning salient object detection from sparse annotations, especially in the remote sensing field. In addition, the sparse annotation usually contains scanty information, which makes it challenging to train a well-performing model, resulting in its performance largely lagging behind the fully-supervised models. Although some SOD methods adopt some prior cues to improve the detection performance, they usually lack targeted discrimination of object boundaries and thus provide saliency maps with poor boundary localization. To this end, in this paper, we propose a novel weakly-supervised salient object detection framework to predict the saliency of remote sensing images from sparse scribble annotations. To implement it, we first construct the scribble-based remote sensing saliency dataset by relabelling an existing large-scale SOD dataset with scribbles, namely S-EOR dataset. After that, we present a novel scribble-based boundary-aware network (SBA-Net) for remote sensing salient object detection. Specifically, we design a boundary-aware module (BAM) to explore the object boundary semantics, which is explicitly supervised by the high-confidence object boundary (pseudo) labels generated by the boundary label generation (BLG) module, forcing the model to learn features that highlight the object structure and thus boosting the boundary localization of objects. Then, the boundary semantics are integrated with high-level features to guide the salient object detection under the supervision of scribble labels.",
      "paper_authors": [
        "Zhou Huang",
        "Tian-Zhu Xiang",
        "Huai-Xin Chen",
        "Hang Dai"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-02-07",
      "update_time": "2022-02-07",
      "comments": "33 pages, 10 figures",
      "repo_url": "https://github.com/zhouhuang23/sba-net"
    },
    "2202.02925": {
      "paper_id": "2202.02925v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2202.02925v1",
      "paper_key": "2202.02925",
      "paper_title": "Benchmarking Deep Models for Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2202.02925v1",
      "paper_abstract": "In recent years, deep network-based methods have continuously refreshed state-of-the-art performance on Salient Object Detection (SOD) task. However, the performance discrepancy caused by different implementation details may conceal the real progress in this task. Making an impartial comparison is required for future researches. To meet this need, we construct a general SALient Object Detection (SALOD) benchmark to conduct a comprehensive comparison among several representative SOD methods. Specifically, we re-implement 14 representative SOD methods by using consistent settings for training. Moreover, two additional protocols are set up in our benchmark to investigate the robustness of existing methods in some limited conditions. In the first protocol, we enlarge the difference between objectness distributions of train and test sets to evaluate the robustness of these SOD methods. In the second protocol, we build multiple train subsets with different scales to validate whether these methods can extract discriminative features from only a few samples. In the above experiments, we find that existing loss functions usually specialized in some metrics but reported inferior results on the others. Therefore, we propose a novel Edge-Aware (EA) loss that promotes deep networks to learn more discriminative features by integrating both pixel- and image-level supervision signals. Experiments prove that our EA loss reports more robust performances compared to existing losses.",
      "paper_authors": [
        "Huajun Zhou",
        "Yang Lin",
        "Lingxiao Yang",
        "Jianhuang Lai",
        "Xiaohua Xie"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-02-07",
      "update_time": "2022-02-07",
      "comments": "24 pages",
      "repo_url": "https://github.com/moothes/salod"
    },
    "2201.09574": {
      "paper_id": "2201.09574v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2201.09574v1",
      "paper_key": "2201.09574",
      "paper_title": "Multi-Scale Iterative Refinement Network for RGB-D Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2201.09574v1",
      "paper_abstract": "The extensive research leveraging RGB-D information has been exploited in salient object detection. However, salient visual cues appear in various scales and resolutions of RGB images due to semantic gaps at different feature levels. Meanwhile, similar salient patterns are available in cross-modal depth images as well as multi-scale versions. Cross-modal fusion and multi-scale refinement are still an open problem in RGB-D salient object detection task. In this paper, we begin by introducing top-down and bottom-up iterative refinement architecture to leverage multi-scale features, and then devise attention based fusion module (ABF) to address on cross-modal correlation. We conduct extensive experiments on seven public datasets. The experimental results show the effectiveness of our devised method",
      "paper_authors": [
        "Ze-yu Liu",
        "Jian-wei Liu",
        "Xin Zuo",
        "Ming-fei Hu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-01-24",
      "update_time": "2022-01-24",
      "comments": "40 pages",
      "repo_url": "#"
    },
    "2201.08049": {
      "paper_id": "2201.08049v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2201.08049v1",
      "paper_key": "2201.08049",
      "paper_title": "Lightweight Salient Object Detection in Optical Remote Sensing Images via Feature Correlation",
      "paper_url": "http://arxiv.org/abs/2201.08049v1",
      "paper_abstract": "Salient object detection in optical remote sensing images (ORSI-SOD) has been widely explored for understanding ORSIs. However, previous methods focus mainly on improving the detection accuracy while neglecting the cost in memory and computation, which may hinder their real-world applications. In this paper, we propose a novel lightweight ORSI-SOD solution, named CorrNet, to address these issues. In CorrNet, we first lighten the backbone (VGG-16) and build a lightweight subnet for feature extraction. Then, following the coarse-to-fine strategy, we generate an initial coarse saliency map from high-level semantic features in a Correlation Module (CorrM). The coarse saliency map serves as the location guidance for low-level features. In CorrM, we mine the object location information between high-level semantic features through the cross-layer correlation operation. Finally, based on low-level detailed features, we refine the coarse saliency map in the refinement subnet equipped with Dense Lightweight Refinement Blocks, and produce the final fine saliency map. By reducing the parameters and computations of each component, CorrNet ends up having only 4.09M parameters and running with 21.09G FLOPs. Experimental results on two public datasets demonstrate that our lightweight CorrNet achieves competitive or even better performance compared with 26 state-of-the-art methods (including 16 large CNN-based methods and 2 lightweight methods), and meanwhile enjoys the clear memory and run time efficiency. The code and results of our method are available at https://github.com/MathLee/CorrNet.",
      "paper_authors": [
        "Gongyang Li",
        "Zhi Liu",
        "Zhen Bai",
        "Weisi Lin",
        "and Haibin Ling"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-01-20",
      "update_time": "2022-01-20",
      "comments": "11 pages, 6 figures, Accepted by IEEE Transactions on Geoscience and\n  Remote Sensing 2022",
      "repo_url": "https://github.com/mathlee/corrnet"
    },
    "2201.00439": {
      "paper_id": "2201.00439v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2201.00439v1",
      "paper_key": "2201.00439",
      "paper_title": "Salient Object Detection by LTP Texture Characterization on Opposing Color Pairs under SLICO Superpixel Constraint",
      "paper_url": "http://arxiv.org/abs/2201.00439v1",
      "paper_abstract": "The effortless detection of salient objects by humans has been the subject of research in several fields, including computer vision as it has many applications. However, salient object detection remains a challenge for many computer models dealing with color and textured images. Herein, we propose a novel and efficient strategy, through a simple model, almost without internal parameters, which generates a robust saliency map for a natural image. This strategy consists of integrating color information into local textural patterns to characterize a color micro-texture. Most models in the literature that use the color and texture features treat them separately. In our case, it is the simple, yet powerful LTP (Local Ternary Patterns) texture descriptor applied to opposing color pairs of a color space that allows us to achieve this end. Each color micro-texture is represented by vector whose components are from a superpixel obtained by SLICO (Simple Linear Iterative Clustering with zero parameter) algorithm which is simple, fast and exhibits state-of-the-art boundary adherence. The degree of dissimilarity between each pair of color micro-texture is computed by the FastMap method, a fast version of MDS (Multi-dimensional Scaling), that considers the color micro-textures non-linearity while preserving their distances. These degrees of dissimilarity give us an intermediate saliency map for each RGB, HSL, LUV and CMY color spaces. The final saliency map is their combination to take advantage of the strength of each of them. The MAE (Mean Absolute Error) and F$_{\\beta}$ measures of our saliency maps, on the complex ECSSD dataset show that our model is both simple and efficient, outperforming several state-of-the-art models.",
      "paper_authors": [
        "Didier Ndayikengurukiye",
        "Max Mignotte"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-01-03",
      "update_time": "2022-01-03",
      "comments": null,
      "repo_url": "#"
    },
    "2201.00100": {
      "paper_id": "2201.00100v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2201.00100v1",
      "paper_key": "2201.00100",
      "paper_title": "Boosting RGB-D Saliency Detection by Leveraging Unlabeled RGB Images",
      "paper_url": "http://arxiv.org/abs/2201.00100v1",
      "paper_abstract": "Training deep models for RGB-D salient object detection (SOD) often requires a large number of labeled RGB-D images. However, RGB-D data is not easily acquired, which limits the development of RGB-D SOD techniques. To alleviate this issue, we present a Dual-Semi RGB-D Salient Object Detection Network (DS-Net) to leverage unlabeled RGB images for boosting RGB-D saliency detection. We first devise a depth decoupling convolutional neural network (DDCNN), which contains a depth estimation branch and a saliency detection branch. The depth estimation branch is trained with RGB-D images and then used to estimate the pseudo depth maps for all unlabeled RGB images to form the paired data. The saliency detection branch is used to fuse the RGB feature and depth feature to predict the RGB-D saliency. Then, the whole DDCNN is assigned as the backbone in a teacher-student framework for semi-supervised learning. Moreover, we also introduce a consistency loss on the intermediate attention and saliency maps for the unlabeled data, as well as a supervised depth and saliency loss for labeled data. Experimental results on seven widely-used benchmark datasets demonstrate that our DDCNN outperforms state-of-the-art methods both quantitatively and qualitatively. We also demonstrate that our semi-supervised DS-Net can further improve the performance, even when using an RGB image with the pseudo depth map.",
      "paper_authors": [
        "Xiaoqiang Wang",
        "Lei Zhu",
        "Siliang Tang",
        "Huazhu Fu",
        "Ping Li",
        "Fei Wu",
        "Yi Yang",
        "Yueting Zhuang"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2022-01-01",
      "update_time": "2022-01-01",
      "comments": "Accepted by IEEE TIP",
      "repo_url": "https://github.com/robert-xiaoqiang/ds-net"
    },
    "2112.14019": {
      "paper_id": "2112.14019v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2112.14019v2",
      "paper_key": "2112.14019",
      "paper_title": "Semi-supervised Salient Object Detection with Effective Confidence Estimation",
      "paper_url": "http://arxiv.org/abs/2112.14019v2",
      "paper_abstract": "The success of existing salient object detection models relies on a large pixel-wise labeled training dataset, which is time-consuming and expensive to obtain. We study semi-supervised salient object detection, with access to a small number of labeled samples and a large number of unlabeled samples. Specifically, we present a pseudo label based learn-ing framework with a Conditional Energy-based Model. We model the stochastic nature of human saliency labels using the stochastic latent variable of the Conditional Energy-based Model. It further enables generation of a high-quality pixel-wise uncertainty map, highlighting the reliability of corresponding pseudo label generated for the unlabeled sample. This minimises the contribution of low-certainty pseudo labels in optimising the model, preventing the error propagation. Experimental results show that the proposed strategy can effectively explore the contribution of unlabeled data. With only 1/16 labeled samples, our model achieves competitive performance compared with state-of-the-art fully-supervised models.",
      "paper_authors": [
        "Jiawei Liu",
        "Jing Zhang",
        "Nick Barnes"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-12-28",
      "update_time": "2023-11-26",
      "comments": null,
      "repo_url": "#"
    },
    "2112.13528": {
      "paper_id": "2112.13528v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2112.13528v1",
      "paper_key": "2112.13528",
      "paper_title": "Learning Generative Vision Transformer with Energy-Based Latent Space for Saliency Prediction",
      "paper_url": "http://arxiv.org/abs/2112.13528v1",
      "paper_abstract": "Vision transformer networks have shown superiority in many computer vision tasks. In this paper, we take a step further by proposing a novel generative vision transformer with latent variables following an informative energy-based prior for salient object detection. Both the vision transformer network and the energy-based prior model are jointly trained via Markov chain Monte Carlo-based maximum likelihood estimation, in which the sampling from the intractable posterior and prior distributions of the latent variables are performed by Langevin dynamics. Further, with the generative vision transformer, we can easily obtain a pixel-wise uncertainty map from an image, which indicates the model confidence in predicting saliency from the image. Different from the existing generative models which define the prior distribution of the latent variables as a simple isotropic Gaussian distribution, our model uses an energy-based informative prior which can be more expressive to capture the latent space of the data. We apply the proposed framework to both RGB and RGB-D salient object detection tasks. Extensive experimental results show that our framework can achieve not only accurate saliency predictions but also meaningful uncertainty maps that are consistent with the human perception.",
      "paper_authors": [
        "Jing Zhang",
        "Jianwen Xie",
        "Nick Barnes",
        "Ping Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-12-27",
      "update_time": "2021-12-27",
      "comments": "NeurIPS 2021",
      "repo_url": "#"
    },
    "2112.10481": {
      "paper_id": "2112.10481v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2112.10481v1",
      "paper_key": "2112.10481",
      "paper_title": "a novel attention-based network for fast salient object detection",
      "paper_url": "http://arxiv.org/abs/2112.10481v1",
      "paper_abstract": "In the current salient object detection network, the most popular method is using U-shape structure. However, the massive number of parameters leads to more consumption of computing and storage resources which are not feasible to deploy on the limited memory device. Some others shallow layer network will not maintain the same accuracy compared with U-shape structure and the deep network structure with more parameters will not converge to a global minimum loss with great speed. To overcome all of these disadvantages, we proposed a new deep convolution network architecture with three contributions: (1) using smaller convolution neural networks (CNNs) to compress the model in our improved salient object features compression and reinforcement extraction module (ISFCREM) to reduce parameters of the model. (2) introducing channel attention mechanism in ISFCREM to weigh different channels for improving the ability of feature representation. (3) applying a new optimizer to accumulate the long-term gradient information during training to adaptively tune the learning rate. The results demonstrate that the proposed method can compress the model to 1/3 of the original size nearly without losing the accuracy and converging faster and more smoothly on six widely used datasets of salient object detection compared with the others models. Our code is published in https://gitee.com/binzhangbinzhangbin/code-a-novel-attention-based-network-for-fast-salient-object-detection.git",
      "paper_authors": [
        "Bin Zhang",
        "Yang Wu",
        "Xiaojing Zhang",
        "Ming Ma"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-12-20",
      "update_time": "2021-12-20",
      "comments": null,
      "repo_url": "#"
    },
    "2112.07380": {
      "paper_id": "2112.07380v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2112.07380v2",
      "paper_key": "2112.07380",
      "paper_title": "TRACER: Extreme Attention Guided Salient Object Tracing Network",
      "paper_url": "http://arxiv.org/abs/2112.07380v2",
      "paper_abstract": "Existing studies on salient object detection (SOD) focus on extracting distinct objects with edge information and aggregating multi-level features to improve SOD performance. To achieve satisfactory performance, the methods employ refined edge information and low multi-level discrepancy. However, both performance gain and computational efficiency cannot be attained, which has motivated us to study the inefficiencies in existing encoder-decoder structures to avoid this trade-off. We propose TRACER, which detects salient objects with explicit edges by incorporating attention guided tracing modules. We employ a masked edge attention module at the end of the first encoder using a fast Fourier transform to propagate the refined edge information to the downstream feature extraction. In the multi-level aggregation phase, the union attention module identifies the complementary channel and important spatial information. To improve the decoder performance and computational efficiency, we minimize the decoder block usage with object attention module. This module extracts undetected objects and edge information from refined channels and spatial representations. Subsequently, we propose an adaptive pixel intensity loss function to deal with the relatively important pixels unlike conventional loss functions which treat all pixels equally. A comparison with 13 existing methods reveals that TRACER achieves state-of-the-art performance on five benchmark datasets. We have released TRACER at https://github.com/Karel911/TRACER.",
      "paper_authors": [
        "Min Seok Lee",
        "Wooseok Shin",
        "Sung Won Han"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-12-14",
      "update_time": "2022-06-27",
      "comments": "AAAI 2022, SA poster session accepted paper",
      "repo_url": "https://github.com/Karel911/TRACER"
    },
    "2112.03650": {
      "paper_id": "2112.03650v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2112.03650v3",
      "paper_key": "2112.03650",
      "paper_title": "Activation to Saliency: Forming High-Quality Labels for Completely Unsupervised Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2112.03650v3",
      "paper_abstract": "Existing deep learning-based Unsupervised Salient Object Detection (USOD) methods rely on supervised pre-trained deep models. Moreover, they generate pseudo labels based on hand-crafted features, which lack high-level semantic information. In order to overcome these shortcomings, we propose a new two-stage Activation-to-Saliency (A2S) framework that effectively excavates high-quality saliency cues to train a robust saliency detector. It is worth noting that our method does not require any manual annotation, even in the pre-training phase. In the first stage, we transform an unsupervisedly pre-trained network to aggregate multi-level features to a single activation map, where an Adaptive Decision Boundary (ADB) is proposed to assist the training of the transformed network. Moreover, a new loss function is proposed to facilitate the generation of high-quality pseudo labels. In the second stage, a self-rectification learning paradigm strategy is developed to train a saliency detector and refine the pseudo labels online. In addition, we construct a lightweight saliency detector using two Residual Attention Modules (RAMs) to largely reduce the risk of overfitting. Extensive experiments on several SOD benchmarks prove that our framework reports significant performance compared with existing USOD methods. Moreover, training our framework on 3,000 images consumes about 1 hour, which is over 30$\\times$ faster than previous state-of-the-art methods.",
      "paper_authors": [
        "Huajun Zhou",
        "Peijia Chen",
        "Lingxiao Yang",
        "Jianhuang Lai",
        "Xiaohua Xie"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-12-07",
      "update_time": "2021-12-24",
      "comments": "11 pages",
      "repo_url": "https://github.com/moothes/a2s-usod"
    },
    "2112.02363": {
      "paper_id": "2112.02363v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2112.02363v3",
      "paper_key": "2112.02363",
      "paper_title": "CAVER: Cross-Modal View-Mixed Transformer for Bi-Modal Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2112.02363v3",
      "paper_abstract": "Most of the existing bi-modal (RGB-D and RGB-T) salient object detection methods utilize the convolution operation and construct complex interweave fusion structures to achieve cross-modal information integration. The inherent local connectivity of the convolution operation constrains the performance of the convolution-based methods to a ceiling. In this work, we rethink these tasks from the perspective of global information alignment and transformation. Specifically, the proposed \\underline{c}ross-mod\\underline{a}l \\underline{v}iew-mixed transform\\underline{er} (CAVER) cascades several cross-modal integration units to construct a top-down transformer-based information propagation path. CAVER treats the multi-scale and multi-modal feature integration as a sequence-to-sequence context propagation and update process built on a novel view-mixed attention mechanism. Besides, considering the quadratic complexity w.r.t. the number of input tokens, we design a parameter-free patch-wise token re-embedding strategy to simplify operations. Extensive experimental results on RGB-D and RGB-T SOD datasets demonstrate that such a simple two-stream encoder-decoder framework can surpass recent state-of-the-art methods when it is equipped with the proposed components. Code and pretrained models will be available at \\href{https://github.com/lartpang/CAVER}{the link}.",
      "paper_authors": [
        "Youwei Pang",
        "Xiaoqi Zhao",
        "Lihe Zhang",
        "Huchuan Lu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-12-04",
      "update_time": "2023-02-16",
      "comments": "Accepted by TIP-2023. Add more details and update the weight\n  illustration",
      "repo_url": "https://github.com/lartpang/caver"
    },
    "2112.01732": {
      "paper_id": "2112.01732v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2112.01732v1",
      "paper_key": "2112.01732",
      "paper_title": "MFNet: Multi-filter Directive Network for Weakly Supervised Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2112.01732v1",
      "paper_abstract": "Weakly supervised salient object detection (WSOD) targets to train a CNNs-based saliency network using only low-cost annotations. Existing WSOD methods take various techniques to pursue single \"high-quality\" pseudo label from low-cost annotations and then develop their saliency networks. Though these methods have achieved good performance, the generated single label is inevitably affected by adopted refinement algorithms and shows prejudiced characteristics which further influence the saliency networks. In this work, we introduce a new multiple-pseudo-label framework to integrate more comprehensive and accurate saliency cues from multiple labels, avoiding the aforementioned problem. Specifically, we propose a multi-filter directive network (MFNet) including a saliency network as well as multiple directive filters. The directive filter (DF) is designed to extract and filter more accurate saliency cues from the noisy pseudo labels. The multiple accurate cues from multiple DFs are then simultaneously propagated to the saliency network with a multi-guidance loss. Extensive experiments on five datasets over four metrics demonstrate that our method outperforms all the existing congeneric methods. Moreover, it is also worth noting that our framework is flexible enough to apply to existing methods and improve their performance.",
      "paper_authors": [
        "Yongri Piao",
        "Jian Wang",
        "Miao Zhang",
        "Huchuan Lu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-12-03",
      "update_time": "2021-12-03",
      "comments": "accepted by ICCV-2021",
      "repo_url": "https://github.com/oiplab-dut/mfnet"
    },
    "2112.01932": {
      "paper_id": "2112.01932v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2112.01932v1",
      "paper_key": "2112.01932",
      "paper_title": "Multi-Content Complementation Network for Salient Object Detection in Optical Remote Sensing Images",
      "paper_url": "http://arxiv.org/abs/2112.01932v1",
      "paper_abstract": "In the computer vision community, great progresses have been achieved in salient object detection from natural scene images (NSI-SOD); by contrast, salient object detection in optical remote sensing images (RSI-SOD) remains to be a challenging emerging topic. The unique characteristics of optical RSIs, such as scales, illuminations and imaging orientations, bring significant differences between NSI-SOD and RSI-SOD. In this paper, we propose a novel Multi-Content Complementation Network (MCCNet) to explore the complementarity of multiple content for RSI-SOD. Specifically, MCCNet is based on the general encoder-decoder architecture, and contains a novel key component named Multi-Content Complementation Module (MCCM), which bridges the encoder and the decoder. In MCCM, we consider multiple types of features that are critical to RSI-SOD, including foreground features, edge features, background features, and global image-level features, and exploit the content complementarity between them to highlight salient regions over various scales in RSI features through the attention mechanism. Besides, we comprehensively introduce pixel-level, map-level and metric-aware losses in the training phase. Extensive experiments on two popular datasets demonstrate that the proposed MCCNet outperforms 23 state-of-the-art methods, including both NSI-SOD and RSI-SOD methods. The code and results of our method are available at https://github.com/MathLee/MCCNet.",
      "paper_authors": [
        "Gongyang Li",
        "Zhi Liu",
        "Weisi Lin",
        "Haibin Ling"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-12-02",
      "update_time": "2021-12-02",
      "comments": "12 pages, 7 figures, Accepted by IEEE Transactions on Geoscience and\n  Remote Sensing 2021",
      "repo_url": "https://github.com/mathlee/mccnet"
    },
    "2111.11827": {
      "paper_id": "2111.11827v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2111.11827v2",
      "paper_key": "2111.11827",
      "paper_title": "A General Divergence Modeling Strategy for Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2111.11827v2",
      "paper_abstract": "Salient object detection is subjective in nature, which implies that multiple estimations should be related to the same input image. Most existing salient object detection models are deterministic following a point to point estimation learning pipeline, making them incapable of estimating the predictive distribution. Although latent variable model based stochastic prediction networks exist to model the prediction variants, the latent space based on the single clean saliency annotation is less reliable in exploring the subjective nature of saliency, leading to less effective saliency divergence modeling. Given multiple saliency annotations, we introduce a general divergence modeling strategy via random sampling, and apply our strategy to an ensemble based framework and three latent variable model based solutions to explore the subjective nature of saliency. Experimental results prove the superior performance of our general divergence modeling strategy.",
      "paper_authors": [
        "Xinyu Tian",
        "Jing Zhang",
        "Yuchao Dai"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-11-23",
      "update_time": "2022-10-03",
      "comments": "Code is available at: https://npucvr.github.io/Divergence_SOD/",
      "repo_url": "#"
    },
    "2111.03195": {
      "paper_id": "2111.03195v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2111.03195v1",
      "paper_key": "2111.03195",
      "paper_title": "Addressing Multiple Salient Object Detection via Dual-Space Long-Range Dependencies",
      "paper_url": "http://arxiv.org/abs/2111.03195v1",
      "paper_abstract": "Salient object detection plays an important role in many downstream tasks. However, complex real-world scenes with varying scales and numbers of salient objects still pose a challenge. In this paper, we directly address the problem of detecting multiple salient objects across complex scenes. We propose a network architecture incorporating non-local feature information in both the spatial and channel spaces, capturing the long-range dependencies between separate objects. Traditional bottom-up and non-local features are combined with edge features within a feature fusion gate that progressively refines the salient object prediction in the decoder. We show that our approach accurately locates multiple salient regions even in complex scenarios. To demonstrate the efficacy of our approach to the multiple salient objects problem, we curate a new dataset containing only multiple salient objects. Our experiments demonstrate the proposed method presents state-of-the-art results on five widely used datasets without any pre-processing and post-processing. We obtain a further performance improvement against competing techniques on our multi-objects dataset. The dataset and source code are avaliable at: https://github.com/EricDengbowen/DSLRDNet.",
      "paper_authors": [
        "Bowen Deng",
        "Andrew P. French",
        "Michael P. Pound"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-11-04",
      "update_time": "2021-11-04",
      "comments": "10 pages, 9 figures",
      "repo_url": "https://github.com/ericdengbowen/dslrdnet"
    },
    "2111.02368": {
      "paper_id": "2111.02368v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2111.02368v1",
      "paper_key": "2111.02368",
      "paper_title": "Video Salient Object Detection via Contrastive Features and Attention Modules",
      "paper_url": "http://arxiv.org/abs/2111.02368v1",
      "paper_abstract": "Video salient object detection aims to find the most visually distinctive objects in a video. To explore the temporal dependencies, existing methods usually resort to recurrent neural networks or optical flow. However, these approaches require high computational cost, and tend to accumulate inaccuracies over time. In this paper, we propose a network with attention modules to learn contrastive features for video salient object detection without the high computational temporal modeling techniques. We develop a non-local self-attention scheme to capture the global information in the video frame. A co-attention formulation is utilized to combine the low-level and high-level features. We further apply the contrastive learning to improve the feature representations, where foreground region pairs from the same video are pulled together, and foreground-background region pairs are pushed away in the latent space. The intra-frame contrastive loss helps separate the foreground and background features, and the inter-frame contrastive loss improves the temporal consistency. We conduct extensive experiments on several benchmark datasets for video salient object detection and unsupervised video object segmentation, and show that the proposed method requires less computation, and performs favorably against the state-of-the-art approaches.",
      "paper_authors": [
        "Yi-Wen Chen",
        "Xiaojie Jin",
        "Xiaohui Shen",
        "Ming-Hsuan Yang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-11-03",
      "update_time": "2021-11-03",
      "comments": "Accepted in WACV 2022",
      "repo_url": "#"
    },
    "2110.14223": {
      "paper_id": "2110.14223v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2110.14223v2",
      "paper_key": "2110.14223",
      "paper_title": "RRNet: Relational Reasoning Network with Parallel Multi-scale Attention for Salient Object Detection in Optical Remote Sensing Images",
      "paper_url": "http://arxiv.org/abs/2110.14223v2",
      "paper_abstract": "Salient object detection (SOD) for optical remote sensing images (RSIs) aims at locating and extracting visually distinctive objects/regions from the optical RSIs. Despite some saliency models were proposed to solve the intrinsic problem of optical RSIs (such as complex background and scale-variant objects), the accuracy and completeness are still unsatisfactory. To this end, we propose a relational reasoning network with parallel multi-scale attention for SOD in optical RSIs in this paper. The relational reasoning module that integrates the spatial and the channel dimensions is designed to infer the semantic relationship by utilizing high-level encoder features, thereby promoting the generation of more complete detection results. The parallel multi-scale attention module is proposed to effectively restore the detail information and address the scale variation of salient objects by using the low-level features refined by multi-scale attention. Extensive experiments on two datasets demonstrate that our proposed RRNet outperforms the existing state-of-the-art SOD competitors both qualitatively and quantitatively.",
      "paper_authors": [
        "Runmin Cong",
        "Yumo Zhang",
        "Leyuan Fang",
        "Jun Li",
        "Yao Zhao",
        "Sam Kwong"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-10-27",
      "update_time": "2022-02-21",
      "comments": "11 pages, 9 figures, Accepted by IEEE Transactions on Geoscience and\n  Remote Sensing 2021, project: https://rmcong.github.io/proj_RRNet.html",
      "repo_url": "#"
    },
    "2110.11887": {
      "paper_id": "2110.11887v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2110.11887v1",
      "paper_key": "2110.11887",
      "paper_title": "C$^{4}$Net: Contextual Compression and Complementary Combination Network for Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2110.11887v1",
      "paper_abstract": "Deep learning solutions of the salient object detection problem have achieved great results in recent years. The majority of these models are based on encoders and decoders, with a different multi-feature combination. In this paper, we show that feature concatenation works better than other combination methods like multiplication or addition. Also, joint feature learning gives better results, because of the information sharing during their processing. We designed a Complementary Extraction Module (CEM) to extract necessary features with edge preservation. Our proposed Excessiveness Loss (EL) function helps to reduce false-positive predictions and purifies the edges with other weighted loss functions. Our designed Pyramid-Semantic Module (PSM) with Global guiding flow (G) makes the prediction more accurate by providing high-level complementary information to shallower layers. Experimental results show that the proposed model outperforms the state-of-the-art methods on all benchmark datasets under three evaluation metrics.",
      "paper_authors": [
        "Hazarapet Tunanyan"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-10-22",
      "update_time": "2021-10-22",
      "comments": null,
      "repo_url": "#"
    },
    "2110.10869": {
      "paper_id": "2110.10869v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2110.10869v1",
      "paper_key": "2110.10869",
      "paper_title": "LC3Net: Ladder context correlation complementary network for salient object detection",
      "paper_url": "http://arxiv.org/abs/2110.10869v1",
      "paper_abstract": "Currently, existing salient object detection methods based on convolutional neural networks commonly resort to constructing discriminative networks to aggregate high level and low level features. However, contextual information is always not fully and reasonably utilized, which usually causes either the absence of useful features or contamination of redundant features. To address these issues, we propose a novel ladder context correlation complementary network (LC3Net) in this paper, which is equipped with three crucial components. At the beginning, we propose a filterable convolution block (FCB) to assist the automatic collection of information on the diversity of initial features, and it is simple yet practical. Besides, we propose a dense cross module (DCM) to facilitate the intimate aggregation of different levels of features by validly integrating semantic information and detailed information of both adjacent and non-adjacent layers. Furthermore, we propose a bidirectional compression decoder (BCD) to help the progressive shrinkage of multi-scale features from coarse to fine by leveraging multiple pairs of alternating top-down and bottom-up feature interaction flows. Extensive experiments demonstrate the superiority of our method against 16 state-of-the-art methods.",
      "paper_authors": [
        "Xian Fang",
        "Jinchao Zhu",
        "Xiuli Shao",
        "Hongpeng Wang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-10-21",
      "update_time": "2021-10-21",
      "comments": null,
      "repo_url": "#"
    },
    "2110.07859": {
      "paper_id": "2110.07859v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2110.07859v1",
      "paper_key": "2110.07859",
      "paper_title": "Receptive Field Broadening and Boosting for Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2110.07859v1",
      "paper_abstract": "Salient object detection requires a comprehensive and scalable receptive field to locate the visually significant objects in the image. Recently, the emergence of visual transformers and multi-branch modules has significantly enhanced the ability of neural networks to perceive objects at different scales. However, compared to the traditional backbone, the calculation process of transformers is time-consuming. Moreover, different branches of the multi-branch modules could cause the same error back propagation in each training iteration, which is not conducive to extracting discriminative features. To solve these problems, we propose a bilateral network based on transformer and CNN to efficiently broaden local details and global semantic information simultaneously. Besides, a Multi-Head Boosting (MHB) strategy is proposed to enhance the specificity of different network branches. By calculating the errors of different prediction heads, each branch can separately pay more attention to the pixels that other branches predict incorrectly. Moreover, Unlike multi-path parallel training, MHB randomly selects one branch each time for gradient back propagation in a boosting way. Additionally, an Attention Feature Fusion Module (AF) is proposed to fuse two types of features according to respective characteristics. Comprehensive experiments on five benchmark datasets demonstrate that the proposed method can achieve a significant performance improvement compared with the state-of-the-art methods.",
      "paper_authors": [
        "Mingcan Ma",
        "Changqun Xia",
        "Chenxi Xie",
        "Xiaowu Chen",
        "Jia Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-10-15",
      "update_time": "2021-10-15",
      "comments": "9 pages, 5 figures",
      "repo_url": "#"
    },
    "2110.06550": {
      "paper_id": "2110.06550v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2110.06550v1",
      "paper_key": "2110.06550",
      "paper_title": "Saliency Detection via Global Context Enhanced Feature Fusion and Edge Weighted Loss",
      "paper_url": "http://arxiv.org/abs/2110.06550v1",
      "paper_abstract": "UNet-based methods have shown outstanding performance in salient object detection (SOD), but are problematic in two aspects. 1) Indiscriminately integrating the encoder feature, which contains spatial information for multiple objects, and the decoder feature, which contains global information of the salient object, is likely to convey unnecessary details of non-salient objects to the decoder, hindering saliency detection. 2) To deal with ambiguous object boundaries and generate accurate saliency maps, the model needs additional branches, such as edge reconstructions, which leads to increasing computational cost. To address the problems, we propose a context fusion decoder network (CFDN) and near edge weighted loss (NEWLoss) function. The CFDN creates an accurate saliency map by integrating global context information and thus suppressing the influence of the unnecessary spatial information. NEWLoss accelerates learning of obscure boundaries without additional modules by generating weight maps on object boundaries. Our method is evaluated on four benchmarks and achieves state-of-the-art performance. We prove the effectiveness of the proposed method through comparative experiments.",
      "paper_authors": [
        "Chaewon Park",
        "Minhyeok Lee",
        "MyeongAh Cho",
        "Sangyoun Lee"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-10-13",
      "update_time": "2021-10-13",
      "comments": "4 pages, 5 figures",
      "repo_url": "#"
    },
    "2110.04904": {
      "paper_id": "2110.04904v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2110.04904v2",
      "paper_key": "2110.04904",
      "paper_title": "Modality-Guided Subnetwork for Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2110.04904v2",
      "paper_abstract": "Recent RGBD-based models for saliency detection have attracted research attention. The depth clues such as boundary clues, surface normal, shape attribute, etc., contribute to the identification of salient objects with complicated scenarios. However, most RGBD networks require multi-modalities from the input side and feed them separately through a two-stream design, which inevitably results in extra costs on depth sensors and computation. To tackle these inconveniences, we present in this paper a novel fusion design named modality-guided subnetwork (MGSnet). It has the following superior designs: 1) Our model works for both RGB and RGBD data, and dynamically estimating depth if not available. Taking the inner workings of depth-prediction networks into account, we propose to estimate the pseudo-geometry maps from RGB input - essentially mimicking the multi-modality input. 2) Our MGSnet for RGB SOD results in real-time inference but achieves state-of-the-art performance compared to other RGB models. 3) The flexible and lightweight design of MGS facilitates the integration into RGBD two-streaming models. The introduced fusion design enables a cross-modality interaction to enable further progress but with a minimal cost.",
      "paper_authors": [
        "Zongwei Wu",
        "Guillaume Allibert",
        "Christophe Stolz",
        "Chao Ma",
        "C\u00e9dric Demonceaux"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-10-10",
      "update_time": "2021-10-25",
      "comments": "Accepted to 3DV 2021",
      "repo_url": "#"
    },
    "2109.07922": {
      "paper_id": "2109.07922v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2109.07922v1",
      "paper_key": "2109.07922",
      "paper_title": "M2RNet: Multi-modal and Multi-scale Refined Network for RGB-D Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2109.07922v1",
      "paper_abstract": "Salient object detection is a fundamental topic in computer vision. Previous methods based on RGB-D often suffer from the incompatibility of multi-modal feature fusion and the insufficiency of multi-scale feature aggregation. To tackle these two dilemmas, we propose a novel multi-modal and multi-scale refined network (M2RNet). Three essential components are presented in this network. The nested dual attention module (NDAM) explicitly exploits the combined features of RGB and depth flows. The adjacent interactive aggregation module (AIAM) gradually integrates the neighbor features of high, middle and low levels. The joint hybrid optimization loss (JHOL) makes the predictions have a prominent outline. Extensive experiments demonstrate that our method outperforms other state-of-the-art approaches.",
      "paper_authors": [
        "Xian Fang",
        "Jinchao Zhu",
        "Ruixun Zhang",
        "Xiuli Shao",
        "Hongpeng Wang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-09-16",
      "update_time": "2021-09-16",
      "comments": null,
      "repo_url": "#"
    },
    "2109.04627": {
      "paper_id": "2109.04627v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2109.04627v1",
      "paper_key": "2109.04627",
      "paper_title": "ACFNet: Adaptively-Cooperative Fusion Network for RGB-D Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2109.04627v1",
      "paper_abstract": "The reasonable employment of RGB and depth data show great significance in promoting the development of computer vision tasks and robot-environment interaction. However, there are different advantages and disadvantages in the early and late fusion of the two types of data. Besides, due to the diversity of object information, using a single type of data in a specific scenario tends to result in semantic misleading. Based on the above considerations, we propose an adaptively-cooperative fusion network (ACFNet) with ResinRes structure for salient object detection. This structure is designed to flexibly utilize the advantages of feature fusion in early and late stages. Secondly, an adaptively-cooperative semantic guidance (ACG) scheme is designed to suppress inaccurate features in the guidance phase. Further, we proposed a type-based attention module (TAM) to optimize the network and enhance the multi-scale perception of different objects. For different objects, the features generated by different types of convolution are enhanced or suppressed by the gated mechanism for segmentation optimization. ACG and TAM optimize the transfer of feature streams according to their data attributes and convolution attributes, respectively. Sufficient experiments conducted on RGB-D SOD datasets illustrate that the proposed network performs favorably against 18 state-of-the-art algorithms.",
      "paper_authors": [
        "Jinchao Zhu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-09-10",
      "update_time": "2021-09-10",
      "comments": null,
      "repo_url": "#"
    },
    "2109.03425": {
      "paper_id": "2109.03425v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2109.03425v1",
      "paper_key": "2109.03425",
      "paper_title": "RGB-D Salient Object Detection with Ubiquitous Target Awareness",
      "paper_url": "http://arxiv.org/abs/2109.03425v1",
      "paper_abstract": "Conventional RGB-D salient object detection methods aim to leverage depth as complementary information to find the salient regions in both modalities. However, the salient object detection results heavily rely on the quality of captured depth data which sometimes are unavailable. In this work, we make the first attempt to solve the RGB-D salient object detection problem with a novel depth-awareness framework. This framework only relies on RGB data in the testing phase, utilizing captured depth data as supervision for representation learning. To construct our framework as well as achieving accurate salient detection results, we propose a Ubiquitous Target Awareness (UTA) network to solve three important challenges in RGB-D SOD task: 1) a depth awareness module to excavate depth information and to mine ambiguous regions via adaptive depth-error weights, 2) a spatial-aware cross-modal interaction and a channel-aware cross-level interaction, exploiting the low-level boundary cues and amplifying high-level salient channels, and 3) a gated multi-scale predictor module to perceive the object saliency in different contextual scales. Besides its high performance, our proposed UTA network is depth-free for inference and runs in real-time with 43 FPS. Experimental evidence demonstrates that our proposed network not only surpasses the state-of-the-art methods on five public RGB-D SOD benchmarks by a large margin, but also verifies its extensibility on five public RGB SOD benchmarks.",
      "paper_authors": [
        "Yifan Zhao",
        "Jiawei Zhao",
        "Jia Li",
        "Xiaowu Chen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-09-08",
      "update_time": "2021-09-08",
      "comments": "15 pages, 13 figures, Accepted by IEEE Transactions on Image\n  Processing (2021). arXiv admin note: text overlap with arXiv:2006.00269",
      "repo_url": "#"
    },
    "2109.01770": {
      "paper_id": "2109.01770v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2109.01770v1",
      "paper_key": "2109.01770",
      "paper_title": "To be Critical: Self-Calibrated Weakly Supervised Learning for Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2109.01770v1",
      "paper_abstract": "Weakly-supervised salient object detection (WSOD) aims to develop saliency models using image-level annotations. Despite of the success of previous works, explorations on an effective training strategy for the saliency network and accurate matches between image-level annotations and salient objects are still inadequate. In this work, 1) we propose a self-calibrated training strategy by explicitly establishing a mutual calibration loop between pseudo labels and network predictions, liberating the saliency network from error-prone propagation caused by pseudo labels. 2) we prove that even a much smaller dataset (merely 1.8% of ImageNet) with well-matched annotations can facilitate models to achieve better performance as well as generalizability. This sheds new light on the development of WSOD and encourages more contributions to the community. Comprehensive experiments demonstrate that our method outperforms all the existing WSOD methods by adopting the self-calibrated strategy only. Steady improvements are further achieved by training on the proposed dataset. Additionally, our method achieves 94.7% of the performance of fully-supervised methods on average. And what is more, the fully supervised models adopting our predicted results as \"ground truths\" achieve successful results (95.6% for BASNet and 97.3% for ITSD on F-measure), while costing only 0.32% of labeling time for pixel-level annotation.",
      "paper_authors": [
        "Yongri Piao",
        "Jian Wang",
        "Miao Zhang",
        "Zhengxuan Ma",
        "Huchuan Lu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-09-04",
      "update_time": "2021-09-04",
      "comments": "In the manuscript",
      "repo_url": "#"
    },
    "2108.09408": {
      "paper_id": "2108.09408v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2108.09408v1",
      "paper_key": "2108.09408",
      "paper_title": "Multi-scale Edge-based U-shape Network for Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2108.09408v1",
      "paper_abstract": "Deep-learning based salient object detection methods achieve great improvements. However, there are still problems existing in the predictions, such as blurry boundary and inaccurate location, which is mainly caused by inadequate feature extraction and integration. In this paper, we propose a Multi-scale Edge-based U-shape Network (MEUN) to integrate various features at different scales to achieve better performance. To extract more useful information for boundary prediction, U-shape Edge Network modules are embedded in each decoder units. Besides, the additional down-sampling module alleviates the location inaccuracy. Experimental results on four benchmark datasets demonstrate the validity and reliability of the proposed method. Multi-scale Edge based U-shape Network also shows its superiority when compared with 15 state-of-the-art salient object detection methods.",
      "paper_authors": [
        "Han Sun",
        "Yetong Bian",
        "Ningzhong Liu",
        "Huiyu Zhou"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-08-21",
      "update_time": "2021-08-21",
      "comments": "14pages, 5 figures. accepted by PRICAI 2021, code:\n  https://github.com/bellatong/MEUNet",
      "repo_url": "https://github.com/bellatong/meunet"
    },
    "2108.08162": {
      "paper_id": "2108.08162v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2108.08162v2",
      "paper_key": "2108.08162",
      "paper_title": "Specificity-preserving RGB-D Saliency Detection",
      "paper_url": "http://arxiv.org/abs/2108.08162v2",
      "paper_abstract": "Salient object detection (SOD) on RGB and depth images has attracted more and more research interests, due to its effectiveness and the fact that depth cues can now be conveniently captured. Existing RGB-D SOD models usually adopt different fusion strategies to learn a shared representation from the two modalities (\\ie, RGB and depth), while few methods explicitly consider how to preserve modality-specific characteristics. In this study, we propose a novel framework, termed SPNet} (Specificity-preserving network), which benefits SOD performance by exploring both the shared information and modality-specific properties (\\eg, specificity). Specifically, we propose to adopt two modality-specific networks and a shared learning network to generate individual and shared saliency prediction maps, respectively. To effectively fuse cross-modal features in the shared learning network, we propose a cross-enhanced integration module (CIM) and then propagate the fused feature to the next layer for integrating cross-level information. Moreover, to capture rich complementary multi-modal information for boosting the SOD performance, we propose a multi-modal feature aggregation (MFA) module to integrate the modality-specific features from each individual decoder into the shared decoder. By using a skip connection, the hierarchical features between the encoder and decoder layers can be fully combined. Extensive experiments demonstrate that our~\\ours~outperforms cutting-edge approaches on six popular RGB-D SOD and three camouflaged object detection benchmarks. The project is publicly available at: https://github.com/taozh2017/SPNet.",
      "paper_authors": [
        "Tao Zhou",
        "Deng-Ping Fan",
        "Geng Chen",
        "Yi Zhou",
        "Huazhu Fu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-08-18",
      "update_time": "2022-01-09",
      "comments": "This is an extensive version and has been accepted by Computational\n  Visual Media",
      "repo_url": "https://github.com/taozh2017/RGBD-SODsurvey"
    },
    "2108.07851": {
      "paper_id": "2108.07851v6",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2108.07851v6",
      "paper_key": "2108.07851",
      "paper_title": "Boosting Salient Object Detection with Transformer-based Asymmetric Bilateral U-Net",
      "paper_url": "http://arxiv.org/abs/2108.07851v6",
      "paper_abstract": "Existing salient object detection (SOD) methods mainly rely on U-shaped convolution neural networks (CNNs) with skip connections to combine the global contexts and local spatial details that are crucial for locating salient objects and refining object details, respectively. Despite great successes, the ability of CNNs in learning global contexts is limited. Recently, the vision transformer has achieved revolutionary progress in computer vision owing to its powerful modeling of global dependencies. However, directly applying the transformer to SOD is suboptimal because the transformer lacks the ability to learn local spatial representations. To this end, this paper explores the combination of transformers and CNNs to learn both global and local representations for SOD. We propose a transformer-based Asymmetric Bilateral U-Net (ABiU-Net). The asymmetric bilateral encoder has a transformer path and a lightweight CNN path, where the two paths communicate at each encoder stage to learn complementary global contexts and local spatial details, respectively. The asymmetric bilateral decoder also consists of two paths to process features from the transformer and CNN encoder paths, with communication at each decoder stage for decoding coarse salient object locations and fine-grained object details, respectively. Such communication between the two encoder/decoder paths enables AbiU-Net to learn complementary global and local representations, taking advantage of the natural merits of transformers and CNNs, respectively. Hence, ABiU-Net provides a new perspective for transformer-based SOD. Extensive experiments demonstrate that ABiU-Net performs favorably against previous state-of-the-art SOD methods. The code is available at https://github.com/yuqiuyuqiu/ABiU-Net.",
      "paper_authors": [
        "Yu Qiu",
        "Yun Liu",
        "Le Zhang",
        "Jing Xu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-08-17",
      "update_time": "2023-08-21",
      "comments": "Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT)",
      "repo_url": "https://github.com/yuqiuyuqiu/abiu-net"
    },
    "2108.06281": {
      "paper_id": "2108.06281v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2108.06281v2",
      "paper_key": "2108.06281",
      "paper_title": "Modal-Adaptive Gated Recoding Network for RGB-D Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2108.06281v2",
      "paper_abstract": "The multi-modal salient object detection model based on RGB-D information has better robustness in the real world. However, it remains nontrivial to better adaptively balance effective multi-modal information in the feature fusion phase. In this letter, we propose a novel gated recoding network (GRNet) to evaluate the information validity of the two modes, and balance their influence. Our framework is divided into three phases: perception phase, recoding mixing phase and feature integration phase. First, A perception encoder is adopted to extract multi-level single-modal features, which lays the foundation for multi-modal semantic comparative analysis. Then, a modal-adaptive gate unit (MGU) is proposed to suppress the invalid information and transfer the effective modal features to the recoding mixer and the hybrid branch decoder. The recoding mixer is responsible for recoding and mixing the balanced multi-modal information. Finally, the hybrid branch decoder completes the multi-level feature integration under the guidance of an optional edge guidance stream (OEGS). Experiments and analysis on eight popular benchmarks verify that our framework performs favorably against 9 state-of-art methods.",
      "paper_authors": [
        "Jinchao Zhu",
        "Xiaoyu Zhang",
        "Xian Fang",
        "Feng Dong",
        "Qiu Yu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-08-13",
      "update_time": "2021-11-09",
      "comments": null,
      "repo_url": "#"
    },
    "2108.03990": {
      "paper_id": "2108.03990v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2108.03990v1",
      "paper_key": "2108.03990",
      "paper_title": "TriTransNet: RGB-D Salient Object Detection with a Triplet Transformer Embedding Network",
      "paper_url": "http://arxiv.org/abs/2108.03990v1",
      "paper_abstract": "Salient object detection is the pixel-level dense prediction task which can highlight the prominent object in the scene. Recently U-Net framework is widely used, and continuous convolution and pooling operations generate multi-level features which are complementary with each other. In view of the more contribution of high-level features for the performance, we propose a triplet transformer embedding module to enhance them by learning long-range dependencies across layers. It is the first to use three transformer encoders with shared weights to enhance multi-level features. By further designing scale adjustment module to process the input, devising three-stream decoder to process the output and attaching depth features to color features for the multi-modal fusion, the proposed triplet transformer embedding network (TriTransNet) achieves the state-of-the-art performance in RGB-D salient object detection, and pushes the performance to a new level. Experimental results demonstrate the effectiveness of the proposed modules and the competition of TriTransNet.",
      "paper_authors": [
        "Zhengyi Liu",
        "Yuan Wang",
        "Zhengzheng Tu",
        "Yun Xiao",
        "Bin Tang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-08-09",
      "update_time": "2021-08-09",
      "comments": null,
      "repo_url": "https://github.com/liuzywen/tritransnet"
    },
    "2108.03618": {
      "paper_id": "2108.03618v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2108.03618v1",
      "paper_key": "2108.03618",
      "paper_title": "MPI: Multi-receptive and Parallel Integration for Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2108.03618v1",
      "paper_abstract": "The semantic representation of deep features is essential for image context understanding, and effective fusion of features with different semantic representations can significantly improve the model's performance on salient object detection. In this paper, a novel method called MPI is proposed for salient object detection. Firstly, a multi-receptive enhancement module (MRE) is designed to effectively expand the receptive fields of features from different layers and generate features with different receptive fields. MRE can enhance the semantic representation and improve the model's perception of the image context, which enables the model to locate the salient object accurately. Secondly, in order to reduce the reuse of redundant information in the complex top-down fusion method and weaken the differences between semantic features, a relatively simple but effective parallel fusion strategy (PFS) is proposed. It allows multi-scale features to better interact with each other, thus improving the overall performance of the model. Experimental results on multiple datasets demonstrate that the proposed method outperforms state-of-the-art methods under different evaluation metrics.",
      "paper_authors": [
        "Han Sun",
        "Jun Cen",
        "Ningzhong Liu",
        "Dong Liang",
        "Huiyu Zhou"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-08-08",
      "update_time": "2021-08-08",
      "comments": "10 pages, 10 figures, accepted by IET Image Processing, code:\n  https://github.com/NuaaCJ/MPI",
      "repo_url": "https://github.com/nuaacj/mpi"
    },
    "2108.03551": {
      "paper_id": "2108.03551v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2108.03551v2",
      "paper_key": "2108.03551",
      "paper_title": "Disentangled High Quality Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2108.03551v2",
      "paper_abstract": "Aiming at discovering and locating most distinctive objects from visual scenes, salient object detection (SOD) plays an essential role in various computer vision systems. Coming to the era of high resolution, SOD methods are facing new challenges. The major limitation of previous methods is that they try to identify the salient regions and estimate the accurate objects boundaries simultaneously with a single regression task at low-resolution. This practice ignores the inherent difference between the two difficult problems, resulting in poor detection quality. In this paper, we propose a novel deep learning framework for high-resolution SOD task, which disentangles the task into a low-resolution saliency classification network (LRSCN) and a high-resolution refinement network (HRRN). As a pixel-wise classification task, LRSCN is designed to capture sufficient semantics at low-resolution to identify the definite salient, background and uncertain image regions. HRRN is a regression task, which aims at accurately refining the saliency value of pixels in the uncertain region to preserve a clear object boundary at high-resolution with limited GPU memory. It is worth noting that by introducing uncertainty into the training process, our HRRN can well address the high-resolution refinement task without using any high-resolution training data. Extensive experiments on high-resolution saliency datasets as well as some widely used saliency benchmarks show that the proposed method achieves superior performance compared to the state-of-the-art methods.",
      "paper_authors": [
        "Lv Tang",
        "Bo Li",
        "Shouhong Ding",
        "Mofei Song"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-08-08",
      "update_time": "2021-09-02",
      "comments": "Full version of ICCV2021",
      "repo_url": "https://github.com/luckybird1994/hqsod"
    },
    "2108.03151": {
      "paper_id": "2108.03151v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2108.03151v3",
      "paper_key": "2108.03151",
      "paper_title": "Full-Duplex Strategy for Video Object Segmentation",
      "paper_url": "http://arxiv.org/abs/2108.03151v3",
      "paper_abstract": "Previous video object segmentation approaches mainly focus on using simplex solutions between appearance and motion, limiting feature collaboration efficiency among and across these two cues. In this work, we study a novel and efficient full-duplex strategy network (FSNet) to address this issue, by considering a better mutual restraint scheme between motion and appearance in exploiting the cross-modal features from the fusion and decoding stage. Specifically, we introduce the relational cross-attention module (RCAM) to achieve bidirectional message propagation across embedding sub-spaces. To improve the model's robustness and update the inconsistent features from the spatial-temporal embeddings, we adopt the bidirectional purification module (BPM) after the RCAM. Extensive experiments on five popular benchmarks show that our FSNet is robust to various challenging scenarios (e.g., motion blur, occlusion) and achieves favourable performance against existing cutting-edges both in the video object segmentation and video salient object detection tasks. The project is publicly available at: https://dpfan.net/FSNet.",
      "paper_authors": [
        "Ge-Peng Ji",
        "Deng-Ping Fan",
        "Keren Fu",
        "Zhe Wu",
        "Jianbing Shen",
        "Ling Shao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-08-06",
      "update_time": "2021-09-03",
      "comments": "Accepted at ICCV-2021 (Journal Submission). Project Page:\n  http://dpfan.net/FSNet/",
      "repo_url": "https://github.com/GewelsJI/FSNet"
    },
    "2409.02368": {
      "paper_id": "2409.02368v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.02368v1",
      "paper_key": "2409.02368",
      "paper_title": "Pluralistic Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2409.02368v1",
      "paper_abstract": "We introduce pluralistic salient object detection (PSOD), a novel task aimed at generating multiple plausible salient segmentation results for a given input image. Unlike conventional SOD methods that produce a single segmentation mask for salient objects, this new setting recognizes the inherent complexity of real-world images, comprising multiple objects, and the ambiguity in defining salient objects due to different user intentions. To study this task, we present two new SOD datasets \"DUTS-MM\" and \"DUS-MQ\", along with newly designed evaluation metrics. DUTS-MM builds upon the DUTS dataset but enriches the ground-truth mask annotations from three aspects which 1) improves the mask quality especially for boundary and fine-grained structures; 2) alleviates the annotation inconsistency issue; and 3) provides multiple ground-truth masks for images with saliency ambiguity. DUTS-MQ consists of approximately 100K image-mask pairs with human-annotated preference scores, enabling the learning of real human preferences in measuring mask quality. Building upon these two datasets, we propose a simple yet effective pluralistic SOD baseline based on a Mixture-of-Experts (MOE) design. Equipped with two prediction heads, it simultaneously predicts multiple masks using different query prompts and predicts human preference scores for each mask candidate. Extensive experiments and analyses underscore the significance of our proposed datasets and affirm the effectiveness of our PSOD framework.",
      "paper_authors": [
        "Xuelu Feng",
        "Yunsheng Li",
        "Dongdong Chen",
        "Chunming Qiao",
        "Junsong Yuan",
        "Lu Yuan",
        "Gang Hua"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-04",
      "update_time": "2024-09-04",
      "comments": null,
      "repo_url": "#"
    },
    "2409.01021": {
      "paper_id": "2409.01021v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.01021v2",
      "paper_key": "2409.01021",
      "paper_title": "CONDA: Condensed Deep Association Learning for Co-Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2409.01021v2",
      "paper_abstract": "Inter-image association modeling is crucial for co-salient object detection. Despite satisfactory performance, previous methods still have limitations on sufficient inter-image association modeling. Because most of them focus on image feature optimization under the guidance of heuristically calculated raw inter-image associations. They directly rely on raw associations which are not reliable in complex scenarios, and their image feature optimization approach is not explicit for inter-image association modeling. To alleviate these limitations, this paper proposes a deep association learning strategy that deploys deep networks on raw associations to explicitly transform them into deep association features. Specifically, we first create hyperassociations to collect dense pixel-pair-wise raw associations and then deploys deep aggregation networks on them. We design a progressive association generation module for this purpose with additional enhancement of the hyperassociation calculation. More importantly, we propose a correspondence-induced association condensation module that introduces a pretext task, i.e. semantic correspondence estimation, to condense the hyperassociations for computational burden reduction and noise elimination. We also design an object-aware cycle consistency loss for high-quality correspondence estimations. Experimental results in three benchmark datasets demonstrate the remarkable effectiveness of our proposed method with various training settings.",
      "paper_authors": [
        "Long Li",
        "Nian Liu",
        "Dingwen Zhang",
        "Zhongyu Li",
        "Salman Khan",
        "Rao Anwer",
        "Hisham Cholakkal",
        "Junwei Han",
        "Fahad Shahbaz Khan"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-02",
      "update_time": "2024-09-04",
      "comments": "There is an error. In Sec 4.1, the number of images in some dataset\n  is incorrect and needs to be revised",
      "repo_url": "#"
    },
    "2409.04817": {
      "paper_id": "2409.04817v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.04817v1",
      "paper_key": "2409.04817",
      "paper_title": "SSFam: Scribble Supervised Salient Object Detection Family",
      "paper_url": "http://arxiv.org/abs/2409.04817v1",
      "paper_abstract": "Scribble supervised salient object detection (SSSOD) constructs segmentation ability of attractive objects from surroundings under the supervision of sparse scribble labels. For the better segmentation, depth and thermal infrared modalities serve as the supplement to RGB images in the complex scenes. Existing methods specifically design various feature extraction and multi-modal fusion strategies for RGB, RGB-Depth, RGB-Thermal, and Visual-Depth-Thermal image input respectively, leading to similar model flood. As the recently proposed Segment Anything Model (SAM) possesses extraordinary segmentation and prompt interactive capability, we propose an SSSOD family based on SAM, named SSFam, for the combination input with different modalities. Firstly, different modal-aware modulators are designed to attain modal-specific knowledge which cooperates with modal-agnostic information extracted from the frozen SAM encoder for the better feature ensemble. Secondly, a siamese decoder is tailored to bridge the gap between the training with scribble prompt and the testing with no prompt for the stronger decoding ability. Our model demonstrates the remarkable performance among combinations of different modalities and refreshes the highest level of scribble supervised methods and comes close to the ones of fully supervised methods. https://github.com/liuzywen/SSFam",
      "paper_authors": [
        "Zhengyi Liu",
        "Sheng Deng",
        "Xinrui Wang",
        "Linbo Wang",
        "Xianyong Fang",
        "Bin Tang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-07",
      "update_time": "2024-09-07",
      "comments": "Accepted by TMM 2024",
      "repo_url": "https://github.com/liuzywen/ssfam"
    }
  },
  "Camouflaged Object Detection": {
    "2408.15020": {
      "paper_id": "2408.15020v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.15020v1",
      "paper_key": "2408.15020",
      "paper_title": "Hierarchical Graph Interaction Transformer with Dynamic Token Clustering for Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2408.15020v1",
      "paper_abstract": "Camouflaged object detection (COD) aims to identify the objects that seamlessly blend into the surrounding backgrounds. Due to the intrinsic similarity between the camouflaged objects and the background region, it is extremely challenging to precisely distinguish the camouflaged objects by existing approaches. In this paper, we propose a hierarchical graph interaction network termed HGINet for camouflaged object detection, which is capable of discovering imperceptible objects via effective graph interaction among the hierarchical tokenized features. Specifically, we first design a region-aware token focusing attention (RTFA) with dynamic token clustering to excavate the potentially distinguishable tokens in the local region. Afterwards, a hierarchical graph interaction transformer (HGIT) is proposed to construct bi-directional aligned communication between hierarchical features in the latent interaction space for visual semantics enhancement. Furthermore, we propose a decoder network with confidence aggregated feature fusion (CAFF) modules, which progressively fuses the hierarchical interacted features to refine the local detail in ambiguous regions. Extensive experiments conducted on the prevalent datasets, i.e. COD10K, CAMO, NC4K and CHAMELEON demonstrate the superior performance of HGINet compared to existing state-of-the-art methods. Our code is available at https://github.com/Garyson1204/HGINet.",
      "paper_authors": [
        "Siyuan Yao",
        "Hao Sun",
        "Tian-Zhu Xiang",
        "Xiao Wang",
        "Xiaochun Cao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-27",
      "update_time": "2024-08-27",
      "comments": "Submitted to IEEE Transactions on Image Processing",
      "repo_url": "https://github.com/garyson1204/hginet"
    },
    "2408.14562": {
      "paper_id": "2408.14562v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.14562v1",
      "paper_key": "2408.14562",
      "paper_title": "A Survey of Camouflaged Object Detection and Beyond",
      "paper_url": "http://arxiv.org/abs/2408.14562v1",
      "paper_abstract": "Camouflaged Object Detection (COD) refers to the task of identifying and segmenting objects that blend seamlessly into their surroundings, posing a significant challenge for computer vision systems. In recent years, COD has garnered widespread attention due to its potential applications in surveillance, wildlife conservation, autonomous systems, and more. While several surveys on COD exist, they often have limitations in terms of the number and scope of papers covered, particularly regarding the rapid advancements made in the field since mid-2023. To address this void, we present the most comprehensive review of COD to date, encompassing both theoretical frameworks and practical contributions to the field. This paper explores various COD methods across four domains, including both image-level and video-level solutions, from the perspectives of traditional and deep learning approaches. We thoroughly investigate the correlations between COD and other camouflaged scenario methods, thereby laying the theoretical foundation for subsequent analyses. Beyond object-level detection, we also summarize extended methods for instance-level tasks, including camouflaged instance segmentation, counting, and ranking. Additionally, we provide an overview of commonly used benchmarks and evaluation metrics in COD tasks, conducting a comprehensive evaluation of deep learning-based techniques in both image and video domains, considering both qualitative and quantitative performance. Finally, we discuss the limitations of current COD models and propose 9 promising directions for future research, focusing on addressing inherent challenges and exploring novel, meaningful technologies. For those interested, a curated list of COD-related techniques, datasets, and additional resources can be found at https://github.com/ChunmingHe/awesome-concealed-object-segmentation",
      "paper_authors": [
        "Fengyang Xiao",
        "Sujie Hu",
        "Yuqi Shen",
        "Chengyu Fang",
        "Jinfa Huang",
        "Chunming He",
        "Longxiang Tang",
        "Ziyun Yang",
        "Xiu Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-26",
      "update_time": "2024-08-26",
      "comments": "26 pages, 10 figures, 8 tables",
      "repo_url": "https://github.com/chunminghe/awesome-concealed-object-segmentation"
    },
    "2408.10777": {
      "paper_id": "2408.10777v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.10777v1",
      "paper_key": "2408.10777",
      "paper_title": "Just a Hint: Point-Supervised Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2408.10777v1",
      "paper_abstract": "Camouflaged Object Detection (COD) demands models to expeditiously and accurately distinguish objects which conceal themselves seamlessly in the environment. Owing to the subtle differences and ambiguous boundaries, COD is not only a remarkably challenging task for models but also for human annotators, requiring huge efforts to provide pixel-wise annotations. To alleviate the heavy annotation burden, we propose to fulfill this task with the help of only one point supervision. Specifically, by swiftly clicking on each object, we first adaptively expand the original point-based annotation to a reasonable hint area. Then, to avoid partial localization around discriminative parts, we propose an attention regulator to scatter model attention to the whole object through partially masking labeled regions. Moreover, to solve the unstable feature representation of camouflaged objects under only point-based annotation, we perform unsupervised contrastive learning based on differently augmented image pairs (e.g. changing color or doing translation). On three mainstream COD benchmarks, experimental results show that our model outperforms several weakly-supervised methods by a large margin across various metrics.",
      "paper_authors": [
        "Huafeng Chen",
        "Dian Shao",
        "Guangqian Guo",
        "Shan Gao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-20",
      "update_time": "2024-08-20",
      "comments": "Accepted by ECCV2024",
      "repo_url": "#"
    },
    "2408.10760": {
      "paper_id": "2408.10760v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.10760v1",
      "paper_key": "2408.10760",
      "paper_title": "SAM-COD: SAM-guided Unified Framework for Weakly-Supervised Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2408.10760v1",
      "paper_abstract": "Most Camouflaged Object Detection (COD) methods heavily rely on mask annotations, which are time-consuming and labor-intensive to acquire. Existing weakly-supervised COD approaches exhibit significantly inferior performance compared to fully-supervised methods and struggle to simultaneously support all the existing types of camouflaged object labels, including scribbles, bounding boxes, and points. Even for Segment Anything Model (SAM), it is still problematic to handle the weakly-supervised COD and it typically encounters challenges of prompt compatibility of the scribble labels, extreme response, semantically erroneous response, and unstable feature representations, producing unsatisfactory results in camouflaged scenes. To mitigate these issues, we propose a unified COD framework in this paper, termed SAM-COD, which is capable of supporting arbitrary weakly-supervised labels. Our SAM-COD employs a prompt adapter to handle scribbles as prompts based on SAM. Meanwhile, we introduce response filter and semantic matcher modules to improve the quality of the masks obtained by SAM under COD prompts. To alleviate the negative impacts of inaccurate mask predictions, a new strategy of prompt-adaptive knowledge distillation is utilized to ensure a reliable feature representation. To validate the effectiveness of our approach, we have conducted extensive empirical experiments on three mainstream COD benchmarks. The results demonstrate the superiority of our method against state-of-the-art weakly-supervised and even fully-supervised methods.",
      "paper_authors": [
        "Huafeng Chen",
        "Pengxu Wei",
        "Guangqian Guo",
        "Shan Gao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-20",
      "update_time": "2024-08-20",
      "comments": "Accepted by ECCV2024",
      "repo_url": "#"
    },
    "2408.09097": {
      "paper_id": "2408.09097v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.09097v1",
      "paper_key": "2408.09097",
      "paper_title": "Depth-guided Texture Diffusion for Image Semantic Segmentation",
      "paper_url": "http://arxiv.org/abs/2408.09097v1",
      "paper_abstract": "Depth information provides valuable insights into the 3D structure especially the outline of objects, which can be utilized to improve the semantic segmentation tasks. However, a naive fusion of depth information can disrupt feature and compromise accuracy due to the modality gap between the depth and the vision. In this work, we introduce a Depth-guided Texture Diffusion approach that effectively tackles the outlined challenge. Our method extracts low-level features from edges and textures to create a texture image. This image is then selectively diffused across the depth map, enhancing structural information vital for precisely extracting object outlines. By integrating this enriched depth map with the original RGB image into a joint feature embedding, our method effectively bridges the disparity between the depth map and the image, enabling more accurate semantic segmentation. We conduct comprehensive experiments across diverse, commonly-used datasets spanning a wide range of semantic segmentation tasks, including Camouflaged Object Detection (COD), Salient Object Detection (SOD), and indoor semantic segmentation. With source-free estimated depth or depth captured by depth cameras, our method consistently outperforms existing baselines and achieves new state-of-theart results, demonstrating the effectiveness of our Depth-guided Texture Diffusion for image semantic segmentation.",
      "paper_authors": [
        "Wei Sun",
        "Yuan Li",
        "Qixiang Ye",
        "Jianbin Jiao",
        "Yanzhao Zhou"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-17",
      "update_time": "2024-08-17",
      "comments": null,
      "repo_url": "#"
    },
    "2408.08870": {
      "paper_id": "2408.08870v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.08870v1",
      "paper_key": "2408.08870",
      "paper_title": "SAM2-UNet: Segment Anything 2 Makes Strong Encoder for Natural and Medical Image Segmentation",
      "paper_url": "http://arxiv.org/abs/2408.08870v1",
      "paper_abstract": "Image segmentation plays an important role in vision understanding. Recently, the emerging vision foundation models continuously achieved superior performance on various tasks. Following such success, in this paper, we prove that the Segment Anything Model 2 (SAM2) can be a strong encoder for U-shaped segmentation models. We propose a simple but effective framework, termed SAM2-UNet, for versatile image segmentation. Specifically, SAM2-UNet adopts the Hiera backbone of SAM2 as the encoder, while the decoder uses the classic U-shaped design. Additionally, adapters are inserted into the encoder to allow parameter-efficient fine-tuning. Preliminary experiments on various downstream tasks, such as camouflaged object detection, salient object detection, marine animal segmentation, mirror detection, and polyp segmentation, demonstrate that our SAM2-UNet can simply beat existing specialized state-of-the-art methods without bells and whistles. Project page: \\url{https://github.com/WZH0120/SAM2-UNet}.",
      "paper_authors": [
        "Xinyu Xiong",
        "Zihuang Wu",
        "Shuangyi Tan",
        "Wenxue Li",
        "Feilong Tang",
        "Ying Chen",
        "Siying Li",
        "Jie Ma",
        "Guanbin Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-16",
      "update_time": "2024-08-16",
      "comments": "Technical Report",
      "repo_url": "https://github.com/wzh0120/sam2-unet"
    },
    "2408.08050": {
      "paper_id": "2408.08050v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.08050v1",
      "paper_key": "2408.08050",
      "paper_title": "CamoTeacher: Dual-Rotation Consistency Learning for Semi-Supervised Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2408.08050v1",
      "paper_abstract": "Existing camouflaged object detection~(COD) methods depend heavily on large-scale pixel-level annotations.However, acquiring such annotations is laborious due to the inherent camouflage characteristics of the objects.Semi-supervised learning offers a promising solution to this challenge.Yet, its application in COD is hindered by significant pseudo-label noise, both pixel-level and instance-level.We introduce CamoTeacher, a novel semi-supervised COD framework, utilizing Dual-Rotation Consistency Learning~(DRCL) to effectively address these noise issues.Specifically, DRCL minimizes pseudo-label noise by leveraging rotation views' consistency in pixel-level and instance-level.First, it employs Pixel-wise Consistency Learning~(PCL) to deal with pixel-level noise by reweighting the different parts within the pseudo-label.Second, Instance-wise Consistency Learning~(ICL) is used to adjust weights for pseudo-labels, which handles instance-level noise.Extensive experiments on four COD benchmark datasets demonstrate that the proposed CamoTeacher not only achieves state-of-the-art compared with semi-supervised learning methods, but also rivals established fully-supervised learning methods.Our code will be available soon.",
      "paper_authors": [
        "Xunfa Lai",
        "Zhiyu Yang",
        "Jie Hu",
        "Shengchuan Zhang",
        "Liujuan Cao",
        "Guannan Jiang",
        "Zhiyu Wang",
        "Songan Zhang",
        "Rongrong Ji"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-15",
      "update_time": "2024-08-15",
      "comments": "Accepted to ECCV 2024",
      "repo_url": "#"
    },
    "2408.05936": {
      "paper_id": "2408.05936v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.05936v1",
      "paper_key": "2408.05936",
      "paper_title": "Multi-scale Contrastive Adaptor Learning for Segmenting Anything in Underperformed Scenes",
      "paper_url": "http://arxiv.org/abs/2408.05936v1",
      "paper_abstract": "Foundational vision models, such as the Segment Anything Model (SAM), have achieved significant breakthroughs through extensive pre-training on large-scale visual datasets. Despite their general success, these models may fall short in specialized tasks with limited data, and fine-tuning such large-scale models is often not feasible. Current strategies involve incorporating adaptors into the pre-trained SAM to facilitate downstream task performance with minimal model adjustment. However, these strategies can be hampered by suboptimal learning approaches for the adaptors. In this paper, we introduce a novel Multi-scale Contrastive Adaptor learning method named MCA-SAM, which enhances adaptor performance through a meticulously designed contrastive learning framework at both token and sample levels. Our Token-level Contrastive adaptor (TC-adaptor) focuses on refining local representations by improving the discriminability of patch tokens, while the Sample-level Contrastive adaptor (SC-adaptor) amplifies global understanding across different samples. Together, these adaptors synergistically enhance feature comparison within and across samples, bolstering the model's representational strength and its ability to adapt to new tasks. Empirical results demonstrate that MCA-SAM sets new benchmarks, outperforming existing methods in three challenging domains: camouflage object detection, shadow segmentation, and polyp segmentation. Specifically, MCA-SAM exhibits substantial relative performance enhancements, achieving a 20.0% improvement in MAE on the COD10K dataset, a 6.0% improvement in MAE on the CAMO dataset, a 15.4% improvement in BER on the ISTD dataset, and a 7.9% improvement in mDice on the Kvasir-SEG dataset.",
      "paper_authors": [
        "Ke Zhou",
        "Zhongwei Qiu",
        "Dongmei Fu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-12",
      "update_time": "2024-08-12",
      "comments": null,
      "repo_url": "#"
    },
    "2408.01137": {
      "paper_id": "2408.01137v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.01137v1",
      "paper_key": "2408.01137",
      "paper_title": "PGNeXt: High-Resolution Salient Object Detection via Pyramid Grafting Network",
      "paper_url": "http://arxiv.org/abs/2408.01137v1",
      "paper_abstract": "We present an advanced study on more challenging high-resolution salient object detection (HRSOD) from both dataset and network framework perspectives. To compensate for the lack of HRSOD dataset, we thoughtfully collect a large-scale high resolution salient object detection dataset, called UHRSD, containing 5,920 images from real-world complex scenarios at 4K-8K resolutions. All the images are finely annotated in pixel-level, far exceeding previous low-resolution SOD datasets. Aiming at overcoming the contradiction between the sampling depth and the receptive field size in the past methods, we propose a novel one-stage framework for HR-SOD task using pyramid grafting mechanism. In general, transformer-based and CNN-based backbones are adopted to extract features from different resolution images independently and then these features are grafted from transformer branch to CNN branch. An attention-based Cross-Model Grafting Module (CMGM) is proposed to enable CNN branch to combine broken detailed information more holistically, guided by different source feature during decoding process. Moreover, we design an Attention Guided Loss (AGL) to explicitly supervise the attention matrix generated by CMGM to help the network better interact with the attention from different branches. Comprehensive experiments on UHRSD and widely-used SOD datasets demonstrate that our method can simultaneously locate salient object and preserve rich details, outperforming state-of-the-art methods. To verify the generalization ability of the proposed framework, we apply it to the camouflaged object detection (COD) task. Notably, our method performs superior to most state-of-the-art COD methods without bells and whistles.",
      "paper_authors": [
        "Changqun Xia",
        "Chenxi Xie",
        "Zhentao He",
        "Tianshu Yu",
        "Jia Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-02",
      "update_time": "2024-08-02",
      "comments": null,
      "repo_url": "#"
    },
    "2407.21596": {
      "paper_id": "2407.21596v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.21596v1",
      "paper_key": "2407.21596",
      "paper_title": "Evaluating SAM2's Role in Camouflaged Object Detection: From SAM to SAM2",
      "paper_url": "http://arxiv.org/abs/2407.21596v1",
      "paper_abstract": "The Segment Anything Model (SAM), introduced by Meta AI Research as a generic object segmentation model, quickly garnered widespread attention and significantly influenced the academic community. To extend its application to video, Meta further develops Segment Anything Model 2 (SAM2), a unified model capable of both video and image segmentation. SAM2 shows notable improvements over its predecessor in terms of applicable domains, promptable segmentation accuracy, and running speed. However, this report reveals a decline in SAM2's ability to perceive different objects in images without prompts in its auto mode, compared to SAM. Specifically, we employ the challenging task of camouflaged object detection to assess this performance decrease, hoping to inspire further exploration of the SAM model family by researchers. The results of this paper are provided in \\url{https://github.com/luckybird1994/SAMCOD}.",
      "paper_authors": [
        "Lv Tang",
        "Bo Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-31",
      "update_time": "2024-07-31",
      "comments": null,
      "repo_url": "#"
    },
    "2407.13157": {
      "paper_id": "2407.13157v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.13157v1",
      "paper_key": "2407.13157",
      "paper_title": "Learning Camouflaged Object Detection from Noisy Pseudo Label",
      "paper_url": "http://arxiv.org/abs/2407.13157v1",
      "paper_abstract": "Existing Camouflaged Object Detection (COD) methods rely heavily on large-scale pixel-annotated training sets, which are both time-consuming and labor-intensive. Although weakly supervised methods offer higher annotation efficiency, their performance is far behind due to the unclear visual demarcations between foreground and background in camouflaged images. In this paper, we explore the potential of using boxes as prompts in camouflaged scenes and introduce the first weakly semi-supervised COD method, aiming for budget-efficient and high-precision camouflaged object segmentation with an extremely limited number of fully labeled images. Critically, learning from such limited set inevitably generates pseudo labels with serious noisy pixels. To address this, we propose a noise correction loss that facilitates the model's learning of correct pixels in the early learning stage, and corrects the error risk gradients dominated by noisy pixels in the memorization stage, ultimately achieving accurate segmentation of camouflaged objects from noisy labels. When using only 20% of fully labeled data, our method shows superior performance over the state-of-the-art methods.",
      "paper_authors": [
        "Jin Zhang",
        "Ruiheng Zhang",
        "Yanjiao Shi",
        "Zhe Cao",
        "Nian Liu",
        "Fahad Shahbaz Khan"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-18",
      "update_time": "2024-07-18",
      "comments": "Accepted by ECCV2024",
      "repo_url": "#"
    },
    "2407.13133": {
      "paper_id": "2407.13133v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.13133v1",
      "paper_key": "2407.13133",
      "paper_title": "FocusDiffuser: Perceiving Local Disparities for Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2407.13133v1",
      "paper_abstract": "Detecting objects seamlessly blended into their surroundings represents a complex task for both human cognitive capabilities and advanced artificial intelligence algorithms. Currently, the majority of methodologies for detecting camouflaged objects mainly focus on utilizing discriminative models with various unique designs. However, it has been observed that generative models, such as Stable Diffusion, possess stronger capabilities for understanding various objects in complex environments; Yet their potential for the cognition and detection of camouflaged objects has not been extensively explored. In this study, we present a novel denoising diffusion model, namely FocusDiffuser, to investigate how generative models can enhance the detection and interpretation of camouflaged objects. We believe that the secret to spotting camouflaged objects lies in catching the subtle nuances in details. Consequently, our FocusDiffuser innovatively integrates specialized enhancements, notably the Boundary-Driven LookUp (BDLU) module and Cyclic Positioning (CP) module, to elevate standard diffusion models, significantly boosting the detail-oriented analytical capabilities. Our experiments demonstrate that FocusDiffuser, from a generative perspective, effectively addresses the challenge of camouflaged object detection, surpassing leading models on benchmarks like CAMO, COD10K and NC4K.",
      "paper_authors": [
        "Jianwei Zhao",
        "Xin Li",
        "Fan Yang",
        "Qiang Zhai",
        "Ao Luo",
        "Zicheng Jiao",
        "Hong Cheng"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-18",
      "update_time": "2024-07-18",
      "comments": "18 pages,7figures",
      "repo_url": "#"
    },
    "2407.12339": {
      "paper_id": "2407.12339v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.12339v1",
      "paper_key": "2407.12339",
      "paper_title": "Exploring Deeper! Segment Anything Model with Depth Perception for Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2407.12339v1",
      "paper_abstract": "This paper introduces a new Segment Anything Model with Depth Perception (DSAM) for Camouflaged Object Detection (COD). DSAM exploits the zero-shot capability of SAM to realize precise segmentation in the RGB-D domain. It consists of the Prompt-Deeper Module and the Finer Module. The Prompt-Deeper Module utilizes knowledge distillation and the Bias Correction Module to achieve the interaction between RGB features and depth features, especially using depth features to correct erroneous parts in RGB features. Then, the interacted features are combined with the box prompt in SAM to create a prompt with depth perception. The Finer Module explores the possibility of accurately segmenting highly camouflaged targets from a depth perspective. It uncovers depth cues in areas missed by SAM through mask reversion, self-filtering, and self-attention operations, compensating for its defects in the COD domain. DSAM represents the first step towards the SAM-based RGB-D COD model. It maximizes the utilization of depth features while synergizing with RGB features to achieve multimodal complementarity, thereby overcoming the segmentation limitations of SAM and improving its accuracy in COD. Experimental results on COD benchmarks demonstrate that DSAM achieves excellent segmentation performance and reaches the state-of-the-art (SOTA) on COD benchmarks with less consumption of training resources. The code will be available at https://github.com/guobaoxiao/DSAM.",
      "paper_authors": [
        "Zhenni Yu",
        "Xiaoqin Zhang",
        "Li Zhao",
        "Yi Bin",
        "Guobao Xiao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-17",
      "update_time": "2024-07-17",
      "comments": "ACM MM 2024",
      "repo_url": "#"
    },
    "2406.11641": {
      "paper_id": "2406.11641v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.11641v1",
      "paper_key": "2406.11641",
      "paper_title": "YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection",
      "paper_url": "http://arxiv.org/abs/2406.11641v1",
      "paper_abstract": "Predominant methods for image-based drone detection frequently rely on employing generic object detection algorithms like YOLOv5. While proficient in identifying drones against homogeneous backgrounds, these algorithms often struggle in complex, highly textured environments. In such scenarios, drones seamlessly integrate into the background, creating camouflage effects that adversely affect the detection quality. To address this issue, we introduce a novel deep learning architecture called YOLO-FEDER FusionNet. Unlike conventional approaches, YOLO-FEDER FusionNet combines generic object detection methods with the specialized strength of camouflage object detection techniques to enhance drone detection capabilities. Comprehensive evaluations of YOLO-FEDER FusionNet show the efficiency of the proposed model and demonstrate substantial improvements in both reducing missed detections and false alarms.",
      "paper_authors": [
        "Tamara R. Lenhard",
        "Andreas Weinmann",
        "Stefan J\u00e4ger",
        "Tobias Koch"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-17",
      "update_time": "2024-06-17",
      "comments": "7 pages, 4 figures, 6 tables, to be published in the conference\n  proceedings of the 2024 IEEE International Conference on Image Processing\n  (ICIP)",
      "repo_url": "#"
    },
    "2406.05802": {
      "paper_id": "2406.05802v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.05802v1",
      "paper_key": "2406.05802",
      "paper_title": "SAM-PM: Enhancing Video Camouflaged Object Detection using Spatio-Temporal Attention",
      "paper_url": "http://arxiv.org/abs/2406.05802v1",
      "paper_abstract": "In the domain of large foundation models, the Segment Anything Model (SAM) has gained notable recognition for its exceptional performance in image segmentation. However, tackling the video camouflage object detection (VCOD) task presents a unique challenge. Camouflaged objects typically blend into the background, making them difficult to distinguish in still images. Additionally, ensuring temporal consistency in this context is a challenging problem. As a result, SAM encounters limitations and falls short when applied to the VCOD task. To overcome these challenges, we propose a new method called the SAM Propagation Module (SAM-PM). Our propagation module enforces temporal consistency within SAM by employing spatio-temporal cross-attention mechanisms. Moreover, we exclusively train the propagation module while keeping the SAM network weights frozen, allowing us to integrate task-specific insights with the vast knowledge accumulated by the large model. Our method effectively incorporates temporal consistency and domain-specific expertise into the segmentation network with an addition of less than 1% of SAM's parameters. Extensive experimentation reveals a substantial performance improvement in the VCOD benchmark when compared to the most recent state-of-the-art techniques. Code and pre-trained weights are open-sourced at https://github.com/SpiderNitt/SAM-PM",
      "paper_authors": [
        "Muhammad Nawfal Meeran",
        "Gokul Adethya T",
        "Bhanu Pratyush Mantha"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-09",
      "update_time": "2024-06-09",
      "comments": null,
      "repo_url": "https://github.com/spidernitt/sam-pm"
    },
    "2406.05776": {
      "paper_id": "2406.05776v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.05776v1",
      "paper_key": "2406.05776",
      "paper_title": "Utilizing Grounded SAM for self-supervised frugal camouflaged human detection",
      "paper_url": "http://arxiv.org/abs/2406.05776v1",
      "paper_abstract": "Visually detecting camouflaged objects is a hard problem for both humans and computer vision algorithms. Strong similarities between object and background appearance make the task significantly more challenging than traditional object detection or segmentation tasks. Current state-of-the-art models use either convolutional neural networks or vision transformers as feature extractors. They are trained in a fully supervised manner and thus need a large amount of labeled training data. In this paper, both self-supervised and frugal learning methods are introduced to the task of Camouflaged Object Detection (COD). The overall goal is to fine-tune two COD reference methods, namely SINet-V2 and HitNet, pre-trained for camouflaged animal detection to the task of camouflaged human detection. Therefore, we use the public dataset CPD1K that contains camouflaged humans in a forest environment. We create a strong baseline using supervised frugal transfer learning for the fine-tuning task. Then, we analyze three pseudo-labeling approaches to perform the fine-tuning task in a self-supervised manner. Our experiments show that we achieve similar performance by pure self-supervision compared to fully supervised frugal learning.",
      "paper_authors": [
        "Matthias Pijarowski",
        "Alexander Wolpert",
        "Martin Heckmann",
        "Michael Teutsch"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-09",
      "update_time": "2024-06-09",
      "comments": null,
      "repo_url": "#"
    },
    "2405.16144": {
      "paper_id": "2405.16144v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.16144v1",
      "paper_key": "2405.16144",
      "paper_title": "GreenCOD: A Green Camouflaged Object Detection Method",
      "paper_url": "http://arxiv.org/abs/2405.16144v1",
      "paper_abstract": "We introduce GreenCOD, a green method for detecting camouflaged objects, distinct in its avoidance of backpropagation techniques. GreenCOD leverages gradient boosting and deep features extracted from pre-trained Deep Neural Networks (DNNs). Traditional camouflaged object detection (COD) approaches often rely on complex deep neural network architectures, seeking performance improvements through backpropagation-based fine-tuning. However, such methods are typically computationally demanding and exhibit only marginal performance variations across different models. This raises the question of whether effective training can be achieved without backpropagation. Addressing this, our work proposes a new paradigm that utilizes gradient boosting for COD. This approach significantly simplifies the model design, resulting in a system that requires fewer parameters and operations and maintains high performance compared to state-of-the-art deep learning models. Remarkably, our models are trained without backpropagation and achieve the best performance with fewer than 20G Multiply-Accumulate Operations (MACs). This new, more efficient paradigm opens avenues for further exploration in green, backpropagation-free model training.",
      "paper_authors": [
        "Hong-Shuo Chen",
        "Yao Zhu",
        "Suya You",
        "Azad M. Madni",
        "C. -C. Jay Kuo"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-25",
      "update_time": "2024-05-25",
      "comments": null,
      "repo_url": "#"
    },
    "2405.05614": {
      "paper_id": "2405.05614v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.05614v1",
      "paper_key": "2405.05614",
      "paper_title": "Depth Awakens: A Depth-perceptual Attention Fusion Network for RGB-D Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2405.05614v1",
      "paper_abstract": "Camouflaged object detection (COD) presents a persistent challenge in accurately identifying objects that seamlessly blend into their surroundings. However, most existing COD models overlook the fact that visual systems operate within a genuine 3D environment. The scene depth inherent in a single 2D image provides rich spatial clues that can assist in the detection of camouflaged objects. Therefore, we propose a novel depth-perception attention fusion network that leverages the depth map as an auxiliary input to enhance the network's ability to perceive 3D information, which is typically challenging for the human eye to discern from 2D images. The network uses a trident-branch encoder to extract chromatic and depth information and their communications. Recognizing that certain regions of a depth map may not effectively highlight the camouflaged object, we introduce a depth-weighted cross-attention fusion module to dynamically adjust the fusion weights on depth and RGB feature maps. To keep the model simple without compromising effectiveness, we design a straightforward feature aggregation decoder that adaptively fuses the enhanced aggregated features. Experiments demonstrate the significant superiority of our proposed method over other states of the arts, which further validates the contribution of depth information in camouflaged object detection. The code will be available at https://github.com/xinran-liu00/DAF-Net.",
      "paper_authors": [
        "Xinran Liua",
        "Lin Qia",
        "Yuxuan Songa",
        "Qi Wen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-09",
      "update_time": "2024-05-09",
      "comments": null,
      "repo_url": "#"
    },
    "2405.02824": {
      "paper_id": "2405.02824v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.02824v2",
      "paper_key": "2405.02824",
      "paper_title": "Adaptive Guidance Learning for Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2405.02824v2",
      "paper_abstract": "Camouflaged object detection (COD) aims to segment objects visually embedded in their surroundings, which is a very challenging task due to the high similarity between the objects and the background. To address it, most methods often incorporate additional information (e.g., boundary, texture, and frequency clues) to guide feature learning for better detecting camouflaged objects from the background. Although progress has been made, these methods are basically individually tailored to specific auxiliary cues, thus lacking adaptability and not consistently achieving high segmentation performance. To this end, this paper proposes an adaptive guidance learning network, dubbed \\textit{AGLNet}, which is a unified end-to-end learnable model for exploring and adapting different additional cues in CNN models to guide accurate camouflaged feature learning. Specifically, we first design a straightforward additional information generation (AIG) module to learn additional camouflaged object cues, which can be adapted for the exploration of effective camouflaged features. Then we present a hierarchical feature combination (HFC) module to deeply integrate additional cues and image features to guide camouflaged feature learning in a multi-level fusion manner.Followed by a recalibration decoder (RD), different features are further aggregated and refined for accurate object prediction. Extensive experiments on three widely used COD benchmark datasets demonstrate that the proposed method achieves significant performance improvements under different additional cues, and outperforms the recent 20 state-of-the-art methods by a large margin. Our code will be made publicly available at: \\textcolor{blue}{{https://github.com/ZNan-Chen/AGLNet}}.",
      "paper_authors": [
        "Zhennan Chen",
        "Xuying Zhang",
        "Tian-Zhu Xiang",
        "Ying Tai"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-05",
      "update_time": "2024-05-07",
      "comments": null,
      "repo_url": "#"
    },
    "2404.08936": {
      "paper_id": "2404.08936v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.08936v1",
      "paper_key": "2404.08936",
      "paper_title": "Shifting Spotlight for Co-supervision: A Simple yet Efficient Single-branch Network to See Through Camouflage",
      "paper_url": "http://arxiv.org/abs/2404.08936v1",
      "paper_abstract": "Efficient and accurate camouflaged object detection (COD) poses a challenge in the field of computer vision. Recent approaches explored the utility of edge information for network co-supervision, achieving notable advancements. However, these approaches introduce an extra branch for complex edge extraction, complicate the model architecture and increases computational demands. Addressing this issue, our work replicates the effect that animal's camouflage can be easily revealed under a shifting spotlight, and leverages it for network co-supervision to form a compact yet efficient single-branch network, the Co-Supervised Spotlight Shifting Network (CS$^3$Net). The spotlight shifting strategy allows CS$^3$Net to learn additional prior within a single-branch framework, obviating the need for resource demanding multi-branch design. To leverage the prior of spotlight shifting co-supervision, we propose Shadow Refinement Module (SRM) and Projection Aware Attention (PAA) for feature refinement and enhancement. To ensure the continuity of multi-scale features aggregation, we utilize the Extended Neighbor Connection Decoder (ENCD) for generating the final predictions. Empirical evaluations on public datasets confirm that our CS$^3$Net offers an optimal balance between efficiency and performance: it accomplishes a 32.13% reduction in Multiply-Accumulate (MACs) operations compared to leading efficient COD models, while also delivering superior performance.",
      "paper_authors": [
        "Yang Hu",
        "Jinxia Zhang",
        "Kaihua Zhang",
        "Yin Yuan"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-04-13",
      "update_time": "2024-04-13",
      "comments": null,
      "repo_url": "#"
    },
    "2403.01968": {
      "paper_id": "2403.01968v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.01968v1",
      "paper_key": "2403.01968",
      "paper_title": "Explicit Motion Handling and Interactive Prompting for Video Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2403.01968v1",
      "paper_abstract": "Camouflage poses challenges in distinguishing a static target, whereas any movement of the target can break this disguise. Existing video camouflaged object detection (VCOD) approaches take noisy motion estimation as input or model motion implicitly, restricting detection performance in complex dynamic scenes. In this paper, we propose a novel Explicit Motion handling and Interactive Prompting framework for VCOD, dubbed EMIP, which handles motion cues explicitly using a frozen pre-trained optical flow fundamental model. EMIP is characterized by a two-stream architecture for simultaneously conducting camouflaged segmentation and optical flow estimation. Interactions across the dual streams are realized in an interactive prompting way that is inspired by emerging visual prompt learning. Two learnable modules, i.e. the camouflaged feeder and motion collector, are designed to incorporate segmentation-to-motion and motion-to-segmentation prompts, respectively, and enhance outputs of the both streams. The prompt fed to the motion stream is learned by supervising optical flow in a self-supervised manner. Furthermore, we show that long-term historical information can also be incorporated as a prompt into EMIP and achieve more robust results with temporal consistency. Experimental results demonstrate that our EMIP achieves new state-of-the-art records on popular VCOD benchmarks. The code will be publicly available.",
      "paper_authors": [
        "Xin Zhang",
        "Tao Xiao",
        "Gepeng Ji",
        "Xuan Wu",
        "Keren Fu",
        "Qijun Zhao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-03-04",
      "update_time": "2024-03-04",
      "comments": "9 pages, 6 figures",
      "repo_url": "#"
    },
    "2402.18922": {
      "paper_id": "2402.18922v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.18922v1",
      "paper_key": "2402.18922",
      "paper_title": "A Simple yet Effective Network based on Vision Transformer for Camouflaged Object and Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2402.18922v1",
      "paper_abstract": "Camouflaged object detection (COD) and salient object detection (SOD) are two distinct yet closely-related computer vision tasks widely studied during the past decades. Though sharing the same purpose of segmenting an image into binary foreground and background regions, their distinction lies in the fact that COD focuses on concealed objects hidden in the image, while SOD concentrates on the most prominent objects in the image. Previous works achieved good performance by stacking various hand-designed modules and multi-scale features. However, these carefully-designed complex networks often performed well on one task but not on another. In this work, we propose a simple yet effective network (SENet) based on vision Transformer (ViT), by employing a simple design of an asymmetric ViT-based encoder-decoder structure, we yield competitive results on both tasks, exhibiting greater versatility than meticulously crafted ones. Furthermore, to enhance the Transformer's ability to model local information, which is important for pixel-level binary segmentation tasks, we propose a local information capture module (LICM). We also propose a dynamic weighted loss (DW loss) based on Binary Cross-Entropy (BCE) and Intersection over Union (IoU) loss, which guides the network to pay more attention to those smaller and more difficult-to-find target objects according to their size. Moreover, we explore the issue of joint training of SOD and COD, and propose a preliminary solution to the conflict in joint training, further improving the performance of SOD. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our method. The code is available at https://github.com/linuxsino/SENet.",
      "paper_authors": [
        "Chao Hao",
        "Zitong Yu",
        "Xin Liu",
        "Jun Xu",
        "Huanjing Yue",
        "Jingyu Yang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-02-29",
      "update_time": "2024-02-29",
      "comments": "submitted to IEEE TIP",
      "repo_url": "#"
    },
    "2402.18698": {
      "paper_id": "2402.18698v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.18698v2",
      "paper_key": "2402.18698",
      "paper_title": "Spatial Coherence Loss: All Objects Matter in Salient and Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2402.18698v2",
      "paper_abstract": "Generic object detection is a category-independent task that relies on accurate modeling of objectness. We show that for accurate semantic analysis, the network needs to learn all object-level predictions that appear at any stage of learning, including the pre-defined ground truth (GT) objects and the ambiguous decoy objects that the network misidentifies as foreground. Yet, most relevant models focused mainly on improving the learning of the GT objects. A few methods that consider decoy objects utilize loss functions that only focus on the single-response, i.e., the loss response of a single ambiguous pixel, and thus do not benefit from the wealth of information that an object-level ambiguity learning design can provide. Inspired by the human visual system, which first discerns the boundaries of ambiguous regions before delving into the semantic meaning, we propose a novel loss function, Spatial Coherence Loss (SCLoss), that incorporates the mutual response between adjacent pixels into the widely-used single-response loss functions. We demonstrate that the proposed SCLoss can gradually learn the ambiguous regions by detecting and emphasizing their boundaries in a self-adaptive manner. Through comprehensive experiments, we demonstrate that replacing popular loss functions with SCLoss can improve the performance of current state-of-the-art (SOTA) salient or camouflaged object detection (SOD or COD) models. We also demonstrate that combining SCLoss with other loss functions can further improve performance and result in SOTA outcomes for different applications.",
      "paper_authors": [
        "Ziyun Yang",
        "Kevin Choy",
        "Sina Farsiu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-02-28",
      "update_time": "2024-07-16",
      "comments": null,
      "repo_url": "#"
    },
    "2402.02217": {
      "paper_id": "2402.02217v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.02217v1",
      "paper_key": "2402.02217",
      "paper_title": "CoFiNet: Unveiling Camouflaged Objects with Multi-Scale Finesse",
      "paper_url": "http://arxiv.org/abs/2402.02217v1",
      "paper_abstract": "Camouflaged Object Detection (COD) is a critical aspect of computer vision aimed at identifying concealed objects, with applications spanning military, industrial, medical and monitoring domains. To address the problem of poor detail segmentation effect, we introduce a novel method for camouflage object detection, named CoFiNet. Our approach primarily focuses on multi-scale feature fusion and extraction, with special attention to the model's segmentation effectiveness for detailed features, enhancing its ability to effectively detect camouflaged objects. CoFiNet adopts a coarse-to-fine strategy. A multi-scale feature integration module is laveraged to enhance the model's capability of fusing context feature. A multi-activation selective kernel module is leveraged to grant the model the ability to autonomously alter its receptive field, enabling it to selectively choose an appropriate receptive field for camouflaged objects of different sizes. During mask generation, we employ the dual-mask strategy for image segmentation, separating the reconstruction of coarse and fine masks, which significantly enhances the model's learning capacity for details. Comprehensive experiments were conducted on four different datasets, demonstrating that CoFiNet achieves state-of-the-art performance across all datasets. The experiment results of CoFiNet underscore its effectiveness in camouflage object detection and highlight its potential in various practical application scenarios.",
      "paper_authors": [
        "Cunhan Guo",
        "Heyan Huang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-02-03",
      "update_time": "2024-02-03",
      "comments": null,
      "repo_url": "#"
    },
    "2401.17803": {
      "paper_id": "2401.17803v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.17803v2",
      "paper_key": "2401.17803",
      "paper_title": "SU-SAM: A Simple Unified Framework for Adapting Segment Anything Model in Underperformed Scenes",
      "paper_url": "http://arxiv.org/abs/2401.17803v2",
      "paper_abstract": "Segment anything model (SAM) has demonstrated excellent generalizability in common vision scenarios, yet falling short of the ability to understand specialized data. Recently, several methods have combined parameter-efficient techniques with task-specific designs to fine-tune SAM on particular tasks. However, these methods heavily rely on handcraft, complicated, and task-specific designs, and pre/post-processing to achieve acceptable performances on downstream tasks. As a result, this severely restricts generalizability to other downstream tasks. To address this issue, we present a simple and unified framework, namely SU-SAM, that can easily and efficiently fine-tune the SAM model with parameter-efficient techniques while maintaining excellent generalizability toward various downstream tasks. SU-SAM does not require any task-specific designs and aims to improve the adaptability of SAM-like models significantly toward underperformed scenes. Concretely, we abstract parameter-efficient modules of different methods into basic design elements in our framework. Besides, we propose four variants of SU-SAM, i.e., series, parallel, mixed, and LoRA structures. Comprehensive experiments on nine datasets and six downstream tasks to verify the effectiveness of SU-SAM, including medical image segmentation, camouflage object detection, salient object segmentation, surface defect segmentation, complex object shapes, and shadow masking. Our experimental results demonstrate that SU-SAM achieves competitive or superior accuracy compared to state-of-the-art methods. Furthermore, we provide in-depth analyses highlighting the effectiveness of different parameter-efficient designs within SU-SAM. In addition, we propose a generalized model and benchmark, showcasing SU-SAM's generalizability across all diverse datasets simultaneously.",
      "paper_authors": [
        "Yiran Song",
        "Qianyu Zhou",
        "Xuequan Lu",
        "Zhiwen Shao",
        "Lizhuang Ma"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-01-31",
      "update_time": "2024-07-29",
      "comments": null,
      "repo_url": "https://github.com/zongzi13545329/simada"
    },
    "2401.11767": {
      "paper_id": "2401.11767v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.11767v1",
      "paper_key": "2401.11767",
      "paper_title": "Concealed Object Segmentation with Hierarchical Coherence Modeling",
      "paper_url": "http://arxiv.org/abs/2401.11767v1",
      "paper_abstract": "Concealed object segmentation (COS) is a challenging task that involves localizing and segmenting those concealed objects that are visually blended with their surrounding environments. Despite achieving remarkable success, existing COS segmenters still struggle to achieve complete segmentation results in extremely concealed scenarios. In this paper, we propose a Hierarchical Coherence Modeling (HCM) segmenter for COS, aiming to address this incomplete segmentation limitation. In specific, HCM promotes feature coherence by leveraging the intra-stage coherence and cross-stage coherence modules, exploring feature correlations at both the single-stage and contextual levels. Additionally, we introduce the reversible re-calibration decoder to detect previously undetected parts in low-confidence regions, resulting in further enhancing segmentation performance. Extensive experiments conducted on three COS tasks, including camouflaged object detection, polyp image segmentation, and transparent object detection, demonstrate the promising results achieved by the proposed HCM segmenter.",
      "paper_authors": [
        "Fengyang Xiao",
        "Pan Zhang",
        "Chunming He",
        "Runze Hu",
        "Yutao Liu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-01-22",
      "update_time": "2024-01-22",
      "comments": "Accepted to CICAI 2023. 13 pages, 6 figures, 4 tables",
      "repo_url": "#"
    },
    "2312.07374": {
      "paper_id": "2312.07374v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.07374v3",
      "paper_key": "2312.07374",
      "paper_title": "Relax Image-Specific Prompt Requirement in SAM: A Single Generic Prompt for Segmenting Camouflaged Objects",
      "paper_url": "http://arxiv.org/abs/2312.07374v3",
      "paper_abstract": "Camouflaged object detection (COD) approaches heavily rely on pixel-level annotated datasets. Weakly-supervised COD (WSCOD) approaches use sparse annotations like scribbles or points to reduce annotation effort, but this can lead to decreased accuracy. The Segment Anything Model (SAM) shows remarkable segmentation ability with sparse prompts like points. However, manual prompt is not always feasible, as it may not be accessible in real-world application. Additionally, it only provides localization information instead of semantic one, which can intrinsically cause ambiguity in interpreting the targets. In this work, we aim to eliminate the need for manual prompt. The key idea is to employ Cross-modal Chains of Thought Prompting (CCTP) to reason visual prompts using the semantic information given by a generic text prompt. To that end, we introduce a test-time adaptation per-instance mechanism called Generalizable SAM (GenSAM) to automatically enerate and optimize visual prompts the generic task prompt for WSCOD. In particular, CCTP maps a single generic text prompt onto image-specific consensus foreground and background heatmaps using vision-language models, acquiring reliable visual prompts. Moreover, to test-time adapt the visual prompts, we further propose Progressive Mask Generation (PMG) to iteratively reweight the input image, guiding the model to focus on the targets in a coarse-to-fine manner. Crucially, all network parameters are fixed, avoiding the need for additional training. Experiments demonstrate the superiority of GenSAM. Experiments on three benchmarks demonstrate that GenSAM outperforms point supervision approaches and achieves comparable results to scribble supervision ones, solely relying on general task descriptions as prompts. our codes is in: https://lwpyh.github.io/GenSAM/.",
      "paper_authors": [
        "Jian Hu",
        "Jiayi Lin",
        "Weitong Cai",
        "Shaogang Gong"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-12-12",
      "update_time": "2023-12-18",
      "comments": "Accepted by AAAI2024",
      "repo_url": "https://github.com/jyLin8100/GenSAM"
    },
    "2311.17122": {
      "paper_id": "2311.17122v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.17122v1",
      "paper_key": "2311.17122",
      "paper_title": "Large Model Based Referring Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2311.17122v1",
      "paper_abstract": "Referring camouflaged object detection (Ref-COD) is a recently-proposed problem aiming to segment out specified camouflaged objects matched with a textual or visual reference. This task involves two major challenges: the COD domain-specific perception and multimodal reference-image alignment. Our motivation is to make full use of the semantic intelligence and intrinsic knowledge of recent Multimodal Large Language Models (MLLMs) to decompose this complex task in a human-like way. As language is highly condensed and inductive, linguistic expression is the main media of human knowledge learning, and the transmission of knowledge information follows a multi-level progression from simplicity to complexity. In this paper, we propose a large-model-based Multi-Level Knowledge-Guided multimodal method for Ref-COD termed MLKG, where multi-level knowledge descriptions from MLLM are organized to guide the large vision model of segmentation to perceive the camouflage-targets and camouflage-scene progressively and meanwhile deeply align the textual references with camouflaged photos. To our knowledge, our contributions mainly include: (1) This is the first time that the MLLM knowledge is studied for Ref-COD and COD. (2) We, for the first time, propose decomposing Ref-COD into two main perspectives of perceiving the target and scene by integrating MLLM knowledge, and contribute a multi-level knowledge-guided method. (3) Our method achieves the state-of-the-art on the Ref-COD benchmark outperforming numerous strong competitors. Moreover, thanks to the injected rich knowledge, it demonstrates zero-shot generalization ability on uni-modal COD datasets. We will release our code soon.",
      "paper_authors": [
        "Shupeng Cheng",
        "Ge-Peng Ji",
        "Pengda Qin",
        "Deng-Ping Fan",
        "Bowen Zhou",
        "Peng Xu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-28",
      "update_time": "2023-11-28",
      "comments": null,
      "repo_url": "#"
    },
    "2311.16618": {
      "paper_id": "2311.16618v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.16618v2",
      "paper_key": "2311.16618",
      "paper_title": "Cross-level Attention with Overlapped Windows for Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2311.16618v2",
      "paper_abstract": "Camouflaged objects adaptively fit their color and texture with the environment, which makes them indistinguishable from the surroundings. Current methods revealed that high-level semantic features can highlight the differences between camouflaged objects and the backgrounds. Consequently, they integrate high-level semantic features with low-level detailed features for accurate camouflaged object detection (COD). Unlike previous designs for multi-level feature fusion, we state that enhancing low-level features is more impending for COD. In this paper, we propose an overlapped window cross-level attention (OWinCA) to achieve the low-level feature enhancement guided by the highest-level features. By sliding an aligned window pair on both the highest- and low-level feature maps, the high-level semantics are explicitly integrated into the low-level details via cross-level attention. Additionally, it employs an overlapped window partition strategy to alleviate the incoherence among windows, which prevents the loss of global information. These adoptions enable the proposed OWinCA to enhance low-level features by promoting the separability of camouflaged objects. The associated proposed OWinCANet fuses these enhanced multi-level features by simple convolution operation to achieve the final COD. Experiments conducted on three large-scale COD datasets demonstrate that our OWinCANet significantly surpasses the current state-of-the-art COD methods.",
      "paper_authors": [
        "Jiepan Li",
        "Fangxiao Lu",
        "Nan Xue",
        "Zhuohong Li",
        "Hongyan Zhang",
        "Wei He"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-28",
      "update_time": "2024-09-20",
      "comments": "I am withdrawing this submission as I have developed a significantly\n  expanded and improved version of this work. The new research incorporates\n  substantial advancements, which extend beyond the scope of the original\n  paper. To avoid redundancy and ensure the highest quality of contribution, I\n  will submit the enhanced version in a future manuscript.",
      "repo_url": "#"
    },
    "2311.15011": {
      "paper_id": "2311.15011v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.15011v3",
      "paper_key": "2311.15011",
      "paper_title": "VSCode: General Visual Salient and Camouflaged Object Detection with 2D Prompt Learning",
      "paper_url": "http://arxiv.org/abs/2311.15011v3",
      "paper_abstract": "Salient object detection (SOD) and camouflaged object detection (COD) are related yet distinct binary mapping tasks. These tasks involve multiple modalities, sharing commonalities and unique cues. Existing research often employs intricate task-specific specialist models, potentially leading to redundancy and suboptimal results. We introduce VSCode, a generalist model with novel 2D prompt learning, to jointly address four SOD tasks and three COD tasks. We utilize VST as the foundation model and introduce 2D prompts within the encoder-decoder architecture to learn domain and task-specific knowledge on two separate dimensions. A prompt discrimination loss helps disentangle peculiarities to benefit model optimization. VSCode outperforms state-of-the-art methods across six tasks on 26 datasets and exhibits zero-shot generalization to unseen tasks by combining 2D prompts, such as RGB-D COD. Source code has been available at https://github.com/Sssssuperior/VSCode.",
      "paper_authors": [
        "Ziyang Luo",
        "Nian Liu",
        "Wangbo Zhao",
        "Xuguang Yang",
        "Dingwen Zhang",
        "Deng-Ping Fan",
        "Fahad Khan",
        "Junwei Han"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-25",
      "update_time": "2024-04-11",
      "comments": "Accepted by CVPR2024",
      "repo_url": "https://github.com/sssssuperior/vscode"
    },
    "2311.11273": {
      "paper_id": "2311.11273v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.11273v2",
      "paper_key": "2311.11273",
      "paper_title": "Chain of Visual Perception: Harnessing Multimodal Large Language Models for Zero-shot Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2311.11273v2",
      "paper_abstract": "In this paper, we introduce a novel multimodal camo-perceptive framework (MMCPF) aimed at handling zero-shot Camouflaged Object Detection (COD) by leveraging the powerful capabilities of Multimodal Large Language Models (MLLMs). Recognizing the inherent limitations of current COD methodologies, which predominantly rely on supervised learning models demanding extensive and accurately annotated datasets, resulting in weak generalization, our research proposes a zero-shot MMCPF that circumvents these challenges. Although MLLMs hold significant potential for broad applications, their effectiveness in COD is hindered and they would make misinterpretations of camouflaged objects. To address this challenge, we further propose a strategic enhancement called the Chain of Visual Perception (CoVP), which significantly improves the perceptual capabilities of MLLMs in camouflaged scenes by leveraging both linguistic and visual cues more effectively. We validate the effectiveness of MMCPF on five widely used COD datasets, containing CAMO, COD10K, NC4K, MoCA-Mask and OVCamo. Experiments show that MMCPF can outperform all existing state-of-the-art zero-shot COD methods, and achieve competitive performance compared to weakly-supervised and fully-supervised methods, which demonstrates the potential of MMCPF. The Github link of this paper is \\url{https://github.com/luckybird1994/MMCPF}.",
      "paper_authors": [
        "Lv Tang",
        "Peng-Tao Jiang",
        "Zhihao Shen",
        "Hao Zhang",
        "Jinwei Chen",
        "Bo Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-19",
      "update_time": "2024-07-30",
      "comments": "Accepted by ACMMM2024",
      "repo_url": "https://github.com/luckybird1994/mmcpf"
    },
    "2311.02535": {
      "paper_id": "2311.02535v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.02535v2",
      "paper_key": "2311.02535",
      "paper_title": "TokenMotion: Motion-Guided Vision Transformer for Video Camouflaged Object Detection Via Learnable Token Selection",
      "paper_url": "http://arxiv.org/abs/2311.02535v2",
      "paper_abstract": "The area of Video Camouflaged Object Detection (VCOD) presents unique challenges in the field of computer vision due to texture similarities between target objects and their surroundings, as well as irregular motion patterns caused by both objects and camera movement. In this paper, we introduce TokenMotion (TMNet), which employs a transformer-based model to enhance VCOD by extracting motion-guided features using a learnable token selection. Evaluated on the challenging MoCA-Mask dataset, TMNet achieves state-of-the-art performance in VCOD. It outperforms the existing state-of-the-art method by a 12.8% improvement in weighted F-measure, an 8.4% enhancement in S-measure, and a 10.7% boost in mean IoU. The results demonstrate the benefits of utilizing motion-guided features via learnable token selection within a transformer-based framework to tackle the intricate task of VCOD.",
      "paper_authors": [
        "Zifan Yu",
        "Erfan Bank Tavakoli",
        "Meida Chen",
        "Suya You",
        "Raghuveer Rao",
        "Sanjeev Agarwal",
        "Fengbo Ren"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-05",
      "update_time": "2024-02-01",
      "comments": "Revising Needed",
      "repo_url": "#"
    },
    "2310.20208": {
      "paper_id": "2310.20208v4",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.20208v4",
      "paper_key": "2310.20208",
      "paper_title": "ZoomNeXt: A Unified Collaborative Pyramid Network for Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2310.20208v4",
      "paper_abstract": "Recent camouflaged object detection (COD) attempts to segment objects visually blended into their surroundings, which is extremely complex and difficult in real-world scenarios. Apart from the high intrinsic similarity between camouflaged objects and their background, objects are usually diverse in scale, fuzzy in appearance, and even severely occluded. To this end, we propose an effective unified collaborative pyramid network that mimics human behavior when observing vague images and videos, \\ie zooming in and out. Specifically, our approach employs the zooming strategy to learn discriminative mixed-scale semantics by the multi-head scale integration and rich granularity perception units, which are designed to fully explore imperceptible clues between candidate objects and background surroundings. The former's intrinsic multi-head aggregation provides more diverse visual patterns. The latter's routing mechanism can effectively propagate inter-frame differences in spatiotemporal scenarios and be adaptively deactivated and output all-zero results for static representations. They provide a solid foundation for realizing a unified architecture for static and dynamic COD. Moreover, considering the uncertainty and ambiguity derived from indistinguishable textures, we construct a simple yet effective regularization, uncertainty awareness loss, to encourage predictions with higher confidence in candidate regions. Our highly task-friendly framework consistently outperforms existing state-of-the-art methods in image and video COD benchmarks. Our code can be found at {https://github.com/lartpang/ZoomNeXt}.",
      "paper_authors": [
        "Youwei Pang",
        "Xiaoqi Zhao",
        "Tian-Zhu Xiang",
        "Lihe Zhang",
        "Huchuan Lu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-10-31",
      "update_time": "2024-07-14",
      "comments": "Extensions to the conference version accepted by TPAMI 2024. Fixed\n  value errors. arXiv admin note: substantial text overlap with\n  arXiv:2203.02688",
      "repo_url": "https://github.com/lartpang/zoomnext"
    },
    "2310.04253": {
      "paper_id": "2310.04253v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.04253v1",
      "paper_key": "2310.04253",
      "paper_title": "Collaborative Camouflaged Object Detection: A Large-Scale Dataset and Benchmark",
      "paper_url": "http://arxiv.org/abs/2310.04253v1",
      "paper_abstract": "In this paper, we provide a comprehensive study on a new task called collaborative camouflaged object detection (CoCOD), which aims to simultaneously detect camouflaged objects with the same properties from a group of relevant images. To this end, we meticulously construct the first large-scale dataset, termed CoCOD8K, which consists of 8,528 high-quality and elaborately selected images with object mask annotations, covering 5 superclasses and 70 subclasses. The dataset spans a wide range of natural and artificial camouflage scenes with diverse object appearances and backgrounds, making it a very challenging dataset for CoCOD. Besides, we propose the first baseline model for CoCOD, named bilateral-branch network (BBNet), which explores and aggregates co-camouflaged cues within a single image and between images within a group, respectively, for accurate camouflaged object detection in given images. This is implemented by an inter-image collaborative feature exploration (CFE) module, an intra-image object feature search (OFS) module, and a local-global refinement (LGR) module. We benchmark 18 state-of-the-art models, including 12 COD algorithms and 6 CoSOD algorithms, on the proposed CoCOD8K dataset under 5 widely used evaluation metrics. Extensive experiments demonstrate the effectiveness of the proposed method and the significantly superior performance compared to other competitors. We hope that our proposed dataset and model will boost growth in the COD community. The dataset, model, and results will be available at: https://github.com/zc199823/BBNet--CoCOD.",
      "paper_authors": [
        "Cong Zhang",
        "Hongbo Bi",
        "Tian-Zhu Xiang",
        "Ranwan Wu",
        "Jinghui Tong",
        "Xiufang Wang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-10-06",
      "update_time": "2023-10-06",
      "comments": "Accepted by IEEE Transactions on Neural Networks and Learning Systems\n  (TNNLS)",
      "repo_url": "https://github.com/zc199823/bbnet--cocod"
    },
    "2310.00702": {
      "paper_id": "2310.00702v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.00702v1",
      "paper_key": "2310.00702",
      "paper_title": "You Do Not Need Additional Priors in Camouflage Object Detection",
      "paper_url": "http://arxiv.org/abs/2310.00702v1",
      "paper_abstract": "Camouflage object detection (COD) poses a significant challenge due to the high resemblance between camouflaged objects and their surroundings. Although current deep learning methods have made significant progress in detecting camouflaged objects, many of them heavily rely on additional prior information. However, acquiring such additional prior information is both expensive and impractical in real-world scenarios. Therefore, there is a need to develop a network for camouflage object detection that does not depend on additional priors. In this paper, we propose a novel adaptive feature aggregation method that effectively combines multi-layer feature information to generate guidance information. In contrast to previous approaches that rely on edge or ranking priors, our method directly leverages information extracted from image features to guide model training. Through extensive experimental results, we demonstrate that our proposed method achieves comparable or superior performance when compared to state-of-the-art approaches.",
      "paper_authors": [
        "Yuchen Dong",
        "Heng Zhou",
        "Chengyang Li",
        "Junjie Xie",
        "Yongqiang Xie",
        "Zhongbo Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-10-01",
      "update_time": "2023-10-01",
      "comments": null,
      "repo_url": "#"
    },
    "2308.15660": {
      "paper_id": "2308.15660v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.15660v1",
      "paper_key": "2308.15660",
      "paper_title": "Unveiling Camouflage: A Learnable Fourier-based Augmentation for Camouflaged Object Detection and Instance Segmentation",
      "paper_url": "http://arxiv.org/abs/2308.15660v1",
      "paper_abstract": "Camouflaged object detection (COD) and camouflaged instance segmentation (CIS) aim to recognize and segment objects that are blended into their surroundings, respectively. While several deep neural network models have been proposed to tackle those tasks, augmentation methods for COD and CIS have not been thoroughly explored. Augmentation strategies can help improve the performance of models by increasing the size and diversity of the training data and exposing the model to a wider range of variations in the data. Besides, we aim to automatically learn transformations that help to reveal the underlying structure of camouflaged objects and allow the model to learn to better identify and segment camouflaged objects. To achieve this, we propose a learnable augmentation method in the frequency domain for COD and CIS via Fourier transform approach, dubbed CamoFourier. Our method leverages a conditional generative adversarial network and cross-attention mechanism to generate a reference image and an adaptive hybrid swapping with parameters to mix the low-frequency component of the reference image and the high-frequency component of the input image. This approach aims to make camouflaged objects more visible for detection and segmentation models. Without bells and whistles, our proposed augmentation method boosts the performance of camouflaged object detectors and camouflaged instance segmenters by large margins.",
      "paper_authors": [
        "Minh-Quan Le",
        "Minh-Triet Tran",
        "Trung-Nghia Le",
        "Tam V. Nguyen",
        "Thanh-Toan Do"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-08-29",
      "update_time": "2023-08-29",
      "comments": null,
      "repo_url": "#"
    },
    "2308.08924": {
      "paper_id": "2308.08924v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.08924v1",
      "paper_key": "2308.08924",
      "paper_title": "Frequency Perception Network for Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2308.08924v1",
      "paper_abstract": "Camouflaged object detection (COD) aims to accurately detect objects hidden in the surrounding environment. However, the existing COD methods mainly locate camouflaged objects in the RGB domain, their performance has not been fully exploited in many challenging scenarios. Considering that the features of the camouflaged object and the background are more discriminative in the frequency domain, we propose a novel learnable and separable frequency perception mechanism driven by the semantic hierarchy in the frequency domain. Our entire network adopts a two-stage model, including a frequency-guided coarse localization stage and a detail-preserving fine localization stage. With the multi-level features extracted by the backbone, we design a flexible frequency perception module based on octave convolution for coarse positioning. Then, we design the correction fusion module to step-by-step integrate the high-level features through the prior-guided correction and cross-layer feature channel association, and finally combine them with the shallow features to achieve the detailed correction of the camouflaged objects. Compared with the currently existing models, our proposed method achieves competitive performance in three popular benchmark datasets both qualitatively and quantitatively.",
      "paper_authors": [
        "Runmin Cong",
        "Mengyao Sun",
        "Sanyi Zhang",
        "Xiaofei Zhou",
        "Wei Zhang",
        "Yao Zhao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-08-17",
      "update_time": "2023-08-17",
      "comments": "Accepted by ACM MM 2023",
      "repo_url": "https://gitee.com/HarveyYeung/FPNet-MindSpore.git"
    },
    "2308.06701": {
      "paper_id": "2308.06701v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.06701v1",
      "paper_key": "2308.06701",
      "paper_title": "Camouflaged Image Synthesis Is All You Need to Boost Camouflaged Detection",
      "paper_url": "http://arxiv.org/abs/2308.06701v1",
      "paper_abstract": "Camouflaged objects that blend into natural scenes pose significant challenges for deep-learning models to detect and synthesize. While camouflaged object detection is a crucial task in computer vision with diverse real-world applications, this research topic has been constrained by limited data availability. We propose a framework for synthesizing camouflage data to enhance the detection of camouflaged objects in natural scenes. Our approach employs a generative model to produce realistic camouflage images, which can be used to train existing object detection models. Specifically, we use a camouflage environment generator supervised by a camouflage distribution classifier to synthesize the camouflage images, which are then fed into our generator to expand the dataset. Our framework outperforms the current state-of-the-art method on three datasets (COD10k, CAMO, and CHAMELEON), demonstrating its effectiveness in improving camouflaged object detection. This approach can serve as a plug-and-play data generation and augmentation module for existing camouflaged object detection tasks and provides a novel way to introduce more diversity and distributions into current camouflage datasets.",
      "paper_authors": [
        "Haichao Zhang",
        "Can Qin",
        "Yu Yin",
        "Yun Fu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-08-13",
      "update_time": "2023-08-13",
      "comments": null,
      "repo_url": "#"
    },
    "2308.04370": {
      "paper_id": "2308.04370v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.04370v1",
      "paper_key": "2308.04370",
      "paper_title": "When Super-Resolution Meets Camouflaged Object Detection: A Comparison Study",
      "paper_url": "http://arxiv.org/abs/2308.04370v1",
      "paper_abstract": "Super Resolution (SR) and Camouflaged Object Detection (COD) are two hot topics in computer vision with various joint applications. For instance, low-resolution surveillance images can be successively processed by super-resolution techniques and camouflaged object detection. However, in previous work, these two areas are always studied in isolation. In this paper, we, for the first time, conduct an integrated comparative evaluation for both. Specifically, we benchmark different super-resolution methods on commonly used COD datasets, and meanwhile, we evaluate the robustness of different COD models by using COD data processed by SR methods. Our goal is to bridge these two domains, discover novel experimental phenomena, summarize new experim.",
      "paper_authors": [
        "Juan Wen",
        "Shupeng Cheng",
        "Peng Xu",
        "Bowen Zhou",
        "Radu Timofte",
        "Weiyan Hou",
        "Luc Van Gool"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-08-08",
      "update_time": "2023-08-08",
      "comments": "23 pages with 8 figures",
      "repo_url": "#"
    },
    "2308.03166": {
      "paper_id": "2308.03166v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.03166v2",
      "paper_key": "2308.03166",
      "paper_title": "Strategic Preys Make Acute Predators: Enhancing Camouflaged Object Detectors by Generating Camouflaged Objects",
      "paper_url": "http://arxiv.org/abs/2308.03166v2",
      "paper_abstract": "Camouflaged object detection (COD) is the challenging task of identifying camouflaged objects visually blended into surroundings. Albeit achieving remarkable success, existing COD detectors still struggle to obtain precise results in some challenging cases. To handle this problem, we draw inspiration from the prey-vs-predator game that leads preys to develop better camouflage and predators to acquire more acute vision systems and develop algorithms from both the prey side and the predator side. On the prey side, we propose an adversarial training framework, Camouflageator, which introduces an auxiliary generator to generate more camouflaged objects that are harder for a COD method to detect. Camouflageator trains the generator and detector in an adversarial way such that the enhanced auxiliary generator helps produce a stronger detector. On the predator side, we introduce a novel COD method, called Internal Coherence and Edge Guidance (ICEG), which introduces a camouflaged feature coherence module to excavate the internal coherence of camouflaged objects, striving to obtain more complete segmentation results. Additionally, ICEG proposes a novel edge-guided separated calibration module to remove false predictions to avoid obtaining ambiguous boundaries. Extensive experiments show that ICEG outperforms existing COD detectors and Camouflageator is flexible to improve various COD detectors, including ICEG, which brings state-of-the-art COD performance.",
      "paper_authors": [
        "Chunming He",
        "Kai Li",
        "Yachao Zhang",
        "Yulun Zhang",
        "Zhenhua Guo",
        "Xiu Li",
        "Martin Danelljan",
        "Fisher Yu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-08-06",
      "update_time": "2024-03-10",
      "comments": "Accepted at ICLR 2024",
      "repo_url": "https://github.com/chunminghe/camouflageator"
    },
    "2308.00303": {
      "paper_id": "2308.00303v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2308.00303v2",
      "paper_key": "2308.00303",
      "paper_title": "Diffusion Model for Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2308.00303v2",
      "paper_abstract": "Camouflaged object detection is a challenging task that aims to identify objects that are highly similar to their background. Due to the powerful noise-to-image denoising capability of denoising diffusion models, in this paper, we propose a diffusion-based framework for camouflaged object detection, termed diffCOD, a new framework that considers the camouflaged object segmentation task as a denoising diffusion process from noisy masks to object masks. Specifically, the object mask diffuses from the ground-truth masks to a random distribution, and the designed model learns to reverse this noising process. To strengthen the denoising learning, the input image prior is encoded and integrated into the denoising diffusion model to guide the diffusion process. Furthermore, we design an injection attention module (IAM) to interact conditional semantic features extracted from the image with the diffusion noise embedding via the cross-attention mechanism to enhance denoising learning. Extensive experiments on four widely used COD benchmark datasets demonstrate that the proposed method achieves favorable performance compared to the existing 11 state-of-the-art methods, especially in the detailed texture segmentation of camouflaged objects. Our code will be made publicly available at: https://github.com/ZNan-Chen/diffCOD.",
      "paper_authors": [
        "Zhennan Chen",
        "Rongrong Gao",
        "Tian-Zhu Xiang",
        "Fan Lin"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-08-01",
      "update_time": "2023-08-05",
      "comments": "Accepted by ECAI2023",
      "repo_url": "#"
    },
    "2307.10685": {
      "paper_id": "2307.10685v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.10685v2",
      "paper_key": "2307.10685",
      "paper_title": "Pre-train, Adapt and Detect: Multi-Task Adapter Tuning for Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2307.10685v2",
      "paper_abstract": "Camouflaged object detection (COD), aiming to segment camouflaged objects which exhibit similar patterns with the background, is a challenging task. Most existing works are dedicated to establishing specialized modules to identify camouflaged objects with complete and fine details, while the boundary can not be well located for the lack of object-related semantics. In this paper, we propose a novel ``pre-train, adapt and detect\" paradigm to detect camouflaged objects. By introducing a large pre-trained model, abundant knowledge learned from massive multi-modal data can be directly transferred to COD. A lightweight parallel adapter is inserted to adjust the features suitable for the downstream COD task. Extensive experiments on four challenging benchmark datasets demonstrate that our method outperforms existing state-of-the-art COD models by large margins. Moreover, we design a multi-task learning scheme for tuning the adapter to exploit the shareable knowledge across different semantic classes. Comprehensive experimental results showed that the generalization ability of our model can be substantially improved with multi-task adapter initialization on source tasks and multi-task adaptation on target tasks.",
      "paper_authors": [
        "Yinghui Xing",
        "Dexuan Kong",
        "Shizhou Zhang",
        "Geng Chen",
        "Lingyan Ran",
        "Peng Wang",
        "Yanning Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-07-20",
      "update_time": "2023-08-22",
      "comments": null,
      "repo_url": "#"
    },
    "2307.04651": {
      "paper_id": "2307.04651v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.04651v1",
      "paper_key": "2307.04651",
      "paper_title": "Joint Salient Object Detection and Camouflaged Object Detection via Uncertainty-aware Learning",
      "paper_url": "http://arxiv.org/abs/2307.04651v1",
      "paper_abstract": "Salient objects attract human attention and usually stand out clearly from their surroundings. In contrast, camouflaged objects share similar colors or textures with the environment. In this case, salient objects are typically non-camouflaged, and camouflaged objects are usually not salient. Due to this inherent contradictory attribute, we introduce an uncertainty-aware learning pipeline to extensively explore the contradictory information of salient object detection (SOD) and camouflaged object detection (COD) via data-level and task-wise contradiction modeling. We first exploit the dataset correlation of these two tasks and claim that the easy samples in the COD dataset can serve as hard samples for SOD to improve the robustness of the SOD model. Based on the assumption that these two models should lead to activation maps highlighting different regions of the same input image, we further introduce a contrastive module with a joint-task contrastive learning framework to explicitly model the contradictory attributes of these two tasks. Different from conventional intra-task contrastive learning for unsupervised representation learning, our contrastive module is designed to model the task-wise correlation, leading to cross-task representation learning. To better understand the two tasks from the perspective of uncertainty, we extensively investigate the uncertainty estimation techniques for modeling the main uncertainties of the two tasks, namely task uncertainty (for SOD) and data uncertainty (for COD), and aiming to effectively estimate the challenging regions for each task to achieve difficulty-aware learning. Experimental results on benchmark datasets demonstrate that our solution leads to both state-of-the-art performance and informative uncertainty estimation.",
      "paper_authors": [
        "Aixuan Li",
        "Jing Zhang",
        "Yunqiu Lv",
        "Tong Zhang",
        "Yiran Zhong",
        "Mingyi He",
        "Yuchao Dai"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-07-10",
      "update_time": "2023-07-10",
      "comments": null,
      "repo_url": "#"
    },
    "2307.03943": {
      "paper_id": "2307.03943v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.03943v1",
      "paper_key": "2307.03943",
      "paper_title": "Camouflaged Object Detection with Feature Grafting and Distractor Aware",
      "paper_url": "http://arxiv.org/abs/2307.03943v1",
      "paper_abstract": "The task of Camouflaged Object Detection (COD) aims to accurately segment camouflaged objects that integrated into the environment, which is more challenging than ordinary detection as the texture between the target and background is visually indistinguishable. In this paper, we proposed a novel Feature Grafting and Distractor Aware network (FDNet) to handle the COD task. Specifically, we use CNN and Transformer to encode multi-scale images in parallel. In order to better explore the advantages of the two encoders, we design a cross-attention-based Feature Grafting Module to graft features extracted from Transformer branch into CNN branch, after which the features are aggregated in the Feature Fusion Module. A Distractor Aware Module is designed to explicitly model the two possible distractors in the COD task to refine the coarse camouflage map. We also proposed the largest artificial camouflaged object dataset which contains 2000 images with annotations, named ACOD2K. We conducted extensive experiments on four widely used benchmark datasets and the ACOD2K dataset. The results show that our method significantly outperforms other state-of-the-art methods. The code and the ACOD2K will be available at https://github.com/syxvision/FDNet.",
      "paper_authors": [
        "Yuxuan Song",
        "Xinyue Li",
        "Lin Qi"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-07-08",
      "update_time": "2023-07-08",
      "comments": "ICME2023 paper",
      "repo_url": "https://github.com/syxvision/fdnet"
    },
    "2307.03932": {
      "paper_id": "2307.03932v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2307.03932v1",
      "paper_key": "2307.03932",
      "paper_title": "Edge-Aware Mirror Network for Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2307.03932v1",
      "paper_abstract": "Existing edge-aware camouflaged object detection (COD) methods normally output the edge prediction in the early stage. However, edges are important and fundamental factors in the following segmentation task. Due to the high visual similarity between camouflaged targets and the surroundings, edge prior predicted in early stage usually introduces erroneous foreground-background and contaminates features for segmentation. To tackle this problem, we propose a novel Edge-aware Mirror Network (EAMNet), which models edge detection and camouflaged object segmentation as a cross refinement process. More specifically, EAMNet has a two-branch architecture, where a segmentation-induced edge aggregation module and an edge-induced integrity aggregation module are designed to cross-guide the segmentation branch and edge detection branch. A guided-residual channel attention module which leverages the residual connection and gated convolution finally better extracts structural details from low-level features. Quantitative and qualitative experiment results show that EAMNet outperforms existing cutting-edge baselines on three widely used COD datasets. Codes are available at https://github.com/sdy1999/EAMNet.",
      "paper_authors": [
        "Dongyue Sun",
        "Shiyao Jiang",
        "Lin Qi"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-07-08",
      "update_time": "2023-07-08",
      "comments": "ICME2023 paper",
      "repo_url": "https://github.com/sdy1999/eamnet"
    },
    "2306.07532": {
      "paper_id": "2306.07532v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2306.07532v2",
      "paper_key": "2306.07532",
      "paper_title": "Referring Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2306.07532v2",
      "paper_abstract": "We consider the problem of referring camouflaged object detection (Ref-COD), a new task that aims to segment specified camouflaged objects based on a small set of referring images with salient target objects. We first assemble a large-scale dataset, called R2C7K, which consists of 7K images covering 64 object categories in real-world scenarios. Then, we develop a simple but strong dual-branch framework, dubbed R2CNet, with a reference branch embedding the common representations of target objects from referring images and a segmentation branch identifying and segmenting camouflaged objects under the guidance of the common representations. In particular, we design a Referring Mask Generation module to generate pixel-level prior mask and a Referring Feature Enrichment module to enhance the capability of identifying specified camouflaged objects. Extensive experiments show the superiority of our Ref-COD methods over their COD counterparts in segmenting specified camouflaged objects and identifying the main body of target objects. Our code and dataset are publicly available at https://github.com/zhangxuying1004/RefCOD.",
      "paper_authors": [
        "Xuying Zhang",
        "Bowen Yin",
        "Zheng Lin",
        "Qibin Hou",
        "Deng-Ping Fan",
        "Ming-Ming Cheng"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-06-13",
      "update_time": "2023-07-11",
      "comments": null,
      "repo_url": "https://github.com/zhangxuying1004/refcod"
    },
    "2305.18476": {
      "paper_id": "2305.18476v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2305.18476v1",
      "paper_key": "2305.18476",
      "paper_title": "Explicit Visual Prompting for Universal Foreground Segmentations",
      "paper_url": "http://arxiv.org/abs/2305.18476v1",
      "paper_abstract": "Foreground segmentation is a fundamental problem in computer vision, which includes salient object detection, forgery detection, defocus blur detection, shadow detection, and camouflage object detection. Previous works have typically relied on domain-specific solutions to address accuracy and robustness issues in those applications. In this paper, we present a unified framework for a number of foreground segmentation tasks without any task-specific designs. We take inspiration from the widely-used pre-training and then prompt tuning protocols in NLP and propose a new visual prompting model, named Explicit Visual Prompting (EVP). Different from the previous visual prompting which is typically a dataset-level implicit embedding, our key insight is to enforce the tunable parameters focusing on the explicit visual content from each individual image, i.e., the features from frozen patch embeddings and high-frequency components. Our method freezes a pre-trained model and then learns task-specific knowledge using a few extra parameters. Despite introducing only a small number of tunable parameters, EVP achieves superior performance than full fine-tuning and other parameter-efficient fine-tuning methods. Experiments in fourteen datasets across five tasks show the proposed method outperforms other task-specific methods while being considerably simple. The proposed method demonstrates the scalability in different architectures, pre-trained weights, and tasks. The code is available at: https://github.com/NiFangBaAGe/Explicit-Visual-Prompt.",
      "paper_authors": [
        "Weihuang Liu",
        "Xi Shen",
        "Chi-Man Pun",
        "Xiaodong Cun"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-05-29",
      "update_time": "2023-05-29",
      "comments": "arXiv admin note: substantial text overlap with arXiv:2303.10883",
      "repo_url": "https://github.com/nifangbaage/explicit-visual-prompt"
    },
    "2305.17932": {
      "paper_id": "2305.17932v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2305.17932v1",
      "paper_key": "2305.17932",
      "paper_title": "CamoDiffusion: Camouflaged Object Detection via Conditional Diffusion Models",
      "paper_url": "http://arxiv.org/abs/2305.17932v1",
      "paper_abstract": "Camouflaged Object Detection (COD) is a challenging task in computer vision due to the high similarity between camouflaged objects and their surroundings. Existing COD methods primarily employ semantic segmentation, which suffers from overconfident incorrect predictions. In this paper, we propose a new paradigm that treats COD as a conditional mask-generation task leveraging diffusion models. Our method, dubbed CamoDiffusion, employs the denoising process of diffusion models to iteratively reduce the noise of the mask. Due to the stochastic sampling process of diffusion, our model is capable of sampling multiple possible predictions from the mask distribution, avoiding the problem of overconfident point estimation. Moreover, we develop specialized learning strategies that include an innovative ensemble approach for generating robust predictions and tailored forward diffusion methods for efficient training, specifically for the COD task. Extensive experiments on three COD datasets attest the superior performance of our model compared to existing state-of-the-art methods, particularly on the most challenging COD10K dataset, where our approach achieves 0.019 in terms of MAE.",
      "paper_authors": [
        "Zhongxi Chen",
        "Ke Sun",
        "Xianming Lin",
        "Rongrong Ji"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-05-29",
      "update_time": "2023-05-29",
      "comments": null,
      "repo_url": "https://github.com/rapisurazurite/camodiffusion"
    },
    "2305.12635": {
      "paper_id": "2305.12635v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2305.12635v2",
      "paper_key": "2305.12635",
      "paper_title": "A bioinspired three-stage model for camouflaged object detection",
      "paper_url": "http://arxiv.org/abs/2305.12635v2",
      "paper_abstract": "Camouflaged objects are typically assimilated into their backgrounds and exhibit fuzzy boundaries. The complex environmental conditions and the high intrinsic similarity between camouflaged targets and their surroundings pose significant challenges in accurately locating and segmenting these objects in their entirety. While existing methods have demonstrated remarkable performance in various real-world scenarios, they still face limitations when confronted with difficult cases, such as small targets, thin structures, and indistinct boundaries. Drawing inspiration from human visual perception when observing images containing camouflaged objects, we propose a three-stage model that enables coarse-to-fine segmentation in a single iteration. Specifically, our model employs three decoders to sequentially process subsampled features, cropped features, and high-resolution original features. This proposed approach not only reduces computational overhead but also mitigates interference caused by background noise. Furthermore, considering the significance of multi-scale information, we have designed a multi-scale feature enhancement module that enlarges the receptive field while preserving detailed structural cues. Additionally, a boundary enhancement module has been developed to enhance performance by leveraging boundary information. Subsequently, a mask-guided fusion module is proposed to generate fine-grained results by integrating coarse prediction maps with high-resolution feature maps. Our network surpasses state-of-the-art CNN-based counterparts without unnecessary complexities. Upon acceptance of the paper, the source code will be made publicly available at https://github.com/clelouch/BTSNet.",
      "paper_authors": [
        "Tianyou Chen",
        "Jin Xiao",
        "Xiaoguang Hu",
        "Guofeng Zhang",
        "Shaojie Wang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-05-22",
      "update_time": "2023-06-06",
      "comments": null,
      "repo_url": "#"
    },
    "2305.11513": {
      "paper_id": "2305.11513v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2305.11513v1",
      "paper_key": "2305.11513",
      "paper_title": "When SAM Meets Shadow Detection",
      "paper_url": "http://arxiv.org/abs/2305.11513v1",
      "paper_abstract": "As a promptable generic object segmentation model, segment anything model (SAM) has recently attracted significant attention, and also demonstrates its powerful performance. Nevertheless, it still meets its Waterloo when encountering several tasks, e.g., medical image segmentation, camouflaged object detection, etc. In this report, we try SAM on an unexplored popular task: shadow detection. Specifically, four benchmarks were chosen and evaluated with widely used metrics. The experimental results show that the performance for shadow detection using SAM is not satisfactory, especially when comparing with the elaborate models. Code is available at https://github.com/LeipingJie/SAMSh.",
      "paper_authors": [
        "Leiping Jie",
        "Hui Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-05-19",
      "update_time": "2023-05-19",
      "comments": "Technical Report",
      "repo_url": "https://github.com/leipingjie/samshadow"
    },
    "2304.09148": {
      "paper_id": "2304.09148v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2304.09148v3",
      "paper_key": "2304.09148",
      "paper_title": "SAM Fails to Segment Anything? -- SAM-Adapter: Adapting SAM in Underperformed Scenes: Camouflage, Shadow, Medical Image Segmentation, and More",
      "paper_url": "http://arxiv.org/abs/2304.09148v3",
      "paper_abstract": "The emergence of large models, also known as foundation models, has brought significant advancements to AI research. One such model is Segment Anything (SAM), which is designed for image segmentation tasks. However, as with other foundation models, our experimental findings suggest that SAM may fail or perform poorly in certain segmentation tasks, such as shadow detection and camouflaged object detection (concealed object detection). This study first paves the way for applying the large pre-trained image segmentation model SAM to these downstream tasks, even in situations where SAM performs poorly. Rather than fine-tuning the SAM network, we propose \\textbf{SAM-Adapter}, which incorporates domain-specific information or visual prompts into the segmentation network by using simple yet effective adapters. By integrating task-specific knowledge with general knowledge learnt by the large model, SAM-Adapter can significantly elevate the performance of SAM in challenging tasks as shown in extensive experiments. We can even outperform task-specific network models and achieve state-of-the-art performance in the task we tested: camouflaged object detection, shadow detection. We also tested polyp segmentation (medical image segmentation) and achieves better results. We believe our work opens up opportunities for utilizing SAM in downstream tasks, with potential applications in various fields, including medical image processing, agriculture, remote sensing, and more.",
      "paper_authors": [
        "Tianrun Chen",
        "Lanyun Zhu",
        "Chaotao Ding",
        "Runlong Cao",
        "Yan Wang",
        "Zejian Li",
        "Lingyun Sun",
        "Papa Mao",
        "Ying Zang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-04-18",
      "update_time": "2023-05-02",
      "comments": null,
      "repo_url": "#"
    },
    "2304.07444": {
      "paper_id": "2304.07444v4",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2304.07444v4",
      "paper_key": "2304.07444",
      "paper_title": "The Art of Camouflage: Few-Shot Learning for Animal Detection and Segmentation",
      "paper_url": "http://arxiv.org/abs/2304.07444v4",
      "paper_abstract": "Camouflaged object detection and segmentation is a new and challenging research topic in computer vision. There is a serious issue of lacking data on concealed objects such as camouflaged animals in natural scenes. In this paper, we address the problem of few-shot learning for camouflaged object detection and segmentation. To this end, we first collect a new dataset, CAMO-FS, for the benchmark. As camouflaged instances are challenging to recognize due to their similarity compared to the surroundings, we guide our models to obtain camouflaged features that highly distinguish the instances from the background. In this work, we propose FS-CDIS, a framework to efficiently detect and segment camouflaged instances via two loss functions contributing to the training process. Firstly, the instance triplet loss with the characteristic of differentiating the anchor, which is the mean of all camouflaged foreground points, and the background points are employed to work at the instance level. Secondly, to consolidate the generalization at the class level, we present instance memory storage with the scope of storing camouflaged features of the same category, allowing the model to capture further class-level information during the learning process. The extensive experiments demonstrated that our proposed method achieves state-of-the-art performance on the newly collected dataset. Code is available at https://github.com/danhntd/FS-CDIS.",
      "paper_authors": [
        "Thanh-Danh Nguyen",
        "Anh-Khoa Nguyen Vu",
        "Nhat-Duy Nguyen",
        "Vinh-Tiep Nguyen",
        "Thanh Duc Ngo",
        "Thanh-Toan Do",
        "Minh-Triet Tran",
        "Tam V. Nguyen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-04-15",
      "update_time": "2024-08-06",
      "comments": "IEEE Access 2024",
      "repo_url": "https://github.com/danhntd/FS-CDIS"
    },
    "2304.05469": {
      "paper_id": "2304.05469v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2304.05469v1",
      "paper_key": "2304.05469",
      "paper_title": "CamDiff: Camouflage Image Augmentation via Diffusion Model",
      "paper_url": "http://arxiv.org/abs/2304.05469v1",
      "paper_abstract": "The burgeoning field of camouflaged object detection (COD) seeks to identify objects that blend into their surroundings. Despite the impressive performance of recent models, we have identified a limitation in their robustness, where existing methods may misclassify salient objects as camouflaged ones, despite these two characteristics being contradictory. This limitation may stem from lacking multi-pattern training images, leading to less saliency robustness. To address this issue, we introduce CamDiff, a novel approach inspired by AI-Generated Content (AIGC) that overcomes the scarcity of multi-pattern training images. Specifically, we leverage the latent diffusion model to synthesize salient objects in camouflaged scenes, while using the zero-shot image classification ability of the Contrastive Language-Image Pre-training (CLIP) model to prevent synthesis failures and ensure the synthesized object aligns with the input prompt. Consequently, the synthesized image retains its original camouflage label while incorporating salient objects, yielding camouflage samples with richer characteristics. The results of user studies show that the salient objects in the scenes synthesized by our framework attract the user's attention more; thus, such samples pose a greater challenge to the existing COD models. Our approach enables flexible editing and efficient large-scale dataset generation at a low cost. It significantly enhances COD baselines' training and testing phases, emphasizing robustness across diverse domains. Our newly-generated datasets and source code are available at https://github.com/drlxj/CamDiff.",
      "paper_authors": [
        "Xue-Jing Luo",
        "Shuo Wang",
        "Zongwei Wu",
        "Christos Sakaridis",
        "Yun Cheng",
        "Deng-Ping Fan",
        "Luc Van Gool"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-04-11",
      "update_time": "2023-04-11",
      "comments": null,
      "repo_url": "https://github.com/drlxj/camdiff"
    },
    "2304.04709": {
      "paper_id": "2304.04709v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2304.04709v2",
      "paper_key": "2304.04709",
      "paper_title": "Can SAM Segment Anything? When SAM Meets Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2304.04709v2",
      "paper_abstract": "SAM is a segmentation model recently released by Meta AI Research and has been gaining attention quickly due to its impressive performance in generic object segmentation. However, its ability to generalize to specific scenes such as camouflaged scenes is still unknown. Camouflaged object detection (COD) involves identifying objects that are seamlessly integrated into their surroundings and has numerous practical applications in fields such as medicine, art, and agriculture. In this study, we try to ask if SAM can address the COD task and evaluate the performance of SAM on the COD benchmark by employing maximum segmentation evaluation and camouflage location evaluation. We also compare SAM's performance with 22 state-of-the-art COD methods. Our results indicate that while SAM shows promise in generic object segmentation, its performance on the COD task is limited. This presents an opportunity for further research to explore how to build a stronger SAM that may address the COD task. The results of this paper are provided in \\url{https://github.com/luckybird1994/SAMCOD}.",
      "paper_authors": [
        "Lv Tang",
        "Haoke Xiao",
        "Bo Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-04-10",
      "update_time": "2023-04-11",
      "comments": null,
      "repo_url": "https://github.com/luckybird1994/samcod"
    },
    "2303.14816": {
      "paper_id": "2303.14816v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2303.14816v1",
      "paper_key": "2303.14816",
      "paper_title": "Feature Shrinkage Pyramid for Camouflaged Object Detection with Transformers",
      "paper_url": "http://arxiv.org/abs/2303.14816v1",
      "paper_abstract": "Vision transformers have recently shown strong global context modeling capabilities in camouflaged object detection. However, they suffer from two major limitations: less effective locality modeling and insufficient feature aggregation in decoders, which are not conducive to camouflaged object detection that explores subtle cues from indistinguishable backgrounds. To address these issues, in this paper, we propose a novel transformer-based Feature Shrinkage Pyramid Network (FSPNet), which aims to hierarchically decode locality-enhanced neighboring transformer features through progressive shrinking for camouflaged object detection. Specifically, we propose a nonlocal token enhancement module (NL-TEM) that employs the non-local mechanism to interact neighboring tokens and explore graph-based high-order relations within tokens to enhance local representations of transformers. Moreover, we design a feature shrinkage decoder (FSD) with adjacent interaction modules (AIM), which progressively aggregates adjacent transformer features through a layer-bylayer shrinkage pyramid to accumulate imperceptible but effective cues as much as possible for object information decoding. Extensive quantitative and qualitative experiments demonstrate that the proposed model significantly outperforms the existing 24 competitors on three challenging COD benchmark datasets under six widely-used evaluation metrics. Our code is publicly available at https://github.com/ZhouHuang23/FSPNet.",
      "paper_authors": [
        "Zhou Huang",
        "Hang Dai",
        "Tian-Zhu Xiang",
        "Shuo Wang",
        "Huai-Xin Chen",
        "Jie Qin",
        "Huan Xiong"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-03-26",
      "update_time": "2023-03-26",
      "comments": "CVPR 2023. Project webpage at:\n  https://tzxiang.github.io/project/COD-FSPNet/index.html",
      "repo_url": "https://github.com/zhouhuang23/fspnet"
    },
    "2303.01734": {
      "paper_id": "2303.01734v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2303.01734v2",
      "paper_key": "2303.01734",
      "paper_title": "AdvART: Adversarial Art for Camouflaged Object Detection Attacks",
      "paper_url": "http://arxiv.org/abs/2303.01734v2",
      "paper_abstract": "Physical adversarial attacks pose a significant practical threat as it deceives deep learning systems operating in the real world by producing prominent and maliciously designed physical perturbations. Emphasizing the evaluation of naturalness is crucial in such attacks, as humans can readily detect and eliminate unnatural manipulations. To overcome this limitation, recent work has proposed leveraging generative adversarial networks (GANs) to generate naturalistic patches, which may not catch human's attention. However, these approaches suffer from a limited latent space which leads to an inevitable trade-off between naturalness and attack efficiency. In this paper, we propose a novel approach to generate naturalistic and inconspicuous adversarial patches. Specifically, we redefine the optimization problem by introducing an additional loss term to the cost function. This term works as a semantic constraint to ensure that the generated camouflage pattern holds semantic meaning rather than arbitrary patterns. The additional term leverages similarity metrics to construct a similarity loss that we optimize within the global objective function. Our technique is based on directly manipulating the pixel values in the patch, which gives higher flexibility and larger space compared to the GAN-based techniques that are based on indirectly optimizing the patch by modifying the latent vector. Our attack achieves superior success rate of up to 91.19\\% and 72\\%, respectively, in the digital world and when deployed in smart cameras at the edge compared to the GAN-based technique.",
      "paper_authors": [
        "Amira Guesmi",
        "Ioan Marius Bilasco",
        "Muhammad Shafique",
        "Ihsen Alouani"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-03-03",
      "update_time": "2024-02-09",
      "comments": null,
      "repo_url": "#"
    },
    "2303.12946": {
      "paper_id": "2303.12946v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2303.12946v1",
      "paper_key": "2303.12946",
      "paper_title": "Underwater Camouflage Object Detection Dataset",
      "paper_url": "http://arxiv.org/abs/2303.12946v1",
      "paper_abstract": "We have made a dataset of camouflage object detection mainly for complex seabed scenes, and named it UnderWater RGB&Sonar,or UW-RS for short. The UW-RS dataset contains a total of 1972 image data. The dataset mainly consists of two parts, namely underwater optical data part (UW-R dataset) and underwater sonar data part (UW-S dataset).",
      "paper_authors": [
        "Feng Dong",
        "Jinchao Zhu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-03-01",
      "update_time": "2023-03-01",
      "comments": null,
      "repo_url": "#"
    },
    "2212.08296": {
      "paper_id": "2212.08296v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2212.08296v1",
      "paper_key": "2212.08296",
      "paper_title": "DQnet: Cross-Model Detail Querying for Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2212.08296v1",
      "paper_abstract": "Camouflaged objects are seamlessly blended in with their surroundings, which brings a challenging detection task in computer vision. Optimizing a convolutional neural network (CNN) for camouflaged object detection (COD) tends to activate local discriminative regions while ignoring complete object extent, causing the partial activation issue which inevitably leads to missing or redundant regions of objects. In this paper, we argue that partial activation is caused by the intrinsic characteristics of CNN, where the convolution operations produce local receptive fields and experience difficulty to capture long-range feature dependency among image regions. In order to obtain feature maps that could activate full object extent, keeping the segmental results from being overwhelmed by noisy features, a novel framework termed Cross-Model Detail Querying network (DQnet) is proposed. It reasons the relations between long-range-aware representations and multi-scale local details to make the enhanced representation fully highlight the object regions and eliminate noise on non-object regions. Specifically, a vanilla ViT pretrained with self-supervised learning (SSL) is employed to model long-range dependencies among image regions. A ResNet is employed to enable learning fine-grained spatial local details in multiple scales. Then, to effectively retrieve object-related details, a Relation-Based Querying (RBQ) module is proposed to explore window-based interactions between the global representations and the multi-scale local details. Extensive experiments are conducted on the widely used COD datasets and show that our DQnet outperforms the current state-of-the-arts.",
      "paper_authors": [
        "Wei Sun",
        "Chengao Liu",
        "Linyan Zhang",
        "Yu Li",
        "Pengxu Wei",
        "Chang Liu",
        "Jialing Zou",
        "Jianbin Jiao",
        "Qixiang Ye"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-12-16",
      "update_time": "2022-12-16",
      "comments": null,
      "repo_url": "#"
    },
    "2212.05370": {
      "paper_id": "2212.05370v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2212.05370v3",
      "paper_key": "2212.05370",
      "paper_title": "Source-free Depth for Object Pop-out",
      "paper_url": "http://arxiv.org/abs/2212.05370v3",
      "paper_abstract": "Depth cues are known to be useful for visual perception. However, direct measurement of depth is often impracticable. Fortunately, though, modern learning-based methods offer promising depth maps by inference in the wild. In this work, we adapt such depth inference models for object segmentation using the objects' \"pop-out\" prior in 3D. The \"pop-out\" is a simple composition prior that assumes objects reside on the background surface. Such compositional prior allows us to reason about objects in the 3D space. More specifically, we adapt the inferred depth maps such that objects can be localized using only 3D information. Such separation, however, requires knowledge about contact surface which we learn using the weak supervision of the segmentation mask. Our intermediate representation of contact surface, and thereby reasoning about objects purely in 3D, allows us to better transfer the depth knowledge into semantics. The proposed adaptation method uses only the depth model without needing the source data used for training, making the learning process efficient and practical. Our experiments on eight datasets of two challenging tasks, namely camouflaged object detection and salient object detection, consistently demonstrate the benefit of our method in terms of both performance and generalizability.",
      "paper_authors": [
        "Zongwei Wu",
        "Danda Pani Paudel",
        "Deng-Ping Fan",
        "Jingjing Wang",
        "Shuo Wang",
        "C\u00e9dric Demonceaux",
        "Radu Timofte",
        "Luc Van Gool"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-12-10",
      "update_time": "2023-09-25",
      "comments": "Accepted to ICCV 2023",
      "repo_url": "https://github.com/zongwei97/popnet"
    },
    "2212.06570": {
      "paper_id": "2212.06570v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2212.06570v1",
      "paper_key": "2212.06570",
      "paper_title": "CamoFormer: Masked Separable Attention for Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2212.06570v1",
      "paper_abstract": "How to identify and segment camouflaged objects from the background is challenging. Inspired by the multi-head self-attention in Transformers, we present a simple masked separable attention (MSA) for camouflaged object detection. We first separate the multi-head self-attention into three parts, which are responsible for distinguishing the camouflaged objects from the background using different mask strategies. Furthermore, we propose to capture high-resolution semantic representations progressively based on a simple top-down decoder with the proposed MSA to attain precise segmentation results. These structures plus a backbone encoder form a new model, dubbed CamoFormer. Extensive experiments show that CamoFormer surpasses all existing state-of-the-art methods on three widely-used camouflaged object detection benchmarks. There are on average around 5% relative improvements over previous methods in terms of S-measure and weighted F-measure.",
      "paper_authors": [
        "Bowen Yin",
        "Xuying Zhang",
        "Qibin Hou",
        "Bo-Yuan Sun",
        "Deng-Ping Fan",
        "Luc Van Gool"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-12-10",
      "update_time": "2022-12-10",
      "comments": null,
      "repo_url": "https://github.com/hvision-nku/camoformer"
    },
    "2212.00990": {
      "paper_id": "2212.00990v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2212.00990v1",
      "paper_key": "2212.00990",
      "paper_title": "Feature Aggregation and Propagation Network for Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2212.00990v1",
      "paper_abstract": "Camouflaged object detection (COD) aims to detect/segment camouflaged objects embedded in the environment, which has attracted increasing attention over the past decades. Although several COD methods have been developed, they still suffer from unsatisfactory performance due to the intrinsic similarities between the foreground objects and background surroundings. In this paper, we propose a novel Feature Aggregation and Propagation Network (FAP-Net) for camouflaged object detection. Specifically, we propose a Boundary Guidance Module (BGM) to explicitly model the boundary characteristic, which can provide boundary-enhanced features to boost the COD performance. To capture the scale variations of the camouflaged objects, we propose a Multi-scale Feature Aggregation Module (MFAM) to characterize the multi-scale information from each layer and obtain the aggregated feature representations. Furthermore, we propose a Cross-level Fusion and Propagation Module (CFPM). In the CFPM, the feature fusion part can effectively integrate the features from adjacent layers to exploit the cross-level correlations, and the feature propagation part can transmit valuable context information from the encoder to the decoder network via a gate unit. Finally, we formulate a unified and end-to-end trainable framework where cross-level features can be effectively fused and propagated for capturing rich context information. Extensive experiments on three benchmark camouflaged datasets demonstrate that our FAP-Net outperforms other state-of-the-art COD models. Moreover, our model can be extended to the polyp segmentation task, and the comparison results further validate the effectiveness of the proposed model in segmenting polyps. The source code and results will be released at https://github.com/taozh2017/FAPNet.",
      "paper_authors": [
        "Tao Zhou",
        "Yi Zhou",
        "Chen Gong",
        "Jian Yang",
        "Yu Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-12-02",
      "update_time": "2022-12-02",
      "comments": "12 pages, 6 figures, accepted by IEEE Transactions on Image\n  Processing",
      "repo_url": "https://github.com/taozh2017/fapnet"
    },
    "2211.12048": {
      "paper_id": "2211.12048v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2211.12048v2",
      "paper_key": "2211.12048",
      "paper_title": "Boundary-aware Camouflaged Object Detection via Deformable Point Sampling",
      "paper_url": "http://arxiv.org/abs/2211.12048v2",
      "paper_abstract": "The camouflaged object detection (COD) task aims to identify and segment objects that blend into the background due to their similar color or texture. Despite the inherent difficulties of the task, COD has gained considerable attention in several fields, such as medicine, life-saving, and anti-military fields. In this paper, we propose a novel solution called the Deformable Point Sampling network (DPS-Net) to address the challenges associated with COD. The proposed DPS-Net utilizes a Deformable Point Sampling transformer (DPS transformer) that can effectively capture sparse local boundary information of significant object boundaries in COD using a deformable point sampling method. Moreover, the DPS transformer demonstrates robust COD performance by extracting contextual features for target object localization through integrating rough global positional information of objects with boundary local information. We evaluate our method on three prominent datasets and achieve state-of-the-art performance. Our results demonstrate the effectiveness of the proposed method through comparative experiments.",
      "paper_authors": [
        "Minhyeok Lee",
        "Suhwan Cho",
        "Chaewon Park",
        "Dogyoon Lee",
        "Jungho Lee",
        "Sangyoun Lee"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-11-22",
      "update_time": "2023-03-11",
      "comments": null,
      "repo_url": "#"
    },
    "2210.06361": {
      "paper_id": "2210.06361v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2210.06361v3",
      "paper_key": "2210.06361",
      "paper_title": "MFFN: Multi-view Feature Fusion Network for Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2210.06361v3",
      "paper_abstract": "Recent research about camouflaged object detection (COD) aims to segment highly concealed objects hidden in complex surroundings. The tiny, fuzzy camouflaged objects result in visually indistinguishable properties. However, current single-view COD detectors are sensitive to background distractors. Therefore, blurred boundaries and variable shapes of the camouflaged objects are challenging to be fully captured with a single-view detector. To overcome these obstacles, we propose a behavior-inspired framework, called Multi-view Feature Fusion Network (MFFN), which mimics the human behaviors of finding indistinct objects in images, i.e., observing from multiple angles, distances, perspectives. Specifically, the key idea behind it is to generate multiple ways of observation (multi-view) by data augmentation and apply them as inputs. MFFN captures critical boundary and semantic information by comparing and fusing extracted multi-view features. In addition, our MFFN exploits the dependence and interaction between views and channels. Specifically, our methods leverage the complementary information between different views through a two-stage attention module called Co-attention of Multi-view (CAMV). And we design a local-overall module called Channel Fusion Unit (CFU) to explore the channel-wise contextual clues of diverse feature maps in an iterative manner. The experiment results show that our method performs favorably against existing state-of-the-art methods via training with the same data. The code will be available at https://github.com/dwardzheng/MFFN_COD.",
      "paper_authors": [
        "Dehua Zheng",
        "Xiaochen Zheng",
        "Laurence T. Yang",
        "Yuan Gao",
        "Chenlu Zhu",
        "Yiheng Ruan"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-10-12",
      "update_time": "2022-10-19",
      "comments": "In Proceedings of the 2023 IEEE/CVF Winter Conference on Applications\n  of Computer Vision (WACV)",
      "repo_url": "https://github.com/dwardzheng/mffn_cod"
    },
    "2207.14083": {
      "paper_id": "2207.14083v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2207.14083v2",
      "paper_key": "2207.14083",
      "paper_title": "Weakly-Supervised Camouflaged Object Detection with Scribble Annotations",
      "paper_url": "http://arxiv.org/abs/2207.14083v2",
      "paper_abstract": "Existing camouflaged object detection (COD) methods rely heavily on large-scale datasets with pixel-wise annotations. However, due to the ambiguous boundary, annotating camouflage objects pixel-wisely is very time-consuming and labor-intensive, taking ~60mins to label one image. In this paper, we propose the first weakly-supervised COD method, using scribble annotations as supervision. To achieve this, we first relabel 4,040 images in existing camouflaged object datasets with scribbles, which takes ~10s to label one image. As scribble annotations only describe the primary structure of objects without details, for the network to learn to localize the boundaries of camouflaged objects, we propose a novel consistency loss composed of two parts: a cross-view loss to attain reliable consistency over different images, and an inside-view loss to maintain consistency inside a single prediction map. Besides, we observe that humans use semantic information to segment regions near the boundaries of camouflaged objects. Hence, we further propose a feature-guided loss, which includes visual features directly extracted from images and semantically significant features captured by the model. Finally, we propose a novel network for COD via scribble learning on structural information and semantic relations. Our network has two novel modules: the local-context contrasted (LCC) module, which mimics visual inhibition to enhance image contrast/sharpness and expand the scribbles into potential camouflaged regions, and the logical semantic relation (LSR) module, which analyzes the semantic relation to determine the regions representing the camouflaged object. Experimental results show that our model outperforms relevant SOTA methods on three COD benchmarks with an average improvement of 11.0% on MAE, 3.2% on S-measure, 2.5% on E-measure, and 4.4% on weighted F-measure.",
      "paper_authors": [
        "Ruozhen He",
        "Qihua Dong",
        "Jiaying Lin",
        "Rynson W. H. Lau"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-07-28",
      "update_time": "2022-11-28",
      "comments": "Accepted to AAAI 2023. The code and dataset are available at\n  https://github.com/dddraxxx/Weakly-Supervised-Camouflaged-Object-Detection-with-Scribble-Annotations",
      "repo_url": "https://github.com/dddraxxx/weakly-supervised-camouflaged-object-detection-with-scribble-annotations"
    },
    "2207.13362": {
      "paper_id": "2207.13362v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2207.13362v1",
      "paper_key": "2207.13362",
      "paper_title": "Camouflaged Object Detection via Context-aware Cross-level Fusion",
      "paper_url": "http://arxiv.org/abs/2207.13362v1",
      "paper_abstract": "Camouflaged object detection (COD) aims to identify the objects that conceal themselves in natural scenes. Accurate COD suffers from a number of challenges associated with low boundary contrast and the large variation of object appearances, e.g., object size and shape. To address these challenges, we propose a novel Context-aware Cross-level Fusion Network (C2F-Net), which fuses context-aware cross-level features for accurately identifying camouflaged objects. Specifically, we compute informative attention coefficients from multi-level features with our Attention-induced Cross-level Fusion Module (ACFM), which further integrates the features under the guidance of attention coefficients. We then propose a Dual-branch Global Context Module (DGCM) to refine the fused features for informative feature representations by exploiting rich global context information. Multiple ACFMs and DGCMs are integrated in a cascaded manner for generating a coarse prediction from high-level features. The coarse prediction acts as an attention map to refine the low-level features before passing them to our Camouflage Inference Module (CIM) to generate the final prediction. We perform extensive experiments on three widely used benchmark datasets and compare C2F-Net with state-of-the-art (SOTA) models. The results show that C2F-Net is an effective COD model and outperforms SOTA models remarkably. Further, an evaluation on polyp segmentation datasets demonstrates the promising potentials of our C2F-Net in COD downstream applications. Our code is publicly available at: https://github.com/Ben57882/C2FNet-TSCVT.",
      "paper_authors": [
        "Geng Chen",
        "Si-Jie Liu",
        "Yu-Jia Sun",
        "Ge-Peng Ji",
        "Ya-Feng Wu",
        "Tao Zhou"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-07-27",
      "update_time": "2022-07-27",
      "comments": null,
      "repo_url": "https://github.com/ben57882/c2fnet-tscvt"
    },
    "2207.00794": {
      "paper_id": "2207.00794v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2207.00794v1",
      "paper_key": "2207.00794",
      "paper_title": "Boundary-Guided Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2207.00794v1",
      "paper_abstract": "Camouflaged object detection (COD), segmenting objects that are elegantly blended into their surroundings, is a valuable yet challenging task. Existing deep-learning methods often fall into the difficulty of accurately identifying the camouflaged object with complete and fine object structure. To this end, in this paper, we propose a novel boundary-guided network (BGNet) for camouflaged object detection. Our method explores valuable and extra object-related edge semantics to guide representation learning of COD, which forces the model to generate features that highlight object structure, thereby promoting camouflaged object detection of accurate boundary localization. Extensive experiments on three challenging benchmark datasets demonstrate that our BGNet significantly outperforms the existing 18 state-of-the-art methods under four widely-used evaluation metrics. Our code is publicly available at: https://github.com/thograce/BGNet.",
      "paper_authors": [
        "Yujia Sun",
        "Shuo Wang",
        "Chenglizhao Chen",
        "Tian-Zhu Xiang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-07-02",
      "update_time": "2022-07-02",
      "comments": "Accepted by IJCAI2022",
      "repo_url": "https://github.com/thograce/bgnet"
    },
    "2205.12853": {
      "paper_id": "2205.12853v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2205.12853v2",
      "paper_key": "2205.12853",
      "paper_title": "Deep Gradient Learning for Efficient Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2205.12853v2",
      "paper_abstract": "This paper introduces DGNet, a novel deep framework that exploits object gradient supervision for camouflaged object detection (COD). It decouples the task into two connected branches, i.e., a context and a texture encoder. The essential connection is the gradient-induced transition, representing a soft grouping between context and texture features. Benefiting from the simple but efficient framework, DGNet outperforms existing state-of-the-art COD models by a large margin. Notably, our efficient version, DGNet-S, runs in real-time (80 fps) and achieves comparable results to the cutting-edge model JCSOD-CVPR$_{21}$ with only 6.82% parameters. Application results also show that the proposed DGNet performs well in polyp segmentation, defect detection, and transparent object segmentation tasks. Codes will be made available at https://github.com/GewelsJI/DGNet.",
      "paper_authors": [
        "Ge-Peng Ji",
        "Deng-Ping Fan",
        "Yu-Cheng Chou",
        "Dengxin Dai",
        "Alexander Liniger",
        "Luc Van Gool"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-05-25",
      "update_time": "2022-08-08",
      "comments": "Accepted by Machine Intelligence Research",
      "repo_url": "https://github.com/gewelsji/dgnet"
    },
    "2205.11333": {
      "paper_id": "2205.11333v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2205.11333v2",
      "paper_key": "2205.11333",
      "paper_title": "Towards Deeper Understanding of Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2205.11333v2",
      "paper_abstract": "Preys in the wild evolve to be camouflaged to avoid being recognized by predators. In this way, camouflage acts as a key defence mechanism across species that is critical to survival. To detect and segment the whole scope of a camouflaged object, camouflaged object detection (COD) is introduced as a binary segmentation task, with the binary ground truth camouflage map indicating the exact regions of the camouflaged objects. In this paper, we revisit this task and argue that the binary segmentation setting fails to fully understand the concept of camouflage. We find that explicitly modeling the conspicuousness of camouflaged objects against their particular backgrounds can not only lead to a better understanding about camouflage, but also provide guidance to designing more sophisticated camouflage techniques. Furthermore, we observe that it is some specific parts of camouflaged objects that make them detectable by predators. With the above understanding about camouflaged objects, we present the first triple-task learning framework to simultaneously localize, segment, and rank camouflaged objects, indicating the conspicuousness level of camouflage. As no corresponding datasets exist for either the localization model or the ranking model, we generate localization maps with an eye tracker, which are then processed according to the instance level labels to generate our ranking-based training and testing dataset. We also contribute the largest COD testing set to comprehensively analyse performance of the COD models. Experimental results show that our triple-task learning framework achieves new state-of-the-art, leading to a more explainable COD network. Our code, data, and results are available at: \\url{https://github.com/JingZhang617/COD-Rank-Localize-and-Segment}.",
      "paper_authors": [
        "Yunqiu Lv",
        "Jing Zhang",
        "Yuchao Dai",
        "Aixuan Li",
        "Nick Barnes",
        "Deng-Ping Fan"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-05-23",
      "update_time": "2023-01-03",
      "comments": "IEEE Transactions on Circuits and Systems for Video Technology 2023",
      "repo_url": "https://github.com/JingZhang617/COD-Rank-Localize-and-Segment"
    },
    "2205.10579": {
      "paper_id": "2205.10579v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2205.10579v1",
      "paper_key": "2205.10579",
      "paper_title": "Boosting Camouflaged Object Detection with Dual-Task Interactive Transformer",
      "paper_url": "http://arxiv.org/abs/2205.10579v1",
      "paper_abstract": "Camouflaged object detection intends to discover the concealed objects hidden in the surroundings. Existing methods follow the bio-inspired framework, which first locates the object and second refines the boundary. We argue that the discovery of camouflaged objects depends on the recurrent search for the object and the boundary. The recurrent processing makes the human tired and helpless, but it is just the advantage of the transformer with global search ability. Therefore, a dual-task interactive transformer is proposed to detect both accurate position of the camouflaged object and its detailed boundary. The boundary feature is considered as Query to improve the camouflaged object detection, and meanwhile the object feature is considered as Query to improve the boundary detection. The camouflaged object detection and the boundary detection are fully interacted by multi-head self-attention. Besides, to obtain the initial object feature and boundary feature, transformer-based backbones are adopted to extract the foreground and background. The foreground is just object, while foreground minus background is considered as boundary. Here, the boundary feature can be obtained from blurry boundary region of the foreground and background. Supervised by the object, the background and the boundary ground truth, the proposed model achieves state-of-the-art performance in public datasets. https://github.com/liuzywen/COD",
      "paper_authors": [
        "Zhengyi Liu",
        "Zhili Zhang",
        "Wei Wu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-05-21",
      "update_time": "2022-05-21",
      "comments": "Accepted by ICPR2022",
      "repo_url": "https://github.com/liuzywen/cod"
    },
    "2203.11624": {
      "paper_id": "2203.11624v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2203.11624v2",
      "paper_key": "2203.11624",
      "paper_title": "High-resolution Iterative Feedback Network for Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2203.11624v2",
      "paper_abstract": "Spotting camouflaged objects that are visually assimilated into the background is tricky for both object detection algorithms and humans who are usually confused or cheated by the perfectly intrinsic similarities between the foreground objects and the background surroundings. To tackle this challenge, we aim to extract the high-resolution texture details to avoid the detail degradation that causes blurred vision in edges and boundaries. We introduce a novel HitNet to refine the low-resolution representations by high-resolution features in an iterative feedback manner, essentially a global loop-based connection among the multi-scale resolutions. In addition, an iterative feedback loss is proposed to impose more constraints on each feedback connection. Extensive experiments on four challenging datasets demonstrate that our \\ourmodel~breaks the performance bottleneck and achieves significant improvements compared with 29 state-of-the-art methods. To address the data scarcity in camouflaged scenarios, we provide an application example by employing cross-domain learning to extract the features that can reflect the camouflaged object properties and embed the features into salient objects, thereby generating more camouflaged training samples from the diverse salient object datasets The code will be available at https://github.com/HUuxiaobin/HitNet.",
      "paper_authors": [
        "Xiaobin Hu",
        "Shuo Wang",
        "Xuebin Qin",
        "Hang Dai",
        "Wenqi Ren",
        "Ying Tai",
        "Chengjie Wang",
        "Ling Shao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-03-22",
      "update_time": "2023-02-03",
      "comments": null,
      "repo_url": "https://github.com/huuxiaobin/hitnet"
    },
    "2203.07363": {
      "paper_id": "2203.07363v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2203.07363v2",
      "paper_key": "2203.07363",
      "paper_title": "Implicit Motion Handling for Video Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2203.07363v2",
      "paper_abstract": "We propose a new video camouflaged object detection (VCOD) framework that can exploit both short-term dynamics and long-term temporal consistency to detect camouflaged objects from video frames. An essential property of camouflaged objects is that they usually exhibit patterns similar to the background and thus make them hard to identify from still images. Therefore, effectively handling temporal dynamics in videos becomes the key for the VCOD task as the camouflaged objects will be noticeable when they move. However, current VCOD methods often leverage homography or optical flows to represent motions, where the detection error may accumulate from both the motion estimation error and the segmentation error. On the other hand, our method unifies motion estimation and object segmentation within a single optimization framework. Specifically, we build a dense correlation volume to implicitly capture motions between neighbouring frames and utilize the final segmentation supervision to optimize the implicit motion estimation and segmentation jointly. Furthermore, to enforce temporal consistency within a video sequence, we jointly utilize a spatio-temporal transformer to refine the short-term predictions. Extensive experiments on VCOD benchmarks demonstrate the architectural effectiveness of our approach. We also provide a large-scale VCOD dataset named MoCA-Mask with pixel-level handcrafted ground-truth masks and construct a comprehensive VCOD benchmark with previous methods to facilitate research in this direction. Dataset Link: https://xueliancheng.github.io/SLT-Net-project.",
      "paper_authors": [
        "Xuelian Cheng",
        "Huan Xiong",
        "Deng-Ping Fan",
        "Yiran Zhong",
        "Mehrtash Harandi",
        "Tom Drummond",
        "Zongyuan Ge"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-03-14",
      "update_time": "2022-03-15",
      "comments": "Accepted to CVPR 2022; Xuelian Cheng and Huan Xiong made equal\n  contributions; Corresponding author: Deng-Ping Fan (dengpfan@gmail.com).\n  Dataset: https://xueliancheng.github.io/SLT-Net-project",
      "repo_url": "#"
    },
    "2203.02688": {
      "paper_id": "2203.02688v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2203.02688v1",
      "paper_key": "2203.02688",
      "paper_title": "Zoom In and Out: A Mixed-scale Triplet Network for Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2203.02688v1",
      "paper_abstract": "The recently proposed camouflaged object detection (COD) attempts to segment objects that are visually blended into their surroundings, which is extremely complex and difficult in real-world scenarios. Apart from high intrinsic similarity between the camouflaged objects and their background, the objects are usually diverse in scale, fuzzy in appearance, and even severely occluded. To deal with these problems, we propose a mixed-scale triplet network, \\textbf{ZoomNet}, which mimics the behavior of humans when observing vague images, i.e., zooming in and out. Specifically, our ZoomNet employs the zoom strategy to learn the discriminative mixed-scale semantics by the designed scale integration unit and hierarchical mixed-scale unit, which fully explores imperceptible clues between the candidate objects and background surroundings. Moreover, considering the uncertainty and ambiguity derived from indistinguishable textures, we construct a simple yet effective regularization constraint, uncertainty-aware loss, to promote the model to accurately produce predictions with higher confidence in candidate regions. Without bells and whistles, our proposed highly task-friendly model consistently surpasses the existing 23 state-of-the-art methods on four public datasets. Besides, the superior performance over the recent cutting-edge models on the SOD task also verifies the effectiveness and generality of our model. The code will be available at \\url{https://github.com/lartpang/ZoomNet}.",
      "paper_authors": [
        "Youwei Pang",
        "Xiaoqi Zhao",
        "Tian-Zhu Xiang",
        "Lihe Zhang",
        "Huchuan Lu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2022-03-05",
      "update_time": "2022-03-05",
      "comments": "Accepted by CVPR2022. This is the arxiv version that contains the\n  appendix section",
      "repo_url": "https://github.com/lartpang/zoomnet"
    },
    "2111.11430": {
      "paper_id": "2111.11430v6",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2111.11430v6",
      "paper_key": "2111.11430",
      "paper_title": "Class-agnostic Object Detection with Multi-modal Transformer",
      "paper_url": "http://arxiv.org/abs/2111.11430v6",
      "paper_abstract": "What constitutes an object? This has been a long-standing question in computer vision. Towards this goal, numerous learning-free and learning-based approaches have been developed to score objectness. However, they generally do not scale well across new domains and novel objects. In this paper, we advocate that existing methods lack a top-down supervision signal governed by human-understandable semantics. For the first time in literature, we demonstrate that Multi-modal Vision Transformers (MViT) trained with aligned image-text pairs can effectively bridge this gap. Our extensive experiments across various domains and novel objects show the state-of-the-art performance of MViTs to localize generic objects in images. Based on the observation that existing MViTs do not include multi-scale feature processing and usually require longer training schedules, we develop an efficient MViT architecture using multi-scale deformable attention and late vision-language fusion. We show the significance of MViT proposals in a diverse range of applications including open-world object detection, salient and camouflage object detection, supervised and self-supervised detection tasks. Further, MViTs can adaptively generate proposals given a specific language query and thus offer enhanced interactability. Code: \\url{https://git.io/J1HPY}.",
      "paper_authors": [
        "Muhammad Maaz",
        "Hanoona Rasheed",
        "Salman Khan",
        "Fahad Shahbaz Khan",
        "Rao Muhammad Anwer",
        "Ming-Hsuan Yang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-11-22",
      "update_time": "2022-07-19",
      "comments": "Accepted at ECCV 2022",
      "repo_url": "https://github.com/mmaaz60/mvits_for_class_agnostic_od"
    },
    "2111.11055": {
      "paper_id": "2111.11055v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2111.11055v1",
      "paper_key": "2111.11055",
      "paper_title": "Dense Uncertainty Estimation via an Ensemble-based Conditional Latent Variable Model",
      "paper_url": "http://arxiv.org/abs/2111.11055v1",
      "paper_abstract": "Uncertainty estimation has been extensively studied in recent literature, which can usually be classified as aleatoric uncertainty and epistemic uncertainty. In current aleatoric uncertainty estimation frameworks, it is often neglected that the aleatoric uncertainty is an inherent attribute of the data and can only be correctly estimated with an unbiased oracle model. Since the oracle model is inaccessible in most cases, we propose a new sampling and selection strategy at train time to approximate the oracle model for aleatoric uncertainty estimation. Further, we show a trivial solution in the dual-head based heteroscedastic aleatoric uncertainty estimation framework and introduce a new uncertainty consistency loss to avoid it. For epistemic uncertainty estimation, we argue that the internal variable in a conditional latent variable model is another source of epistemic uncertainty to model the predictive distribution and explore the limited knowledge about the hidden true model. We validate our observation on a dense prediction task, i.e., camouflaged object detection. Our results show that our solution achieves both accurate deterministic results and reliable uncertainty estimation.",
      "paper_authors": [
        "Jing Zhang",
        "Yuchao Dai",
        "Mehrtash Harandi",
        "Yiran Zhong",
        "Nick Barnes",
        "Richard Hartley"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-11-22",
      "update_time": "2021-11-22",
      "comments": null,
      "repo_url": "#"
    },
    "2111.03216": {
      "paper_id": "2111.03216v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2111.03216v1",
      "paper_key": "2111.03216",
      "paper_title": "Fast Camouflaged Object Detection via Edge-based Reversible Re-calibration Network",
      "paper_url": "http://arxiv.org/abs/2111.03216v1",
      "paper_abstract": "Camouflaged Object Detection (COD) aims to detect objects with similar patterns (e.g., texture, intensity, colour, etc) to their surroundings, and recently has attracted growing research interest. As camouflaged objects often present very ambiguous boundaries, how to determine object locations as well as their weak boundaries is challenging and also the key to this task. Inspired by the biological visual perception process when a human observer discovers camouflaged objects, this paper proposes a novel edge-based reversible re-calibration network called ERRNet. Our model is characterized by two innovative designs, namely Selective Edge Aggregation (SEA) and Reversible Re-calibration Unit (RRU), which aim to model the visual perception behaviour and achieve effective edge prior and cross-comparison between potential camouflaged regions and background. More importantly, RRU incorporates diverse priors with more comprehensive information comparing to existing COD models. Experimental results show that ERRNet outperforms existing cutting-edge baselines on three COD datasets and five medical image segmentation datasets. Especially, compared with the existing top-1 model SINet, ERRNet significantly improves the performance by $\\sim$6% (mean E-measure) with notably high speed (79.3 FPS), showing that ERRNet could be a general and robust solution for the COD task.",
      "paper_authors": [
        "Ge-Peng Ji",
        "Lei Zhu",
        "Mingchen Zhuge",
        "Keren Fu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-11-05",
      "update_time": "2021-11-05",
      "comments": "35 pages, 7 figures, 5 tables (Accepted by Pattern Recognition 2022)",
      "repo_url": "https://github.com/gewelsji/errnet"
    },
    "2110.15606": {
      "paper_id": "2110.15606v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2110.15606v1",
      "paper_key": "2110.15606",
      "paper_title": "Improving Camouflaged Object Detection with the Uncertainty of Pseudo-edge Labels",
      "paper_url": "http://arxiv.org/abs/2110.15606v1",
      "paper_abstract": "This paper focuses on camouflaged object detection (COD), which is a task to detect objects hidden in the background. Most of the current COD models aim to highlight the target object directly while outputting ambiguous camouflaged boundaries. On the other hand, the performance of the models considering edge information is not yet satisfactory. To this end, we propose a new framework that makes full use of multiple visual cues, i.e., saliency as well as edges, to refine the predicted camouflaged map. This framework consists of three key components, i.e., a pseudo-edge generator, a pseudo-map generator, and an uncertainty-aware refinement module. In particular, the pseudo-edge generator estimates the boundary that outputs the pseudo-edge label, and the conventional COD method serves as the pseudo-map generator that outputs the pseudo-map label. Then, we propose an uncertainty-based module to reduce the uncertainty and noise of such two pseudo labels, which takes both pseudo labels as input and outputs an edge-accurate camouflaged map. Experiments on various COD datasets demonstrate the effectiveness of our method with superior performance to the existing state-of-the-art methods.",
      "paper_authors": [
        "Nobukatsu Kajiura",
        "Hong Liu",
        "Shin'ichi Satoh"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-10-29",
      "update_time": "2021-10-29",
      "comments": "Accepted to ACM Multimedia Asia 2021",
      "repo_url": "https://github.com/nobukatsu-kajiura/UR-COD"
    },
    "2108.08162": {
      "paper_id": "2108.08162v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2108.08162v2",
      "paper_key": "2108.08162",
      "paper_title": "Specificity-preserving RGB-D Saliency Detection",
      "paper_url": "http://arxiv.org/abs/2108.08162v2",
      "paper_abstract": "Salient object detection (SOD) on RGB and depth images has attracted more and more research interests, due to its effectiveness and the fact that depth cues can now be conveniently captured. Existing RGB-D SOD models usually adopt different fusion strategies to learn a shared representation from the two modalities (\\ie, RGB and depth), while few methods explicitly consider how to preserve modality-specific characteristics. In this study, we propose a novel framework, termed SPNet} (Specificity-preserving network), which benefits SOD performance by exploring both the shared information and modality-specific properties (\\eg, specificity). Specifically, we propose to adopt two modality-specific networks and a shared learning network to generate individual and shared saliency prediction maps, respectively. To effectively fuse cross-modal features in the shared learning network, we propose a cross-enhanced integration module (CIM) and then propagate the fused feature to the next layer for integrating cross-level information. Moreover, to capture rich complementary multi-modal information for boosting the SOD performance, we propose a multi-modal feature aggregation (MFA) module to integrate the modality-specific features from each individual decoder into the shared decoder. By using a skip connection, the hierarchical features between the encoder and decoder layers can be fully combined. Extensive experiments demonstrate that our~\\ours~outperforms cutting-edge approaches on six popular RGB-D SOD and three camouflaged object detection benchmarks. The project is publicly available at: https://github.com/taozh2017/SPNet.",
      "paper_authors": [
        "Tao Zhou",
        "Deng-Ping Fan",
        "Geng Chen",
        "Yi Zhou",
        "Huazhu Fu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-08-18",
      "update_time": "2022-01-09",
      "comments": "This is an extensive version and has been accepted by Computational\n  Visual Media",
      "repo_url": "https://github.com/taozh2017/RGBD-SODsurvey"
    },
    "2106.13217": {
      "paper_id": "2106.13217v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2106.13217v3",
      "paper_key": "2106.13217",
      "paper_title": "Exploring Depth Contribution for Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2106.13217v3",
      "paper_abstract": "Camouflaged object detection (COD) aims to segment camouflaged objects hiding in the environment, which is challenging due to the similar appearance of camouflaged objects and their surroundings. Research in biology suggests depth can provide useful object localization cues for camouflaged object discovery. In this paper, we study the depth contribution for camouflaged object detection, where the depth maps are generated with existing monocular depth estimation (MDE) methods. Due to the domain gap between the MDE dataset and our COD dataset, the generated depth maps are not accurate enough to be directly used. We then introduce two solutions to avoid the noisy depth maps from dominating the training process. Firstly, we present an auxiliary depth estimation branch (\"ADE\"), aiming to regress the depth maps. We find that \"ADE\" is especially necessary for our \"generated depth\" scenario. Secondly, we introduce a multi-modal confidence-aware loss function via a generative adversarial network to weigh the contribution of depth for camouflaged object detection. Our extensive experiments on various camouflaged object detection datasets explain that the existing \"sensor depth\" based RGB-D segmentation techniques work poorly with \"generated depth\", and our proposed two solutions work cooperatively, achieving effective depth contribution exploration for camouflaged object detection.",
      "paper_authors": [
        "Mochu Xiang",
        "Jing Zhang",
        "Yunqiu Lv",
        "Aixuan Li",
        "Yiran Zhong",
        "Yuchao Dai"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-06-24",
      "update_time": "2022-01-13",
      "comments": "The first work in RGB-D Camouflaged object detection (COD)",
      "repo_url": "#"
    },
    "2106.11641": {
      "paper_id": "2106.11641v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2106.11641v1",
      "paper_key": "2106.11641",
      "paper_title": "Confidence-Aware Learning for Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2106.11641v1",
      "paper_abstract": "Confidence-aware learning is proven as an effective solution to prevent networks becoming overconfident. We present a confidence-aware camouflaged object detection framework using dynamic supervision to produce both accurate camouflage map and meaningful \"confidence\" representing model awareness about the current prediction. A camouflaged object detection network is designed to produce our camouflage prediction. Then, we concatenate it with the input image and feed it to the confidence estimation network to produce an one channel confidence map.We generate dynamic supervision for the confidence estimation network, representing the agreement of camouflage prediction with the ground truth camouflage map. With the produced confidence map, we introduce confidence-aware learning with the confidence map as guidance to pay more attention to the hard/low-confidence pixels in the loss function. We claim that, once trained, our confidence estimation network can evaluate pixel-wise accuracy of the prediction without relying on the ground truth camouflage map. Extensive results on four camouflaged object detection testing datasets illustrate the superior performance of the proposed model in explaining the camouflage prediction.",
      "paper_authors": [
        "Jiawei Liu",
        "Jing Zhang",
        "Nick Barnes"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-06-22",
      "update_time": "2021-06-22",
      "comments": null,
      "repo_url": "#"
    },
    "2105.12555": {
      "paper_id": "2105.12555v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2105.12555v1",
      "paper_key": "2105.12555",
      "paper_title": "Context-aware Cross-level Fusion Network for Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2105.12555v1",
      "paper_abstract": "Camouflaged object detection (COD) is a challenging task due to the low boundary contrast between the object and its surroundings. In addition, the appearance of camouflaged objects varies significantly, e.g., object size and shape, aggravating the difficulties of accurate COD. In this paper, we propose a novel Context-aware Cross-level Fusion Network (C2F-Net) to address the challenging COD task. Specifically, we propose an Attention-induced Cross-level Fusion Module (ACFM) to integrate the multi-level features with informative attention coefficients. The fused features are then fed to the proposed Dual-branch Global Context Module (DGCM), which yields multi-scale feature representations for exploiting rich global context information. In C2F-Net, the two modules are conducted on high-level features using a cascaded manner. Extensive experiments on three widely used benchmark datasets demonstrate that our C2F-Net is an effective COD model and outperforms state-of-the-art models remarkably. Our code is publicly available at: https://github.com/thograce/C2FNet.",
      "paper_authors": [
        "Yujia Sun",
        "Geng Chen",
        "Tao Zhou",
        "Yi Zhang",
        "Nian Liu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-05-26",
      "update_time": "2021-05-26",
      "comments": "7 pages, 4 figures. Accepted by IJCAI-2021",
      "repo_url": "https://github.com/thograce/C2FNet"
    },
    "2104.02628": {
      "paper_id": "2104.02628v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2104.02628v1",
      "paper_key": "2104.02628",
      "paper_title": "Uncertainty-aware Joint Salient Object and Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2104.02628v1",
      "paper_abstract": "Visual salient object detection (SOD) aims at finding the salient object(s) that attract human attention, while camouflaged object detection (COD) on the contrary intends to discover the camouflaged object(s) that hidden in the surrounding. In this paper, we propose a paradigm of leveraging the contradictory information to enhance the detection ability of both salient object detection and camouflaged object detection. We start by exploiting the easy positive samples in the COD dataset to serve as hard positive samples in the SOD task to improve the robustness of the SOD model. Then, we introduce a similarity measure module to explicitly model the contradicting attributes of these two tasks. Furthermore, considering the uncertainty of labeling in both tasks' datasets, we propose an adversarial learning network to achieve both higher order similarity measure and network confidence estimation. Experimental results on benchmark datasets demonstrate that our solution leads to state-of-the-art (SOTA) performance for both tasks.",
      "paper_authors": [
        "Aixuan Li",
        "Jing Zhang",
        "Yunqiu Lv",
        "Bowen Liu",
        "Tong Zhang",
        "Yuchao Dai"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-04-06",
      "update_time": "2021-04-06",
      "comments": "Accepted to IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2021. Aixuan Li and Jing Zhang contributed equally",
      "repo_url": "https://github.com/JingZhang617/Joint_COD_SOD"
    },
    "2104.02613": {
      "paper_id": "2104.02613v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2104.02613v1",
      "paper_key": "2104.02613",
      "paper_title": "Mutual Graph Learning for Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2104.02613v1",
      "paper_abstract": "Automatically detecting/segmenting object(s) that blend in with their surroundings is difficult for current models. A major challenge is that the intrinsic similarities between such foreground objects and background surroundings make the features extracted by deep model indistinguishable. To overcome this challenge, an ideal model should be able to seek valuable, extra clues from the given scene and incorporate them into a joint learning framework for representation co-enhancement. With this inspiration, we design a novel Mutual Graph Learning (MGL) model, which generalizes the idea of conventional mutual learning from regular grids to the graph domain. Specifically, MGL decouples an image into two task-specific feature maps -- one for roughly locating the target and the other for accurately capturing its boundary details -- and fully exploits the mutual benefits by recurrently reasoning their high-order relations through graphs. Importantly, in contrast to most mutual learning approaches that use a shared function to model all between-task interactions, MGL is equipped with typed functions for handling different complementary relations to maximize information interactions. Experiments on challenging datasets, including CHAMELEON, CAMO and COD10K, demonstrate the effectiveness of our MGL with superior performance to existing state-of-the-art methods.",
      "paper_authors": [
        "Qiang Zhai",
        "Xin Li",
        "Fan Yang",
        "Chenglizhao Chen",
        "Hong Cheng",
        "Deng-Ping Fan"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-04-03",
      "update_time": "2021-04-03",
      "comments": null,
      "repo_url": "https://github.com/fanyang587/MGL"
    },
    "2103.13279": {
      "paper_id": "2103.13279v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2103.13279v2",
      "paper_key": "2103.13279",
      "paper_title": "FakeMix Augmentation Improves Transparent Object Detection",
      "paper_url": "http://arxiv.org/abs/2103.13279v2",
      "paper_abstract": "Detecting transparent objects in natural scenes is challenging due to the low contrast in texture, brightness and colors. Recent deep-learning-based works reveal that it is effective to leverage boundaries for transparent object detection (TOD). However, these methods usually encounter boundary-related imbalance problem, leading to limited generation capability. Detailly, a kind of boundaries in the background, which share the same characteristics with boundaries of transparent objects but have much smaller amounts, usually hurt the performance. To conquer the boundary-related imbalance problem, we propose a novel content-dependent data augmentation method termed FakeMix. Considering collecting these trouble-maker boundaries in the background is hard without corresponding annotations, we elaborately generate them by appending the boundaries of transparent objects from other samples into the current image during training, which adjusts the data space and improves the generalization of the models. Further, we present AdaptiveASPP, an enhanced version of ASPP, that can capture multi-scale and cross-modality features dynamically. Extensive experiments demonstrate that our methods clearly outperform the state-of-the-art methods. We also show that our approach can also transfer well on related tasks, in which the model meets similar troubles, such as mirror detection, glass detection, and camouflaged object detection. Code will be made publicly available.",
      "paper_authors": [
        "Yang Cao",
        "Zhengqiang Zhang",
        "Enze Xie",
        "Qibin Hou",
        "Kai Zhao",
        "Xiangui Luo",
        "Jian Tuo"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-03-24",
      "update_time": "2021-10-19",
      "comments": null,
      "repo_url": "https://github.com/yangcao1996/fanet"
    },
    "2103.04011": {
      "paper_id": "2103.04011v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2103.04011v2",
      "paper_key": "2103.04011",
      "paper_title": "Simultaneously Localize, Segment and Rank the Camouflaged Objects",
      "paper_url": "http://arxiv.org/abs/2103.04011v2",
      "paper_abstract": "Camouflage is a key defence mechanism across species that is critical to survival. Common strategies for camouflage include background matching, imitating the color and pattern of the environment, and disruptive coloration, disguising body outlines [35]. Camouflaged object detection (COD) aims to segment camouflaged objects hiding in their surroundings. Existing COD models are built upon binary ground truth to segment the camouflaged objects without illustrating the level of camouflage. In this paper, we revisit this task and argue that explicitly modeling the conspicuousness of camouflaged objects against their particular backgrounds can not only lead to a better understanding about camouflage and evolution of animals, but also provide guidance to design more sophisticated camouflage techniques. Furthermore, we observe that it is some specific parts of the camouflaged objects that make them detectable by predators. With the above understanding about camouflaged objects, we present the first ranking based COD network (Rank-Net) to simultaneously localize, segment and rank camouflaged objects. The localization model is proposed to find the discriminative regions that make the camouflaged object obvious. The segmentation model segments the full scope of the camouflaged objects. And, the ranking model infers the detectability of different camouflaged objects. Moreover, we contribute a large COD testing set to evaluate the generalization ability of COD models. Experimental results show that our model achieves new state-of-the-art, leading to a more interpretable COD network.",
      "paper_authors": [
        "Yunqiu Lv",
        "Jing Zhang",
        "Yuchao Dai",
        "Aixuan Li",
        "Bowen Liu",
        "Nick Barnes",
        "Deng-Ping Fan"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-03-06",
      "update_time": "2021-04-13",
      "comments": "Accepted to IEEE/CVF CVPR 2021. Our code and dataset are publicly\n  available at https://github.com/JingZhang617/COD-Rank-Localize-and-Segment",
      "repo_url": "https://github.com/JingZhang617/COD-Rank-Localize-and-Segment"
    },
    "2102.02996": {
      "paper_id": "2102.02996v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2102.02996v1",
      "paper_key": "2102.02996",
      "paper_title": "Deep Texture-Aware Features for Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2102.02996v1",
      "paper_abstract": "Camouflaged object detection is a challenging task that aims to identify objects having similar texture to the surroundings. This paper presents to amplify the subtle texture difference between camouflaged objects and the background for camouflaged object detection by formulating multiple texture-aware refinement modules to learn the texture-aware features in a deep convolutional neural network. The texture-aware refinement module computes the covariance matrices of feature responses to extract the texture information, designs an affinity loss to learn a set of parameter maps that help to separate the texture between camouflaged objects and the background, and adopts a boundary-consistency loss to explore the object detail structures.We evaluate our network on the benchmark dataset for camouflaged object detection both qualitatively and quantitatively. Experimental results show that our approach outperforms various state-of-the-art methods by a large margin.",
      "paper_authors": [
        "Jingjing Ren",
        "Xiaowei Hu",
        "Lei Zhu",
        "Xuemiao Xu",
        "Yangyang Xu",
        "Weiming Wang",
        "Zijun Deng",
        "Pheng-Ann Heng"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-02-05",
      "update_time": "2021-02-05",
      "comments": null,
      "repo_url": "#"
    },
    "2101.05687": {
      "paper_id": "2101.05687v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2101.05687v3",
      "paper_key": "2101.05687",
      "paper_title": "Towards Accurate Camouflaged Object Detection with Mixture Convolution and Interactive Fusion",
      "paper_url": "http://arxiv.org/abs/2101.05687v3",
      "paper_abstract": "Camouflaged object detection (COD), which aims to identify the objects that conceal themselves into the surroundings, has recently drawn increasing research efforts in the field of computer vision. In practice, the success of deep learning based COD is mainly determined by two key factors, including (i) A significantly large receptive field, which provides rich context information, and (ii) An effective fusion strategy, which aggregates the rich multi-level features for accurate COD. Motivated by these observations, in this paper, we propose a novel deep learning based COD approach, which integrates the large receptive field and effective feature fusion into a unified framework. Specifically, we first extract multi-level features from a backbone network. The resulting features are then fed to the proposed dual-branch mixture convolution modules, each of which utilizes multiple asymmetric convolutional layers and two dilated convolutional layers to extract rich context features from a large receptive field. Finally, we fuse the features using specially-designed multilevel interactive fusion modules, each of which employs an attention mechanism along with feature interaction for effective feature fusion. Our method detects camouflaged objects with an effective fusion strategy, which aggregates the rich context information from a large receptive field. All of these designs meet the requirements of COD well, allowing the accurate detection of camouflaged objects. Extensive experiments on widely-used benchmark datasets demonstrate that our method is capable of accurately detecting camouflaged objects and outperforms the state-of-the-art methods.",
      "paper_authors": [
        "Geng Chen",
        "Xinrui Chen",
        "Bo Dong",
        "Mingchen Zhuge",
        "Yongxiong Wang",
        "Hongbo Bi",
        "Jian Chen",
        "Peng Wang",
        "Yanning Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2021-01-14",
      "update_time": "2024-07-19",
      "comments": null,
      "repo_url": "#"
    },
    "2012.13581": {
      "paper_id": "2012.13581v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2012.13581v1",
      "paper_key": "2012.13581",
      "paper_title": "Camouflaged Object Detection and Tracking: A Survey",
      "paper_url": "http://arxiv.org/abs/2012.13581v1",
      "paper_abstract": "Moving object detection and tracking have various applications, including surveillance, anomaly detection, vehicle navigation, etc. The literature on object detection and tracking is rich enough, and several essential survey papers exist. However, the research on camouflage object detection and tracking limited due to the complexity of the problem. Existing work on this problem has been done based on either biological characteristics of the camouflaged objects or computer vision techniques. In this article, we review the existing camouflaged object detection and tracking techniques using computer vision algorithms from the theoretical point of view. This article also addresses several issues of interest as well as future research direction on this area. We hope this review will help the reader to learn the recent advances in camouflaged object detection and tracking.",
      "paper_authors": [
        "Ajoy Mondal"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2020-12-25",
      "update_time": "2020-12-25",
      "comments": null,
      "repo_url": "#"
    },
    "2409.01686": {
      "paper_id": "2409.01686v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.01686v1",
      "paper_key": "2409.01686",
      "paper_title": "Frequency-Spatial Entanglement Learning for Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2409.01686v1",
      "paper_abstract": "Camouflaged object detection has attracted a lot of attention in computer vision. The main challenge lies in the high degree of similarity between camouflaged objects and their surroundings in the spatial domain, making identification difficult. Existing methods attempt to reduce the impact of pixel similarity by maximizing the distinguishing ability of spatial features with complicated design, but often ignore the sensitivity and locality of features in the spatial domain, leading to sub-optimal results. In this paper, we propose a new approach to address this issue by jointly exploring the representation in the frequency and spatial domains, introducing the Frequency-Spatial Entanglement Learning (FSEL) method. This method consists of a series of well-designed Entanglement Transformer Blocks (ETB) for representation learning, a Joint Domain Perception Module for semantic enhancement, and a Dual-domain Reverse Parser for feature integration in the frequency and spatial domains. Specifically, the ETB utilizes frequency self-attention to effectively characterize the relationship between different frequency bands, while the entanglement feed-forward network facilitates information interaction between features of different domains through entanglement learning. Our extensive experiments demonstrate the superiority of our FSEL over 21 state-of-the-art methods, through comprehensive quantitative and qualitative comparisons in three widely-used datasets. The source code is available at: https://github.com/CSYSI/FSEL.",
      "paper_authors": [
        "Yanguang Sun",
        "Chunyan Xu",
        "Jian Yang",
        "Hanyu Xuan",
        "Lei Luo"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-03",
      "update_time": "2024-09-03",
      "comments": "Accepted at ECCV 2024",
      "repo_url": "https://github.com/csysi/fsel"
    },
    "2409.09588": {
      "paper_id": "2409.09588v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09588v1",
      "paper_key": "2409.09588",
      "paper_title": "GLCONet: Learning Multi-source Perception Representation for Camouflaged Object Detection",
      "paper_url": "http://arxiv.org/abs/2409.09588v1",
      "paper_abstract": "Recently, biological perception has been a powerful tool for handling the camouflaged object detection (COD) task. However, most existing methods are heavily dependent on the local spatial information of diverse scales from convolutional operations to optimize initial features. A commonly neglected point in these methods is the long-range dependencies between feature pixels from different scale spaces that can help the model build a global structure of the object, inducing a more precise image representation. In this paper, we propose a novel Global-Local Collaborative Optimization Network, called GLCONet. Technically, we first design a collaborative optimization strategy from the perspective of multi-source perception to simultaneously model the local details and global long-range relationships, which can provide features with abundant discriminative information to boost the accuracy in detecting camouflaged objects. Furthermore, we introduce an adjacent reverse decoder that contains cross-layer aggregation and reverse optimization to integrate complementary information from different levels for generating high-quality representations. Extensive experiments demonstrate that the proposed GLCONet method with different backbones can effectively activate potentially significant pixels in an image, outperforming twenty state-of-the-art methods on three public COD datasets. The source code is available at: \\https://github.com/CSYSI/GLCONet.",
      "paper_authors": [
        "Yanguang Sun",
        "Hanyu Xuan",
        "Jian Yang",
        "Lei Luo"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": "Accepted at TNNLS 2024",
      "repo_url": "https://github.com/csysi/glconet"
    }
  },
  "Change Detection": {
    "2408.15993": {
      "paper_id": "2408.15993v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.15993v1",
      "paper_key": "2408.15993",
      "paper_title": "ClimDetect: A Benchmark Dataset for Climate Change Detection and Attribution",
      "paper_url": "http://arxiv.org/abs/2408.15993v1",
      "paper_abstract": "Detecting and attributing temperature increases due to climate change is crucial for understanding global warming and guiding adaptation strategies. The complexity of distinguishing human-induced climate signals from natural variability has challenged traditional detection and attribution (D&A) approaches, which seek to identify specific \"fingerprints\" in climate response variables. Deep learning offers potential for discerning these complex patterns in expansive spatial datasets. However, lack of standard protocols has hindered consistent comparisons across studies. We introduce ClimDetect, a standardized dataset of over 816k daily climate snapshots, designed to enhance model accuracy in identifying climate change signals. ClimDetect integrates various input and target variables used in past research, ensuring comparability and consistency. We also explore the application of vision transformers (ViT) to climate data, a novel and modernizing approach in this context. Our open-access data and code serve as a benchmark for advancing climate science through improved model evaluations. ClimDetect is publicly accessible via Huggingface dataet respository at: https://huggingface.co/datasets/ClimDetect/ClimDetect.",
      "paper_authors": [
        "Sungduk Yu",
        "Brian L. White",
        "Anahita Bhiwandiwalla",
        "Musashi Hinck",
        "Matthew Lyle Olson",
        "Tung Nguyen",
        "Vasudev Lal"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-28",
      "update_time": "2024-08-28",
      "comments": null,
      "repo_url": "#"
    },
    "2408.15689": {
      "paper_id": "2408.15689v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.15689v1",
      "paper_key": "2408.15689",
      "paper_title": "TempoFormer: A Transformer for Temporally-aware Representations in Change Detection",
      "paper_url": "http://arxiv.org/abs/2408.15689v1",
      "paper_abstract": "Dynamic representation learning plays a pivotal role in understanding the evolution of linguistic content over time. On this front both context and time dynamics as well as their interplay are of prime importance. Current approaches model context via pre-trained representations, which are typically temporally agnostic. Previous work on modeling context and temporal dynamics has used recurrent methods, which are slow and prone to overfitting. Here we introduce TempoFormer, the fist task-agnostic transformer-based and temporally-aware model for dynamic representation learning. Our approach is jointly trained on inter and intra context dynamics and introduces a novel temporal variation of rotary positional embeddings. The architecture is flexible and can be used as the temporal representation foundation of other models or applied to different transformer-based architectures. We show new SOTA performance on three different real-time change detection tasks.",
      "paper_authors": [
        "Talia Tseriotou",
        "Adam Tsakalidis",
        "Maria Liakata"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-08-28",
      "update_time": "2024-08-28",
      "comments": null,
      "repo_url": "#"
    },
    "2408.15678": {
      "paper_id": "2408.15678v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.15678v2",
      "paper_key": "2408.15678",
      "paper_title": "Deep Learning Based Speckle Filtering for Polarimetric SAR Images. Application to Sentinel-1",
      "paper_url": "http://arxiv.org/abs/2408.15678v2",
      "paper_abstract": "Speckle suppression in synthetic aperture radar (SAR) images is a key processing step which continues to be a research topic. A wide variety of methods, using either spatially-based approaches or transform-based strategies, have been developed and have shown to provide outstanding results. However, recent advances in deep learning techniques and their application to SAR image despeckling have been demonstrated to offer state-of-the-art results. Unfortunately, they have been mostly applied to single-polarimetric images. The extension of a deep learning-based approach for speckle removal to polarimetric SAR (PolSAR) images is complicated because of the complex nature of the measured covariance matrices for every image pixel, the properties of which must be preserved during filtering. In this work, we propose a complete framework to remove speckle in polarimetric SAR images using a convolutional neural network. The methodology includes a reversible transformation of the original complex covariance matrix to obtain a set of real-valued intensity bands which are fed to the neural network. In addition, the proposed method includes a change detection strategy to avoid the neural network to learn erroneous features in areas strongly affected by temporal changes, so that the network only learns the underlying speckle component present in the data. The method is implemented and tested with dual-polarimetric images acquired by Sentinel-1. Experiments show that the proposed approach offers exceptional results in both speckle reduction and resolution preservation. More importantly, it is also shown that the neural network is not generating artifacts or introducing bias in the filtered images, making them suitable for further polarimetric processing and exploitation.",
      "paper_authors": [
        "Alejandro Mestre-Quereda",
        "Juan M. Lopez-Sanchez"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-28",
      "update_time": "2024-08-29",
      "comments": "23 pages, 32 figures",
      "repo_url": "https://github.com/alejandromestrequereda/CNN_PolSAR_Despeckling"
    },
    "2408.14431": {
      "paper_id": "2408.14431v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.14431v1",
      "paper_key": "2408.14431",
      "paper_title": "Towards Better Comprehension of Breaking Changes in the NPM Ecosystem",
      "paper_url": "http://arxiv.org/abs/2408.14431v1",
      "paper_abstract": "Breaking changes cause a lot of effort to both downstream and upstream developers: downstream developers need to adapt to breaking changes and upstream developers are responsible for identifying and documenting them. In the NPM ecosystem, characterized by frequent code changes and a high tolerance for making breaking changes, the effort is larger.   For better comprehension of breaking changes in the NPM ecosystem and to enhance breaking change detection tools, we conduct a large-scale empirical study to investigate breaking changes in the NPM ecosystem. We construct a dataset of explicitly documented breaking changes from 381 popular NPM projects. We find that 93.6% of the detected breaking changes can be covered by developers' documentation, and about 19% of the breaking changes cannot be detected by regression testing. Then in the process of investigating source code of our collected breaking changes, we yield a taxonomy of JavaScript and TypeScript-specific syntactic breaking changes and a taxonomy of major types of behavioral breaking changes. Additionally, we investigate the reasons why developers make breaking changes in NPM and find three major reasons, i.e., to reduce code redundancy, to improve identifier name, and to improve API design, and each category contains several sub-items.   We provide actionable implications for future research, e.g., automatic naming and renaming techniques should be applied in JavaScript projects to improve identifier names, future research can try to detect more types of behavioral breaking changes. By presenting the implications, we also discuss the weakness of automatic renaming and BC detection approaches.",
      "paper_authors": [
        "Dezhen Kong",
        "Jiakun Liu",
        "Lingfeng Bao",
        "David Lo"
      ],
      "primary_category": "cs.SE",
      "publish_time": "2024-08-26",
      "update_time": "2024-08-26",
      "comments": null,
      "repo_url": "#"
    },
    "2408.12945": {
      "paper_id": "2408.12945v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.12945v1",
      "paper_key": "2408.12945",
      "paper_title": "Find the Assembly Mistakes: Error Segmentation for Industrial Applications",
      "paper_url": "http://arxiv.org/abs/2408.12945v1",
      "paper_abstract": "Recognizing errors in assembly and maintenance procedures is valuable for industrial applications, since it can increase worker efficiency and prevent unplanned down-time. Although assembly state recognition is gaining attention, none of the current works investigate assembly error localization. Therefore, we propose StateDiffNet, which localizes assembly errors based on detecting the differences between a (correct) intended assembly state and a test image from a similar viewpoint. StateDiffNet is trained on synthetically generated image pairs, providing full control over the type of meaningful change that should be detected. The proposed approach is the first to correctly localize assembly errors taken from real ego-centric video data for both states and error types that are never presented during training. Furthermore, the deployment of change detection to this industrial application provides valuable insights and considerations into the mechanisms of state-of-the-art change detection algorithms. The code and data generation pipeline are publicly available at: https://timschoonbeek.github.io/error_seg.",
      "paper_authors": [
        "Dan Lehman",
        "Tim J. Schoonbeek",
        "Shao-Hsuan Hung",
        "Jacek Kustra",
        "Peter H. N. de With",
        "Fons van der Sommen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-23",
      "update_time": "2024-08-23",
      "comments": "23 pages (14 main paper, 2 references, 7 supplementary), 15 figures\n  (8 main paper, 7 supplementary). Accepted at ECCV Vision-based InduStrial\n  InspectiON (VISION) workshop",
      "repo_url": "#"
    },
    "2408.12527": {
      "paper_id": "2408.12527v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.12527v1",
      "paper_key": "2408.12527",
      "paper_title": "UMAD: University of Macau Anomaly Detection Benchmark Dataset",
      "paper_url": "http://arxiv.org/abs/2408.12527v1",
      "paper_abstract": "Anomaly detection is critical in surveillance systems and patrol robots by identifying anomalous regions in images for early warning. Depending on whether reference data are utilized, anomaly detection can be categorized into anomaly detection with reference and anomaly detection without reference. Currently, anomaly detection without reference, which is closely related to out-of-distribution (OoD) object detection, struggles with learning anomalous patterns due to the difficulty of collecting sufficiently large and diverse anomaly datasets with the inherent rarity and novelty of anomalies. Alternatively, anomaly detection with reference employs the scheme of change detection to identify anomalies by comparing semantic changes between a reference image and a query one. However, there are very few ADr works due to the scarcity of public datasets in this domain. In this paper, we aim to address this gap by introducing the UMAD Benchmark Dataset. To our best knowledge, this is the first benchmark dataset designed specifically for anomaly detection with reference in robotic patrolling scenarios, e.g., where an autonomous robot is employed to detect anomalous objects by comparing a reference and a query video sequences. The reference sequences can be taken by the robot along a specified route when there are no anomalous objects in the scene. The query sequences are captured online by the robot when it is patrolling in the same scene following the same route. Our benchmark dataset is elaborated such that each query image can find a corresponding reference based on accurate robot localization along the same route in the prebuilt 3D map, with which the reference and query images can be geometrically aligned using adaptive warping. Besides the proposed benchmark dataset, we evaluate the baseline models of ADr on this dataset.",
      "paper_authors": [
        "Dong Li",
        "Lineng Chen",
        "Cheng-Zhong Xu",
        "Hui Kong"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-08-22",
      "update_time": "2024-08-22",
      "comments": "Accepted by the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS) 2024, project code at https://github.com/IMRL/UMAD",
      "repo_url": "https://github.com/imrl/umad"
    },
    "2408.11240": {
      "paper_id": "2408.11240v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.11240v1",
      "paper_key": "2408.11240",
      "paper_title": "Asymmetric Graph Error Control with Low Complexity in Causal Bandits",
      "paper_url": "http://arxiv.org/abs/2408.11240v1",
      "paper_abstract": "In this paper, the causal bandit problem is investigated, in which the objective is to select an optimal sequence of interventions on nodes in a causal graph. It is assumed that the graph is governed by linear structural equations; it is further assumed that both the causal topology and the distribution of interventions are unknown. By exploiting the causal relationships between the nodes whose signals contribute to the reward, interventions are optimized. First, based on the difference between the two types of graph identification errors (false positives and negatives), a causal graph learning method is proposed, which strongly reduces sample complexity relative to the prior art by learning sub-graphs. Under the assumption of Gaussian exogenous inputs and minimum-mean squared error weight estimation, a new uncertainty bound tailored to the causal bandit problem is derived. This uncertainty bound drives an upper confidence bound based intervention selection to optimize the reward. To cope with non-stationary bandits, a sub-graph change detection mechanism is proposed, with high sample efficiency. Numerical results compare the new methodology to existing schemes and show a substantial performance improvement in both stationary and non-stationary settings. Compared to existing approaches, the proposed scheme takes 67% fewer samples to learn the causal structure and achieves an average reward gain of 85%.",
      "paper_authors": [
        "Chen Peng",
        "Di Zhang",
        "Urbashi Mitra"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-08-20",
      "update_time": "2024-08-20",
      "comments": null,
      "repo_url": "#"
    },
    "2408.10619": {
      "paper_id": "2408.10619v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.10619v1",
      "paper_key": "2408.10619",
      "paper_title": "Novel Change Detection Framework in Remote Sensing Imagery Using Diffusion Models and Structural Similarity Index (SSIM)",
      "paper_url": "http://arxiv.org/abs/2408.10619v1",
      "paper_abstract": "Change detection is a crucial task in remote sensing, enabling the monitoring of environmental changes, urban growth, and disaster impact. Conventional change detection techniques, such as image differencing and ratioing, often struggle with noise and fail to capture complex variations in imagery. Recent advancements in machine learning, particularly generative models like diffusion models, offer new opportunities for enhancing change detection accuracy. In this paper, we propose a novel change detection framework that combines the strengths of Stable Diffusion models with the Structural Similarity Index (SSIM) to create robust and interpretable change maps. Our approach, named Diffusion Based Change Detector, is evaluated on both synthetic and real-world remote sensing datasets and compared with state-of-the-art methods. The results demonstrate that our method significantly outperforms traditional differencing techniques and recent deep learning-based methods, particularly in scenarios with complex changes and noise.",
      "paper_authors": [
        "Andrew Kiruluta",
        "Eric Lundy",
        "Andreas Lemos"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-20",
      "update_time": "2024-08-20",
      "comments": null,
      "repo_url": "#"
    },
    "2408.08078": {
      "paper_id": "2408.08078v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.08078v1",
      "paper_key": "2408.08078",
      "paper_title": "Treat Stillness with Movement: Remote Sensing Change Detection via Coarse-grained Temporal Foregrounds Mining",
      "paper_url": "http://arxiv.org/abs/2408.08078v1",
      "paper_abstract": "Current works focus on addressing the remote sensing change detection task using bi-temporal images. Although good performance can be achieved, however, seldom of they consider the motion cues which may also be vital. In this work, we revisit the widely adopted bi-temporal images-based framework and propose a novel Coarse-grained Temporal Mining Augmented (CTMA) framework. To be specific, given the bi-temporal images, we first transform them into a video using interpolation operations. Then, a set of temporal encoders is adopted to extract the motion features from the obtained video for coarse-grained changed region prediction. Subsequently, we design a novel Coarse-grained Foregrounds Augmented Spatial Encoder module to integrate both global and local information. We also introduce a motion augmented strategy that leverages motion cues as an additional output to aggregate with the spatial features for improved results. Meanwhile, we feed the input image pairs into the ResNet to get the different features and also the spatial blocks for fine-grained feature learning. More importantly, we propose a mask augmented strategy that utilizes coarse-grained changed regions, incorporating them into the decoder blocks to enhance the final changed prediction. Extensive experiments conducted on multiple benchmark datasets fully validated the effectiveness of our proposed framework for remote sensing image change detection. The source code of this paper will be released on https://github.com/Event-AHU/CTM_Remote_Sensing_Change_Detection",
      "paper_authors": [
        "Xixi Wang",
        "Zitian Wang",
        "Jingtao Jiang",
        "Lan Chen",
        "Xiao Wang",
        "Bo Jiang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-15",
      "update_time": "2024-08-15",
      "comments": "In Peer Review",
      "repo_url": "https://github.com/event-ahu/ctm_remote_sensing_change_detection"
    },
    "2408.16004": {
      "paper_id": "2408.16004v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.16004v1",
      "paper_key": "2408.16004",
      "paper_title": "Granger causal inference for climate change attribution",
      "paper_url": "http://arxiv.org/abs/2408.16004v1",
      "paper_abstract": "Climate change detection and attribution (D&A) is concerned with determining the extent to which anthropogenic activities have influenced specific aspects of the global climate system. D&A fits within the broader field of causal inference, the collection of statistical methods that identify cause and effect relationships. There are a wide variety of methods for making attribution statements, each of which require different types of input data and focus on different types of weather and climate events and each of which are conditional to varying extents. Some methods are based on Pearl causality while others leverage Granger causality, and the causal framing provides important context for how the resulting attribution conclusion should be interpreted. However, while Granger-causal attribution analyses have become more common, there is no clear statement of their strengths and weaknesses and no clear consensus on where and when Granger-causal perspectives are appropriate. In this prospective paper, we provide a formal definition for Granger-based approaches to trend and event attribution and a clear comparison with more traditional methods for assessing the human influence on extreme weather and climate events. Broadly speaking, Granger-causal attribution statements can be constructed quickly from observations and do not require computationally-intesive dynamical experiments. These analyses also enable rapid attribution, which is useful in the aftermath of a severe weather event, and provide multiple lines of evidence for anthropogenic climate change when paired with Pearl-causal attribution. Confidence in attribution statements is increased when different methodologies arrive at similar conclusions. Moving forward, we encourage the D&A community to embrace hybrid approaches to climate change attribution that leverage the strengths of both Granger and Pearl causality.",
      "paper_authors": [
        "Mark D. Risser",
        "Mohammed Ombadi",
        "Michael F. Wehner"
      ],
      "primary_category": "stat.AP",
      "publish_time": "2024-08-13",
      "update_time": "2024-08-13",
      "comments": null,
      "repo_url": "#"
    },
    "2408.06644": {
      "paper_id": "2408.06644v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.06644v1",
      "paper_key": "2408.06644",
      "paper_title": "Specialized Change Detection using Segment Anything",
      "paper_url": "http://arxiv.org/abs/2408.06644v1",
      "paper_abstract": "Change detection (CD) is a fundamental task in Earth observation. While most change detection methods detect all changes, there is a growing need for specialized methods targeting specific changes relevant to particular applications while discarding the other changes. For instance, urban management might prioritize detecting the disappearance of buildings due to natural disasters or other reasons. Furthermore, while most supervised change detection methods require large-scale training datasets, in many applications only one or two training examples might be available instead of large datasets. Addressing such needs, we propose a focused CD approach using the Segment Anything Model (SAM), a versatile vision foundation model. Our method leverages a binary mask of the object of interest in pre-change images to detect their disappearance in post-change images. By using SAM's robust segmentation capabilities, we create prompts from the pre-change mask, use those prompts to segment the post-change image, and identify missing objects. This unsupervised approach demonstrated for building disappearance detection, is adaptable to various domains requiring specialized CD. Our contributions include defining a novel CD problem, proposing a method using SAM, and demonstrating its effectiveness. The proposed method also has benefits related to privacy preservation.",
      "paper_authors": [
        "Tahir Ahmad",
        "Sudipan Saha"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2024-08-13",
      "update_time": "2024-08-13",
      "comments": null,
      "repo_url": "#"
    },
    "2408.15268": {
      "paper_id": "2408.15268v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.15268v2",
      "paper_key": "2408.15268",
      "paper_title": "Anomaly Detection in Time Series of EDFA Pump Currents to Monitor Degeneration Processes using Fuzzy Clustering",
      "paper_url": "http://arxiv.org/abs/2408.15268v2",
      "paper_abstract": "This article proposes a novel fuzzy clustering based anomaly detection method for pump current time series of EDFA systems. The proposed change detection framework (CDF) strategically combines the advantages of entropy analysis (EA) and principle component analysis (PCA) with fuzzy clustering procedures. In the framework, EA is applied for dynamic selection of features for reduction of the feature space and increase of computational performance. Furthermore, PCA is utilized to extract features from the raw feature space to enable generalization capability of the subsequent fuzzy clustering procedures. Three different fuzzy clustering methods, more precisely the fuzzy clustering algorithm, a probabilistic clustering algorithm and a possibilistic clustering algorithm are evaluated for performance and generalization. Hence, the proposed framework has the innovative feature to detect changes in pump current time series at an early stage for arbitrary points of operation, compared to state-of-the-art predefined alarms in commercially used EDFAs. Moreover, the approach is implemented and tested using experimental data. In addition, the proposed framework enables further approaches of applying decentralized predictive maintenance for optical fiber networks.",
      "paper_authors": [
        "Dominic Schneider",
        "Lutz Rapp",
        "Christoph Ament"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2024-08-12",
      "update_time": "2024-08-30",
      "comments": "6 pages, 6 figures",
      "repo_url": "#"
    },
    "2408.05817": {
      "paper_id": "2408.05817v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.05817v2",
      "paper_key": "2408.05817",
      "paper_title": "High Probability Latency Sequential Change Detection over an Unknown Finite Horizon",
      "paper_url": "http://arxiv.org/abs/2408.05817v2",
      "paper_abstract": "A finite horizon variant of the quickest change detection problem is studied, in which the goal is to minimize a delay threshold (latency), under constraints on the probability of false alarm and the probability that the latency is exceeded. In addition, the horizon is not known to the change detector. A variant of the cumulative sum (CuSum) test with a threshold that increasing logarithmically with time is proposed as a candidate solution to the problem. An information-theoretic lower bound on the minimum value of the latency under the constraints is then developed. This lower bound is used to establish certain asymptotic optimality properties of the proposed test in terms of the horizon and the false alarm probability. Some experimental results are given to illustrate the performance of the test.",
      "paper_authors": [
        "Yu-Han Huang",
        "Venugopal V. Veeravalli"
      ],
      "primary_category": "cs.DS",
      "publish_time": "2024-08-11",
      "update_time": "2024-09-18",
      "comments": "7 pages, 2 figures, International Symposium of Information Theory",
      "repo_url": "#"
    },
    "2408.04144": {
      "paper_id": "2408.04144v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.04144v1",
      "paper_key": "2408.04144",
      "paper_title": "Integrated Dynamic Phenological Feature for Remote Sensing Image Land Cover Change Detection",
      "paper_url": "http://arxiv.org/abs/2408.04144v1",
      "paper_abstract": "Remote sensing image change detection (CD) is essential for analyzing land surface changes over time, with a significant challenge being the differentiation of actual changes from complex scenes while filtering out pseudo-changes. A primary contributor to this challenge is the intra-class dynamic changes due to phenological characteristics in natural areas. To overcome this, we introduce the InPhea model, which integrates phenological features into a remote sensing image CD framework. The model features a detector with a differential attention module for improved feature representation of change information, coupled with high-resolution feature extraction and spatial pyramid blocks to enhance performance. Additionally, a constrainer with four constraint modules and a multi-stage contrastive learning approach is employed to aid in the model's understanding of phenological characteristics. Experiments on the HRSCD, SECD, and PSCD-Wuhan datasets reveal that InPhea outperforms other models, confirming its effectiveness in addressing phenological pseudo-changes and its overall model superiority.",
      "paper_authors": [
        "Yi Liu",
        "Chenhao Sun",
        "Hao Ye",
        "Xiangying Liu",
        "Weilong Ju"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-08",
      "update_time": "2024-08-08",
      "comments": null,
      "repo_url": "#"
    },
    "2408.04056": {
      "paper_id": "2408.04056v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.04056v1",
      "paper_key": "2408.04056",
      "paper_title": "Testing for a general changepoint in psychometric studies: changes detection and sample size planning",
      "paper_url": "http://arxiv.org/abs/2408.04056v1",
      "paper_abstract": "This paper introduces a new method for change detection in psychometric studies based on the recently introduced pseudo Score statistic, for which the sampling distribution under the alternative hypothesis has been determined. Our approach has the advantage of simplicity in its computation, eliminating the need for resampling or simulations to obtain critical values. Additionally, it comes with a known null/alternative distribution, facilitating easy calculations for power levels and sample size planning. The paper indeed also discusses the topic of power analysis in segmented regression, namely the estimation of sample size or power level when the study data being collected focuses on a covariate expected to affect the mean response via a piecewise relationship with an unknown breakpoint. We run simulation results showing that our method outperforms other Tests for a Change Point (TFCP) with both normally distributed and binary data and carry out a real SAT Critical reading data analysis. The proposed test contributes to the framework of psychometric research, and it is available on the Comprehensive R Archive Network (CRAN) and in a more user-friendly Shiny App, both illustrated at the end of the paper.",
      "paper_authors": [
        "Nicoletta D'Angelo"
      ],
      "primary_category": "stat.ME",
      "publish_time": "2024-08-07",
      "update_time": "2024-08-07",
      "comments": null,
      "repo_url": "#"
    },
    "2407.16158": {
      "paper_id": "2407.16158v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.16158v1",
      "paper_key": "2407.16158",
      "paper_title": "Cross-Domain Separable Translation Network for Multimodal Image Change Detection",
      "paper_url": "http://arxiv.org/abs/2407.16158v1",
      "paper_abstract": "In the remote sensing community, multimodal change detection (MCD) is particularly critical due to its ability to track changes across different imaging conditions and sensor types, making it highly applicable to a wide range of real-world scenarios. This paper focuses on addressing the challenges of MCD, especially the difficulty in comparing images from different sensors with varying styles and statistical characteristics of geospatial objects. Traditional MCD methods often struggle with these variations, leading to inaccurate and unreliable results. To overcome these limitations, a novel unsupervised cross-domain separable translation network (CSTN) is proposed, which uniquely integrates a within-domain self-reconstruction and a cross-domain image translation and cycle-reconstruction workflow with change detection constraints. The model is optimized by implementing both the tasks of image translation and MCD simultaneously, thereby guaranteeing the comparability of learned features from multimodal images. Specifically, a simple yet efficient dual-branch convolutional architecture is employed to separate the content and style information of multimodal images. This process generates a style-independent content-comparable feature space, which is crucial for achieving accurate change detection even in the presence of significant sensor variations. Extensive experimental results demonstrate the effectiveness of the proposed method, showing remarkable improvements over state-of-the-art approaches in terms of accuracy and efficacy for MCD. The implementation of our method will be publicly available at \\url{https://github.com/OMEGA-RS/CSTN}",
      "paper_authors": [
        "Tao Zhan",
        "Yuanyuan Zhu",
        "Jie Lan",
        "Qianlong Dang"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2024-07-23",
      "update_time": "2024-07-23",
      "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible",
      "repo_url": "https://github.com/omega-rs/cstn"
    },
    "2407.15999": {
      "paper_id": "2407.15999v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.15999v1",
      "paper_key": "2407.15999",
      "paper_title": "EfficientCD: A New Strategy For Change Detection Based With Bi-temporal Layers Exchanged",
      "paper_url": "http://arxiv.org/abs/2407.15999v1",
      "paper_abstract": "With the widespread application of remote sensing technology in environmental monitoring, the demand for efficient and accurate remote sensing image change detection (CD) for natural environments is growing. We propose a novel deep learning framework named EfficientCD, specifically designed for remote sensing image change detection. The framework employs EfficientNet as its backbone network for feature extraction. To enhance the information exchange between bi-temporal image feature maps, we have designed a new Feature Pyramid Network module targeted at remote sensing change detection, named ChangeFPN. Additionally, to make full use of the multi-level feature maps in the decoding stage, we have developed a layer-by-layer feature upsampling module combined with Euclidean distance to improve feature fusion and reconstruction during the decoding stage. The EfficientCD has been experimentally validated on four remote sensing datasets: LEVIR-CD, SYSU-CD, CLCD, and WHUCD. The experimental results demonstrate that EfficientCD exhibits outstanding performance in change detection accuracy. The code and pretrained models will be released at https://github.com/dyzy41/mmrscd.",
      "paper_authors": [
        "Sijun Dong",
        "Yuwei Zhu",
        "Geng Chen",
        "Xiaoliang Meng"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-22",
      "update_time": "2024-07-22",
      "comments": null,
      "repo_url": "https://github.com/dyzy41/mmrscd"
    },
    "2407.15317": {
      "paper_id": "2407.15317v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.15317v1",
      "paper_key": "2407.15317",
      "paper_title": "Open-CD: A Comprehensive Toolbox for Change Detection",
      "paper_url": "http://arxiv.org/abs/2407.15317v1",
      "paper_abstract": "We present Open-CD, a change detection toolbox that contains a rich set of change detection methods as well as related components and modules. The toolbox started from a series of open source general vision task tools, including OpenMMLab Toolkits, PyTorch Image Models, etc. It gradually evolves into a unified platform that covers many popular change detection methods and contemporary modules. It not only includes training and inference codes, but also provides some useful scripts for data analysis. We believe this toolbox is by far the most complete change detection toolbox. In this report, we introduce the various features, supported methods and applications of Open-CD. In addition, we also conduct a benchmarking study on different methods and components. We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new change detectors. Code and models are available at \\url{https://github.com/likyoo/open-cd}. Pioneeringly, this report also includes brief descriptions of the algorithms supported in Open-CD, mainly contributed by their authors. We sincerely encourage researchers in this field to participate in this project and work together to create a more open community. This toolkit and report will be kept updated.",
      "paper_authors": [
        "Kaiyu Li",
        "Jiawei Jiang",
        "Andrea Codegoni",
        "Chengxi Han",
        "Yupeng Deng",
        "Keyan Chen",
        "Zhuo Zheng",
        "Hao Chen",
        "Zhengxia Zou",
        "Zhenwei Shi",
        "Sheng Fang",
        "Deyu Meng",
        "Zhi Wang",
        "Xiangyong Cao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-22",
      "update_time": "2024-07-22",
      "comments": "9 pages",
      "repo_url": "https://github.com/likyoo/open-cd"
    },
    "2407.14032": {
      "paper_id": "2407.14032v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.14032v1",
      "paper_key": "2407.14032",
      "paper_title": "Semantic-CC: Boosting Remote Sensing Image Change Captioning via Foundational Knowledge and Semantic Guidance",
      "paper_url": "http://arxiv.org/abs/2407.14032v1",
      "paper_abstract": "Remote sensing image change captioning (RSICC) aims to articulate the changes in objects of interest within bi-temporal remote sensing images using natural language. Given the limitations of current RSICC methods in expressing general features across multi-temporal and spatial scenarios, and their deficiency in providing granular, robust, and precise change descriptions, we introduce a novel change captioning (CC) method based on the foundational knowledge and semantic guidance, which we term Semantic-CC. Semantic-CC alleviates the dependency of high-generalization algorithms on extensive annotations by harnessing the latent knowledge of foundation models, and it generates more comprehensive and accurate change descriptions guided by pixel-level semantics from change detection (CD). Specifically, we propose a bi-temporal SAM-based encoder for dual-image feature extraction; a multi-task semantic aggregation neck for facilitating information interaction between heterogeneous tasks; a straightforward multi-scale change detection decoder to provide pixel-level semantic guidance; and a change caption decoder based on the large language model (LLM) to generate change description sentences. Moreover, to ensure the stability of the joint training of CD and CC, we propose a three-stage training strategy that supervises different tasks at various stages. We validate the proposed method on the LEVIR-CC and LEVIR-CD datasets. The experimental results corroborate the complementarity of CD and CC, demonstrating that Semantic-CC can generate more accurate change descriptions and achieve optimal performance across both tasks.",
      "paper_authors": [
        "Yongshuo Zhu",
        "Lu Li",
        "Keyan Chen",
        "Chenyang Liu",
        "Fugen Zhou",
        "Zhenwei Shi"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-19",
      "update_time": "2024-07-19",
      "comments": null,
      "repo_url": "#"
    },
    "2407.13151": {
      "paper_id": "2407.13151v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.13151v1",
      "paper_key": "2407.13151",
      "paper_title": "Wavelet-based Bi-dimensional Aggregation Network for SAR Image Change Detection",
      "paper_url": "http://arxiv.org/abs/2407.13151v1",
      "paper_abstract": "Synthetic aperture radar (SAR) image change detection is critical in remote sensing image analysis. Recently, the attention mechanism has been widely used in change detection tasks. However, existing attention mechanisms often employ down-sampling operations such as average pooling on the Key and Value components to enhance computational efficiency. These irreversible operations result in the loss of high-frequency components and other important information. To address this limitation, we develop Wavelet-based Bi-dimensional Aggregation Network (WBANet) for SAR image change detection. We design a wavelet-based self-attention block that includes discrete wavelet transform and inverse discrete wavelet transform operations on Key and Value components. Hence, the feature undergoes downsampling without any loss of information, while simultaneously enhancing local contextual awareness through an expanded receptive field. Additionally, we have incorporated a bi-dimensional aggregation module that boosts the non-linear representation capability by merging spatial and channel information via broadcast mechanism. Experimental results on three SAR datasets demonstrate that our WBANet significantly outperforms contemporary state-of-the-art methods. Specifically, our WBANet achieves 98.33\\%, 96.65\\%, and 96.62\\% of percentage of correct classification (PCC) on the respective datasets, highlighting its superior performance. Source codes are available at \\url{https://github.com/summitgao/WBANet}.",
      "paper_authors": [
        "Jiangwei Xie",
        "Feng Gao",
        "Xiaowei Zhou",
        "Junyu Dong"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2024-07-18",
      "update_time": "2024-07-18",
      "comments": "IEEE GRSL 2024",
      "repo_url": "https://github.com/summitgao/WBANet"
    },
    "2407.11578": {
      "paper_id": "2407.11578v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.11578v2",
      "paper_key": "2407.11578",
      "paper_title": "UP-Diff: Latent Diffusion Model for Remote Sensing Urban Prediction",
      "paper_url": "http://arxiv.org/abs/2407.11578v2",
      "paper_abstract": "This study introduces a novel Remote Sensing (RS) Urban Prediction (UP) task focused on future urban planning, which aims to forecast urban layouts by utilizing information from existing urban layouts and planned change maps. To address the proposed RS UP task, we propose UP-Diff, which leverages a Latent Diffusion Model (LDM) to capture positionaware embeddings of pre-change urban layouts and planned change maps. In specific, the trainable cross-attention layers within UP-Diff's iterative diffusion modules enable the model to dynamically highlight crucial regions for targeted modifications. By utilizing our UP-Diff, designers can effectively refine and adjust future urban city plans by making modifications to the change maps in a dynamic and adaptive manner. Compared with conventional RS Change Detection (CD) methods, the proposed UP-Diff for the RS UP task avoids the requirement of paired prechange and post-change images, which enhances the practical usage in city development. Experimental results on LEVIRCD and SYSU-CD datasets show UP-Diff's ability to accurately predict future urban layouts with high fidelity, demonstrating its potential for urban planning. Code and model weights are available at https://github.com/zeyuwang-zju/UP-Diff.",
      "paper_authors": [
        "Zeyu Wang",
        "Zecheng Hao",
        "Jingyu Lin",
        "Yuchao Feng",
        "Yufei Guo"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-16",
      "update_time": "2024-07-17",
      "comments": "5 pages, 4 figures",
      "repo_url": "#"
    },
    "2407.11094": {
      "paper_id": "2407.11094v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.11094v2",
      "paper_key": "2407.11094",
      "paper_title": "Robust Score-Based Quickest Change Detection",
      "paper_url": "http://arxiv.org/abs/2407.11094v2",
      "paper_abstract": "Methods in the field of quickest change detection rapidly detect in real-time a change in the data-generating distribution of an online data stream. Existing methods have been able to detect this change point when the densities of the pre- and post-change distributions are known. Recent work has extended these results to the case where the pre- and post-change distributions are known only by their score functions. This work considers the case where the pre- and post-change score functions are known only to correspond to distributions in two disjoint sets. This work employs a pair of \"least-favorable\" distributions to robustify the existing score-based quickest change detection algorithm, the properties of which are studied. This paper calculates the least-favorable distributions for specific model classes and provides methods of estimating the least-favorable distributions for common constructions. Simulation results are provided demonstrating the performance of our robust change detection algorithm.",
      "paper_authors": [
        "Sean Moushegian",
        "Suya Wu",
        "Enmao Diao",
        "Jie Ding",
        "Taposh Banerjee",
        "Vahid Tarokh"
      ],
      "primary_category": "stat.ME",
      "publish_time": "2024-07-15",
      "update_time": "2024-09-20",
      "comments": "arXiv admin note: text overlap with arXiv:2306.05091",
      "repo_url": "#"
    },
    "2407.09874": {
      "paper_id": "2407.09874v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.09874v1",
      "paper_key": "2407.09874",
      "paper_title": "SeFi-CD: A Semantic First Change Detection Paradigm That Can Detect Any Change You Want",
      "paper_url": "http://arxiv.org/abs/2407.09874v1",
      "paper_abstract": "The existing change detection(CD) methods can be summarized as the visual-first change detection (ViFi-CD) paradigm, which first extracts change features from visual differences and then assigns them specific semantic information. However, CD is essentially dependent on change regions of interest (CRoIs), meaning that the CD results are directly determined by the semantics changes of interest, making its primary image factor semantic of interest rather than visual. The ViFi-CD paradigm can only assign specific semantics of interest to specific change features extracted from visual differences, leading to the inevitable omission of potential CRoIs and the inability to adapt to different CRoI CD tasks. In other words, changes in other CRoIs cannot be detected by the ViFi-CD method without retraining the model or significantly modifying the method. This paper introduces a new CD paradigm, the semantic-first CD (SeFi-CD) paradigm. The core idea of SeFi-CD is to first perceive the dynamic semantics of interest and then visually search for change features related to the semantics. Based on the SeFi-CD paradigm, we designed Anything You Want Change Detection (AUWCD). Experiments on public datasets demonstrate that the AUWCD outperforms the current state-of-the-art CD methods, achieving an average F1 score 5.01\\% higher than that of these advanced supervised baselines on the SECOND dataset, with a maximum increase of 13.17\\%. The proposed SeFi-CD offers a novel CD perspective and approach.",
      "paper_authors": [
        "Ling Zhao",
        "Zhenyang Huang",
        "Dongsheng Kuang",
        "Chengli Peng",
        "Jun Gan",
        "Haifeng Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-13",
      "update_time": "2024-07-13",
      "comments": null,
      "repo_url": "#"
    },
    "2407.09392": {
      "paper_id": "2407.09392v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.09392v2",
      "paper_key": "2407.09392",
      "paper_title": "Open-Canopy: A Country-Scale Benchmark for Canopy Height Estimation at Very High Resolution",
      "paper_url": "http://arxiv.org/abs/2407.09392v2",
      "paper_abstract": "Estimating canopy height and canopy height change at meter resolution from satellite imagery has numerous applications, such as monitoring forest health, logging activities, wood resources, and carbon stocks. However, many existing forest datasets are based on commercial or closed data sources, restricting the reproducibility and evaluation of new approaches. To address this gap, we introduce Open-Canopy, the first open-access and country-scale benchmark for very high resolution (1.5 m) canopy height estimation. Covering more than 87,000 km$^2$ across France, Open-Canopy combines SPOT satellite imagery with high resolution aerial LiDAR data. We also propose Open-Canopy-$\\Delta$, the first benchmark for canopy height change detection between two images taken at different years, a particularly challenging task even for recent models. To establish a robust foundation for these benchmarks, we evaluate a comprehensive list of state-of-the-art computer vision models for canopy height estimation. The dataset and associated codes can be accessed at https://github.com/fajwel/Open-Canopy.",
      "paper_authors": [
        "Fajwel Fogel",
        "Yohann Perron",
        "Nikola Besic",
        "Laurent Saint-Andr\u00e9",
        "Agn\u00e8s Pellissier-Tanon",
        "Martin Schwartz",
        "Thomas Boudras",
        "Ibrahim Fayad",
        "Alexandre d'Aspremont",
        "Loic Landrieu",
        "Philippe Ciais"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-12",
      "update_time": "2024-07-18",
      "comments": "22 pages, 8 figures, Submitted to NeurIPS 2024 Datasets and\n  Benchmarks Track",
      "repo_url": "https://github.com/fajwel/open-canopy"
    },
    "2407.08448": {
      "paper_id": "2407.08448v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.08448v1",
      "paper_key": "2407.08448",
      "paper_title": "Paving the way toward foundation models for irregular and unaligned Satellite Image Time Series",
      "paper_url": "http://arxiv.org/abs/2407.08448v1",
      "paper_abstract": "Although recently several foundation models for satellite remote sensing imagery have been proposed, they fail to address major challenges of real/operational applications. Indeed, embeddings that don't take into account the spectral, spatial and temporal dimensions of the data as well as the irregular or unaligned temporal sampling are of little use for most real world uses.As a consequence, we propose an ALIgned Sits Encoder (ALISE), a novel approach that leverages the spatial, spectral, and temporal dimensions of irregular and unaligned SITS while producing aligned latent representations. Unlike SSL models currently available for SITS, ALISE incorporates a flexible query mechanism to project the SITS into a common and learned temporal projection space. Additionally, thanks to a multi-view framework, we explore integration of instance discrimination along a masked autoencoding task to SITS. The quality of the produced representation is assessed through three downstream tasks: crop segmentation (PASTIS), land cover segmentation (MultiSenGE), and a novel crop change detection dataset. Furthermore, the change detection task is performed without supervision. The results suggest that the use of aligned representations is more effective than previous SSL methods for linear probing segmentation tasks.",
      "paper_authors": [
        "Iris Dumeur",
        "Silvia Valero",
        "Jordi Inglada"
      ],
      "primary_category": "cs.AI",
      "publish_time": "2024-07-11",
      "update_time": "2024-07-11",
      "comments": null,
      "repo_url": "#"
    },
    "2407.07616": {
      "paper_id": "2407.07616v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.07616v1",
      "paper_key": "2407.07616",
      "paper_title": "Satellite Image Time Series Semantic Change Detection: Novel Architecture and Analysis of Domain Shift",
      "paper_url": "http://arxiv.org/abs/2407.07616v1",
      "paper_abstract": "Satellite imagery plays a crucial role in monitoring changes happening on Earth's surface and aiding in climate analysis, ecosystem assessment, and disaster response. In this paper, we tackle semantic change detection with satellite image time series (SITS-SCD) which encompasses both change detection and semantic segmentation tasks. We propose a new architecture that improves over the state of the art, scales better with the number of parameters, and leverages long-term temporal information. However, for practical use cases, models need to adapt to spatial and temporal shifts, which remains a challenge. We investigate the impact of temporal and spatial shifts separately on global, multi-year SITS datasets using DynamicEarthNet and MUDS. We show that the spatial domain shift represents the most complex setting and that the impact of temporal shift on performance is more pronounced on change detection than on semantic segmentation, highlighting that it is a specific issue deserving further attention.",
      "paper_authors": [
        "Elliot Vincent",
        "Jean Ponce",
        "Mathieu Aubry"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-10",
      "update_time": "2024-07-10",
      "comments": null,
      "repo_url": "https://github.com/elliotvincent/sitsscd"
    },
    "2407.06839": {
      "paper_id": "2407.06839v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.06839v1",
      "paper_key": "2407.06839",
      "paper_title": "A Mamba-based Siamese Network for Remote Sensing Change Detection",
      "paper_url": "http://arxiv.org/abs/2407.06839v1",
      "paper_abstract": "Change detection in remote sensing images is an essential tool for analyzing a region at different times. It finds varied applications in monitoring environmental changes, man-made changes as well as corresponding decision-making and prediction of future trends. Deep learning methods like Convolutional Neural Networks (CNNs) and Transformers have achieved remarkable success in detecting significant changes, given two images at different times. In this paper, we propose a Mamba-based Change Detector (M-CD) that segments out the regions of interest even better. Mamba-based architectures demonstrate linear-time training capabilities and an improved receptive field over transformers. Our experiments on four widely used change detection datasets demonstrate significant improvements over existing state-of-the-art (SOTA) methods. Our code and pre-trained models are available at https://github.com/JayParanjape/M-CD",
      "paper_authors": [
        "Jay N. Paranjape",
        "Celso de Melo",
        "Vishal M. Patel"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-08",
      "update_time": "2024-07-08",
      "comments": "11 pages, 7 figures",
      "repo_url": "https://github.com/jayparanjape/m-cd"
    },
    "2407.05063": {
      "paper_id": "2407.05063v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.05063v1",
      "paper_key": "2407.05063",
      "paper_title": "Co-Scale Cross-Attentional Transformer for Rearrangement Target Detection",
      "paper_url": "http://arxiv.org/abs/2407.05063v1",
      "paper_abstract": "Rearranging objects (e.g. vase, door) back in their original positions is one of the most fundamental skills for domestic service robots (DSRs). In rearrangement tasks, it is crucial to detect the objects that need to be rearranged according to the goal and current states. In this study, we focus on Rearrangement Target Detection (RTD), where the model generates a change mask for objects that should be rearranged. Although many studies have been conducted in the field of Scene Change Detection (SCD), most SCD methods often fail to segment objects with complex shapes and fail to detect the change in the angle of objects that can be opened or closed. In this study, we propose a Co-Scale Cross-Attentional Transformer for RTD. We introduce the Serial Encoder which consists of a sequence of serial blocks and the Cross-Attentional Encoder which models the relationship between the goal and current states. We built a new dataset consisting of RGB images and change masks regarding the goal and current states. We validated our method on the dataset and the results demonstrated that our method outperformed baseline methods on $F_1$-score and mean IoU.",
      "paper_authors": [
        "Haruka Matsuo",
        "Shintaro Ishikawa",
        "Komei Sugiura"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-07-06",
      "update_time": "2024-07-06",
      "comments": "Accepted for publication in Advanced Robotics",
      "repo_url": "https://github.com/keio-smilab24/Co-Scale_Cross-Attentional_Transformer"
    },
    "2407.04444": {
      "paper_id": "2407.04444v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.04444v1",
      "paper_key": "2407.04444",
      "paper_title": "TokenVerse: Unifying Speech and NLP Tasks via Transducer-based ASR",
      "paper_url": "http://arxiv.org/abs/2407.04444v1",
      "paper_abstract": "In traditional conversational intelligence from speech, a cascaded pipeline is used, involving tasks such as voice activity detection, diarization, transcription, and subsequent processing with different NLP models for tasks like semantic endpointing and named entity recognition (NER). Our paper introduces TokenVerse, a single Transducer-based model designed to handle multiple tasks. This is achieved by integrating task-specific tokens into the reference text during ASR model training, streamlining the inference and eliminating the need for separate NLP models. In addition to ASR, we conduct experiments on 3 different tasks: speaker change detection, endpointing, and NER. Our experiments on a public and a private dataset show that the proposed method improves ASR by up to 7.7% in relative WER while outperforming the cascaded pipeline approach in individual task performance. Additionally, we present task transfer learning to a new task within an existing TokenVerse.",
      "paper_authors": [
        "Shashi Kumar",
        "Srikanth Madikeri",
        "Juan Zuluaga-Gomez",
        "Iuliia Nigmatulina",
        "Esa\u00fa Villatoro-Tello",
        "Sergio Burdisso",
        "Petr Motlicek",
        "Karthik Pandia",
        "Aravind Ganapathiraju"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-07-05",
      "update_time": "2024-07-05",
      "comments": "5 pages, double column",
      "repo_url": "#"
    },
    "2407.03971": {
      "paper_id": "2407.03971v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.03971v1",
      "paper_key": "2407.03971",
      "paper_title": "MineNetCD: A Benchmark for Global Mining Change Detection on Remote Sensing Imagery",
      "paper_url": "http://arxiv.org/abs/2407.03971v1",
      "paper_abstract": "Monitoring changes triggered by mining activities is crucial for industrial controlling, environmental management and regulatory compliance, yet it poses significant challenges due to the vast and often remote locations of mining sites. Remote sensing technologies have increasingly become indispensable to detect and analyze these changes over time. We thus introduce MineNetCD, a comprehensive benchmark designed for global mining change detection using remote sensing imagery. The benchmark comprises three key contributions. First, we establish a global mining change detection dataset featuring more than 70k paired patches of bi-temporal high-resolution remote sensing images and pixel-level annotations from 100 mining sites worldwide. Second, we develop a novel baseline model based on a change-aware Fast Fourier Transform (ChangeFFT) module, which enhances various backbones by leveraging essential spectrum components within features in the frequency domain and capturing the channel-wise correlation of bi-temporal feature differences to learn change-aware representations. Third, we construct a unified change detection (UCD) framework that integrates over 13 advanced change detection models. This framework is designed for streamlined and efficient processing, utilizing the cloud platform hosted by HuggingFace. Extensive experiments have been conducted to demonstrate the superiority of the proposed baseline model compared with 12 state-of-the-art change detection approaches. Empirical studies on modularized backbones comprehensively confirm the efficacy of different representation learners on change detection. This contribution represents significant advancements in the field of remote sensing and change detection, providing a robust resource for future research and applications in global mining monitoring. Dataset and Codes are available via the link.",
      "paper_authors": [
        "Weikang Yu",
        "Xiaokang Zhang",
        "Xiao Xiang Zhu",
        "Richard Gloaguen",
        "Pedram Ghamisi"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-04",
      "update_time": "2024-07-04",
      "comments": null,
      "repo_url": "#"
    },
    "2407.03695": {
      "paper_id": "2407.03695v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.03695v1",
      "paper_key": "2407.03695",
      "paper_title": "M^3:Manipulation Mask Manufacturer for Arbitrary-Scale Super-Resolution Mask",
      "paper_url": "http://arxiv.org/abs/2407.03695v1",
      "paper_abstract": "In the field of image manipulation localization (IML), the small quantity and poor quality of existing datasets have always been major issues. A dataset containing various types of manipulations will greatly help improve the accuracy of IML models. Images on the internet (such as those on Baidu Tieba's PS Bar) are manipulated using various techniques, and creating a dataset from these images will significantly enrich the types of manipulations in our data. However, images on the internet suffer from resolution and clarity issues, and the masks obtained by simply subtracting the manipulated image from the original contain various noises. These noises are difficult to remove, rendering the masks unusable for IML models. Inspired by the field of change detection, we treat the original and manipulated images as changes over time for the same image and view the data generation task as a change detection task. However, due to clarity issues between images, conventional change detection models perform poorly. Therefore, we introduced a super-resolution module and proposed the Manipulation Mask Manufacturer (MMM) framework. It enhances the resolution of both the original and tampered images, thereby improving image details for better comparison. Simultaneously, the framework converts the original and tampered images into feature embeddings and concatenates them, effectively modeling the context. Additionally, we created the Manipulation Mask Manufacturer Dataset (MMMD), a dataset that covers a wide range of manipulation techniques. We aim to contribute to the fields of image forensics and manipulation detection by providing more realistic manipulation data through MMM and MMMD. Detailed information about MMMD and the download link can be found at: the code and datasets will be made available.",
      "paper_authors": [
        "Xinyu Yang",
        "Xiaochen Ma",
        "Xuekang Zhu",
        "Bo Du",
        "Lei Su",
        "Bingkui Tong",
        "Zeyu Lei",
        "Jizhe Zhou"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-04",
      "update_time": "2024-07-04",
      "comments": null,
      "repo_url": "#"
    },
    "2407.03178": {
      "paper_id": "2407.03178v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.03178v1",
      "paper_key": "2407.03178",
      "paper_title": "Relating CNN-Transformer Fusion Network for Change Detection",
      "paper_url": "http://arxiv.org/abs/2407.03178v1",
      "paper_abstract": "While deep learning, particularly convolutional neural networks (CNNs), has revolutionized remote sensing (RS) change detection (CD), existing approaches often miss crucial features due to neglecting global context and incomplete change learning. Additionally, transformer networks struggle with low-level details. RCTNet addresses these limitations by introducing \\textbf{(1)} an early fusion backbone to exploit both spatial and temporal features early on, \\textbf{(2)} a Cross-Stage Aggregation (CSA) module for enhanced temporal representation, \\textbf{(3)} a Multi-Scale Feature Fusion (MSF) module for enriched feature extraction in the decoder, and \\textbf{(4)} an Efficient Self-deciphering Attention (ESA) module utilizing transformers to capture global information and fine-grained details for accurate change detection. Extensive experiments demonstrate RCTNet's clear superiority over traditional RS image CD methods, showing significant improvement and an optimal balance between accuracy and computational cost.",
      "paper_authors": [
        "Yuhao Gao",
        "Gensheng Pei",
        "Mengmeng Sheng",
        "Zeren Sun",
        "Tao Chen",
        "Yazhou Yao"
      ],
      "primary_category": "cs.MM",
      "publish_time": "2024-07-03",
      "update_time": "2024-07-03",
      "comments": "accepted by IEEE Conference on Multimedia Expo",
      "repo_url": "https://github.com/nust-machine-intelligence-laboratory/rctnet"
    },
    "2407.02820": {
      "paper_id": "2407.02820v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.02820v1",
      "paper_key": "2407.02820",
      "paper_title": "Investigating the Contextualised Word Embedding Dimensions Responsible for Contextual and Temporal Semantic Changes",
      "paper_url": "http://arxiv.org/abs/2407.02820v1",
      "paper_abstract": "Words change their meaning over time as well as in different contexts. The sense-aware contextualised word embeddings (SCWEs) such as the ones produced by XL-LEXEME by fine-tuning masked langauge models (MLMs) on Word-in-Context (WiC) data attempt to encode such semantic changes of words within the contextualised word embedding (CWE) spaces. Despite the superior performance of SCWEs in contextual/temporal semantic change detection (SCD) benchmarks, it remains unclear as to how the meaning changes are encoded in the embedding space. To study this, we compare pre-trained CWEs and their fine-tuned versions on contextual and temporal semantic change benchmarks under Principal Component Analysis (PCA) and Independent Component Analysis (ICA) transformations. Our experimental results reveal several novel insights such as (a) although there exist a smaller number of axes that are responsible for semantic changes of words in the pre-trained CWE space, this information gets distributed across all dimensions when fine-tuned, and (b) in contrast to prior work studying the geometry of CWEs, we find that PCA to better represent semantic changes than ICA. Source code is available at https://github.com/LivNLP/svp-dims .",
      "paper_authors": [
        "Taichi Aida",
        "Danushka Bollegala"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-07-03",
      "update_time": "2024-07-03",
      "comments": null,
      "repo_url": "https://github.com/livnlp/svp-dims"
    },
    "2407.00949": {
      "paper_id": "2407.00949v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.00949v1",
      "paper_key": "2407.00949",
      "paper_title": "SpectralKAN: Kolmogorov-Arnold Network for Hyperspectral Images Change Detection",
      "paper_url": "http://arxiv.org/abs/2407.00949v1",
      "paper_abstract": "It has been verified that deep learning methods, including convolutional neural networks (CNNs), graph neural networks (GNNs), and transformers, can accurately extract features from hyperspectral images (HSIs). These algorithms perform exceptionally well on HSIs change detection (HSIs-CD). However, the downside of these impressive results is the enormous number of parameters, FLOPs, GPU memory, training and test times required. In this paper, we propose an spectral Kolmogorov-Arnold Network for HSIs-CD (SpectralKAN). SpectralKAN represent a multivariate continuous function with a composition of activation functions to extract HSIs feature and classification. These activation functions are b-spline functions with different parameters that can simulate various functions. In SpectralKAN, a KAN encoder is proposed to enhance computational efficiency for HSIs. And a spatial-spectral KAN encoder is introduced, where the spatial KAN encoder extracts spatial features and compresses the spatial dimensions from patch size to one. The spectral KAN encoder then extracts spectral features and classifies them into changed and unchanged categories. We use five HSIs-CD datasets to verify the effectiveness of SpectralKAN. Experimental verification has shown that SpectralKAN maintains high HSIs-CD accuracy while requiring fewer parameters, FLOPs, GPU memory, training and testing times, thereby increasing the efficiency of HSIs-CD. The code will be available at https://github.com/yanhengwang-heu/SpectralKAN.",
      "paper_authors": [
        "Yanheng Wang",
        "Xiaohan Yu",
        "Yongsheng Gao",
        "Jianjun Sha",
        "Jian Wang",
        "Lianru Gao",
        "Yonggang Zhang",
        "Xianhui Rong"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-01",
      "update_time": "2024-07-01",
      "comments": null,
      "repo_url": "#"
    },
    "2407.00851": {
      "paper_id": "2407.00851v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.00851v1",
      "paper_key": "2407.00851",
      "paper_title": "SAFE: a SAR Feature Extractor based on self-supervised learning and masked Siamese ViTs",
      "paper_url": "http://arxiv.org/abs/2407.00851v1",
      "paper_abstract": "Due to its all-weather and day-and-night capabilities, Synthetic Aperture Radar imagery is essential for various applications such as disaster management, earth monitoring, change detection and target recognition. However, the scarcity of labeled SAR data limits the performance of most deep learning algorithms. To address this issue, we propose a novel self-supervised learning framework based on masked Siamese Vision Transformers to create a General SAR Feature Extractor coined SAFE. Our method leverages contrastive learning principles to train a model on unlabeled SAR data, extracting robust and generalizable features. SAFE is applicable across multiple SAR acquisition modes and resolutions. We introduce tailored data augmentation techniques specific to SAR imagery, such as sub-aperture decomposition and despeckling. Comprehensive evaluations on various downstream tasks, including few-shot classification, segmentation, visualization, and pattern detection, demonstrate the effectiveness and versatility of the proposed approach. Our network competes with or surpasses other state-of-the-art methods in few-shot classification and segmentation tasks, even without being trained on the sensors used for the evaluation.",
      "paper_authors": [
        "Max Muzeau",
        "Joana Frontera-Pons",
        "Chengfang Ren",
        "Jean-Philippe Ovarlez"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-30",
      "update_time": "2024-06-30",
      "comments": null,
      "repo_url": "https://github.com/muzmax/safe"
    },
    "2406.17998": {
      "paper_id": "2406.17998v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.17998v1",
      "paper_key": "2406.17998",
      "paper_title": "Changen2: Multi-Temporal Remote Sensing Generative Change Foundation Model",
      "paper_url": "http://arxiv.org/abs/2406.17998v1",
      "paper_abstract": "Our understanding of the temporal dynamics of the Earth's surface has been advanced by deep vision models, which often require lots of labeled multi-temporal images for training. However, collecting, preprocessing, and annotating multi-temporal remote sensing images at scale is non-trivial since it is expensive and knowledge-intensive. In this paper, we present change data generators based on generative models, which are cheap and automatic, alleviating these data problems. Our main idea is to simulate a stochastic change process over time. We describe the stochastic change process as a probabilistic graphical model (GPCM), which factorizes the complex simulation problem into two more tractable sub-problems, i.e., change event simulation and semantic change synthesis. To solve these two problems, we present Changen2, a GPCM with a resolution-scalable diffusion transformer which can generate time series of images and their semantic and change labels from labeled or unlabeled single-temporal images. Changen2 is a generative change foundation model that can be trained at scale via self-supervision, and can produce change supervisory signals from unlabeled single-temporal images. Unlike existing foundation models, Changen2 synthesizes change data to train task-specific foundation models for change detection. The resulting model possesses inherent zero-shot change detection capabilities and excellent transferability. Experiments suggest Changen2 has superior spatiotemporal scalability, e.g., Changen2 model trained on 256$^2$ pixel single-temporal images can yield time series of any length and resolutions of 1,024$^2$ pixels. Changen2 pre-trained models exhibit superior zero-shot performance (narrowing the performance gap to 3% on LEVIR-CD and approximately 10% on both S2Looking and SECOND, compared to fully supervised counterparts) and transferability across multiple types of change tasks.",
      "paper_authors": [
        "Zhuo Zheng",
        "Stefano Ermon",
        "Dongjun Kim",
        "Liangpei Zhang",
        "Yanfei Zhong"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-26",
      "update_time": "2024-06-26",
      "comments": "The enhanced extension of our ICCV 2023 (Changen)",
      "repo_url": "https://github.com/Z-Zheng/pytorch-change-models"
    },
    "2406.17458": {
      "paper_id": "2406.17458v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.17458v1",
      "paper_key": "2406.17458",
      "paper_title": "Continuous Urban Change Detection from Satellite Image Time Series with Temporal Feature Refinement and Multi-Task Integration",
      "paper_url": "http://arxiv.org/abs/2406.17458v1",
      "paper_abstract": "Urbanization advances at unprecedented rates, resulting in negative effects on the environment and human well-being. Remote sensing has the potential to mitigate these effects by supporting sustainable development strategies with accurate information on urban growth. Deep learning-based methods have achieved promising urban change detection results from optical satellite image pairs using convolutional neural networks (ConvNets), transformers, and a multi-task learning setup. However, transformers have not been leveraged for urban change detection with multi-temporal data, i.e., >2 images, and multi-task learning methods lack integration approaches that combine change and segmentation outputs. To fill this research gap, we propose a continuous urban change detection method that identifies changes in each consecutive image pair of a satellite image time series. Specifically, we propose a temporal feature refinement (TFR) module that utilizes self-attention to improve ConvNet-based multi-temporal building representations. Furthermore, we propose a multi-task integration (MTI) module that utilizes Markov networks to find an optimal building map time series based on segmentation and dense change outputs. The proposed method effectively identifies urban changes based on high-resolution satellite image time series acquired by the PlanetScope constellation (F1 score 0.551) and Gaofen-2 (F1 score 0.440). Moreover, our experiments on two challenging datasets demonstrate the effectiveness of the proposed method compared to bi-temporal and multi-temporal urban change detection and segmentation methods.",
      "paper_authors": [
        "Sebastian Hafner",
        "Heng Fang",
        "Hossein Azizpour",
        "Yifang Ban"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-25",
      "update_time": "2024-06-25",
      "comments": "Submitted to IEEE Transactions on Geoscience and Remote Sensing, Code\n  will be available at https://github.com/SebastianHafner/ContUrbanCD.git",
      "repo_url": "https://github.com/sebastianhafner/conturbancd"
    },
    "2406.16136": {
      "paper_id": "2406.16136v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.16136v1",
      "paper_key": "2406.16136",
      "paper_title": "Distribution-Free Online Change Detection for Low-Rank Images",
      "paper_url": "http://arxiv.org/abs/2406.16136v1",
      "paper_abstract": "We present a distribution-free CUSUM procedure designed for online change detection in a time series of low-rank images, particularly when the change causes a mean shift. We represent images as matrix data and allow for temporal dependence, in addition to inherent spatial dependence, before and after the change. The marginal distributions are assumed to be general, not limited to any specific parametric distribution. We propose new monitoring statistics that utilize the low-rank structure of the in-control mean matrix. Additionally, we study the properties of the proposed detection procedure, assessing whether the monitoring statistics effectively capture a mean shift and evaluating the rate of increase in average run length relative to the control limit in both in-control and out-of-control cases. The effectiveness of our procedure is demonstrated through simulated and real data experiments.",
      "paper_authors": [
        "Tingnan Gong",
        "Seong-Hee Kim",
        "Yao Xie"
      ],
      "primary_category": "stat.ME",
      "publish_time": "2024-06-23",
      "update_time": "2024-06-23",
      "comments": "29 pages, 7 figures",
      "repo_url": "#"
    },
    "2406.16129": {
      "paper_id": "2406.16129v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.16129v1",
      "paper_key": "2406.16129",
      "paper_title": "UDHF2-Net: An Uncertainty-diffusion-model-based High-Frequency TransFormer Network for High-accuracy Interpretation of Remotely Sensed Imagery",
      "paper_url": "http://arxiv.org/abs/2406.16129v1",
      "paper_abstract": "Remotely sensed image high-accuracy interpretation (RSIHI), including tasks such as semantic segmentation and change detection, faces the three major problems: (1) complementarity problem of spatially stationary-and-non-stationary frequency; (2) edge uncertainty problem caused by down-sampling in the encoder step and intrinsic edge noises; and (3) false detection problem caused by imagery registration error in change detection. To solve the aforementioned problems, an uncertainty-diffusion-model-based high-Frequency TransFormer network (UDHF2-Net) is the proposed for RSIHI, the superiority of which is as following: (1) a spatially-stationary-and-non-stationary high-frequency connection paradigm (SHCP) is proposed to enhance the interaction of spatially stationary and non-stationary frequency features to yield high-fidelity edge extraction result. Inspired by HRFormer, SHCP remains the high-frequency stream through the whole encoder-decoder process with parallel high-to-low frequency streams and reduces the edge loss by a downsampling operation; (2) a mask-and-geo-knowledge-based uncertainty diffusion module (MUDM) is proposed to improve the robustness and edge noise resistance. MUDM could further optimize the uncertain region to improve edge extraction result by gradually removing the multiple geo-knowledge-based noises; (3) a semi-pseudo-Siamese UDHF2-Net for change detection task is proposed to reduce the pseudo change by registration error. It adopts semi-pseudo-Siamese architecture to extract above complemental frequency features for adaptively reducing registration differencing, and MUDM to recover the uncertain region by gradually reducing the registration error besides above edge noises. Comprehensive experiments were performed to demonstrate the superiority of UDHF2-Net. Especially ablation experiments indicate the effectiveness of UDHF2-Net.",
      "paper_authors": [
        "Pengfei Zhang",
        "Chang Li",
        "Yongjun Zhang",
        "Rongjun Qin"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-23",
      "update_time": "2024-06-23",
      "comments": null,
      "repo_url": "#"
    },
    "2406.15725": {
      "paper_id": "2406.15725v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.15725v2",
      "paper_key": "2406.15725",
      "paper_title": "Self Training and Ensembling Frequency Dependent Networks with Coarse Prediction Pooling and Sound Event Bounding Boxes",
      "paper_url": "http://arxiv.org/abs/2406.15725v2",
      "paper_abstract": "To tackle sound event detection (SED), we propose frequency dependent networks (FreDNets), which heavily leverage frequency-dependent methods. We apply frequency warping and FilterAugment, which are frequency-dependent data augmentation methods. The model architecture consists of 3 branches: audio teacher-student transformer (ATST) branch, BEATs branch and CNN branch including either partial dilated frequency dynamic convolution (PDFD conv) or squeeze-and-Excitation (SE) with time-frame frequency-wise SE (tfwSE). To train MAESTRO labels with coarse temporal resolution, we applied max pooling on prediction for the MAESTRO dataset. Using best ensemble model, we applied self training to obtain pseudo label from DESED weak set, unlabeled set and AudioSet. AudioSet pseudo labels, filtered to focus on high-confidence labels, are used to train on DESED dataset only. We used change-detection-based sound event bounding boxes (cSEBBs) as post processing for ensemble models on self training and submission models. The resulting FreDNet was ranked 2nd in DCASE 2024 Challenge Task 4.",
      "paper_authors": [
        "Hyeonuk Nam",
        "Deokki Min",
        "Seungdeok Choi",
        "Inhan Choi",
        "Yong-Hwa Park"
      ],
      "primary_category": "eess.AS",
      "publish_time": "2024-06-22",
      "update_time": "2024-09-20",
      "comments": "DCASE 2024 Challenge Task 4 technical report, DCASE 2024 Workshop\n  accepted",
      "repo_url": "#"
    },
    "2406.15694": {
      "paper_id": "2406.15694v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.15694v1",
      "paper_key": "2406.15694",
      "paper_title": "Single-Temporal Supervised Learning for Universal Remote Sensing Change Detection",
      "paper_url": "http://arxiv.org/abs/2406.15694v1",
      "paper_abstract": "Bitemporal supervised learning paradigm always dominates remote sensing change detection using numerous labeled bitemporal image pairs, especially for high spatial resolution (HSR) remote sensing imagery. However, it is very expensive and labor-intensive to label change regions in large-scale bitemporal HSR remote sensing image pairs. In this paper, we propose single-temporal supervised learning (STAR) for universal remote sensing change detection from a new perspective of exploiting changes between unpaired images as supervisory signals. STAR enables us to train a high-accuracy change detector only using unpaired labeled images and can generalize to real-world bitemporal image pairs. To demonstrate the flexibility and scalability of STAR, we design a simple yet unified change detector, termed ChangeStar2, capable of addressing binary change detection, object change detection, and semantic change detection in one architecture. ChangeStar2 achieves state-of-the-art performances on eight public remote sensing change detection datasets, covering above two supervised settings, multiple change types, multiple scenarios. The code is available at https://github.com/Z-Zheng/pytorch-change-models.",
      "paper_authors": [
        "Zhuo Zheng",
        "Yanfei Zhong",
        "Ailong Ma",
        "Liangpei Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-22",
      "update_time": "2024-06-22",
      "comments": "IJCV 2024. arXiv admin note: text overlap with arXiv:2108.07002",
      "repo_url": "https://github.com/Z-Zheng/pytorch-change-models"
    },
    "2406.15320": {
      "paper_id": "2406.15320v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.15320v1",
      "paper_key": "2406.15320",
      "paper_title": "Rethinking Remote Sensing Change Detection With A Mask View",
      "paper_url": "http://arxiv.org/abs/2406.15320v1",
      "paper_abstract": "Remote sensing change detection aims to compare two or more images recorded for the same area but taken at different time stamps to quantitatively and qualitatively assess changes in geographical entities and environmental factors. Mainstream models usually built on pixel-by-pixel change detection paradigms, which cannot tolerate the diversity of changes due to complex scenes and variation in imaging conditions. To address this shortcoming, this paper rethinks the change detection with the mask view, and further proposes the corresponding: 1) meta-architecture CDMask and 2) instance network CDMaskFormer. Components of CDMask include Siamese backbone, change extractor, pixel decoder, transformer decoder and normalized detector, which ensures the proper functioning of the mask detection paradigm. Since the change query can be adaptively updated based on the bi-temporal feature content, the proposed CDMask can adapt to different latent data distributions, thus accurately identifying regions of interest changes in complex scenarios. Consequently, we further propose the instance network CDMaskFormer customized for the change detection task, which includes: (i) a Spatial-temporal convolutional attention-based instantiated change extractor to capture spatio-temporal context simultaneously with lightweight operations; and (ii) a scene-guided axial attention-instantiated transformer decoder to extract more spatial details. State-of-the-art performance of CDMaskFormer is achieved on five benchmark datasets with a satisfactory efficiency-accuracy trade-off. Code is available at https://github.com/xwmaxwma/rschange.",
      "paper_authors": [
        "Xiaowen Ma",
        "Zhenkai Wu",
        "Rongrong Lian",
        "Wei Zhang",
        "Siyang Song"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-21",
      "update_time": "2024-06-21",
      "comments": "Under review",
      "repo_url": "https://github.com/xwmaxwma/rschange"
    },
    "2406.14167": {
      "paper_id": "2406.14167v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.14167v2",
      "paper_key": "2406.14167",
      "paper_title": "Definition generation for lexical semantic change detection",
      "paper_url": "http://arxiv.org/abs/2406.14167v2",
      "paper_abstract": "We use contextualized word definitions generated by large language models as semantic representations in the task of diachronic lexical semantic change detection (LSCD). In short, generated definitions are used as `senses', and the change score of a target word is retrieved by comparing their distributions in two time periods under comparison. On the material of five datasets and three languages, we show that generated definitions are indeed specific and general enough to convey a signal sufficient to rank sets of words by the degree of their semantic change over time. Our approach is on par with or outperforms prior non-supervised sense-based LSCD methods. At the same time, it preserves interpretability and allows to inspect the reasons behind a specific shift in terms of discrete definitions-as-senses. This is another step in the direction of explainable semantic change modeling.",
      "paper_authors": [
        "Mariia Fedorova",
        "Andrey Kutuzov",
        "Yves Scherrer"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-06-20",
      "update_time": "2024-07-31",
      "comments": "Findings of ACL 2024",
      "repo_url": "https://github.com/ltgoslo/Definition-generation-for-LSCD"
    },
    "2406.14107": {
      "paper_id": "2406.14107v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.14107v1",
      "paper_key": "2406.14107",
      "paper_title": "Efficient Transmission Scheme for LEO Satellite-Based NB-IoT: A Data-Driven Perspective",
      "paper_url": "http://arxiv.org/abs/2406.14107v1",
      "paper_abstract": "This study analyses the medium access control (MAC) layer aspects of a low-Earth-orbit (LEO) satellite-based Internet of Things (IoT) network. A transmission scheme based on change detection is proposed to accommodate more users within the network and improve energy efficiency. Machine learning (ML) algorithms are also proposed to reduce the payload size by leveraging the correlation among the sensed parameters. Real-world data from an IoT testbed deployed for a smart city application is utilised to analyse the performance regarding collision probability, effective data received and average battery lifetime. The findings reveal that the traffic pattern, post-implementation of the proposed scheme, differs from the commonly assumed Poisson traffic, thus proving the effectiveness of having IoT data from actual deployment. It is demonstrated that the transmission scheme facilitates accommodating more devices while targeting a specific collision probability. Considering the link budget for a direct access NB-IoT scenario, more data is effectively offloaded to the server within the limited visibility of LEO satellites. The average battery lifetimes are also demonstrated to increase by many folds by using the proposed access schemes and ML algorithms.",
      "paper_authors": [
        "Ayush Kumar Dwivedi",
        "Houcine Chougrani",
        "Sachin Chaudhari",
        "Neeraj Varshney",
        "Symeon Chatzinotas"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2024-06-20",
      "update_time": "2024-06-20",
      "comments": null,
      "repo_url": "#"
    },
    "2406.13606": {
      "paper_id": "2406.13606v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.13606v1",
      "paper_key": "2406.13606",
      "paper_title": "DDLNet: Boosting Remote Sensing Change Detection with Dual-Domain Learning",
      "paper_url": "http://arxiv.org/abs/2406.13606v1",
      "paper_abstract": "Remote sensing change detection (RSCD) aims to identify the changes of interest in a region by analyzing multi-temporal remote sensing images, and has an outstanding value for local development monitoring. Existing RSCD methods are devoted to contextual modeling in the spatial domain to enhance the changes of interest. Despite the satisfactory performance achieved, the lack of knowledge in the frequency domain limits the further improvement of model performance. In this paper, we propose DDLNet, a RSCD network based on dual-domain learning (i.e., frequency and spatial domains). In particular, we design a Frequency-domain Enhancement Module (FEM) to capture frequency components from the input bi-temporal images using Discrete Cosine Transform (DCT) and thus enhance the changes of interest. Besides, we devise a Spatial-domain Recovery Module (SRM) to fuse spatiotemporal features for reconstructing spatial details of change representations. Extensive experiments on three benchmark RSCD datasets demonstrate that the proposed method achieves state-of-the-art performance and reaches a more satisfactory accuracy-efficiency trade-off. Our code is publicly available at https://github.com/xwmaxwma/rschange.",
      "paper_authors": [
        "Xiaowen Ma",
        "Jiawei Yang",
        "Rui Che",
        "Huanting Zhang",
        "Wei Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-19",
      "update_time": "2024-06-19",
      "comments": "ICME 2024 Oral",
      "repo_url": "https://github.com/xwmaxwma/rschange"
    },
    "2406.13424": {
      "paper_id": "2406.13424v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.13424v1",
      "paper_key": "2406.13424",
      "paper_title": "Towards a multimodal framework for remote sensing image change retrieval and captioning",
      "paper_url": "http://arxiv.org/abs/2406.13424v1",
      "paper_abstract": "Recently, there has been increasing interest in multimodal applications that integrate text with other modalities, such as images, audio and video, to facilitate natural language interactions with multimodal AI systems. While applications involving standard modalities have been extensively explored, there is still a lack of investigation into specific data modalities such as remote sensing (RS) data. Despite the numerous potential applications of RS data, including environmental protection, disaster monitoring and land planning, available solutions are predominantly focused on specific tasks like classification, captioning and retrieval. These solutions often overlook the unique characteristics of RS data, such as its capability to systematically provide information on the same geographical areas over time. This ability enables continuous monitoring of changes in the underlying landscape. To address this gap, we propose a novel foundation model for bi-temporal RS image pairs, in the context of change detection analysis, leveraging Contrastive Learning and the LEVIR-CC dataset for both captioning and text-image retrieval. By jointly training a contrastive encoder and captioning decoder, our model add text-image retrieval capabilities, in the context of bi-temporal change detection, while maintaining captioning performances that are comparable to the state of the art. We release the source code and pretrained weights at: https://github.com/rogerferrod/RSICRC.",
      "paper_authors": [
        "Roger Ferrod",
        "Luigi Di Caro",
        "Dino Ienco"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-19",
      "update_time": "2024-06-19",
      "comments": null,
      "repo_url": "https://github.com/rogerferrod/rsicrc"
    },
    "2406.12847": {
      "paper_id": "2406.12847v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.12847v1",
      "paper_key": "2406.12847",
      "paper_title": "ChangeViT: Unleashing Plain Vision Transformers for Change Detection",
      "paper_url": "http://arxiv.org/abs/2406.12847v1",
      "paper_abstract": "Change detection in remote sensing images is essential for tracking environmental changes on the Earth's surface. Despite the success of vision transformers (ViTs) as backbones in numerous computer vision applications, they remain underutilized in change detection, where convolutional neural networks (CNNs) continue to dominate due to their powerful feature extraction capabilities. In this paper, our study uncovers ViTs' unique advantage in discerning large-scale changes, a capability where CNNs fall short. Capitalizing on this insight, we introduce ChangeViT, a framework that adopts a plain ViT backbone to enhance the performance of large-scale changes. This framework is supplemented by a detail-capture module that generates detailed spatial features and a feature injector that efficiently integrates fine-grained spatial information into high-level semantic learning. The feature integration ensures that ChangeViT excels in both detecting large-scale changes and capturing fine-grained details, providing comprehensive change detection across diverse scales. Without bells and whistles, ChangeViT achieves state-of-the-art performance on three popular high-resolution datasets (i.e., LEVIR-CD, WHU-CD, and CLCD) and one low-resolution dataset (i.e., OSCD), which underscores the unleashed potential of plain ViTs for change detection. Furthermore, thorough quantitative and qualitative analyses validate the efficacy of the introduced modules, solidifying the effectiveness of our approach. The source code is available at https://github.com/zhuduowang/ChangeViT.",
      "paper_authors": [
        "Duowang Zhu",
        "Xiaohu Huang",
        "Haiyan Huang",
        "Zhenfeng Shao",
        "Qimin Cheng"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-18",
      "update_time": "2024-06-18",
      "comments": null,
      "repo_url": "https://github.com/zhuduowang/changevit"
    },
    "2406.12218": {
      "paper_id": "2406.12218v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.12218v1",
      "paper_key": "2406.12218",
      "paper_title": "Scintillation velocity and arc observations of FRB 20201124A",
      "paper_url": "http://arxiv.org/abs/2406.12218v1",
      "paper_abstract": "We present the scintillation velocity measurements of FRB~20201124A from the FAST observations, which reveal an annual variation. This annual variation is further supported by changes detected in the scintillation arc as observed from the secondary spectrum. We attribute the annual velocity variation to the presence of a moderately anisotropic scattering screen located at a distance of 0.4$\\pm$0.1~kpc from Earth. Our results prove that the scintillation of this FRB is mainly caused by material close to Earth on a Galactic scale. However, scintillation observations of other FRBs may expose their surrounding environment or uncover possible orbital motion if scintillation is caused by materials in their host galaxy.",
      "paper_authors": [
        "Ziwei Wu",
        "Weiwei Zhu",
        "Bing Zhang",
        "Yi Feng",
        "JinLin Han",
        "Di Li",
        "Dongzi Li",
        "Rui Luo",
        "Chenhui Niu",
        "Jiarui Niu",
        "Bojun Wang",
        "Fayin Wang",
        "Pei Wang",
        "Weiyang Wang",
        "Heng Xu",
        "Yuanpei Yang",
        "Yongkun Zhang",
        "Dejiang Zhou",
        "Yuhao Zhu",
        "Can-Min Deng",
        "Yonghua Xu"
      ],
      "primary_category": "astro-ph.HE",
      "publish_time": "2024-06-18",
      "update_time": "2024-06-18",
      "comments": "4 figures, 1 tables, accepted for publication in ApJL",
      "repo_url": "#"
    },
    "2406.11210": {
      "paper_id": "2406.11210v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.11210v1",
      "paper_key": "2406.11210",
      "paper_title": "Zero-Shot Scene Change Detection",
      "paper_url": "http://arxiv.org/abs/2406.11210v1",
      "paper_abstract": "We present a novel, training-free approach to scene change detection. Our method leverages tracking models, which inherently perform change detection between consecutive frames of video by identifying common objects and detecting new or missing objects. Specifically, our method takes advantage of the change detection effect of the tracking model by inputting reference and query images instead of consecutive frames. Furthermore, we focus on the content gap and style gap between two input images in change detection, and address both issues by proposing adaptive content threshold and style bridging layers, respectively. Finally, we extend our approach to video to exploit rich temporal information, enhancing scene change detection performance. We compare our approach and baseline through various experiments. While existing train-based baseline tend to specialize only in the trained domain, our method shows consistent performance across various domains, proving the competitiveness of our approach.",
      "paper_authors": [
        "Kyusik Cho",
        "Dong Yeop Kim",
        "Euntai Kim"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-17",
      "update_time": "2024-06-17",
      "comments": "Preprint. Under review",
      "repo_url": "#"
    },
    "2406.10678": {
      "paper_id": "2406.10678v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.10678v1",
      "paper_key": "2406.10678",
      "paper_title": "A Late-Stage Bitemporal Feature Fusion Network for Semantic Change Detection",
      "paper_url": "http://arxiv.org/abs/2406.10678v1",
      "paper_abstract": "Semantic change detection is an important task in geoscience and earth observation. By producing a semantic change map for each temporal phase, both the land use land cover categories and change information can be interpreted. Recently some multi-task learning based semantic change detection methods have been proposed to decompose the task into semantic segmentation and binary change detection subtasks. However, previous works comprise triple branches in an entangled manner, which may not be optimal and hard to adopt foundation models. Besides, lacking explicit refinement of bitemporal features during fusion may cause low accuracy. In this letter, we propose a novel late-stage bitemporal feature fusion network to address the issue. Specifically, we propose local global attentional aggregation module to strengthen feature fusion, and propose local global context enhancement module to highlight pivotal semantics. Comprehensive experiments are conducted on two public datasets, including SECOND and Landsat-SCD. Quantitative and qualitative results show that our proposed model achieves new state-of-the-art performance on both datasets.",
      "paper_authors": [
        "Chenyao Zhou",
        "Haotian Zhang",
        "Han Guo",
        "Zhengxia Zou",
        "Zhenwei Shi"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-15",
      "update_time": "2024-06-15",
      "comments": null,
      "repo_url": "https://github.com/STORMTROOPERRR/RSISCD"
    },
    "2406.09731": {
      "paper_id": "2406.09731v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.09731v1",
      "paper_key": "2406.09731",
      "paper_title": "Automated GIS-Based Framework for Detecting Crosswalk Changes from Bi-Temporal High-Resolution Aerial Images",
      "paper_url": "http://arxiv.org/abs/2406.09731v1",
      "paper_abstract": "Identification of changes in pavement markings has become crucial for infrastructure monitoring, maintenance, development, traffic management, and safety. Automated extraction of roadway geometry is critical in helping with this, given the increasing availability of high-resolution images and advancements in computer vision and object detection. Specifically, due to the substantial volume of satellite and high-resolution aerial images captured at different time instances, change detection has become a viable solution. In this study, an automated framework is developed to detect changes in crosswalks of Orange, Osceola, and Seminole counties in Florida, utilizing data extracted from high-resolution images obtained at various time intervals. Specifically, for Orange County, crosswalk changes between 2019 and 2021 were manually extracted, verified, and categorized as either new or modified crosswalks. For Seminole County, the developed model was used to automatically extract crosswalk changes between 2018 and 2021, while for Osceola County, changes between 2019 and 2020 were extracted. Findings indicate that Orange County witnessed approximately 2,094 crosswalk changes, with 312 occurring on state roads. In Seminole and Osceola counties, on the other hand, 1,040 and 1,402 crosswalk changes were observed on both local and state roads, respectively. Among these, 340 and 344 were identified on state roads in Seminole and Osceola, respectively. Spatiotemporal changes observed in crosswalks can be utilized to regularly update the existing crosswalk inventories, which is essential for agencies engaged in traffic and safety studies. Data extracted from these crosswalk changes can be combined with traffic and crash data to provide valuable insights to policymakers.",
      "paper_authors": [
        "Richard Boadu Antwi",
        "Samuel Takyi",
        "Alican Karaer",
        "Eren Erman Ozguven",
        "Michael Kimollo",
        "Ren Moses",
        "Maxim A. Dulebenets",
        "Thobias Sando"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-14",
      "update_time": "2024-06-14",
      "comments": null,
      "repo_url": "#"
    },
    "2406.08393": {
      "paper_id": "2406.08393v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.08393v1",
      "paper_key": "2406.08393",
      "paper_title": "SCDNet: Self-supervised Learning Feature-based Speaker Change Detection",
      "paper_url": "http://arxiv.org/abs/2406.08393v1",
      "paper_abstract": "Speaker Change Detection (SCD) is to identify boundaries among speakers in a conversation. Motivated by the success of fine-tuning wav2vec 2.0 models for the SCD task, a further investigation of self-supervised learning (SSL) features for SCD is conducted in this work. Specifically, an SCD model, named SCDNet, is proposed. With this model, various state-of-the-art SSL models, including Hubert, wav2vec 2.0, and WavLm are investigated. To discern the most potent layer of SSL models for SCD, a learnable weighting method is employed to analyze the effectiveness of intermediate representations. Additionally, a fine-tuning-based approach is also implemented to further compare the characteristics of SSL models in the SCD task. Furthermore, a contrastive learning method is proposed to mitigate the overfitting tendencies in the training of both the fine-tuning-based method and SCDNet. Experiments showcase the superiority of WavLm in the SCD task and also demonstrate the good design of SCDNet.",
      "paper_authors": [
        "Yue Li",
        "Xinsheng Wang",
        "Li Zhang",
        "Lei Xie"
      ],
      "primary_category": "eess.AS",
      "publish_time": "2024-06-12",
      "update_time": "2024-06-12",
      "comments": null,
      "repo_url": "#"
    },
    "2406.08079": {
      "paper_id": "2406.08079v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.08079v3",
      "paper_key": "2406.08079",
      "paper_title": "A$^{2}$-MAE: A spatial-temporal-spectral unified remote sensing pre-training method based on anchor-aware masked autoencoder",
      "paper_url": "http://arxiv.org/abs/2406.08079v3",
      "paper_abstract": "Vast amounts of remote sensing (RS) data provide Earth observations across multiple dimensions, encompassing critical spatial, temporal, and spectral information which is essential for addressing global-scale challenges such as land use monitoring, disaster prevention, and environmental change mitigation. Despite various pre-training methods tailored to the characteristics of RS data, a key limitation persists: the inability to effectively integrate spatial, temporal, and spectral information within a single unified model. To unlock the potential of RS data, we construct a Spatial-Temporal-Spectral Structured Dataset (STSSD) characterized by the incorporation of multiple RS sources, diverse coverage, unified locations within image sets, and heterogeneity within images. Building upon this structured dataset, we propose an Anchor-Aware Masked AutoEncoder method (A$^{2}$-MAE), leveraging intrinsic complementary information from the different kinds of images and geo-information to reconstruct the masked patches during the pre-training phase. A$^{2}$-MAE integrates an anchor-aware masking strategy and a geographic encoding module to comprehensively exploit the properties of RS images. Specifically, the proposed anchor-aware masking strategy dynamically adapts the masking process based on the meta-information of a pre-selected anchor image, thereby facilitating the training on images captured by diverse types of RS sources within one model. Furthermore, we propose a geographic encoding method to leverage accurate spatial patterns, enhancing the model generalization capabilities for downstream applications that are generally location-related. Extensive experiments demonstrate our method achieves comprehensive improvements across various downstream tasks compared with existing RS pre-training methods, including image classification, semantic segmentation, and change detection tasks.",
      "paper_authors": [
        "Lixian Zhang",
        "Yi Zhao",
        "Runmin Dong",
        "Jinxiao Zhang",
        "Shuai Yuan",
        "Shilei Cao",
        "Mengxuan Chen",
        "Juepeng Zheng",
        "Weijia Li",
        "Wei Liu",
        "Wayne Zhang",
        "Litong Feng",
        "Haohuan Fu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-12",
      "update_time": "2024-06-16",
      "comments": null,
      "repo_url": "#"
    },
    "2406.08020": {
      "paper_id": "2406.08020v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.08020v1",
      "paper_key": "2406.08020",
      "paper_title": "Generalizable Disaster Damage Assessment via Change Detection with Vision Foundation Model",
      "paper_url": "http://arxiv.org/abs/2406.08020v1",
      "paper_abstract": "The increasing frequency and intensity of natural disasters demand more sophisticated approaches for rapid and precise damage assessment. To tackle this issue, researchers have developed various methods on disaster benchmark datasets from satellite imagery to aid in detecting disaster damage. However, the diverse nature of geographical landscapes and disasters makes it challenging to apply existing methods to regions unseen during training. We present DAVI (Disaster Assessment with VIsion foundation model), which overcomes domain disparities and detects structural damage (e.g., building) without requiring ground-truth labels of the target region. DAVI integrates task-specific knowledge from a model trained on source regions with an image segmentation foundation model to generate pseudo labels of possible damage in the target region. It then employs a two-stage refinement process, targeting both the pixel and overall image, to more accurately pinpoint changes in disaster-struck areas based on before-and-after images. Comprehensive evaluations demonstrate that DAVI achieves exceptional performance across diverse terrains (e.g., USA and Mexico) and disaster types (e.g., wildfires, hurricanes, and earthquakes). This confirms its robustness in assessing disaster impact without dependence on ground-truth labels.",
      "paper_authors": [
        "Kyeongjin Ahn",
        "Sungwon Han",
        "Sungwon Park",
        "Jihee Kim",
        "Sangyoon Park",
        "Meeyoung Cha"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-12",
      "update_time": "2024-06-12",
      "comments": "9 pages, 4 figures, 2 tables",
      "repo_url": "#"
    },
    "2406.05668": {
      "paper_id": "2406.05668v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.05668v2",
      "paper_key": "2406.05668",
      "paper_title": "SRC-Net: Bi-Temporal Spatial Relationship Concerned Network for Change Detection",
      "paper_url": "http://arxiv.org/abs/2406.05668v2",
      "paper_abstract": "Change detection (CD) in remote sensing imagery is a crucial task with applications in environmental monitoring, urban development, and disaster management. CD involves utilizing bi-temporal images to identify changes over time. The bi-temporal spatial relationships between features at the same location at different times play a key role in this process. However, existing change detection networks often do not fully leverage these spatial relationships during bi-temporal feature extraction and fusion. In this work, we propose SRC-Net: a bi-temporal spatial relationship concerned network for CD. The proposed SRC-Net includes a Perception and Interaction Module that incorporates spatial relationships and establishes a cross-branch perception mechanism to enhance the precision and robustness of feature extraction. Additionally, a Patch-Mode joint Feature Fusion Module is introduced to address information loss in current methods. It considers different change modes and concerns about spatial relationships, resulting in more expressive fusion features. Furthermore, we construct a novel network using these two relationship concerned modules and conducted experiments on the LEVIR-CD and WHU Building datasets. The experimental results demonstrate that our network outperforms state-of-the-art (SOTA) methods while maintaining a modest parameter count. We believe our approach sets a new paradigm for change detection and will inspire further advancements in the field. The code and models are publicly available at https://github.com/Chnja/SRCNet.",
      "paper_authors": [
        "Hongjia Chen",
        "Xin Xu",
        "Fangling Pu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-09",
      "update_time": "2024-06-27",
      "comments": "13 pages, 12 figures, IEEE Journal of Selected Topics in Applied\n  Earth Observations and Remote Sensing (2024)",
      "repo_url": "https://github.com/Chnja/SRCNet"
    },
    "2406.04212": {
      "paper_id": "2406.04212v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.04212v1",
      "paper_key": "2406.04212",
      "paper_title": "Sound Event Bounding Boxes",
      "paper_url": "http://arxiv.org/abs/2406.04212v1",
      "paper_abstract": "Sound event detection is the task of recognizing sounds and determining their extent (onset/offset times) within an audio clip. Existing systems commonly predict sound presence confidence in short time frames. Then, thresholding produces binary frame-level presence decisions, with the extent of individual events determined by merging consecutive positive frames. In this paper, we show that frame-level thresholding degrades the prediction of the event extent by coupling it with the system's sound presence confidence. We propose to decouple the prediction of event extent and confidence by introducing SEBBs, which format each sound event prediction as a tuple of a class type, extent, and overall confidence. We also propose a change-detection-based algorithm to convert legacy frame-level outputs into SEBBs. We find the algorithm significantly improves the performance of DCASE 2023 Challenge systems, boosting the state of the art from .644 to .686 PSDS1.",
      "paper_authors": [
        "Janek Ebbers",
        "Francois G. Germain",
        "Gordon Wichern",
        "Jonathan Le Roux"
      ],
      "primary_category": "eess.AS",
      "publish_time": "2024-06-06",
      "update_time": "2024-06-06",
      "comments": "Accepted for publication at Interspeech 2024",
      "repo_url": "https://github.com/merlresearch/sebbs"
    },
    "2406.04207": {
      "paper_id": "2406.04207v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.04207v1",
      "paper_key": "2406.04207",
      "paper_title": "CDMamba: Remote Sensing Image Change Detection with Mamba",
      "paper_url": "http://arxiv.org/abs/2406.04207v1",
      "paper_abstract": "Recently, the Mamba architecture based on state space models has demonstrated remarkable performance in a series of natural language processing tasks and has been rapidly applied to remote sensing change detection (CD) tasks. However, most methods enhance the global receptive field by directly modifying the scanning mode of Mamba, neglecting the crucial role that local information plays in dense prediction tasks (e.g., CD). In this article, we propose a model called CDMamba, which effectively combines global and local features for handling CD tasks. Specifically, the Scaled Residual ConvMamba (SRCM) block is proposed to utilize the ability of Mamba to extract global features and convolution to enhance the local details, to alleviate the issue that current Mamba-based methods lack detailed clues and are difficult to achieve fine detection in dense prediction tasks. Furthermore, considering the characteristics of bi-temporal feature interaction required for CD, the Adaptive Global Local Guided Fusion (AGLGF) block is proposed to dynamically facilitate the bi-temporal interaction guided by other temporal global/local features. Our intuition is that more discriminative change features can be acquired with the guidance of other temporal features. Extensive experiments on three datasets demonstrate that our proposed CDMamba outperforms the current state-of-the-art methods. Our code will be open-sourced at https://github.com/zmoka-zht/CDMamba.",
      "paper_authors": [
        "Haotian Zhang",
        "Keyan Chen",
        "Chenyang Liu",
        "Hao Chen",
        "Zhengxia Zou",
        "Zhenwei Shi"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-06",
      "update_time": "2024-06-06",
      "comments": null,
      "repo_url": "https://github.com/zmoka-zht/cdmamba"
    },
    "2406.03452": {
      "paper_id": "2406.03452v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.03452v3",
      "paper_key": "2406.03452",
      "paper_title": "Using Synchronic Definitions and Semantic Relations to Classify Semantic Change Types",
      "paper_url": "http://arxiv.org/abs/2406.03452v3",
      "paper_abstract": "There is abundant evidence of the fact that the way words change their meaning can be classified in different types of change, highlighting the relationship between the old and new meanings (among which generalization, specialization and co-hyponymy transfer). In this paper, we present a way of detecting these types of change by constructing a model that leverages information both from synchronic lexical relations and definitions of word meanings. Specifically, we use synset definitions and hierarchy information from WordNet and test it on a digitized version of Blank's (1997) dataset of semantic change types. Finally, we show how the sense relationships can improve models for both approximation of human judgments of semantic relatedness as well as binary Lexical Semantic Change Detection.",
      "paper_authors": [
        "Pierluigi Cassotti",
        "Stefano De Pascale",
        "Nina Tahmasebi"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-06-05",
      "update_time": "2024-06-11",
      "comments": null,
      "repo_url": "https://github.com/ChangeIsKey/change-type-classification"
    },
    "2406.00656": {
      "paper_id": "2406.00656v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.00656v2",
      "paper_key": "2406.00656",
      "paper_title": "Presence or Absence: Are Unknown Word Usages in Dictionaries?",
      "paper_url": "http://arxiv.org/abs/2406.00656v2",
      "paper_abstract": "There has been a surge of interest in computational modeling of semantic change. The foci of previous works are on detecting and interpreting word senses gained over time; however, it remains unclear whether the gained senses are covered by dictionaries. In this work, we aim to fill this research gap by comparing detected word senses with dictionary sense inventories in order to bridge between the communities of lexical semantic change detection and lexicography. We evaluate our system in the AXOLOTL-24 shared task for Finnish, Russian and German languages \\cite{fedorova-etal-2024-axolotl}. Our system is fully unsupervised. It leverages a graph-based clustering approach to predict mappings between unknown word usages and dictionary entries for Subtask 1, and generates dictionary-like definitions for those novel word usages through the state-of-the-art Large Language Models such as GPT-4 and LLaMA-3 for Subtask 2. In Subtask 1, our system outperforms the baseline system by a large margin, and it offers interpretability for the mapping results by distinguishing between matched and unmatched (novel) word usages through our graph-based clustering approach. Our system ranks first in Finnish and German, and ranks second in Russian on the Subtask 2 test-phase leaderboard. These results show the potential of our system in managing dictionary entries, particularly for updating dictionaries to include novel sense entries. Our code and data are made publicly available\\footnote{\\url{https://github.com/xiaohemaikoo/axolotl24-ABDN-NLP}}.",
      "paper_authors": [
        "Xianghe Ma",
        "Dominik Schlechtweg",
        "Wei Zhao"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-06-02",
      "update_time": "2024-07-04",
      "comments": "LChange24 Camera Ready",
      "repo_url": "https://github.com/xiaohemaikoo/axolotl24-abdn-nlp"
    },
    "2405.20161": {
      "paper_id": "2405.20161v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.20161v1",
      "paper_key": "2405.20161",
      "paper_title": "Landslide mapping from Sentinel-2 imagery through change detection",
      "paper_url": "http://arxiv.org/abs/2405.20161v1",
      "paper_abstract": "Landslides are one of the most critical and destructive geohazards. Widespread development of human activities and settlements combined with the effects of climate change on weather are resulting in a high increase in the frequency and destructive power of landslides, making them a major threat to human life and the economy. In this paper, we explore methodologies to map newly-occurred landslides using Sentinel-2 imagery automatically. All approaches presented are framed as a bi-temporal change detection problem, requiring only a pair of Sentinel-2 images, taken respectively before and after a landslide-triggering event. Furthermore, we introduce a novel deep learning architecture for fusing Sentinel-2 bi-temporal image pairs with Digital Elevation Model (DEM) data, showcasing its promising performances w.r.t. other change detection models in the literature. As a parallel task, we address limitations in existing datasets by creating a novel geodatabase, which includes manually validated open-access landslide inventories over heterogeneous ecoregions of the world. We release both code and dataset with an open-source license.",
      "paper_authors": [
        "Tommaso Monopoli",
        "Fabio Montello",
        "Claudio Rossi"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-30",
      "update_time": "2024-05-30",
      "comments": "to be published in IEEE IGARSS 2024 conference proceedings",
      "repo_url": "https://github.com/links-ads/igarss-landslide-delineation"
    },
    "2405.19055": {
      "paper_id": "2405.19055v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.19055v3",
      "paper_key": "2405.19055",
      "paper_title": "FUSU: A Multi-temporal-source Land Use Change Segmentation Dataset for Fine-grained Urban Semantic Understanding",
      "paper_url": "http://arxiv.org/abs/2405.19055v3",
      "paper_abstract": "Fine urban change segmentation using multi-temporal remote sensing images is essential for understanding human-environment interactions in urban areas. Although there have been advances in high-quality land cover datasets that reveal the physical features of urban landscapes, the lack of fine-grained land use datasets hinders a deeper understanding of how human activities are distributed across the landscape and the impact of these activities on the environment, thus constraining proper technique development. To address this, we introduce FUSU, the first fine-grained land use change segmentation dataset for Fine-grained Urban Semantic Understanding. FUSU features the most detailed land use classification system to date, with 17 classes and 30 billion pixels of annotations. It includes bi-temporal high-resolution satellite images with 0.2-0.5 m ground sample distance and monthly optical and radar satellite time series, covering 847 km^2 across five urban areas in the southern and northern of China with different geographical features. The fine-grained land use pixel-wise annotations and high spatial-temporal resolution data provide a robust foundation for developing proper deep learning models to provide contextual insights on human activities and urbanization. To fully leverage FUSU, we propose a unified time-series architecture for both change detection and segmentation. We benchmark FUSU on various methods for several tasks. Dataset and code are available at: https://github.com/yuanshuai0914/FUSU.",
      "paper_authors": [
        "Shuai Yuan",
        "Guancong Lin",
        "Lixian Zhang",
        "Runmin Dong",
        "Jinxiao Zhang",
        "Shuang Chen",
        "Juepeng Zheng",
        "Jie Wang",
        "Haohuan Fu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-29",
      "update_time": "2024-06-07",
      "comments": null,
      "repo_url": "https://github.com/yuanshuai0914/fusu"
    },
    "2405.18224": {
      "paper_id": "2405.18224v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.18224v1",
      "paper_key": "2405.18224",
      "paper_title": "SSLChange: A Self-supervised Change Detection Framework Based on Domain Adaptation",
      "paper_url": "http://arxiv.org/abs/2405.18224v1",
      "paper_abstract": "In conventional remote sensing change detection (RS CD) procedures, extensive manual labeling for bi-temporal images is first required to maintain the performance of subsequent fully supervised training. However, pixel-level labeling for CD tasks is very complex and time-consuming. In this paper, we explore a novel self-supervised contrastive framework applicable to the RS CD task, which promotes the model to accurately capture spatial, structural, and semantic information through domain adapter and hierarchical contrastive head. The proposed SSLChange framework accomplishes self-learning only by taking a single-temporal sample and can be flexibly transferred to main-stream CD baselines. With self-supervised contrastive learning, feature representation pre-training can be performed directly based on the original data even without labeling. After a certain amount of labels are subsequently obtained, the pre-trained features will be aligned with the labels for fully supervised fine-tuning. Without introducing any additional data or labels, the performance of downstream baselines will experience a significant enhancement. Experimental results on 2 entire datasets and 6 diluted datasets show that our proposed SSLChange improves the performance and stability of CD baseline in data-limited situations. The code of SSLChange will be released at \\url{https://github.com/MarsZhaoYT/SSLChange}",
      "paper_authors": [
        "Yitao Zhao",
        "Turgay Celik",
        "Nanqing Liu",
        "Feng Gao",
        "Heng-Chao Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-28",
      "update_time": "2024-05-28",
      "comments": "This manuscript has been submitted to IEEE TGRS and is under review",
      "repo_url": "https://github.com/marszhaoyt/sslchange"
    },
    "2405.15670": {
      "paper_id": "2405.15670v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.15670v1",
      "paper_key": "2405.15670",
      "paper_title": "Post-selection inference for quantifying uncertainty in changes in variance",
      "paper_url": "http://arxiv.org/abs/2405.15670v1",
      "paper_abstract": "Quantifying uncertainty in detected changepoints is an important problem. However it is challenging as the naive approach would use the data twice, first to detect the changes, and then to test them. This will bias the test, and can lead to anti-conservative p-values. One approach to avoid this is to use ideas from post-selection inference, which conditions on the information in the data used to choose which changes to test. As a result this produces valid p-values; that is, p-values that have a uniform distribution if there is no change. Currently such methods have been developed for detecting changes in mean only. This paper presents two approaches for constructing post-selection p-values for detecting changes in variance. These vary depending on the method use to detect the changes, but are general in terms of being applicable for a range of change-detection methods and a range of hypotheses that we may wish to test.",
      "paper_authors": [
        "Rachel Carrington",
        "Paul Fearnhead"
      ],
      "primary_category": "stat.ME",
      "publish_time": "2024-05-24",
      "update_time": "2024-05-24",
      "comments": "25 pages, 12 figures, plus 6 pages supplementary material",
      "repo_url": "#"
    },
    "2405.14214": {
      "paper_id": "2405.14214v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.14214v1",
      "paper_key": "2405.14214",
      "paper_title": "A Behavior-Aware Approach for Deep Reinforcement Learning in Non-stationary Environments without Known Change Points",
      "paper_url": "http://arxiv.org/abs/2405.14214v1",
      "paper_abstract": "Deep reinforcement learning is used in various domains, but usually under the assumption that the environment has stationary conditions like transitions and state distributions. When this assumption is not met, performance suffers. For this reason, tracking continuous environmental changes and adapting to unpredictable conditions is challenging yet crucial because it ensures that systems remain reliable and flexible in practical scenarios. Our research introduces Behavior-Aware Detection and Adaptation (BADA), an innovative framework that merges environmental change detection with behavior adaptation. The key inspiration behind our method is that policies exhibit different global behaviors in changing environments. Specifically, environmental changes are identified by analyzing variations between behaviors using Wasserstein distances without manually set thresholds. The model adapts to the new environment through behavior regularization based on the extent of changes. The results of a series of experiments demonstrate better performance relative to several current algorithms. This research also indicates significant potential for tackling this long-standing challenge.",
      "paper_authors": [
        "Zihe Liu",
        "Jie Lu",
        "Guangquan Zhang",
        "Junyu Xuan"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-05-23",
      "update_time": "2024-05-23",
      "comments": "Accepted by IJCAI 2024",
      "repo_url": "#"
    },
    "2405.11511": {
      "paper_id": "2405.11511v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.11511v1",
      "paper_key": "2405.11511",
      "paper_title": "Online Action Representation using Change Detection and Symbolic Programming",
      "paper_url": "http://arxiv.org/abs/2405.11511v1",
      "paper_abstract": "This paper addresses the critical need for online action representation, which is essential for various applications like rehabilitation, surveillance, etc. The task can be defined as representation of actions as soon as they happen in a streaming video without access to video frames in the future. Most of the existing methods use predefined window sizes for video segments, which is a restrictive assumption on the dynamics. The proposed method employs a change detection algorithm to automatically segment action sequences, which form meaningful sub-actions and subsequently fit symbolic generative motion programs to the clipped segments. We determine the start time and end time of segments using change detection followed by a piece-wise linear fit algorithm on joint angle and bone length sequences. Domain-specific symbolic primitives are fit to pose keypoint trajectories of those extracted segments in order to obtain a higher level semantic representation. Since this representation is part-based, it is complementary to the compositional nature of human actions, i.e., a complex activity can be broken down into elementary sub-actions. We show the effectiveness of this representation in the downstream task of class agnostic repetition detection. We propose a repetition counting algorithm based on consecutive similarity matching of primitives, which can do online repetition counting. We also compare the results with a similar but offline repetition counting algorithm. The results of the experiments demonstrate that, despite operating online, the proposed method performs better or on par with the existing method.",
      "paper_authors": [
        "Vishnu S Nair",
        "Sneha Sree",
        "Jayaraj Joseph",
        "Mohanasankar Sivaprakasam"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-19",
      "update_time": "2024-05-19",
      "comments": null,
      "repo_url": "#"
    },
    "2405.09896": {
      "paper_id": "2405.09896v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.09896v1",
      "paper_key": "2405.09896",
      "paper_title": "Confidence Estimation in Unsupervised Deep Change Vector Analysis",
      "paper_url": "http://arxiv.org/abs/2405.09896v1",
      "paper_abstract": "Unsupervised transfer learning-based change detection methods exploit the feature extraction capability of pre-trained networks to distinguish changed pixels from the unchanged ones. However, their performance may vary significantly depending on several geographical and model-related aspects. In many applications, it is of utmost importance to provide trustworthy or confident results, even if over a subset of pixels. The core challenge in this problem is to identify changed pixels and confident pixels in an unsupervised manner. To address this, we propose a two-network model - one tasked with mere change detection and the other with confidence estimation. While the change detection network can be used in conjunction with popular transfer learning-based change detection methods such as Deep Change Vector Analysis, the confidence estimation network operates similarly to a randomized smoothing model. By ingesting ensembles of inputs perturbed by noise, it creates a distribution over the output and assigns confidence to each pixel's outcome. We tested the proposed method on three different Earth observation sensors: optical, Synthetic Aperture Radar, and hyperspectral sensors.",
      "paper_authors": [
        "Sudipan Saha"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2024-05-16",
      "update_time": "2024-05-16",
      "comments": null,
      "repo_url": "#"
    },
    "2405.07916": {
      "paper_id": "2405.07916v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.07916v1",
      "paper_key": "2405.07916",
      "paper_title": "IMAFD: An Interpretable Multi-stage Approach to Flood Detection from time series Multispectral Data",
      "paper_url": "http://arxiv.org/abs/2405.07916v1",
      "paper_abstract": "In this paper, we address two critical challenges in the domain of flood detection: the computational expense of large-scale time series change detection and the lack of interpretable decision-making processes on explainable AI (XAI). To overcome these challenges, we proposed an interpretable multi-stage approach to flood detection, IMAFD has been proposed. It provides an automatic, efficient and interpretable solution suitable for large-scale remote sensing tasks and offers insight into the decision-making process. The proposed IMAFD approach combines the analysis of the dynamic time series image sequences to identify images with possible flooding with the static, within-image semantic segmentation. It combines anomaly detection (at both image and pixel level) with semantic segmentation. The flood detection problem is addressed through four stages: (1) at a sequence level: identifying the suspected images (2) at a multi-image level: detecting change within suspected images (3) at an image level: semantic segmentation of images into Land, Water or Cloud class (4) decision making. Our contributions are two folder. First, we efficiently reduced the number of frames to be processed for dense change detection by providing a multi-stage holistic approach to flood detection. Second, the proposed semantic change detection method (stage 3) provides human users with an interpretable decision-making process, while most of the explainable AI (XAI) methods provide post hoc explanations. The evaluation of the proposed IMAFD framework was performed on three datasets, WorldFloods, RavAEn and MediaEval. For all the above datasets, the proposed framework demonstrates a competitive performance compared to other methods offering also interpretability and insight.",
      "paper_authors": [
        "Ziyang Zhang",
        "Plamen Angelov",
        "Dmitry Kangin",
        "Nicolas Long\u00e9p\u00e9"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-13",
      "update_time": "2024-05-13",
      "comments": null,
      "repo_url": "#"
    },
    "2405.06933": {
      "paper_id": "2405.06933v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.06933v1",
      "paper_key": "2405.06933",
      "paper_title": "Syndrome-based Fusion Rules in Heterogeneous Distributed Quickest Change Detection",
      "paper_url": "http://arxiv.org/abs/2405.06933v1",
      "paper_abstract": "In this paper, the heterogeneous distributed quickest change detection (HetDQCD) with 1-bit non-anonymous feedback is studied. The concept of syndromes is introduced and the family of syndrome-based fusion rules is proposed, which encompasses all deterministic fusion rules as special cases. Through the Hasse diagram of syndromes, upper and lower bounds on the second-order performance of expected detection delay as a function of average run length to false alarm are provided. An interesting instance, the weighted voting rule previously proposed in our prior work, is then revisited, for which an efficient pruning method for breadth-first search in the Hasse diagram is proposed to analyze the performance. This in turn assists in the design of the weight threshold in the weighted voting rule. Simulation results corroborate that our analysis is instrumental in identifying a proper design for the weighted voting rule, demonstrating consistent superiority over both the anonymous voting rule and the group selection rule in HetDQCD.",
      "paper_authors": [
        "Wen-Hsuan Li",
        "Yu-Chih Huang"
      ],
      "primary_category": "cs.IT",
      "publish_time": "2024-05-11",
      "update_time": "2024-05-11",
      "comments": null,
      "repo_url": "#"
    },
    "2405.06323": {
      "paper_id": "2405.06323v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.06323v1",
      "paper_key": "2405.06323",
      "paper_title": "Open Access Battle Damage Detection via Pixel-Wise T-Test on Sentinel-1 Imagery",
      "paper_url": "http://arxiv.org/abs/2405.06323v1",
      "paper_abstract": "In the context of recent, highly destructive conflicts in Gaza and Ukraine, reliable estimates of building damage are essential for an informed public discourse, human rights monitoring, and humanitarian aid provision. Given the contentious nature of conflict damage assessment, these estimates must be fully reproducible, explainable, and derived from open access data. This paper introduces a new method for building damage detection-- the Pixel-Wise T-Test (PWTT)-- that satisfies these conditions. Using a combination of freely-available synthetic aperture radar imagery and statistical change detection, the PWTT generates accurate conflict damage estimates across a wide area at regular time intervals. Accuracy is assessed using an original dataset of over half a million labeled building footprints spanning 12 cities across Ukraine, Palestine, Syria, and Iraq. Despite being simple and lightweight, the algorithm achieves building-level accuracy statistics (AUC=0.88 across Ukraine, 0.81 in Gaza) rivalling state of the art methods that use deep learning and high resolution imagery. The workflow is open source and deployed entirely within the Google Earth Engine environment, allowing for the generation of interactive Battle Damage Dashboards for Ukraine and Gaza that update in near-real time, allowing the public and humanitarian practitioners to immediately get estimates of damaged buildings in a given area.",
      "paper_authors": [
        "Ollie Ballinger"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-10",
      "update_time": "2024-05-10",
      "comments": null,
      "repo_url": "#"
    },
    "2405.06246": {
      "paper_id": "2405.06246v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.06246v1",
      "paper_key": "2405.06246",
      "paper_title": "Comparative Analysis of Advanced Feature Matching Algorithms in Challenging High Spatial Resolution Optical Satellite Stereo Scenarios",
      "paper_url": "http://arxiv.org/abs/2405.06246v1",
      "paper_abstract": "Feature matching determines the orientation accuracy for the High Spatial Resolution (HSR) optical satellite stereos, subsequently impacting several significant applications such as 3D reconstruction and change detection. However, the matching of off-track HSR optical satellite stereos often encounters challenging conditions including wide-baseline observation, significant radiometric differences, multi-temporal changes, varying spatial resolutions, inconsistent spectral resolution, and diverse sensors. In this study, we evaluate various advanced feature matching algorithms for HSR optical satellite stereos. Utilizing a specially constructed dataset from five satellites across six challenging scenarios, HSROSS Dataset, we conduct a comparative analysis of four algorithms: the traditional SIFT, and deep-learning based methods including SuperPoint + SuperGlue, SuperPoint + LightGlue, and LoFTR. Our findings highlight overall superior performance of SuperPoint + LightGlue in balancing robustness, accuracy, distribution, and efficiency, showcasing its potential in complex HSR optical satellite scenarios.",
      "paper_authors": [
        "Qiyan Luo",
        "Jidan Zhang",
        "Yuzhen Xie",
        "Xu Huang",
        "Ting Han"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-10",
      "update_time": "2024-05-10",
      "comments": "The manuscript is accepted as Oral Presentation in IEEE International\n  Geoscience and Remote Sensing Symposium(IGARSS 2024)",
      "repo_url": "#"
    },
    "2405.06185": {
      "paper_id": "2405.06185v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.06185v1",
      "paper_key": "2405.06185",
      "paper_title": "Zero-shot Degree of Ill-posedness Estimation for Active Small Object Change Detection",
      "paper_url": "http://arxiv.org/abs/2405.06185v1",
      "paper_abstract": "In everyday indoor navigation, robots often needto detect non-distinctive small-change objects (e.g., stationery,lost items, and junk, etc.) to maintain domain knowledge. Thisis most relevant to ground-view change detection (GVCD), a recently emerging research area in the field of computer vision.However, these existing techniques rely on high-quality class-specific object priors to regularize a change detector modelthat cannot be applied to semantically nondistinctive smallobjects. To address ill-posedness, in this study, we explorethe concept of degree-of-ill-posedness (DoI) from the newperspective of GVCD, aiming to improve both passive and activevision. This novel DoI problem is highly domain-dependent,and manually collecting fine-grained annotated training datais expensive. To regularize this problem, we apply the conceptof self-supervised learning to achieve efficient DoI estimationscheme and investigate its generalization to diverse datasets.Specifically, we tackle the challenging issue of obtaining self-supervision cues for semantically non-distinctive unseen smallobjects and show that novel \"oversegmentation cues\" from openvocabulary semantic segmentation can be effectively exploited.When applied to diverse real datasets, the proposed DoI modelcan boost state-of-the-art change detection models, and it showsstable and consistent improvements when evaluated on real-world datasets.",
      "paper_authors": [
        "Koji Takeda",
        "Kanji Tanaka",
        "Yoshimasa Nakamura",
        "Asako Kanezaki"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-10",
      "update_time": "2024-05-10",
      "comments": "7 pages, 7 figures",
      "repo_url": "#"
    },
    "2405.05724": {
      "paper_id": "2405.05724v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.05724v1",
      "paper_key": "2405.05724",
      "paper_title": "Private Online Community Detection for Censored Block Models",
      "paper_url": "http://arxiv.org/abs/2405.05724v1",
      "paper_abstract": "We study the private online change detection problem for dynamic communities, using a censored block model (CBM). Focusing on the notion of edge differential privacy (DP), we seek to understand the fundamental tradeoffs between the privacy budget, detection delay, and exact community recovery of community labels. We establish the theoretical lower bound on the delay in detecting changes privately and propose an algorithm capable of identifying changes in the community structure, while maintaining user privacy. Further, we provide theoretical guarantees for the effectiveness of our proposed method by showing necessary and sufficient conditions on change detection and exact recovery under edge DP. Simulation and real data examples are provided to validate the proposed method.",
      "paper_authors": [
        "Mohamed Seif",
        "Liyan Xie",
        "Andrea J. Goldsmith",
        "H. Vincent Poor"
      ],
      "primary_category": "cs.SI",
      "publish_time": "2024-05-09",
      "update_time": "2024-05-09",
      "comments": null,
      "repo_url": "#"
    },
    "2405.04788": {
      "paper_id": "2405.04788v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.04788v3",
      "paper_key": "2405.04788",
      "paper_title": "SemiCD-VL: Visual-Language Model Guidance Makes Better Semi-supervised Change Detector",
      "paper_url": "http://arxiv.org/abs/2405.04788v3",
      "paper_abstract": "Change Detection (CD) aims to identify pixels with semantic changes between images. However, annotating massive numbers of pixel-level images is labor-intensive and costly, especially for multi-temporal images, which require pixel-wise comparisons by human experts. Considering the excellent performance of visual language models (VLMs) for zero-shot, open-vocabulary, etc. with prompt-based reasoning, it is promising to utilize VLMs to make better CD under limited labeled data. In this paper, we propose a VLM guidance-based semi-supervised CD method, namely SemiCD-VL. The insight of SemiCD-VL is to synthesize free change labels using VLMs to provide additional supervision signals for unlabeled data. However, almost all current VLMs are designed for single-temporal images and cannot be directly applied to bi- or multi-temporal images. Motivated by this, we first propose a VLM-based mixed change event generation (CEG) strategy to yield pseudo labels for unlabeled CD data. Since the additional supervised signals provided by these VLM-driven pseudo labels may conflict with the pseudo labels from the consistency regularization paradigm (e.g. FixMatch), we propose the dual projection head for de-entangling different signal sources. Further, we explicitly decouple the bi-temporal images semantic representation through two auxiliary segmentation decoders, which are also guided by VLM. Finally, to make the model more adequately capture change representations, we introduce metric-aware supervision by feature-level contrastive loss in auxiliary branches. Extensive experiments show the advantage of SemiCD-VL. For instance, SemiCD-VL improves the FixMatch baseline by +5.3 IoU on WHU-CD and by +2.4 IoU on LEVIR-CD with 5% labels. In addition, our CEG strategy, in an un-supervised manner, can achieve performance far superior to state-of-the-art un-supervised CD methods.",
      "paper_authors": [
        "Kaiyu Li",
        "Xiangyong Cao",
        "Yupeng Deng",
        "Junmin Liu",
        "Deyu Meng",
        "Zhi Wang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-08",
      "update_time": "2024-08-02",
      "comments": "13 pages, 5 figures",
      "repo_url": "https://github.com/likyoo/semicd-vl"
    },
    "2405.02372": {
      "paper_id": "2405.02372v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.02372v2",
      "paper_key": "2405.02372",
      "paper_title": "Triadic-OCD: Asynchronous Online Change Detection with Provable Robustness, Optimality, and Convergence",
      "paper_url": "http://arxiv.org/abs/2405.02372v2",
      "paper_abstract": "The primary goal of online change detection (OCD) is to promptly identify changes in the data stream. OCD problem find a wide variety of applications in diverse areas, e.g., security detection in smart grids and intrusion detection in communication networks. Prior research usually assumes precise knowledge of the system parameters. Nevertheless, this presumption often proves unattainable in practical scenarios due to factors such as estimation errors, system updates, etc. This paper aims to take the first attempt to develop a triadic-OCD framework with certifiable robustness, provable optimality, and guaranteed convergence. In addition, the proposed triadic-OCD algorithm can be realized in a fully asynchronous distributed manner, easing the necessity of transmitting the data to a single server. This asynchronous mechanism could also mitigate the straggler issue that faced by traditional synchronous algorithm. Moreover, the non-asymptotic convergence property of Triadic-OCD is theoretically analyzed, and its iteration complexity to achieve an $\\epsilon$-optimal point is derived. Extensive experiments have been conducted to elucidate the effectiveness of the proposed method.",
      "paper_authors": [
        "Yancheng Huang",
        "Kai Yang",
        "Zelin Zhu",
        "Leian Chen"
      ],
      "primary_category": "stat.ML",
      "publish_time": "2024-05-03",
      "update_time": "2024-06-04",
      "comments": "Accepted at ICML2024",
      "repo_url": "#"
    },
    "2405.01920": {
      "paper_id": "2405.01920v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.01920v1",
      "paper_key": "2405.01920",
      "paper_title": "Lightweight Change Detection in Heterogeneous Remote Sensing Images with Online All-Integer Pruning Training",
      "paper_url": "http://arxiv.org/abs/2405.01920v1",
      "paper_abstract": "Detection of changes in heterogeneous remote sensing images is vital, especially in response to emergencies like earthquakes and floods. Current homogenous transformation-based change detection (CD) methods often suffer from high computation and memory costs, which are not friendly to edge-computation devices like onboard CD devices at satellites. To address this issue, this paper proposes a new lightweight CD method for heterogeneous remote sensing images that employs the online all-integer pruning (OAIP) training strategy to efficiently fine-tune the CD network using the current test data. The proposed CD network consists of two visual geometry group (VGG) subnetworks as the backbone architecture. In the OAIP-based training process, all the weights, gradients, and intermediate data are quantized to integers to speed up training and reduce memory usage, where the per-layer block exponentiation scaling scheme is utilized to reduce the computation errors of network parameters caused by quantization. Second, an adaptive filter-level pruning method based on the L1-norm criterion is employed to further lighten the fine-tuning process of the CD network. Experimental results show that the proposed OAIP-based method attains similar detection performance (but with significantly reduced computation complexity and memory usage) in comparison with state-of-the-art CD methods.",
      "paper_authors": [
        "Chengyang Zhang",
        "Weiming Li",
        "Gang Li",
        "Huina Song",
        "Zhaohui Song",
        "Xueqian Wang",
        "Antonio Plaza"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-03",
      "update_time": "2024-05-03",
      "comments": null,
      "repo_url": "#"
    },
    "2405.01065": {
      "paper_id": "2405.01065v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.01065v1",
      "paper_key": "2405.01065",
      "paper_title": "MFDS-Net: Multi-Scale Feature Depth-Supervised Network for Remote Sensing Change Detection with Global Semantic and Detail Information",
      "paper_url": "http://arxiv.org/abs/2405.01065v1",
      "paper_abstract": "Change detection as an interdisciplinary discipline in the field of computer vision and remote sensing at present has been receiving extensive attention and research. Due to the rapid development of society, the geographic information captured by remote sensing satellites is changing faster and more complex, which undoubtedly poses a higher challenge and highlights the value of change detection tasks. We propose MFDS-Net: Multi-Scale Feature Depth-Supervised Network for Remote Sensing Change Detection with Global Semantic and Detail Information (MFDS-Net) with the aim of achieving a more refined description of changing buildings as well as geographic information, enhancing the localisation of changing targets and the acquisition of weak features. To achieve the research objectives, we use a modified ResNet_34 as backbone network to perform feature extraction and DO-Conv as an alternative to traditional convolution to better focus on the association between feature information and to obtain better training results. We propose the Global Semantic Enhancement Module (GSEM) to enhance the processing of high-level semantic information from a global perspective. The Differential Feature Integration Module (DFIM) is proposed to strengthen the fusion of different depth feature information, achieving learning and extraction of differential features. The entire network is trained and optimized using a deep supervision mechanism.   The experimental outcomes of MFDS-Net surpass those of current mainstream change detection networks. On the LEVIR dataset, it achieved an F1 score of 91.589 and IoU of 84.483, on the WHU dataset, the scores were F1: 92.384 and IoU: 86.807, and on the GZ-CD dataset, the scores were F1: 86.377 and IoU: 76.021. The code is available at https://github.com/AOZAKIiii/MFDS-Net",
      "paper_authors": [
        "Zhenyang Huang",
        "Zhaojin Fu",
        "Song Jintao",
        "Genji Yuan",
        "Jinjiang Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-02",
      "update_time": "2024-05-02",
      "comments": null,
      "repo_url": "https://github.com/aozakiiii/mfds-net"
    },
    "2405.00874": {
      "paper_id": "2405.00874v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.00874v1",
      "paper_key": "2405.00874",
      "paper_title": "Artificial intelligence for context-aware visual change detection in software test automation",
      "paper_url": "http://arxiv.org/abs/2405.00874v1",
      "paper_abstract": "Automated software testing is integral to the software development process, streamlining workflows and ensuring product reliability. Visual testing within this context, especially concerning user interface (UI) and user experience (UX) validation, stands as one of crucial determinants of overall software quality. Nevertheless, conventional methods like pixel-wise comparison and region-based visual change detection fall short in capturing contextual similarities, nuanced alterations, and understanding the spatial relationships between UI elements. In this paper, we introduce a novel graph-based method for visual change detection in software test automation. Leveraging a machine learning model, our method accurately identifies UI controls from software screenshots and constructs a graph representing contextual and spatial relationships between the controls. This information is then used to find correspondence between UI controls within screenshots of different versions of a software. The resulting graph encapsulates the intricate layout of the UI and underlying contextual relations, providing a holistic and context-aware model. This model is finally used to detect and highlight visual regressions in the UI. Comprehensive experiments on different datasets showed that our change detector can accurately detect visual software changes in various simple and complex test scenarios. Moreover, it outperformed pixel-wise comparison and region-based baselines by a large margin in more complex testing scenarios. This work not only contributes to the advancement of visual change detection but also holds practical implications, offering a robust solution for real-world software test automation challenges, enhancing reliability, and ensuring the seamless evolution of software interfaces.",
      "paper_authors": [
        "Milad Moradi",
        "Ke Yan",
        "David Colwell",
        "Rhona Asgari"
      ],
      "primary_category": "cs.SE",
      "publish_time": "2024-05-01",
      "update_time": "2024-05-01",
      "comments": null,
      "repo_url": "#"
    },
    "2405.00842": {
      "paper_id": "2405.00842v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.00842v1",
      "paper_key": "2405.00842",
      "paper_title": "Quickest Change Detection with Confusing Change",
      "paper_url": "http://arxiv.org/abs/2405.00842v1",
      "paper_abstract": "In the problem of quickest change detection (QCD), a change occurs at some unknown time in the distribution of a sequence of independent observations. This work studies a QCD problem where the change is either a bad change, which we aim to detect, or a confusing change, which is not of our interest. Our objective is to detect a bad change as quickly as possible while avoiding raising a false alarm for pre-change or a confusing change. We identify a specific set of pre-change, bad change, and confusing change distributions that pose challenges beyond the capabilities of standard Cumulative Sum (CuSum) procedures. Proposing novel CuSum-based detection procedures, S-CuSum and J-CuSum, leveraging two CuSum statistics, we offer solutions applicable across all kinds of pre-change, bad change, and confusing change distributions. For both S-CuSum and J-CuSum, we provide analytical performance guarantees and validate them by numerical results. Furthermore, both procedures are computationally efficient as they only require simple recursive updates.",
      "paper_authors": [
        "Yu-Zhen Janice Chen",
        "Jinhang Zuo",
        "Venugopal V. Veeravalli",
        "Don Towsley"
      ],
      "primary_category": "math.ST",
      "publish_time": "2024-05-01",
      "update_time": "2024-05-01",
      "comments": null,
      "repo_url": "#"
    },
    "2404.18570": {
      "paper_id": "2404.18570v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.18570v1",
      "paper_key": "2404.18570",
      "paper_title": "Analyzing Semantic Change through Lexical Replacements",
      "paper_url": "http://arxiv.org/abs/2404.18570v1",
      "paper_abstract": "Modern language models are capable of contextualizing words based on their surrounding context. However, this capability is often compromised due to semantic change that leads to words being used in new, unexpected contexts not encountered during pre-training. In this paper, we model \\textit{semantic change} by studying the effect of unexpected contexts introduced by \\textit{lexical replacements}. We propose a \\textit{replacement schema} where a target word is substituted with lexical replacements of varying relatedness, thus simulating different kinds of semantic change. Furthermore, we leverage the replacement schema as a basis for a novel \\textit{interpretable} model for semantic change. We are also the first to evaluate the use of LLaMa for semantic change detection.",
      "paper_authors": [
        "Francesco Periti",
        "Pierluigi Cassotti",
        "Haim Dubossarsky",
        "Nina Tahmasebi"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-04-29",
      "update_time": "2024-04-29",
      "comments": null,
      "repo_url": "https://github.com/changeiskey/asc-lr"
    },
    "2404.17765": {
      "paper_id": "2404.17765v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.17765v1",
      "paper_key": "2404.17765",
      "paper_title": "RFL-CDNet: Towards Accurate Change Detection via Richer Feature Learning",
      "paper_url": "http://arxiv.org/abs/2404.17765v1",
      "paper_abstract": "Change Detection is a crucial but extremely challenging task of remote sensing image analysis, and much progress has been made with the rapid development of deep learning. However, most existing deep learning-based change detection methods mainly focus on intricate feature extraction and multi-scale feature fusion, while ignoring the insufficient utilization of features in the intermediate stages, thus resulting in sub-optimal results. To this end, we propose a novel framework, named RFL-CDNet, that utilizes richer feature learning to boost change detection performance. Specifically, we first introduce deep multiple supervision to enhance intermediate representations, thus unleashing the potential of backbone feature extractor at each stage. Furthermore, we design the Coarse-To-Fine Guiding (C2FG) module and the Learnable Fusion (LF) module to further improve feature learning and obtain more discriminative feature representations. The C2FG module aims to seamlessly integrate the side prediction from the previous coarse-scale into the current fine-scale prediction in a coarse-to-fine manner, while LF module assumes that the contribution of each stage and each spatial location is independent, thus designing a learnable module to fuse multiple predictions. Experiments on several benchmark datasets show that our proposed RFL-CDNet achieves state-of-the-art performance on WHU cultivated land dataset and CDD dataset, and the second-best performance on WHU building dataset. The source code and models are publicly available at https://github.com/Hhaizee/RFL-CDNet.",
      "paper_authors": [
        "Yuhang Gan",
        "Wenjie Xuan",
        "Hang Chen",
        "Juhua Liu",
        "Bo Du"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-04-27",
      "update_time": "2024-04-27",
      "comments": "Accepted by PR, volume 153",
      "repo_url": "https://github.com/hhaizee/rfl-cdnet"
    },
    "2404.17565": {
      "paper_id": "2404.17565v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.17565v1",
      "paper_key": "2404.17565",
      "paper_title": "ChangeBind: A Hybrid Change Encoder for Remote Sensing Change Detection",
      "paper_url": "http://arxiv.org/abs/2404.17565v1",
      "paper_abstract": "Change detection (CD) is a fundamental task in remote sensing (RS) which aims to detect the semantic changes between the same geographical regions at different time stamps. Existing convolutional neural networks (CNNs) based approaches often struggle to capture long-range dependencies. Whereas recent transformer-based methods are prone to the dominant global representation and may limit their capabilities to capture the subtle change regions due to the complexity of the objects in the scene. To address these limitations, we propose an effective Siamese-based framework to encode the semantic changes occurring in the bi-temporal RS images. The main focus of our design is to introduce a change encoder that leverages local and global feature representations to capture both subtle and large change feature information from multi-scale features to precisely estimate the change regions. Our experimental study on two challenging CD datasets reveals the merits of our approach and obtains state-of-the-art performance.",
      "paper_authors": [
        "Mubashir Noman",
        "Mustansar Fiaz",
        "Hisham Cholakkal"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-04-26",
      "update_time": "2024-04-26",
      "comments": "accepted at IGARSS 2024",
      "repo_url": "https://github.com/techmn/changebind"
    },
    "2404.13838": {
      "paper_id": "2404.13838v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.13838v1",
      "paper_key": "2404.13838",
      "paper_title": "C2F-SemiCD: A Coarse-to-Fine Semi-Supervised Change Detection Method Based on Consistency Regularization in High-Resolution Remote Sensing Images",
      "paper_url": "http://arxiv.org/abs/2404.13838v1",
      "paper_abstract": "A high-precision feature extraction model is crucial for change detection (CD). In the past, many deep learning-based supervised CD methods learned to recognize change feature patterns from a large number of labelled bi-temporal images, whereas labelling bi-temporal remote sensing images is very expensive and often time-consuming; therefore, we propose a coarse-to-fine semi-supervised CD method based on consistency regularization (C2F-SemiCD), which includes a coarse-to-fine CD network with a multiscale attention mechanism (C2FNet) and a semi-supervised update method. Among them, the C2FNet network gradually completes the extraction of change features from coarse-grained to fine-grained through multiscale feature fusion, channel attention mechanism, spatial attention mechanism, global context module, feature refine module, initial aggregation module, and final aggregation module. The semi-supervised update method uses the mean teacher method. The parameters of the student model are updated to the parameters of the teacher Model by using the exponential moving average (EMA) method. Through extensive experiments on three datasets and meticulous ablation studies, including crossover experiments across datasets, we verify the significant effectiveness and efficiency of the proposed C2F-SemiCD method. The code will be open at: https://github.com/ChengxiHAN/C2F-SemiCDand-C2FNet.",
      "paper_authors": [
        "Chengxi Han",
        "Chen Wu",
        "Meiqi Hu",
        "Jiepan Li",
        "Hongruixuan Chen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-04-22",
      "update_time": "2024-04-22",
      "comments": null,
      "repo_url": "https://github.com/chengxihan/c2f-semicd-and-c2f-cdnet"
    },
    "2404.12081": {
      "paper_id": "2404.12081v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.12081v1",
      "paper_key": "2404.12081",
      "paper_title": "MaskCD: A Remote Sensing Change Detection Network Based on Mask Classification",
      "paper_url": "http://arxiv.org/abs/2404.12081v1",
      "paper_abstract": "Change detection (CD) from remote sensing (RS) images using deep learning has been widely investigated in the literature. It is typically regarded as a pixel-wise labeling task that aims to classify each pixel as changed or unchanged. Although per-pixel classification networks in encoder-decoder structures have shown dominance, they still suffer from imprecise boundaries and incomplete object delineation at various scenes. For high-resolution RS images, partly or totally changed objects are more worthy of attention rather than a single pixel. Therefore, we revisit the CD task from the mask prediction and classification perspective and propose MaskCD to detect changed areas by adaptively generating categorized masks from input image pairs. Specifically, it utilizes a cross-level change representation perceiver (CLCRP) to learn multiscale change-aware representations and capture spatiotemporal relations from encoded features by exploiting deformable multihead self-attention (DeformMHSA). Subsequently, a masked-attention-based detection transformers (MA-DETR) decoder is developed to accurately locate and identify changed objects based on masked attention and self-attention mechanisms. It reconstructs the desired changed objects by decoding the pixel-wise representations into learnable mask proposals and making final predictions from these candidates. Experimental results on five benchmark datasets demonstrate the proposed approach outperforms other state-of-the-art models. Codes and pretrained models are available online (https://github.com/EricYu97/MaskCD).",
      "paper_authors": [
        "Weikang Yu",
        "Xiaokang Zhang",
        "Samiran Das",
        "Xiao Xiang Zhu",
        "Pedram Ghamisi"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-04-18",
      "update_time": "2024-04-18",
      "comments": null,
      "repo_url": "https://github.com/ericyu97/maskcd"
    },
    "2404.11326": {
      "paper_id": "2404.11326v4",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.11326v4",
      "paper_key": "2404.11326",
      "paper_title": "Single-temporal Supervised Remote Change Detection for Domain Generalization",
      "paper_url": "http://arxiv.org/abs/2404.11326v4",
      "paper_abstract": "Change detection is widely applied in remote sensing image analysis. Existing methods require training models separately for each dataset, which leads to poor domain generalization. Moreover, these methods rely heavily on large amounts of high-quality pair-labelled data for training, which is expensive and impractical. In this paper, we propose a multimodal contrastive learning (ChangeCLIP) based on visual-language pre-training for change detection domain generalization. Additionally, we propose a dynamic context optimization for prompt learning. Meanwhile, to address the data dependency issue of existing methods, we introduce a single-temporal and controllable AI-generated training strategy (SAIN). This allows us to train the model using a large number of single-temporal images without image pairs in the real world, achieving excellent generalization. Extensive experiments on series of real change detection datasets validate the superiority and strong generalization of ChangeCLIP, outperforming state-of-the-art change detection methods. Code will be available.",
      "paper_authors": [
        "Qiangang Du",
        "Jinlong Peng",
        "Xu Chen",
        "Qingdong He",
        "Liren He",
        "Qiang Nie",
        "Wenbing Zhu",
        "Mingmin Chi",
        "Yabiao Wang",
        "Chengjie Wang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-04-17",
      "update_time": "2024-04-23",
      "comments": null,
      "repo_url": "#"
    },
    "2404.11318": {
      "paper_id": "2404.11318v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.11318v3",
      "paper_key": "2404.11318",
      "paper_title": "Leveraging Fine-Grained Information and Noise Decoupling for Remote Sensing Change Detection",
      "paper_url": "http://arxiv.org/abs/2404.11318v3",
      "paper_abstract": "Change detection aims to identify remote sense object changes by analyzing data between bitemporal image pairs. Due to the large temporal and spatial span of data collection in change detection image pairs, there are often a significant amount of task-specific and task-agnostic noise. Previous effort has focused excessively on denoising, with this goes a great deal of loss of fine-grained information. In this paper, we revisit the importance of fine-grained features in change detection and propose a series of operations for fine-grained information compensation and noise decoupling (FINO). First, the context is utilized to compensate for the fine-grained information in the feature space. Next, a shape-aware and a brightness-aware module are designed to improve the capacity for representation learning. The shape-aware module guides the backbone for more precise shape estimation, guiding the backbone network in extracting object shape features. The brightness-aware module learns a overall brightness estimation to improve the model's robustness to task-agnostic noise. Finally, a task-specific noise decoupling structure is designed as a way to improve the model's ability to separate noise interference from feature similarity. With these training schemes, our proposed method achieves new state-of-the-art (SOTA) results in multiple change detection benchmarks. The code will be made available.",
      "paper_authors": [
        "Qiangang Du",
        "Jinlong Peng",
        "Changan Wang",
        "Xu Chen",
        "Qingdong He",
        "Wenbing Zhu",
        "Mingmin Chi",
        "Yabiao Wang",
        "Chengjie Wang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-04-17",
      "update_time": "2024-06-21",
      "comments": null,
      "repo_url": "#"
    },
    "2404.11243": {
      "paper_id": "2404.11243v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.11243v2",
      "paper_key": "2404.11243",
      "paper_title": "Patch-Consistent Optical Translation Across Sensors: Large-Scale Denoising Diffusion with Heterogeneous Change Detection as a Use Case",
      "paper_url": "http://arxiv.org/abs/2404.11243v2",
      "paper_abstract": "In the field of remote sensing, the challenge of comparing images captured by disparate sensors is a common obstacle. This requires image translation -- converting imagery from one sensor domain to another while preserving the original content. Denoising Diffusion Implicit Models (DDIM) are potential state-of-the-art solutions for such domain translation due to their proven superiority in multiple image-to-image translation tasks in classic computer vision. However, these models struggle with large-scale multi-patch imagery, often focusing solely on small patches and resulting in inconsistencies across the full image. To overcome these limitations, we propose a novel method that leverages DDIM for effective optical image translation over large areas. Our approach is tailored to super-resolve large-scale low spatial resolution images into high-resolution equivalents from disparate optical sensors, ensuring uniformity across hundreds of patches. Extensive experiments with a dataset of paired Sentinel-II and Planet Dove images show that our approach provides precise domain adaptation and artifact reduction. Our technique preserves the image content while also improving radiometric (color) accuracy and feature representations. The outcome is a high-resolution large-scale image with consistent patches, vital for applications such as heterogeneous change detection (HCD). We present a unique training and testing algorithm rooted in DDIMs, a thorough image quality assessment, and a comparative study against the standard classifier-free guided DDIM framework and five other leading methods. The efficacy of our approach is further demonstrated by substantial enhancements in HCD tasks performed in the urban settings of Beirut, Lebanon, and Austin, USA.",
      "paper_authors": [
        "Jo\u00e3o Gabriel Vinholi",
        "Marco Chini",
        "Anis Amziane",
        "Renato Machado",
        "Danilo Silva",
        "Patrick Matgen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-04-17",
      "update_time": "2024-07-15",
      "comments": null,
      "repo_url": "#"
    },
    "2404.10842": {
      "paper_id": "2404.10842v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.10842v1",
      "paper_key": "2404.10842",
      "paper_title": "Unsupervised Speaker Diarization in Distributed IoT Networks Using Federated Learning",
      "paper_url": "http://arxiv.org/abs/2404.10842v1",
      "paper_abstract": "This paper presents a computationally efficient and distributed speaker diarization framework for networked IoT-style audio devices. The work proposes a Federated Learning model which can identify the participants in a conversation without the requirement of a large audio database for training. An unsupervised online update mechanism is proposed for the Federated Learning model which depends on cosine similarity of speaker embeddings. Moreover, the proposed diarization system solves the problem of speaker change detection via. unsupervised segmentation techniques using Hotelling's t-squared Statistic and Bayesian Information Criterion. In this new approach, speaker change detection is biased around detected quasi-silences, which reduces the severity of the trade-off between the missed detection and false detection rates. Additionally, the computational overhead due to frame-by-frame identification of speakers is reduced via. unsupervised clustering of speech segments. The results demonstrate the effectiveness of the proposed training method in the presence of non-IID speech data. It also shows a considerable improvement in the reduction of false and missed detection at the segmentation stage, while reducing the computational overhead. Improved accuracy and reduced computational cost makes the mechanism suitable for real-time speaker diarization across a distributed IoT audio network.",
      "paper_authors": [
        "Amit Kumar Bhuyan",
        "Hrishikesh Dutta",
        "Subir Biswas"
      ],
      "primary_category": "cs.SD",
      "publish_time": "2024-04-16",
      "update_time": "2024-04-16",
      "comments": "11 pages, 7 figures, 1 table",
      "repo_url": "#"
    },
    "2404.09179": {
      "paper_id": "2404.09179v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.09179v1",
      "paper_key": "2404.09179",
      "paper_title": "Change Guiding Network: Incorporating Change Prior to Guide Change Detection in Remote Sensing Imagery",
      "paper_url": "http://arxiv.org/abs/2404.09179v1",
      "paper_abstract": "The rapid advancement of automated artificial intelligence algorithms and remote sensing instruments has benefited change detection (CD) tasks. However, there is still a lot of space to study for precise detection, especially the edge integrity and internal holes phenomenon of change features. In order to solve these problems, we design the Change Guiding Network (CGNet), to tackle the insufficient expression problem of change features in the conventional U-Net structure adopted in previous methods, which causes inaccurate edge detection and internal holes. Change maps from deep features with rich semantic information are generated and used as prior information to guide multi-scale feature fusion, which can improve the expression ability of change features. Meanwhile, we propose a self-attention module named Change Guide Module (CGM), which can effectively capture the long-distance dependency among pixels and effectively overcome the problem of the insufficient receptive field of traditional convolutional neural networks. On four major CD datasets, we verify the usefulness and efficiency of the CGNet, and a large number of experiments and ablation studies demonstrate the effectiveness of CGNet. We're going to open-source our code at https://github.com/ChengxiHAN/CGNet-CD.",
      "paper_authors": [
        "Chengxi Han",
        "Chen Wu",
        "Haonan Guo",
        "Meiqi Hu",
        "Jiepan Li",
        "Hongruixuan Chen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-04-14",
      "update_time": "2024-04-14",
      "comments": null,
      "repo_url": "https://github.com/chengxihan/cgnet-cd"
    },
    "2404.09178": {
      "paper_id": "2404.09178v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.09178v1",
      "paper_key": "2404.09178",
      "paper_title": "HANet: A Hierarchical Attention Network for Change Detection With Bitemporal Very-High-Resolution Remote Sensing Images",
      "paper_url": "http://arxiv.org/abs/2404.09178v1",
      "paper_abstract": "Benefiting from the developments in deep learning technology, deep-learning-based algorithms employing automatic feature extraction have achieved remarkable performance on the change detection (CD) task. However, the performance of existing deep-learning-based CD methods is hindered by the imbalance between changed and unchanged pixels. To tackle this problem, a progressive foreground-balanced sampling strategy on the basis of not adding change information is proposed in this article to help the model accurately learn the features of the changed pixels during the early training process and thereby improve detection performance.Furthermore, we design a discriminative Siamese network, hierarchical attention network (HANet), which can integrate multiscale features and refine detailed features. The main part of HANet is the HAN module, which is a lightweight and effective self-attention mechanism. Extensive experiments and ablation studies on two CDdatasets with extremely unbalanced labels validate the effectiveness and efficiency of the proposed method.",
      "paper_authors": [
        "Chengxi Han",
        "Chen Wu",
        "Haonan Guo",
        "Meiqi Hu",
        "Hongruixuan Chen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-04-14",
      "update_time": "2024-04-14",
      "comments": null,
      "repo_url": "https://github.com/chengxihan/hanet-cd"
    },
    "2404.08892": {
      "paper_id": "2404.08892v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.08892v1",
      "paper_key": "2404.08892",
      "paper_title": "ChangeAnywhere: Sample Generation for Remote Sensing Change Detection via Semantic Latent Diffusion Model",
      "paper_url": "http://arxiv.org/abs/2404.08892v1",
      "paper_abstract": "Remote sensing change detection (CD) is a pivotal technique that pinpoints changes on a global scale based on multi-temporal images. With the recent expansion of deep learning, supervised deep learning-based CD models have shown satisfactory performance. However, CD sample labeling is very time-consuming as it is densely labeled and requires expert knowledge. To alleviate this problem, we introduce ChangeAnywhere, a novel CD sample generation method using the semantic latent diffusion model and single-temporal images. Specifically, ChangeAnywhere leverages the relative ease of acquiring large single-temporal semantic datasets to generate large-scale, diverse, and semantically annotated bi-temporal CD datasets. ChangeAnywhere captures the two essentials of CD samples, i.e., change implies semantically different, and non-change implies reasonable change under the same semantic constraints. We generated ChangeAnywhere-100K, the largest synthesis CD dataset with 100,000 pairs of CD samples based on the proposed method. The ChangeAnywhere-100K significantly improved both zero-shot and few-shot performance on two CD benchmark datasets for various deep learning-based CD models, as demonstrated by transfer experiments. This paper delineates the enormous potential of ChangeAnywhere for CD sample generation and demonstrates the subsequent enhancement of model performance. Therefore, ChangeAnywhere offers a potent tool for remote sensing CD. All codes and pre-trained models will be available at https://github.com/tangkai-RS/ChangeAnywhere.",
      "paper_authors": [
        "Kai Tang",
        "Jin Chen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-04-13",
      "update_time": "2024-04-13",
      "comments": "Concise manuscript version of ChangeAnywhere",
      "repo_url": "https://github.com/tangkai-rs/changeanywhere"
    },
    "2404.05486": {
      "paper_id": "2404.05486v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.05486v1",
      "paper_key": "2404.05486",
      "paper_title": "Quickest Change Detection for Multiple Data Streams Using the James-Stein Estimator",
      "paper_url": "http://arxiv.org/abs/2404.05486v1",
      "paper_abstract": "The problem of quickest change detection is studied in the context of detecting an arbitrary unknown mean-shift in multiple independent Gaussian data streams. The James-Stein estimator is used in constructing detection schemes that exhibit strong detection performance both asymptotically and non-asymptotically. First, a James-Stein-based extension of the recently developed windowed CuSum test is introduced. Our results indicate that the proposed scheme constitutes a uniform improvement over its typical maximum likelihood variant. That is, the proposed James-Stein version achieves a smaller detection delay simultaneously for all possible post-change parameter values and every false alarm rate constraint, as long as the number of parallel data streams is greater than three. Additionally, an alternative detection procedure that utilizes the James-Stein estimator is shown to have asymptotic detection delay properties that compare favorably to existing tests. The second-order term of the asymptotic average detection delay is reduced in a predefined low-dimensional subspace of the parameter space, while second-order asymptotic minimaxity is preserved. The results are verified in simulations, where the proposed schemes are shown to achieve smaller detection delays compared to existing alternatives, especially when the number of data streams is large.",
      "paper_authors": [
        "Topi Halme",
        "Venugopal V. Veeravalli",
        "Visa Koivunen"
      ],
      "primary_category": "math.ST",
      "publish_time": "2024-04-08",
      "update_time": "2024-04-08",
      "comments": "27 pages, 5 figures",
      "repo_url": "#"
    },
    "2404.04884": {
      "paper_id": "2404.04884v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.04884v1",
      "paper_key": "2404.04884",
      "paper_title": "LRNet: Change detection of high-resolution remote sensing imagery via strategy of localization-then-refinement",
      "paper_url": "http://arxiv.org/abs/2404.04884v1",
      "paper_abstract": "Change detection, as a research hotspot in the field of remote sensing, has witnessed continuous development and progress. However, the discrimination of boundary details remains a significant bottleneck due to the complexity of surrounding elements between change areas and backgrounds. Discriminating the boundaries of large change areas results in misalignment, while connecting boundaries occurs for small change targets. To address the above issues, a novel network based on the localization-then-refinement strategy is proposed in this paper, namely LRNet. LRNet consists of two stages: localization and refinement. In the localization stage, a three-branch encoder simultaneously extracts original image features and their differential features for interactive localization of the position of each change area. To minimize information loss during feature extraction, learnable optimal pooling (LOP) is proposed to replace the widely used max-pooling. Additionally, this process is trainable and contributes to the overall optimization of the network. To effectively interact features from different branches and accurately locate change areas of various sizes, change alignment attention (C2A) and hierarchical change alignment module (HCA) are proposed. In the refinement stage, the localization results from the localization stage are corrected by constraining the change areas and change edges through the edge-area alignment module (E2A). Subsequently, the decoder, combined with the difference features strengthened by C2A in the localization phase, refines change areas of different sizes, ultimately achieving accurate boundary discrimination of change areas. The proposed LRNet outperforms 13 other state-of-the-art methods in terms of comprehensive evaluation metrics and provides the most precise boundary discrimination results on the LEVIR-CD and WHU-CD datasets.",
      "paper_authors": [
        "Huan Zhong",
        "Chen Wu",
        "Ziqi Xiao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-04-07",
      "update_time": "2024-04-07",
      "comments": "18 pages, 11 figures",
      "repo_url": "#"
    },
    "2404.03425": {
      "paper_id": "2404.03425v6",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.03425v6",
      "paper_key": "2404.03425",
      "paper_title": "ChangeMamba: Remote Sensing Change Detection With Spatiotemporal State Space Model",
      "paper_url": "http://arxiv.org/abs/2404.03425v6",
      "paper_abstract": "Convolutional neural networks (CNN) and Transformers have made impressive progress in the field of remote sensing change detection (CD). However, both architectures have inherent shortcomings: CNN are constrained by a limited receptive field that may hinder their ability to capture broader spatial contexts, while Transformers are computationally intensive, making them costly to train and deploy on large datasets. Recently, the Mamba architecture, based on state space models, has shown remarkable performance in a series of natural language processing tasks, which can effectively compensate for the shortcomings of the above two architectures. In this paper, we explore for the first time the potential of the Mamba architecture for remote sensing CD tasks. We tailor the corresponding frameworks, called MambaBCD, MambaSCD, and MambaBDA, for binary change detection (BCD), semantic change detection (SCD), and building damage assessment (BDA), respectively. All three frameworks adopt the cutting-edge Visual Mamba architecture as the encoder, which allows full learning of global spatial contextual information from the input images. For the change decoder, which is available in all three architectures, we propose three spatio-temporal relationship modeling mechanisms, which can be naturally combined with the Mamba architecture and fully utilize its attribute to achieve spatio-temporal interaction of multi-temporal features, thereby obtaining accurate change information. On five benchmark datasets, our proposed frameworks outperform current CNN- and Transformer-based approaches without using any complex training strategies or tricks, fully demonstrating the potential of the Mamba architecture in CD tasks. Further experiments show that our architecture is quite robust to degraded data. The source code will be available in https://github.com/ChenHongruixuan/MambaCD",
      "paper_authors": [
        "Hongruixuan Chen",
        "Jian Song",
        "Chengxi Han",
        "Junshi Xia",
        "Naoto Yokoya"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2024-04-04",
      "update_time": "2024-07-26",
      "comments": "Accepted by IEEE TGRS: https://ieeexplore.ieee.org/document/10565926",
      "repo_url": "https://github.com/chenhongruixuan/mambacd"
    },
    "2404.02668": {
      "paper_id": "2404.02668v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.02668v2",
      "paper_key": "2404.02668",
      "paper_title": "RS-Mamba for Large Remote Sensing Image Dense Prediction",
      "paper_url": "http://arxiv.org/abs/2404.02668v2",
      "paper_abstract": "Context modeling is critical for remote sensing image dense prediction tasks. Nowadays, the growing size of very-high-resolution (VHR) remote sensing images poses challenges in effectively modeling context. While transformer-based models possess global modeling capabilities, they encounter computational challenges when applied to large VHR images due to their quadratic complexity. The conventional practice of cropping large images into smaller patches results in a notable loss of contextual information. To address these issues, we propose the Remote Sensing Mamba (RSM) for dense prediction tasks in large VHR remote sensing images. RSM is specifically designed to capture the global context of remote sensing images with linear complexity, facilitating the effective processing of large VHR images. Considering that the land covers in remote sensing images are distributed in arbitrary spatial directions due to characteristics of remote sensing over-head imaging, the RSM incorporates an omnidirectional selective scan module to globally model the context of images in multiple directions, capturing large spatial features from various directions. Extensive experiments on semantic segmentation and change detection tasks across various land covers demonstrate the effectiveness of the proposed RSM. We designed simple yet effective models based on RSM, achieving state-of-the-art performance on dense prediction tasks in VHR remote sensing images without fancy training strategies. Leveraging the linear complexity and global modeling capabilities, RSM achieves better efficiency and accuracy than transformer-based models on large remote sensing images. Interestingly, we also demonstrated that our model generally performs better with a larger image size on dense prediction tasks. Our code is available at https://github.com/walking-shadow/Official_Remote_Sensing_Mamba.",
      "paper_authors": [
        "Sijie Zhao",
        "Hao Chen",
        "Xueliang Zhang",
        "Pengfeng Xiao",
        "Lei Bai",
        "Wanli Ouyang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-04-03",
      "update_time": "2024-04-10",
      "comments": "15 pages,8 figures",
      "repo_url": "https://github.com/walking-shadow/Official_Remote_Sensing_Mamba"
    },
    "2404.00538": {
      "paper_id": "2404.00538v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.00538v2",
      "paper_key": "2404.00538",
      "paper_title": "Eclipse Attack Detection on a Blockchain Network as a Non-Parametric Change Detection Problem",
      "paper_url": "http://arxiv.org/abs/2404.00538v2",
      "paper_abstract": "This paper introduces a novel non-parametric change detection algorithm to identify eclipse attacks on a blockchain network; the non-parametric algorithm relies only on the empirical mean and variance of the dataset, making it highly adaptable. An eclipse attack occurs when malicious actors isolate blockchain users, disrupting their ability to reach consensus with the broader network, thereby distorting their local copy of the ledger. To detect an eclipse attack, we monitor changes in the Fr\\'echet mean and variance of the evolving blockchain communication network connecting blockchain users. First, we leverage the Johnson-Lindenstrauss lemma to project large-dimensional networks into a lower-dimensional space, preserving essential statistical properties. Subsequently, we employ a non-parametric change detection procedure, leading to a test statistic that converges weakly to a Brownian bridge process in the absence of an eclipse attack. This enables us to quantify the false alarm rate of the detector. Our detector can be implemented as a smart contract on the blockchain, offering a tamper-proof and reliable solution. Finally, we use numerical examples to compare the proposed eclipse attack detector with a detector based on the random forest model.",
      "paper_authors": [
        "Anurag Gupta",
        "Vikram Krishnamurthy",
        "Brian M. Sadler"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-03-31",
      "update_time": "2024-05-30",
      "comments": null,
      "repo_url": "#"
    },
    "2404.00176": {
      "paper_id": "2404.00176v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.00176v1",
      "paper_key": "2404.00176",
      "paper_title": "The LSCD Benchmark: a Testbed for Diachronic Word Meaning Tasks",
      "paper_url": "http://arxiv.org/abs/2404.00176v1",
      "paper_abstract": "Lexical Semantic Change Detection (LSCD) is a complex, lemma-level task, which is usually operationalized based on two subsequently applied usage-level tasks: First, Word-in-Context (WiC) labels are derived for pairs of usages. Then, these labels are represented in a graph on which Word Sense Induction (WSI) is applied to derive sense clusters. Finally, LSCD labels are derived by comparing sense clusters over time. This modularity is reflected in most LSCD datasets and models. It also leads to a large heterogeneity in modeling options and task definitions, which is exacerbated by a variety of dataset versions, preprocessing options and evaluation metrics. This heterogeneity makes it difficult to evaluate models under comparable conditions, to choose optimal model combinations or to reproduce results. Hence, we provide a benchmark repository standardizing LSCD evaluation. Through transparent implementation results become easily reproducible and by standardization different components can be freely combined. The repository reflects the task's modularity by allowing model evaluation for WiC, WSI and LSCD. This allows for careful evaluation of increasingly complex model components providing new ways of model optimization.",
      "paper_authors": [
        "Dominik Schlechtweg",
        "Shafqat Mumtaz Virk",
        "Nikolay Arefyev"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-03-29",
      "update_time": "2024-03-29",
      "comments": null,
      "repo_url": "#"
    },
    "2403.19646": {
      "paper_id": "2403.19646v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.19646v3",
      "paper_key": "2403.19646",
      "paper_title": "Change-Agent: Towards Interactive Comprehensive Remote Sensing Change Interpretation and Analysis",
      "paper_url": "http://arxiv.org/abs/2403.19646v3",
      "paper_abstract": "Monitoring changes in the Earth's surface is crucial for understanding natural processes and human impacts, necessitating precise and comprehensive interpretation methodologies. Remote sensing satellite imagery offers a unique perspective for monitoring these changes, leading to the emergence of remote sensing image change interpretation (RSICI) as a significant research focus. Current RSICI technology encompasses change detection and change captioning, each with its limitations in providing comprehensive interpretation. To address this, we propose an interactive Change-Agent, which can follow user instructions to achieve comprehensive change interpretation and insightful analysis, such as change detection and change captioning, change object counting, change cause analysis, etc. The Change-Agent integrates a multi-level change interpretation (MCI) model as the eyes and a large language model (LLM) as the brain. The MCI model contains two branches of pixel-level change detection and semantic-level change captioning, in which the BI-temporal Iterative Interaction (BI3) layer is proposed to enhance the model's discriminative feature representation capabilities. To support the training of the MCI model, we build the LEVIR-MCI dataset with a large number of change masks and captions of changes. Experiments demonstrate the SOTA performance of the MCI model in achieving both change detection and change description simultaneously, and highlight the promising application value of our Change-Agent in facilitating comprehensive interpretation of surface changes, which opens up a new avenue for intelligent remote sensing applications. To facilitate future research, we will make our dataset and codebase of the MCI model and Change-Agent publicly available at https://github.com/Chen-Yang-Liu/Change-Agent",
      "paper_authors": [
        "Chenyang Liu",
        "Keyan Chen",
        "Haotian Zhang",
        "Zipeng Qi",
        "Zhengxia Zou",
        "Zhenwei Shi"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-03-28",
      "update_time": "2024-07-16",
      "comments": "IEEE Transactions on Geoscience and Remote Sensing 2024",
      "repo_url": "https://github.com/chen-yang-liu/change-agent"
    },
    "2403.17909": {
      "paper_id": "2403.17909v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.17909v1",
      "paper_key": "2403.17909",
      "paper_title": "ELGC-Net: Efficient Local-Global Context Aggregation for Remote Sensing Change Detection",
      "paper_url": "http://arxiv.org/abs/2403.17909v1",
      "paper_abstract": "Deep learning has shown remarkable success in remote sensing change detection (CD), aiming to identify semantic change regions between co-registered satellite image pairs acquired at distinct time stamps. However, existing convolutional neural network and transformer-based frameworks often struggle to accurately segment semantic change regions. Moreover, transformers-based methods with standard self-attention suffer from quadratic computational complexity with respect to the image resolution, making them less practical for CD tasks with limited training data. To address these issues, we propose an efficient change detection framework, ELGC-Net, which leverages rich contextual information to precisely estimate change regions while reducing the model size. Our ELGC-Net comprises a Siamese encoder, fusion modules, and a decoder. The focus of our design is the introduction of an Efficient Local-Global Context Aggregator module within the encoder, capturing enhanced global context and local spatial information through a novel pooled-transpose (PT) attention and depthwise convolution, respectively. The PT attention employs pooling operations for robust feature extraction and minimizes computational cost with transposed attention. Extensive experiments on three challenging CD datasets demonstrate that ELGC-Net outperforms existing methods. Compared to the recent transformer-based CD approach (ChangeFormer), ELGC-Net achieves a 1.4% gain in intersection over union metric on the LEVIR-CD dataset, while significantly reducing trainable parameters. Our proposed ELGC-Net sets a new state-of-the-art performance in remote sensing change detection benchmarks. Finally, we also introduce ELGC-Net-LW, a lighter variant with significantly reduced computational complexity, suitable for resource-constrained settings, while achieving comparable performance. Project url https://github.com/techmn/elgcnet.",
      "paper_authors": [
        "Mubashir Noman",
        "Mustansar Fiaz",
        "Hisham Cholakkal",
        "Salman Khan",
        "Fahad Shahbaz Khan"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-03-26",
      "update_time": "2024-03-26",
      "comments": "accepted at IEEE TGRS",
      "repo_url": "https://github.com/techmn/elgcnet"
    },
    "2403.16297": {
      "paper_id": "2403.16297v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.16297v1",
      "paper_key": "2403.16297",
      "paper_title": "Round Robin Active Sequential Change Detection for Dependent Multi-Channel Data",
      "paper_url": "http://arxiv.org/abs/2403.16297v1",
      "paper_abstract": "This paper considers the problem of sequentially detecting a change in the joint distribution of multiple data sources under a sampling constraint. Specifically, the channels or sources generate observations that are independent over time, but not necessarily independent at any given time instant. The sources follow an initial joint distribution, and at an unknown time instant, the joint distribution of an unknown subset of sources changes. Importantly, there is a hard constraint that only a fixed number of sources are allowed to be sampled at each time instant. The goal is to sequentially observe the sources according to the constraint, and stop sampling as quickly as possible after the change while controlling the false alarm rate below a user-specified level. The sources can be selected dynamically based on the already collected data, and thus, a policy for this problem consists of a joint sampling and change-detection rule. A non-randomized policy is studied, and an upper bound is established on its worst-case conditional expected detection delay with respect to both the change point and the observations from the affected sources before the change.",
      "paper_authors": [
        "Anamitra Chaudhuri",
        "Georgios Fellouris",
        "Ali Tajer"
      ],
      "primary_category": "stat.ME",
      "publish_time": "2024-03-24",
      "update_time": "2024-03-24",
      "comments": null,
      "repo_url": "#"
    },
    "2403.15943": {
      "paper_id": "2403.15943v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.15943v2",
      "paper_key": "2403.15943",
      "paper_title": "Advanced Feature Manipulation for Enhanced Change Detection Leveraging Natural Language Models",
      "paper_url": "http://arxiv.org/abs/2403.15943v2",
      "paper_abstract": "Change detection is a fundamental task in computer vision that processes a bi-temporal image pair to differentiate between semantically altered and unaltered regions. Large language models (LLMs) have been utilized in various domains for their exceptional feature extraction capabilities and have shown promise in numerous downstream applications. In this study, we harness the power of a pre-trained LLM, extracting feature maps from extensive datasets, and employ an auxiliary network to detect changes. Unlike existing LLM-based change detection methods that solely focus on deriving high-quality feature maps, our approach emphasizes the manipulation of these feature maps to enhance semantic relevance.",
      "paper_authors": [
        "Zhenglin Li",
        "Yangchen Huang",
        "Mengran Zhu",
        "Jingyu Zhang",
        "JingHao Chang",
        "Houze Liu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-03-23",
      "update_time": "2024-06-13",
      "comments": "This version is not our full version based on our new progress,\n  related data, and methodology we are dealing with, and based on the rules and\n  the laws, we are adjusting our current version",
      "repo_url": "#"
    },
    "2403.15032": {
      "paper_id": "2403.15032v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.15032v1",
      "paper_key": "2403.15032",
      "paper_title": "An Integrated Neighborhood and Scale Information Network for Open-Pit Mine Change Detection in High-Resolution Remote Sensing Images",
      "paper_url": "http://arxiv.org/abs/2403.15032v1",
      "paper_abstract": "Open-pit mine change detection (CD) in high-resolution (HR) remote sensing images plays a crucial role in mineral development and environmental protection. Significant progress has been made in this field in recent years, largely due to the advancement of deep learning techniques. However, existing deep-learning-based CD methods encounter challenges in effectively integrating neighborhood and scale information, resulting in suboptimal performance. Therefore, by exploring the influence patterns of neighborhood and scale information, this paper proposes an Integrated Neighborhood and Scale Information Network (INSINet) for open-pit mine CD in HR remote sensing images. Specifically, INSINet introduces 8-neighborhood-image information to acquire a larger receptive field, improving the recognition of center image boundary regions. Drawing on techniques of skip connection, deep supervision, and attention mechanism, the multi-path deep supervised attention (MDSA) module is designed to enhance multi-scale information fusion and change feature extraction. Experimental analysis reveals that incorporating neighborhood and scale information enhances the F1 score of INSINet by 6.40%, with improvements of 3.08% and 3.32% respectively. INSINet outperforms existing methods with an Overall Accuracy of 97.69%, Intersection over Union of 71.26%, and F1 score of 83.22%. INSINet shows significance for open-pit mine CD in HR remote sensing images.",
      "paper_authors": [
        "Zilin Xie",
        "Kangning Li",
        "Jinbao Jiang",
        "Jinzhong Yang",
        "Xiaojun Qiao",
        "Deshuai Yuan",
        "Cheng Nie"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-03-22",
      "update_time": "2024-03-22",
      "comments": null,
      "repo_url": "#"
    },
    "2403.14109": {
      "paper_id": "2403.14109v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.14109v2",
      "paper_key": "2403.14109",
      "paper_title": "Reinforcement Learning Design for Quickest Change Detection",
      "paper_url": "http://arxiv.org/abs/2403.14109v2",
      "paper_abstract": "The field of quickest change detection (QCD) concerns design and analysis of algorithms to estimate in real time the time at which an important event takes place, and identify properties of the post-change behavior. It is shown in this paper that approaches based on reinforcement learning (RL) can be adapted based on any \"surrogate information state\" that is adapted to the observations. Hence we are left to choose both the surrogate information state process and the algorithm. For the former, it is argued that there are many choices available, based on a rich theory of asymptotic statistics for QCD. Two approaches to RL design are considered: (i) Stochastic gradient descent based on an actor-critic formulation. Theory is largely complete for this approach: the algorithm is unbiased, and will converge to a local minimum. However, it is shown that variance of stochastic gradients can be very large, necessitating the need for commensurately long run times; (ii) Q-learning algorithms based on a version of the projected Bellman equation. It is shown that the algorithm is stable, in the sense of bounded sample paths, and that a solution to the projected Bellman equation exists under mild conditions. Numerical experiments illustrate these findings, and provide a roadmap for algorithm design in more general settings.",
      "paper_authors": [
        "Austin Cooper",
        "Sean Meyn"
      ],
      "primary_category": "math.OC",
      "publish_time": "2024-03-21",
      "update_time": "2024-09-13",
      "comments": "Preprint version of \"Reinforcement Learning Design for Quickest\n  Change Detection\", IEEE Conference on Decision and Control, 2024 (to appear)",
      "repo_url": "#"
    },
    "2403.13430": {
      "paper_id": "2403.13430v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.13430v2",
      "paper_key": "2403.13430",
      "paper_title": "MTP: Advancing Remote Sensing Foundation Model via Multi-Task Pretraining",
      "paper_url": "http://arxiv.org/abs/2403.13430v2",
      "paper_abstract": "Foundation models have reshaped the landscape of Remote Sensing (RS) by enhancing various image interpretation tasks. Pretraining is an active research topic, encompassing supervised and self-supervised learning methods to initialize model weights effectively. However, transferring the pretrained models to downstream tasks may encounter task discrepancy due to their formulation of pretraining as image classification or object discrimination tasks. In this study, we explore the Multi-Task Pretraining (MTP) paradigm for RS foundation models to address this issue. Using a shared encoder and task-specific decoder architecture, we conduct multi-task supervised pretraining on the SAMRS dataset, encompassing semantic segmentation, instance segmentation, and rotated object detection. MTP supports both convolutional neural networks and vision transformer foundation models with over 300 million parameters. The pretrained models are finetuned on various RS downstream tasks, such as scene classification, horizontal and rotated object detection, semantic segmentation, and change detection. Extensive experiments across 14 datasets demonstrate the superiority of our models over existing ones of similar size and their competitive performance compared to larger state-of-the-art models, thus validating the effectiveness of MTP.",
      "paper_authors": [
        "Di Wang",
        "Jing Zhang",
        "Minqiang Xu",
        "Lin Liu",
        "Dongsheng Wang",
        "Erzhong Gao",
        "Chengxi Han",
        "Haonan Guo",
        "Bo Du",
        "Dacheng Tao",
        "Liangpei Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-03-20",
      "update_time": "2024-05-30",
      "comments": "Accepted by IEEE JSTARS Special issue on \"Large-Scale Pretraining for\n  Interpretation Promotion in Remote Sensing Domain\". The codes and pretrained\n  models are available at https://github.com/ViTAE-Transformer/MTP",
      "repo_url": "https://github.com/vitae-transformer/mtp"
    },
    "2403.12226": {
      "paper_id": "2403.12226v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.12226v1",
      "paper_key": "2403.12226",
      "paper_title": "Large-scale flood modeling and forecasting with FloodCast",
      "paper_url": "http://arxiv.org/abs/2403.12226v1",
      "paper_abstract": "Large-scale hydrodynamic models generally rely on fixed-resolution spatial grids and model parameters as well as incurring a high computational cost. This limits their ability to accurately forecast flood crests and issue time-critical hazard warnings. In this work, we build a fast, stable, accurate, resolution-invariant, and geometry-adaptative flood modeling and forecasting framework that can perform at large scales, namely FloodCast. The framework comprises two main modules: multi-satellite observation and hydrodynamic modeling. In the multi-satellite observation module, a real-time unsupervised change detection method and a rainfall processing and analysis tool are proposed to harness the full potential of multi-satellite observations in large-scale flood prediction. In the hydrodynamic modeling module, a geometry-adaptive physics-informed neural solver (GeoPINS) is introduced, benefiting from the absence of a requirement for training data in physics-informed neural networks and featuring a fast, accurate, and resolution-invariant architecture with Fourier neural operators. GeoPINS demonstrates impressive performance on popular PDEs across regular and irregular domains. Building upon GeoPINS, we propose a sequence-to-sequence GeoPINS model to handle long-term temporal series and extensive spatial domains in large-scale flood modeling. Next, we establish a benchmark dataset in the 2022 Pakistan flood to assess various flood prediction methods. Finally, we validate the model in three dimensions - flood inundation range, depth, and transferability of spatiotemporal downscaling. Traditional hydrodynamics and sequence-to-sequence GeoPINS exhibit exceptional agreement during high water levels, while comparative assessments with SAR-based flood depth data show that sequence-to-sequence GeoPINS outperforms traditional hydrodynamics, with smaller prediction errors.",
      "paper_authors": [
        "Qingsong Xu",
        "Yilei Shi",
        "Jonathan Bamber",
        "Chaojun Ouyang",
        "Xiao Xiang Zhu"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-03-18",
      "update_time": "2024-03-18",
      "comments": "40 pages, 16 figures, under review",
      "repo_url": "https://github.com/hydropml/floodcast"
    },
    "2403.11434": {
      "paper_id": "2403.11434v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.11434v1",
      "paper_key": "2403.11434",
      "paper_title": "Earth+: on-board satellite imagery compression leveraging historical earth observations",
      "paper_url": "http://arxiv.org/abs/2403.11434v1",
      "paper_abstract": "With the increasing deployment of earth observation satellite constellations, the downlink (satellite-to-ground) capacity often limits the freshness, quality, and coverage of the imagery data available to applications on the ground. To overcome the downlink limitation, we present Earth+, a new satellite imagery compression system that, instead of compressing each image individually, pinpoints and downloads only recent imagery changes with respect to the history reference images. To minimize the amount of changes, it is critical to make reference images as fresh as possible. Earth+ enables each satellite to choose fresh reference images from not only its own history images but also past images of other satellites from an entire satellite constellation. To share reference images across satellites, Earth+ utilizes the limited capacity of the existing uplink (ground-to-satellite) by judiciously selecting and compressing reference images while still allowing accurate change detection. In short, Earth+ is the first to make reference-based compression efficient, by enabling constellation-wide sharing of fresh reference images across satellites. Our evaluation shows that Earth+ can reduce the downlink usage by a factor of 3.3 compared to state-of-the-art on-board image compression techniques while not sacrificing image quality, or using more on-board computing or storage resources, or more uplink bandwidth than currently available.",
      "paper_authors": [
        "Kuntai Du",
        "Yihua Cheng",
        "Peder Olsen",
        "Shadi Noghabi",
        "Ranveer Chandra",
        "Junchen Jiang"
      ],
      "primary_category": "cs.NI",
      "publish_time": "2024-03-18",
      "update_time": "2024-03-18",
      "comments": null,
      "repo_url": "#"
    },
    "2403.08280": {
      "paper_id": "2403.08280v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.08280v1",
      "paper_key": "2403.08280",
      "paper_title": "Pre-examinations Improve Automated Metastases Detection on Cranial MRI",
      "paper_url": "http://arxiv.org/abs/2403.08280v1",
      "paper_abstract": "Materials and methods: First, a dual-time approach was assessed, for which the CNN was provided sequences of the MRI that initially depicted new MM (diagnosis MRI) as well as of a prediagnosis MRI: inclusion of only contrast-enhanced T1-weighted images (CNNdual_ce) was compared with inclusion of also the native T1-weighted images, T2-weighted images, and FLAIR sequences of both time points (CNNdual_all).Second, results were compared with the corresponding single time approaches, in which the CNN was provided exclusively the respective sequences of the diagnosis MRI.Casewise diagnostic performance parameters were calculated from 5-fold cross-validation.   Results: In total, 94 cases with 494 MMs were included. Overall, the highest diagnostic performance was achieved by inclusion of only the contrast-enhanced T1-weighted images of the diagnosis and of a prediagnosis MRI (CNNdual_ce, sensitivity = 73%, PPV = 25%, F1-score = 36%). Using exclusively contrast-enhanced T1-weighted images as input resulted in significantly less false-positives (FPs) compared with inclusion of further sequences beyond contrast-enhanced T1-weighted images (FPs = 5/7 for CNNdual_ce/CNNdual_all, P < 1e-5). Comparison of contrast-enhanced dual and mono time approaches revealed that exclusion of prediagnosis MRI significantly increased FPs (FPs = 5/10 for CNNdual_ce/CNNce, P < 1e-9).Approaches with only native sequences were clearly inferior to CNNs that were provided contrast-enhanced sequences.   Conclusions: Automated MM detection on contrast-enhanced T1-weighted images performed with high sensitivity. Frequent FPs due to artifacts and vessels were significantly reduced by additional inclusion of prediagnosis MRI, but not by inclusion of further sequences beyond contrast-enhanced T1-weighted images. Future studies might investigate different change detection architectures for computer-aided detection.",
      "paper_authors": [
        "Katerina Deike-Hofmann",
        "Dorottya Dancs",
        "Daniel Paech",
        "Heinz-Peter Schlemmer",
        "Klaus Maier-Hein",
        "Philipp B\u00e4umer",
        "Alexander Radbruch",
        "Michael G\u00f6tz"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2024-03-13",
      "update_time": "2024-03-13",
      "comments": null,
      "repo_url": "#"
    },
    "2403.07564": {
      "paper_id": "2403.07564v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.07564v2",
      "paper_key": "2403.07564",
      "paper_title": "RSBuilding: Towards General Remote Sensing Image Building Extraction and Change Detection with Foundation Model",
      "paper_url": "http://arxiv.org/abs/2403.07564v2",
      "paper_abstract": "The intelligent interpretation of buildings plays a significant role in urban planning and management, macroeconomic analysis, population dynamics, etc. Remote sensing image building interpretation primarily encompasses building extraction and change detection. However, current methodologies often treat these two tasks as separate entities, thereby failing to leverage shared knowledge. Moreover, the complexity and diversity of remote sensing image scenes pose additional challenges, as most algorithms are designed to model individual small datasets, thus lacking cross-scene generalization. In this paper, we propose a comprehensive remote sensing image building understanding model, termed RSBuilding, developed from the perspective of the foundation model. RSBuilding is designed to enhance cross-scene generalization and task universality. Specifically, we extract image features based on the prior knowledge of the foundation model and devise a multi-level feature sampler to augment scale information. To unify task representation and integrate image spatiotemporal clues, we introduce a cross-attention decoder with task prompts. Addressing the current shortage of datasets that incorporate annotations for both tasks, we have developed a federated training strategy to facilitate smooth model convergence even when supervision for some tasks is missing, thereby bolstering the complementarity of different tasks. Our model was trained on a dataset comprising up to 245,000 images and validated on multiple building extraction and change detection datasets. The experimental results substantiate that RSBuilding can concurrently handle two structurally distinct tasks and exhibits robust zero-shot generalization capabilities.",
      "paper_authors": [
        "Mingze Wang",
        "Lili Su",
        "Cilin Yan",
        "Sheng Xu",
        "Pengcheng Yuan",
        "Xiaolong Jiang",
        "Baochang Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-03-12",
      "update_time": "2024-04-14",
      "comments": null,
      "repo_url": "https://github.com/meize0729/rsbuilding"
    },
    "2403.07213": {
      "paper_id": "2403.07213v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.07213v1",
      "paper_key": "2403.07213",
      "paper_title": "Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits",
      "paper_url": "http://arxiv.org/abs/2403.07213v1",
      "paper_abstract": "Web-based applications such as chatbots, search engines and news recommendations continue to grow in scale and complexity with the recent surge in the adoption of LLMs. Online model selection has thus garnered increasing attention due to the need to choose the best model among a diverse set while balancing task reward and exploration cost. Organizations faces decisions like whether to employ a costly API-based LLM or a locally finetuned small LLM, weighing cost against performance. Traditional selection methods often evaluate every candidate model before choosing one, which are becoming impractical given the rising costs of training and finetuning LLMs. Moreover, it is undesirable to allocate excessive resources towards exploring poor-performing models. While some recent works leverage online bandit algorithm to manage such exploration-exploitation trade-off in model selection, they tend to overlook the increasing-then-converging trend in model performances as the model is iteratively finetuned, leading to less accurate predictions and suboptimal model selections.   In this paper, we propose a time-increasing bandit algorithm TI-UCB, which effectively predicts the increase of model performances due to finetuning and efficiently balances exploration and exploitation in model selection. To further capture the converging points of models, we develop a change detection mechanism by comparing consecutive increase predictions. We theoretically prove that our algorithm achieves a logarithmic regret upper bound in a typical increasing bandit setting, which implies a fast convergence rate. The advantage of our method is also empirically validated through extensive experiments on classification model selection and online selection of LLMs. Our results highlight the importance of utilizing increasing-then-converging pattern for more efficient and economic model selection in the deployment of LLMs.",
      "paper_authors": [
        "Yu Xia",
        "Fang Kong",
        "Tong Yu",
        "Liya Guo",
        "Ryan A. Rossi",
        "Sungchul Kim",
        "Shuai Li"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-03-11",
      "update_time": "2024-03-11",
      "comments": "Accepted by WWW'24 (Oral)",
      "repo_url": "#"
    },
    "2403.05796": {
      "paper_id": "2403.05796v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.05796v1",
      "paper_key": "2403.05796",
      "paper_title": "Weakly Supervised Change Detection via Knowledge Distillation and Multiscale Sigmoid Inference",
      "paper_url": "http://arxiv.org/abs/2403.05796v1",
      "paper_abstract": "Change detection, which aims to detect spatial changes from a pair of multi-temporal images due to natural or man-made causes, has been widely applied in remote sensing, disaster management, urban management, etc. Most existing change detection approaches, however, are fully supervised and require labor-intensive pixel-level labels. To address this, we develop a novel weakly supervised change detection technique via Knowledge Distillation and Multiscale Sigmoid Inference (KD-MSI) that leverages image-level labels. In our approach, the Class Activation Maps (CAM) are utilized not only to derive a change probability map but also to serve as a foundation for the knowledge distillation process. This is done through a joint training strategy of the teacher and student networks, enabling the student network to highlight potential change areas more accurately than teacher network based on image-level labels. Moreover, we designed a Multiscale Sigmoid Inference (MSI) module as a post processing step to further refine the change probability map from the trained student network. Empirical results on three public datasets, i.e., WHU-CD, DSIFN-CD, and LEVIR-CD, demonstrate that our proposed technique, with its integrated training strategy, significantly outperforms the state-of-the-art.",
      "paper_authors": [
        "Binghao Lu",
        "Caiwen Ding",
        "Jinbo Bi",
        "Dongjin Song"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-03-09",
      "update_time": "2024-03-09",
      "comments": "code is available: https://github.com/BinghaoLu/KD-MSI",
      "repo_url": "https://github.com/binghaolu/kd-msi"
    },
    "2403.04385": {
      "paper_id": "2403.04385v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.04385v2",
      "paper_key": "2403.04385",
      "paper_title": "Impacts of Color and Texture Distortions on Earth Observation Data in Deep Learning",
      "paper_url": "http://arxiv.org/abs/2403.04385v2",
      "paper_abstract": "Land cover classification and change detection are two important applications of remote sensing and Earth observation (EO) that have benefited greatly from the advances of deep learning. Convolutional and transformer-based U-net models are the state-of-the-art architectures for these tasks, and their performances have been boosted by an increased availability of large-scale annotated EO datasets. However, the influence of different visual characteristics of the input EO data on a model's predictions is not well understood. In this work we systematically examine model sensitivities with respect to several color- and texture-based distortions on the input EO data during inference, given models that have been trained without such distortions. We conduct experiments with multiple state-of-the-art segmentation networks for land cover classification and show that they are in general more sensitive to texture than to color distortions. Beyond revealing intriguing characteristics of widely used land cover classification models, our results can also be used to guide the development of more robust models within the EO domain.",
      "paper_authors": [
        "Martin Willbo",
        "Aleksis Pirinen",
        "John Martinsson",
        "Edvin Listo Zec",
        "Olof Mogren",
        "Mikael Nilsson"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-03-07",
      "update_time": "2024-04-12",
      "comments": null,
      "repo_url": "#"
    },
    "2403.02175": {
      "paper_id": "2403.02175v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.02175v2",
      "paper_key": "2403.02175",
      "paper_title": "LiSTA: Geometric Object-Based Change Detection in Cluttered Environments",
      "paper_url": "http://arxiv.org/abs/2403.02175v2",
      "paper_abstract": "We present LiSTA (LiDAR Spatio-Temporal Analysis), a system to detect probabilistic object-level change over time using multi-mission SLAM. Many applications require such a system, including construction, robotic navigation, long-term autonomy, and environmental monitoring. We focus on the semi-static scenario where objects are added, subtracted, or changed in position over weeks or months. Our system combines multi-mission LiDAR SLAM, volumetric differencing, object instance description, and correspondence grouping using learned descriptors to keep track of an open set of objects. Object correspondences between missions are determined by clustering the object's learned descriptors. We demonstrate our approach using datasets collected in a simulated environment and a real-world dataset captured using a LiDAR system mounted on a quadruped robot monitoring an industrial facility containing static, semi-static, and dynamic objects. Our method demonstrates superior performance in detecting changes in semi-static environments compared to existing methods.",
      "paper_authors": [
        "Joseph Rowell",
        "Lintong Zhang",
        "Maurice Fallon"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-03-04",
      "update_time": "2024-03-05",
      "comments": "6+n page limit for (accepted) ICRA 2024 submission",
      "repo_url": "#"
    },
    "2403.00226": {
      "paper_id": "2403.00226v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2403.00226v3",
      "paper_key": "2403.00226",
      "paper_title": "A Semantic Distance Metric Learning approach for Lexical Semantic Change Detection",
      "paper_url": "http://arxiv.org/abs/2403.00226v3",
      "paper_abstract": "Detecting temporal semantic changes of words is an important task for various NLP applications that must make time-sensitive predictions. Lexical Semantic Change Detection (SCD) task involves predicting whether a given target word, $w$, changes its meaning between two different text corpora, $C_1$ and $C_2$. For this purpose, we propose a supervised two-staged SCD method that uses existing Word-in-Context (WiC) datasets. In the first stage, for a target word $w$, we learn two sense-aware encoders that represent the meaning of $w$ in a given sentence selected from a corpus. Next, in the second stage, we learn a sense-aware distance metric that compares the semantic representations of a target word across all of its occurrences in $C_1$ and $C_2$. Experimental results on multiple benchmark datasets for SCD show that our proposed method achieves strong performance in multiple languages. Additionally, our method achieves significant improvements on WiC benchmarks compared to a sense-aware encoder with conventional distance functions. Source code is available at https://github.com/LivNLP/svp-sdml .",
      "paper_authors": [
        "Taichi Aida",
        "Danushka Bollegala"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-03-01",
      "update_time": "2024-06-01",
      "comments": "Findings of ACL2024",
      "repo_url": "https://github.com/livnlp/svp-sdml"
    },
    "2402.17128": {
      "paper_id": "2402.17128v4",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.17128v4",
      "paper_key": "2402.17128",
      "paper_title": "OSCaR: Object State Captioning and State Change Representation",
      "paper_url": "http://arxiv.org/abs/2402.17128v4",
      "paper_abstract": "The capability of intelligent models to extrapolate and comprehend changes in object states is a crucial yet demanding aspect of AI research, particularly through the lens of human interaction in real-world settings. This task involves describing complex visual environments, identifying active objects, and interpreting their changes as conveyed through language. Traditional methods, which isolate object captioning and state change detection, offer a limited view of dynamic environments. Moreover, relying on a small set of symbolic words to represent changes has restricted the expressiveness of the language. To address these challenges, in this paper, we introduce the Object State Captioning and State Change Representation (OSCaR) dataset and benchmark. OSCaR consists of 14,084 annotated video segments with nearly 1,000 unique objects from various egocentric video collections. It sets a new testbed for evaluating multimodal large language models (MLLMs). Our experiments demonstrate that while MLLMs show some skill, they lack a full understanding of object state changes. The benchmark includes a fine-tuned model that, despite initial capabilities, requires significant improvements in accuracy and generalization ability for effective understanding of these changes. Our code and dataset are available at https://github.com/nguyennm1024/OSCaR.",
      "paper_authors": [
        "Nguyen Nguyen",
        "Jing Bi",
        "Ali Vosoughi",
        "Yapeng Tian",
        "Pooyan Fazli",
        "Chenliang Xu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-02-27",
      "update_time": "2024-04-02",
      "comments": "NAACL 2024",
      "repo_url": "https://github.com/nguyennm1024/oscar"
    },
    "2402.16596": {
      "paper_id": "2402.16596v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.16596v1",
      "paper_key": "2402.16596",
      "paper_title": "Semantic change detection for Slovene language: a novel dataset and an approach based on optimal transport",
      "paper_url": "http://arxiv.org/abs/2402.16596v1",
      "paper_abstract": "In this paper, we focus on the detection of semantic changes in Slovene, a less resourced Slavic language with two million speakers. Detecting and tracking semantic changes provides insights into the evolution of the language caused by changes in society and culture. Recently, several systems have been proposed to aid in this study, but all depend on manually annotated gold standard datasets for evaluation. In this paper, we present the first Slovene dataset for evaluating semantic change detection systems, which contains aggregated semantic change scores for 104 target words obtained from more than 3000 manually annotated sentence pairs. We evaluate several existing semantic change detection methods on this dataset and also propose a novel approach based on optimal transport that improves on the existing state-of-the-art systems with an error reduction rate of 22.8%.",
      "paper_authors": [
        "Marko Pranji\u0107",
        "Kaja Dobrovoljc",
        "Senja Pollak",
        "Matej Martinc"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-02-26",
      "update_time": "2024-02-26",
      "comments": null,
      "repo_url": "https://github.com/sharpsy/slovene-ot-semchange"
    },
    "2402.16242": {
      "paper_id": "2402.16242v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.16242v1",
      "paper_key": "2402.16242",
      "paper_title": "HSONet:A Siamese foreground association-driven hard case sample optimization network for high-resolution remote sensing image change detection",
      "paper_url": "http://arxiv.org/abs/2402.16242v1",
      "paper_abstract": "In the later training stages, further improvement of the models ability to determine changes relies on how well the change detection (CD) model learns hard cases; however, there are two additional challenges to learning hard case samples: (1) change labels are limited and tend to pointer only to foreground targets, yet hard case samples are prevalent in the background, which leads to optimizing the loss function focusing on the foreground targets and ignoring the background hard cases, which we call imbalance. (2) Complex situations, such as light shadows, target occlusion, and seasonal changes, induce hard case samples, and in the absence of both supervisory and scene information, it is difficult for the model to learn hard case samples directly to accurately obtain the feature representations of the change information, which we call missingness. We propose a Siamese foreground association-driven hard case sample optimization network (HSONet). To deal with this imbalance, we propose an equilibrium optimization loss function to regulate the optimization focus of the foreground and background, determine the hard case samples through the distribution of the loss values, and introduce dynamic weights in the loss term to gradually shift the optimization focus of the loss from the foreground to the background hard cases as the training progresses. To address this missingness, we understand hard case samples with the help of the scene context, propose the scene-foreground association module, use potential remote sensing spatial scene information to model the association between the target of interest in the foreground and the related context to obtain scene embedding, and apply this information to the feature reinforcement of hard cases. Experiments on four public datasets show that HSONet outperforms current state-of-the-art CD methods, particularly in detecting hard case samples.",
      "paper_authors": [
        "Chao Tao",
        "Dongsheng Kuang",
        "Zhenyang Huang",
        "Chengli Peng",
        "Haifeng Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-02-26",
      "update_time": "2024-02-26",
      "comments": "17 figures, 8 tables, 18 pages",
      "repo_url": "#"
    },
    "2402.12011": {
      "paper_id": "2402.12011v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.12011v3",
      "paper_key": "2402.12011",
      "paper_title": "A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic Change",
      "paper_url": "http://arxiv.org/abs/2402.12011v3",
      "paper_abstract": "Contextualized embeddings are the preferred tool for modeling Lexical Semantic Change (LSC). Current evaluations typically focus on a specific task known as Graded Change Detection (GCD). However, performance comparison across work are often misleading due to their reliance on diverse settings. In this paper, we evaluate state-of-the-art models and approaches for GCD under equal conditions. We further break the LSC problem into Word-in-Context (WiC) and Word Sense Induction (WSI) tasks, and compare models across these different levels. Our evaluation is performed across different languages on eight available benchmarks for LSC, and shows that (i) APD outperforms other approaches for GCD; (ii) XL-LEXEME outperforms other contextualized models for WiC, WSI, and GCD, while being comparable to GPT-4; (iii) there is a clear need for improving the modeling of word meanings, as well as focus on how, when, and why these meanings change, rather than solely focusing on the extent of semantic change.",
      "paper_authors": [
        "Francesco Periti",
        "Nina Tahmasebi"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-02-19",
      "update_time": "2024-03-08",
      "comments": "Submitted to NAACL 2024",
      "repo_url": "https://github.com/francescoperiti/cssdetection"
    },
    "2402.10321": {
      "paper_id": "2402.10321v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.10321v2",
      "paper_key": "2402.10321",
      "paper_title": "LaserSAM: Zero-Shot Change Detection Using Visual Segmentation of Spinning LiDAR",
      "paper_url": "http://arxiv.org/abs/2402.10321v2",
      "paper_abstract": "This paper presents an approach for applying camera perception techniques to spinning LiDAR data. To improve the robustness of long-term change detection from a 3D LiDAR, range and intensity information are rendered into virtual perspectives using a pinhole camera model. Hue-saturation-value image encoding is used to colourize the images by range and near-IR intensity. The LiDAR's active scene illumination makes it invariant to ambient brightness, which enables night-to-day change detection without additional processing. Using the range-colourized, perspective image allows existing foundation models to detect semantic regions. Specifically, the Segment Anything Model detects semantically similar regions in both a previously acquired map and live view from a path-repeating robot. By comparing the masks in both views, changes in the live scan are detected. Results indicate that the Segment Anything Model accurately captures the shape of arbitrary changes introduced into scenes. The proposed method achieves a segmentation intersection over union of 73.3% when evaluated in unstructured environments and 80.4% when evaluated within the planning corridor. Changes can be detected reliably through day-to-night illumination variations. After pixel-level masks are generated, the one-to-one correspondence with 3D points means that the 2D masks can be used directly to recover the 3D location of the changes. The detected 3D changes are avoided in a closed loop by treating them as obstacles in a local motion planner. Experiments on an unmanned ground vehicle demonstrate the performance of the method.",
      "paper_authors": [
        "Alexander Krawciw",
        "Sven Lilge",
        "Timothy D. Barfoot"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-02-15",
      "update_time": "2024-04-29",
      "comments": "9 pages (8 content, 1 references). 9 figures, Presented at 2024\n  Conference on Robots and Vision (CRV)",
      "repo_url": "#"
    },
    "2402.10291": {
      "paper_id": "2402.10291v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.10291v2",
      "paper_key": "2402.10291",
      "paper_title": "An Evaluation of Real-time Adaptive Sampling Change Point Detection Algorithm using KCUSUM",
      "paper_url": "http://arxiv.org/abs/2402.10291v2",
      "paper_abstract": "Detecting abrupt changes in real-time data streams from scientific simulations presents a challenging task, demanding the deployment of accurate and efficient algorithms. Identifying change points in live data stream involves continuous scrutiny of incoming observations for deviations in their statistical characteristics, particularly in high-volume data scenarios. Maintaining a balance between sudden change detection and minimizing false alarms is vital. Many existing algorithms for this purpose rely on known probability distributions, limiting their feasibility. In this study, we introduce the Kernel-based Cumulative Sum (KCUSUM) algorithm, a non-parametric extension of the traditional Cumulative Sum (CUSUM) method, which has gained prominence for its efficacy in online change point detection under less restrictive conditions. KCUSUM splits itself by comparing incoming samples directly with reference samples and computes a statistic grounded in the Maximum Mean Discrepancy (MMD) non-parametric framework. This approach extends KCUSUM's pertinence to scenarios where only reference samples are available, such as atomic trajectories of proteins in vacuum, facilitating the detection of deviations from the reference sample without prior knowledge of the data's underlying distribution. Furthermore, by harnessing MMD's inherent random-walk structure, we can theoretically analyze KCUSUM's performance across various use cases, including metrics like expected delay and mean runtime to false alarms. Finally, we discuss real-world use cases from scientific simulations such as NWChem CODAR and protein folding data, demonstrating KCUSUM's practical effectiveness in online change point detection.",
      "paper_authors": [
        "Vijayalakshmi Saravanan",
        "Perry Siehien",
        "Shinjae Yoo",
        "Hubertus Van Dam",
        "Thomas Flynn",
        "Christopher Kelly",
        "Khaled Z Ibrahim"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-02-15",
      "update_time": "2024-04-05",
      "comments": "16 pages. arXiv admin note: text overlap with arXiv:1903.01661",
      "repo_url": "#"
    },
    "2402.10142": {
      "paper_id": "2402.10142v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.10142v2",
      "paper_key": "2402.10142",
      "paper_title": "Tracking Changing Probabilities via Dynamic Learners",
      "paper_url": "http://arxiv.org/abs/2402.10142v2",
      "paper_abstract": "Consider a predictor, a learner, whose input is a stream of discrete items. The predictor's task, at every time point, is probabilistic multiclass prediction, i.e., to predict which item may occur next by outputting zero or more candidate items, each with a probability, after which the actual item is revealed and the predictor learns from this observation. To output probabilities, the predictor keeps track of the proportions of the items it has seen. The stream is unbounded and the predictor has finite limited space and we seek efficient prediction and update techniques: the set of items is unknown to the predictor and their totality can also grow unbounded. Moreover, there is non-stationarity: the underlying frequencies of items may change, substantially, from time to time. For instance, new items may start appearing and a few recently frequent items may cease to occur again. The predictor, being space-bounded, need only provide probabilities for those items with (currently) sufficiently high frequency, i.e., the salient items. This problem is motivated in the setting of prediction games, a self-supervised learning regime where concepts serve as both the predictors and the predictands, and the set of concepts grows over time, resulting in non-stationarities as new concepts are generated and used. We develop sparse multiclass moving average techniques designed to respond to such non-stationarities in a timely manner. One technique is based on the exponentiated moving average (EMA) and another is based on queuing a few count snapshots. We show that the combination, and in particular supporting dynamic predictand-specific learning rates, offers advantages in terms of faster change detection and convergence.",
      "paper_authors": [
        "Omid Madani"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-02-15",
      "update_time": "2024-04-30",
      "comments": "63 pages, 24 figures, 17 tables",
      "repo_url": "#"
    },
    "2402.09781": {
      "paper_id": "2402.09781v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.09781v1",
      "paper_key": "2402.09781",
      "paper_title": "A Comprehensive Review on Computer Vision Analysis of Aerial Data",
      "paper_url": "http://arxiv.org/abs/2402.09781v1",
      "paper_abstract": "With the emergence of new technologies in the field of airborne platforms and imaging sensors, aerial data analysis is becoming very popular, capitalizing on its advantages over land data. This paper presents a comprehensive review of the computer vision tasks within the domain of aerial data analysis. While addressing fundamental aspects such as object detection and tracking, the primary focus is on pivotal tasks like change detection, object segmentation, and scene-level analysis. The paper provides the comparison of various hyper parameters employed across diverse architectures and tasks. A substantial section is dedicated to an in-depth discussion on libraries, their categorization, and their relevance to different domain expertise. The paper encompasses aerial datasets, the architectural nuances adopted, and the evaluation metrics associated with all the tasks in aerial data analysis. Applications of computer vision tasks in aerial data across different domains are explored, with case studies providing further insights. The paper thoroughly examines the challenges inherent in aerial data analysis, offering practical solutions. Additionally, unresolved issues of significance are identified, paving the way for future research directions in the field of aerial data analysis.",
      "paper_authors": [
        "Vivek Tetarwal",
        "Sandeep Kumar"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-02-15",
      "update_time": "2024-02-15",
      "comments": "112 pages",
      "repo_url": "#"
    },
    "2402.09743": {
      "paper_id": "2402.09743v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.09743v1",
      "paper_key": "2402.09743",
      "paper_title": "Quickest Detection of False Data Injection Attack in Distributed Process Tracking",
      "paper_url": "http://arxiv.org/abs/2402.09743v1",
      "paper_abstract": "This paper addresses the problem of detecting false data injection (FDI) attacks in a distributed network without a fusion center, represented by a connected graph among multiple agent nodes. Each agent node is equipped with a sensor, and uses a Kalman consensus information filter (KCIF) to track a discrete time global process with linear dynamics and additive Gaussian noise. The state estimate of the global process at any sensor is computed from the local observation history and the information received by that agent node from its neighbors. At an unknown time, an attacker starts altering the local observation of one agent node. In the Bayesian setting where there is a known prior distribution of the attack beginning instant, we formulate a Bayesian quickest change detection (QCD) problem for FDI detection in order to minimize the mean detection delay subject to a false alarm probability constraint. While it is well-known that the optimal Bayesian QCD rule involves checking the Shriyaev's statistic against a threshold, we demonstrate how to compute the Shriyaev's statistic at each node in a recursive fashion given our non-i.i.d. observations. Next, we consider non-Bayesian QCD where the attack begins at an arbitrary and unknown time, and the detector seeks to minimize the worst case detection delay subject to a constraint on the mean time to false alarm and probability of misidentification. We use the multiple hypothesis sequential probability ratio test for attack detection and identification at each sensor. For unknown attack strategy, we use the window-limited generalized likelihood ratio (WL-GLR) algorithm to solve the QCD problem. Numerical results demonstrate the performances and trade-offs of the proposed algorithms.",
      "paper_authors": [
        "Saqib Abbas Baba",
        "Arpan Chattopadhyay"
      ],
      "primary_category": "eess.SY",
      "publish_time": "2024-02-15",
      "update_time": "2024-02-15",
      "comments": null,
      "repo_url": "#"
    },
    "2402.06994": {
      "paper_id": "2402.06994v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.06994v2",
      "paper_key": "2402.06994",
      "paper_title": "A Change Detection Reality Check",
      "paper_url": "http://arxiv.org/abs/2402.06994v2",
      "paper_abstract": "In recent years, there has been an explosion of proposed change detection deep learning architectures in the remote sensing literature. These approaches claim to offer state-of-the-art performance on different standard benchmark datasets. However, has the field truly made significant progress? In this paper we perform experiments which conclude a simple U-Net segmentation baseline without training tricks or complicated architectural changes is still a top performer for the task of change detection.",
      "paper_authors": [
        "Isaac Corley",
        "Caleb Robinson",
        "Anthony Ortiz"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-02-10",
      "update_time": "2024-04-12",
      "comments": null,
      "repo_url": "https://github.com/isaaccorley/a-change-detection-reality-check"
    },
    "2402.06531": {
      "paper_id": "2402.06531v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.06531v1",
      "paper_key": "2402.06531",
      "paper_title": "Transferring facade labels between point clouds with semantic octrees while considering change detection",
      "paper_url": "http://arxiv.org/abs/2402.06531v1",
      "paper_abstract": "Point clouds and high-resolution 3D data have become increasingly important in various fields, including surveying, construction, and virtual reality. However, simply having this data is not enough; to extract useful information, semantic labeling is crucial. In this context, we propose a method to transfer annotations from a labeled to an unlabeled point cloud using an octree structure. The structure also analyses changes between the point clouds. Our experiments confirm that our method effectively transfers annotations while addressing changes. The primary contribution of this project is the development of the method for automatic label transfer between two different point clouds that represent the same real-world object. The proposed method can be of great importance for data-driven deep learning algorithms as it can also allow circumventing stochastic transfer learning by deterministic label transfer between datasets depicting the same objects.",
      "paper_authors": [
        "Sophia Schwarz",
        "Tanja Pilz",
        "Olaf Wysocki",
        "Ludwig Hoegner",
        "Uwe Stilla"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-02-09",
      "update_time": "2024-02-09",
      "comments": "Accepted to the Recent Advances in 3D Geoinformation Science,\n  Proceedings of the 18th 3D GeoInfo Conference 2023",
      "repo_url": "https://github.com/schwarzsophia/transferring_urban_labels_between_pointclouds"
    },
    "2402.01188": {
      "paper_id": "2402.01188v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.01188v2",
      "paper_key": "2402.01188",
      "paper_title": "Segment Any Change",
      "paper_url": "http://arxiv.org/abs/2402.01188v2",
      "paper_abstract": "Visual foundation models have achieved remarkable results in zero-shot image classification and segmentation, but zero-shot change detection remains an open problem. In this paper, we propose the segment any change models (AnyChange), a new type of change detection model that supports zero-shot prediction and generalization on unseen change types and data distributions. AnyChange is built on the segment anything model (SAM) via our training-free adaptation method, bitemporal latent matching. By revealing and exploiting intra-image and inter-image semantic similarities in SAM's latent space, bitemporal latent matching endows SAM with zero-shot change detection capabilities in a training-free way. We also propose a point query mechanism to enable AnyChange's zero-shot object-centric change detection capability. We perform extensive experiments to confirm the effectiveness of AnyChange for zero-shot change detection. AnyChange sets a new record on the SECOND benchmark for unsupervised change detection, exceeding the previous SOTA by up to 4.4% F$_1$ score, and achieving comparable accuracy with negligible manual annotations (1 pixel per image) for supervised change detection.",
      "paper_authors": [
        "Zhuo Zheng",
        "Yanfei Zhong",
        "Liangpei Zhang",
        "Stefano Ermon"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-02-02",
      "update_time": "2024-02-15",
      "comments": "technical report, 12 pages",
      "repo_url": "#"
    },
    "2402.01005": {
      "paper_id": "2402.01005v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2402.01005v1",
      "paper_key": "2402.01005",
      "paper_title": "The prices of renewable commodities: A robust stationarity analysis",
      "paper_url": "http://arxiv.org/abs/2402.01005v1",
      "paper_abstract": "This paper addresses the problem of testing for persistence in the effects of the shocks affecting the prices of renewable commodities, which have potential implications on stabilization policies and economic forecasting, among other areas. A robust methodology is employed that enables the determination of the potential presence and number of instant/gradual structural changes in the series, stationarity testing conditional on the number of changes detected, and the detection of change points. This procedure is applied to the annual real prices of eighteen renewable commodities over the period of 1900-2018. Results indicate that most of the series display non-linear features, including quadratic patterns and regime transitions that often coincide with well-known political and economic episodes. The conclusions of stationarity testing suggest that roughly half of the series are integrated. Stationarity fails to be rejected for grains, whereas most livestock and textile commodities do reject stationarity. Evidence is mixed in all soft commodities and tropical crops, where stationarity can be rejected in approximately half of the cases. The implication would be that for these commodities, stabilization schemes would not be recommended.",
      "paper_authors": [
        "Manuel Landajo",
        "Mar\u00eda Jos\u00e9 Presno"
      ],
      "primary_category": "econ.EM",
      "publish_time": "2024-02-01",
      "update_time": "2024-02-01",
      "comments": null,
      "repo_url": "#"
    },
    "2401.17600": {
      "paper_id": "2401.17600v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.17600v1",
      "paper_key": "2401.17600",
      "paper_title": "Good at captioning, bad at counting: Benchmarking GPT-4V on Earth observation data",
      "paper_url": "http://arxiv.org/abs/2401.17600v1",
      "paper_abstract": "Large Vision-Language Models (VLMs) have demonstrated impressive performance on complex tasks involving visual input with natural language instructions. However, it remains unclear to what extent capabilities on natural images transfer to Earth observation (EO) data, which are predominantly satellite and aerial images less common in VLM training data. In this work, we propose a comprehensive benchmark to gauge the progress of VLMs toward being useful tools for EO data by assessing their abilities on scene understanding, localization and counting, and change detection tasks. Motivated by real-world applications, our benchmark includes scenarios like urban monitoring, disaster relief, land use, and conservation. We discover that, although state-of-the-art VLMs like GPT-4V possess extensive world knowledge that leads to strong performance on open-ended tasks like location understanding and image captioning, their poor spatial reasoning limits usefulness on object localization and counting tasks. Our benchmark will be made publicly available at https://vleo.danielz.ch/ and on Hugging Face at https://huggingface.co/collections/mit-ei/vleo-benchmark-datasets-65b789b0466555489cce0d70 for easy model evaluation.",
      "paper_authors": [
        "Chenhui Zhang",
        "Sherrie Wang"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-01-31",
      "update_time": "2024-01-31",
      "comments": "62 pages; work in progress",
      "repo_url": "https://github.com/Earth-Intelligence-Lab/vleo-bench"
    },
    "2401.14040": {
      "paper_id": "2401.14040v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.14040v3",
      "paper_key": "2401.14040",
      "paper_title": "(Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection",
      "paper_url": "http://arxiv.org/abs/2401.14040v3",
      "paper_abstract": "In the universe of Natural Language Processing, Transformer-based language models like BERT and (Chat)GPT have emerged as lexical superheroes with great power to solve open research problems. In this paper, we specifically focus on the temporal problem of semantic change, and evaluate their ability to solve two diachronic extensions of the Word-in-Context (WiC) task: TempoWiC and HistoWiC. In particular, we investigate the potential of a novel, off-the-shelf technology like ChatGPT (and GPT) 3.5 compared to BERT, which represents a family of models that currently stand as the state-of-the-art for modeling semantic change. Our experiments represent the first attempt to assess the use of (Chat)GPT for studying semantic change. Our results indicate that ChatGPT performs significantly worse than the foundational GPT version. Furthermore, our results demonstrate that (Chat)GPT achieves slightly lower performance than BERT in detecting long-term changes but performs significantly worse in detecting short-term changes.",
      "paper_authors": [
        "Francesco Periti",
        "Haim Dubossarsky",
        "Nina Tahmasebi"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-01-25",
      "update_time": "2024-04-29",
      "comments": "Accepted to the Findings of EACL 2024\n  (https://aclanthology.org/2024.findings-eacl.29.pdf)",
      "repo_url": "https://github.com/francescoperiti/chatgptvbert"
    },
    "2401.13877": {
      "paper_id": "2401.13877v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.13877v1",
      "paper_key": "2401.13877",
      "paper_title": "AscDAMs: Advanced SLAM-based channel detection and mapping system",
      "paper_url": "http://arxiv.org/abs/2401.13877v1",
      "paper_abstract": "Obtaining high-resolution, accurate channel topography and deposit conditions is the prior challenge for the study of channelized debris flow. Currently, wide-used mapping technologies including satellite imaging and drone photogrammetry struggle to precisely observe channel interior conditions of mountainous long-deep gullies, particularly those in the Wenchuan Earthquake region. SLAM is an emerging tech for 3D mapping; however, extremely rugged environment in long-deep gullies poses two major challenges even for the state-of-art SLAM: (1) Atypical features; (2) Violent swaying and oscillation of sensors. These issues result in large deviation and lots of noise for SLAM results. To improve SLAM mapping in such environments, we propose an advanced SLAM-based channel detection and mapping system, namely AscDAMs. It features three main enhancements to post-process SLAM results: (1) The digital orthophoto map aided deviation correction algorithm greatly eliminates the systematic error; (2) The point cloud smoothing algorithm substantially diminishes noises; (3) The cross section extraction algorithm enables the quantitative assessment of channel deposits and their changes. Two field experiments were conducted in Chutou Gully, Wenchuan County in China in February and November 2023, representing observations before and after the rainy season. We demonstrate the capability of AscDAMs to greatly improve SLAM results, promoting SLAM for mapping the specially challenging environment. The proposed method compensates for the insufficiencies of existing technologies in detecting debris flow channel interiors including detailed channel morphology, erosion patterns, deposit distinction, volume estimation and change detection. It serves to enhance the study of full-scale debris flow mechanisms, long-term post-seismic evolution, and hazard assessment.",
      "paper_authors": [
        "Tengfei Wang",
        "Fucheng Lu",
        "Jintao Qin",
        "Taosheng Huang",
        "Hui Kong",
        "Ping Shen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-01-25",
      "update_time": "2024-01-25",
      "comments": null,
      "repo_url": "#"
    },
    "2401.12499": {
      "paper_id": "2401.12499v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.12499v1",
      "paper_key": "2401.12499",
      "paper_title": "On the Fundamental Tradeoff of Joint Communication and Quickest Change Detection",
      "paper_url": "http://arxiv.org/abs/2401.12499v1",
      "paper_abstract": "In this work, we take the initiative in studying the fundamental tradeoff between communication and quickest change detection (QCD) under an integrated sensing and communication setting. We formally establish a joint communication and sensing problem for quickest change detection. Then, by utilizing constant subblock-composition codes and a modified QuSum detection rule, which we call subblock QuSum (SQS), we provide an inner bound on the fundamental tradeoff between communication rate and change point detection delay in the asymptotic regime of vanishing false alarm rate. We further provide a partial converse that matches our inner bound for a certain class of codes. This implies that the SQS detection strategy is asymptotically optimal for our codes as the false alarm rate constraint vanishes. We also present some canonical examples of the tradeoff region for a binary channel, a scalar Gaussian channel, and a MIMO Gaussian channel.",
      "paper_authors": [
        "Daewon Seo",
        "Sung Hoon Lim"
      ],
      "primary_category": "cs.IT",
      "publish_time": "2024-01-23",
      "update_time": "2024-01-23",
      "comments": null,
      "repo_url": "#"
    },
    "2401.11489": {
      "paper_id": "2401.11489v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.11489v1",
      "paper_key": "2401.11489",
      "paper_title": "MapChange: Enhancing Semantic Change Detection with Temporal-Invariant Historical Maps Based on Deep Triplet Network",
      "paper_url": "http://arxiv.org/abs/2401.11489v1",
      "paper_abstract": "Semantic Change Detection (SCD) is recognized as both a crucial and challenging task in the field of image analysis. Traditional methods for SCD have predominantly relied on the comparison of image pairs. However, this approach is significantly hindered by substantial imaging differences, which arise due to variations in shooting times, atmospheric conditions, and angles. Such discrepancies lead to two primary issues: the under-detection of minor yet significant changes, and the generation of false alarms due to temporal variances. These factors often result in unchanged objects appearing markedly different in multi-temporal images. In response to these challenges, the MapChange framework has been developed. This framework introduces a novel paradigm that synergizes temporal-invariant historical map data with contemporary high-resolution images. By employing this combination, the temporal variance inherent in conventional image pair comparisons is effectively mitigated. The efficacy of the MapChange framework has been empirically validated through comprehensive testing on two public datasets. These tests have demonstrated the framework's marked superiority over existing state-of-the-art SCD methods.",
      "paper_authors": [
        "Yinhe Liu",
        "Sunan Shi",
        "Zhuo Zheng",
        "Jue Wang",
        "Shiqi Tian",
        "Yanfei Zhong"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-01-21",
      "update_time": "2024-01-21",
      "comments": null,
      "repo_url": "#"
    },
    "2401.10752": {
      "paper_id": "2401.10752v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.10752v1",
      "paper_key": "2401.10752",
      "paper_title": "HiCD: Change Detection in Quality-Varied Images via Hierarchical Correlation Distillation",
      "paper_url": "http://arxiv.org/abs/2401.10752v1",
      "paper_abstract": "Advanced change detection techniques primarily target image pairs of equal and high quality. However, variations in imaging conditions and platforms frequently lead to image pairs with distinct qualities: one image being high-quality, while the other being low-quality. These disparities in image quality present significant challenges for understanding image pairs semantically and extracting change features, ultimately resulting in a notable decline in performance. To tackle this challenge, we introduce an innovative training strategy grounded in knowledge distillation. The core idea revolves around leveraging task knowledge acquired from high-quality image pairs to guide the model's learning process when dealing with image pairs that exhibit differences in quality. Additionally, we develop a hierarchical correlation distillation approach (involving self-correlation, cross-correlation, and global correlation). This approach compels the student model to replicate the correlations inherent in the teacher model, rather than focusing solely on individual features. This ensures effective knowledge transfer while maintaining the student model's training flexibility.",
      "paper_authors": [
        "Chao Pang",
        "Xingxing Weng",
        "Jiang Wu",
        "Qiang Wang",
        "Gui-Song Xia"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-01-19",
      "update_time": "2024-01-19",
      "comments": "accepted by TGRS",
      "repo_url": "https://github.com/fitzpchao/hicd"
    },
    "2401.10006": {
      "paper_id": "2401.10006v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.10006v1",
      "paper_key": "2401.10006",
      "paper_title": "Regime change detection in irregularly sampled time series",
      "paper_url": "http://arxiv.org/abs/2401.10006v1",
      "paper_abstract": "Irregular sampling is a common problem in palaeoclimate studies. We propose a method that provides regularly sampled time series and at the same time a difference filtering of the data. The differences between successive time instances are derived by a transformation costs procedure. A subsequent recurrence analysis is used to investigate regime transitions. This approach is applied on speleothem based palaeoclimate proxy data from the Indonesian-Australian monsoon region. We can clearly identify Heinrich events in the palaeoclimate as characteristic changes in the dynamics.",
      "paper_authors": [
        "Norbert Marwan",
        "Deniz Eroglu",
        "Ibrahim Ozken",
        "Thomas Stemler",
        "Karl-Heinz Wyrwoll",
        "J\u00fcrgen Kurths"
      ],
      "primary_category": "physics.data-an",
      "publish_time": "2024-01-18",
      "update_time": "2024-01-18",
      "comments": "12 pages, 3 figures",
      "repo_url": "#"
    },
    "2401.09325": {
      "paper_id": "2401.09325v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.09325v1",
      "paper_key": "2401.09325",
      "paper_title": "Siamese Meets Diffusion Network: SMDNet for Enhanced Change Detection in High-Resolution RS Imagery",
      "paper_url": "http://arxiv.org/abs/2401.09325v1",
      "paper_abstract": "Recently, the application of deep learning to change detection (CD) has significantly progressed in remote sensing images. In recent years, CD tasks have mostly used architectures such as CNN and Transformer to identify these changes. However, these architectures have shortcomings in representing boundary details and are prone to false alarms and missed detections under complex lighting and weather conditions. For that, we propose a new network, Siamese Meets Diffusion Network (SMDNet). This network combines the Siam-U2Net Feature Differential Encoder (SU-FDE) and the denoising diffusion implicit model to improve the accuracy of image edge change detection and enhance the model's robustness under environmental changes. First, we propose an innovative SU-FDE module that utilizes shared weight features to capture differences between time series images and identify similarities between features to enhance edge detail detection. Furthermore, we add an attention mechanism to identify key coarse features to improve the model's sensitivity and accuracy. Finally, the diffusion model of progressive sampling is used to fuse key coarse features, and the noise reduction ability of the diffusion model and the advantages of capturing the probability distribution of image data are used to enhance the adaptability of the model in different environments. Our method's combination of feature extraction and diffusion models demonstrates effectiveness in change detection in remote sensing images. The performance evaluation of SMDNet on LEVIR-CD, DSIFN-CD, and CDD datasets yields validated F1 scores of 90.99%, 88.40%, and 88.47%, respectively. This substantiates the advanced capabilities of our model in accurately identifying variations and intricate details.",
      "paper_authors": [
        "Jia Jia",
        "Geunho Lee",
        "Zhibo Wang",
        "Lyu Zhi",
        "Yuchu He"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-01-17",
      "update_time": "2024-01-17",
      "comments": "12 pages, 4 figures",
      "repo_url": "#"
    },
    "2401.09022": {
      "paper_id": "2401.09022v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.09022v1",
      "paper_key": "2401.09022",
      "paper_title": "Spatio-temporal analysis for extreme temperature indices over the Levant region",
      "paper_url": "http://arxiv.org/abs/2401.09022v1",
      "paper_abstract": "The temporal and spatial trends of 16 climate extreme indices based on daily maximum and minimum temperatures during the period 1987-2016 at 28 stations distributed across Israel and Palestine in the Levant region were annually and seasonally analysed. The Man-Kendall test and the Sen's slope estimator were employed for the trend analysis. Results showed that the region has significantly experienced a dominant warming trend for the last three decades, with more intense changes for minimum temperatures than for maximum. At annual scale, maximum values of minimum temperatures exhibited significant increasing trends up to 0.68{\\deg}C/decade. Changes detected were more pronounced than those for the absolute extreme temperature indices, with 93 and 89% of stations significantly showed increasing trends in TX90p and TN90p, respectively. The duration and fixed threshold extreme indices confirmed the trend toward a warming, with the 86% of the stations exhibited significant increasing trends in the annual SU25 and TR20. Moreover, 57% of stations showed significant increasing trends in their SU30 index. At seasonal scale, the analysis of trends for extreme temperature indices showed intense and broad significant increasing trends in all absolute extreme temperature indices. In summer, more than 75% of total stations exhibited significant increasing trends for warm days and warm nights. In winter and spring, 71% of the total stations also showed significant increasing trends in SU25 index, whereas the percentage of stations reached 82% in summer and 64% in autumn for significant increasing trends in TR20 index. Finally, the influence of large-scale circulation patterns on temperature extremes was examined. The results highlighted the presence of significant correlations between most of the selected extreme temperature indices and the North Sea-Caspian pattern at annual and seasonal scales.",
      "paper_authors": [
        "Ala A. M. Salameh",
        "Sonia R. G\u00e1miz-Fortis",
        "Yolanda Castro-D\u00edez",
        "Ahmad Abu Hammad",
        "Mar\u00eda Jes\u00fas Esteban-Parra"
      ],
      "primary_category": "physics.ao-ph",
      "publish_time": "2024-01-17",
      "update_time": "2024-01-17",
      "comments": null,
      "repo_url": "#"
    },
    "2401.09019": {
      "paper_id": "2401.09019v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.09019v1",
      "paper_key": "2401.09019",
      "paper_title": "Change Detection Between Optical Remote Sensing Imagery and Map Data via Segment Anything Model (SAM)",
      "paper_url": "http://arxiv.org/abs/2401.09019v1",
      "paper_abstract": "Unsupervised multimodal change detection is pivotal for time-sensitive tasks and comprehensive multi-temporal Earth monitoring. In this study, we explore unsupervised multimodal change detection between two key remote sensing data sources: optical high-resolution imagery and OpenStreetMap (OSM) data. Specifically, we propose to utilize the vision foundation model Segmentation Anything Model (SAM), for addressing our task. Leveraging SAM's exceptional zero-shot transfer capability, high-quality segmentation maps of optical images can be obtained. Thus, we can directly compare these two heterogeneous data forms in the so-called segmentation domain. We then introduce two strategies for guiding SAM's segmentation process: the 'no-prompt' and 'box/mask prompt' methods. The two strategies are designed to detect land-cover changes in general scenarios and to identify new land-cover objects within existing backgrounds, respectively. Experimental results on three datasets indicate that the proposed approach can achieve more competitive results compared to representative unsupervised multimodal change detection methods.",
      "paper_authors": [
        "Hongruixuan Chen",
        "Jian Song",
        "Naoto Yokoya"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2024-01-17",
      "update_time": "2024-01-17",
      "comments": null,
      "repo_url": "#"
    },
    "2401.08837": {
      "paper_id": "2401.08837v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.08837v1",
      "paper_key": "2401.08837",
      "paper_title": "Image Fusion in Remote Sensing: An Overview and Meta Analysis",
      "paper_url": "http://arxiv.org/abs/2401.08837v1",
      "paper_abstract": "Image fusion in Remote Sensing (RS) has been a consistent demand due to its ability to turn raw images of different resolutions, sources, and modalities into accurate, complete, and spatio-temporally coherent images. It greatly facilitates downstream applications such as pan-sharpening, change detection, land-cover classification, etc. Yet, image fusion solutions are highly disparate to various remote sensing problems and thus are often narrowly defined in existing reviews as topical applications, such as pan-sharpening, and spatial-temporal image fusion. Considering that image fusion can be theoretically applied to any gridded data through pixel-level operations, in this paper, we expanded its scope by comprehensively surveying relevant works with a simple taxonomy: 1) many-to-one image fusion; 2) many-to-many image fusion. This simple taxonomy defines image fusion as a mapping problem that turns either a single or a set of images into another single or set of images, depending on the desired coherence, e.g., spectral, spatial/resolution coherence, etc. We show that this simple taxonomy, despite the significant modality difference it covers, can be presented by a conceptually easy framework. In addition, we provide a meta-analysis to review the major papers studying the various types of image fusion and their applications over the years (from the 1980s to date), covering 5,926 peer-reviewed papers. Finally, we discuss the main benefits and emerging challenges to provide open research directions and potential future works.",
      "paper_authors": [
        "Hessah Albanwan",
        "Rongjun Qin",
        "Yang Tang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-01-16",
      "update_time": "2024-01-16",
      "comments": "21pages, 10 figures",
      "repo_url": "#"
    },
    "2401.06752": {
      "paper_id": "2401.06752v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.06752v1",
      "paper_key": "2401.06752",
      "paper_title": "Stylometry Analysis of Multi-authored Documents for Authorship and Author Style Change Detection",
      "paper_url": "http://arxiv.org/abs/2401.06752v1",
      "paper_abstract": "In recent years, the increasing use of Artificial Intelligence based text generation tools has posed new challenges in document provenance, authentication, and authorship detection. However, advancements in stylometry have provided opportunities for automatic authorship and author change detection in multi-authored documents using style analysis techniques. Style analysis can serve as a primary step toward document provenance and authentication through authorship detection. This paper investigates three key tasks of style analysis: (i) classification of single and multi-authored documents, (ii) single change detection, which involves identifying the point where the author switches, and (iii) multiple author-switching detection in multi-authored documents. We formulate all three tasks as classification problems and propose a merit-based fusion framework that integrates several state-of-the-art natural language processing (NLP) algorithms and weight optimization techniques. We also explore the potential of special characters, which are typically removed during pre-processing in NLP applications, on the performance of the proposed methods for these tasks by conducting extensive experiments on both cleaned and raw datasets. Experimental results demonstrate significant improvements over existing solutions for all three tasks on a benchmark dataset.",
      "paper_authors": [
        "Muhammad Tayyab Zamir",
        "Muhammad Asif Ayub",
        "Asma Gul",
        "Nasir Ahmad",
        "Kashif Ahmad"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-01-12",
      "update_time": "2024-01-12",
      "comments": "Tables 7, pages 4;",
      "repo_url": "#"
    },
    "2401.06252": {
      "paper_id": "2401.06252v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.06252v1",
      "paper_key": "2401.06252",
      "paper_title": "AGSPNet: A framework for parcel-scale crop fine-grained semantic change detection from UAV high-resolution imagery with agricultural geographic scene constraints",
      "paper_url": "http://arxiv.org/abs/2401.06252v1",
      "paper_abstract": "Real-time and accurate information on fine-grained changes in crop cultivation is of great significance for crop growth monitoring, yield prediction and agricultural structure adjustment. Aiming at the problems of serious spectral confusion in visible high-resolution unmanned aerial vehicle (UAV) images of different phases, interference of large complex background and salt-and-pepper noise by existing semantic change detection (SCD) algorithms, in order to effectively extract deep image features of crops and meet the demand of agricultural practical engineering applications, this paper designs and proposes an agricultural geographic scene and parcel-scale constrained SCD framework for crops (AGSPNet). AGSPNet framework contains three parts: agricultural geographic scene (AGS) division module, parcel edge extraction module and crop SCD module. Meanwhile, we produce and introduce an UAV image SCD dataset (CSCD) dedicated to agricultural monitoring, encompassing multiple semantic variation types of crops in complex geographical scene. We conduct comparative experiments and accuracy evaluations in two test areas of this dataset, and the results show that the crop SCD results of AGSPNet consistently outperform other deep learning SCD models in terms of quantity and quality, with the evaluation metrics F1-score, kappa, OA, and mIoU obtaining improvements of 0.038, 0.021, 0.011 and 0.062, respectively, on average over the sub-optimal method. The method proposed in this paper can clearly detect the fine-grained change information of crop types in complex scenes, which can provide scientific and technical support for smart agriculture monitoring and management, food policy formulation and food security assurance.",
      "paper_authors": [
        "Shaochun Li",
        "Yanjun Wang",
        "Hengfan Cai",
        "Lina Deng",
        "Yunhao Lin"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-01-11",
      "update_time": "2024-01-11",
      "comments": null,
      "repo_url": "#"
    },
    "2401.05157": {
      "paper_id": "2401.05157v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.05157v1",
      "paper_key": "2401.05157",
      "paper_title": "Toward distortion-aware change detection in realistic scenarios",
      "paper_url": "http://arxiv.org/abs/2401.05157v1",
      "paper_abstract": "In the conventional change detection (CD) pipeline, two manually registered and labeled remote sensing datasets serve as the input of the model for training and prediction. However, in realistic scenarios, data from different periods or sensors could fail to be aligned as a result of various coordinate systems. Geometric distortion caused by coordinate shifting remains a thorny issue for CD algorithms. In this paper, we propose a reusable self-supervised framework for bitemporal geometric distortion in CD tasks. The whole framework is composed of Pretext Representation Pre-training, Bitemporal Image Alignment, and Down-stream Decoder Fine-Tuning. With only single-stage pre-training, the key components of the framework can be reused for assistance in the bitemporal image alignment, while simultaneously enhancing the performance of the CD decoder. Experimental results in 2 large-scale realistic scenarios demonstrate that our proposed method can alleviate the bitemporal geometric distortion in CD tasks.",
      "paper_authors": [
        "Yitao Zhao",
        "Heng-Chao Li",
        "Nanqing Liu",
        "Rui Wang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-01-10",
      "update_time": "2024-01-10",
      "comments": null,
      "repo_url": "#"
    },
    "2401.05093": {
      "paper_id": "2401.05093v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.05093v1",
      "paper_key": "2401.05093",
      "paper_title": "SwiMDiff: Scene-wide Matching Contrastive Learning with Diffusion Constraint for Remote Sensing Image",
      "paper_url": "http://arxiv.org/abs/2401.05093v1",
      "paper_abstract": "With recent advancements in aerospace technology, the volume of unlabeled remote sensing image (RSI) data has increased dramatically. Effectively leveraging this data through self-supervised learning (SSL) is vital in the field of remote sensing. However, current methodologies, particularly contrastive learning (CL), a leading SSL method, encounter specific challenges in this domain. Firstly, CL often mistakenly identifies geographically adjacent samples with similar semantic content as negative pairs, leading to confusion during model training. Secondly, as an instance-level discriminative task, it tends to neglect the essential fine-grained features and complex details inherent in unstructured RSIs. To overcome these obstacles, we introduce SwiMDiff, a novel self-supervised pre-training framework designed for RSIs. SwiMDiff employs a scene-wide matching approach that effectively recalibrates labels to recognize data from the same scene as false negatives. This adjustment makes CL more applicable to the nuances of remote sensing. Additionally, SwiMDiff seamlessly integrates CL with a diffusion model. Through the implementation of pixel-level diffusion constraints, we enhance the encoder's ability to capture both the global semantic information and the fine-grained features of the images more comprehensively. Our proposed framework significantly enriches the information available for downstream tasks in remote sensing. Demonstrating exceptional performance in change detection and land-cover classification tasks, SwiMDiff proves its substantial utility and value in the field of remote sensing.",
      "paper_authors": [
        "Jiayuan Tian",
        "Jie Lei",
        "Jiaqing Zhang",
        "Weiying Xie",
        "Yunsong Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-01-10",
      "update_time": "2024-01-10",
      "comments": null,
      "repo_url": "#"
    },
    "2401.04614": {
      "paper_id": "2401.04614v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.04614v2",
      "paper_key": "2401.04614",
      "paper_title": "Generic Knowledge Boosted Pre-training For Remote Sensing Images",
      "paper_url": "http://arxiv.org/abs/2401.04614v2",
      "paper_abstract": "Deep learning models are essential for scene classification, change detection, land cover segmentation, and other remote sensing image understanding tasks. Most backbones of existing remote sensing deep learning models are typically initialized by pre-trained weights obtained from ImageNet pre-training (IMP). However, domain gaps exist between remote sensing images and natural images (e.g., ImageNet), making deep learning models initialized by pre-trained weights of IMP perform poorly for remote sensing image understanding. Although some pre-training methods are studied in the remote sensing community, current remote sensing pre-training methods face the problem of vague generalization by only using remote sensing images. In this paper, we propose a novel remote sensing pre-training framework, Generic Knowledge Boosted Remote Sensing Pre-training (GeRSP), to learn robust representations from remote sensing and natural images for remote sensing understanding tasks. GeRSP contains two pre-training branches: (1) A self-supervised pre-training branch is adopted to learn domain-related representations from unlabeled remote sensing images. (2) A supervised pre-training branch is integrated into GeRSP for general knowledge learning from labeled natural images. Moreover, GeRSP combines two pre-training branches using a teacher-student architecture to simultaneously learn representations with general and special knowledge, which generates a powerful pre-trained model for deep learning model initialization. Finally, we evaluate GeRSP and other remote sensing pre-training methods on three downstream tasks, i.e., object detection, semantic segmentation, and scene classification. The extensive experimental results consistently demonstrate that GeRSP can effectively learn robust representations in a unified manner, improving the performance of remote sensing downstream tasks.",
      "paper_authors": [
        "Ziyue Huang",
        "Mingming Zhang",
        "Yuan Gong",
        "Qingjie Liu",
        "Yunhong Wang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-01-09",
      "update_time": "2024-01-21",
      "comments": "14 pages, 6 figures",
      "repo_url": "https://github.com/floatingstarZ/GeRSP"
    },
    "2401.04330": {
      "paper_id": "2401.04330v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.04330v2",
      "paper_key": "2401.04330",
      "paper_title": "BD-MSA: Body decouple VHR Remote Sensing Image Change Detection method guided by multi-scale feature information aggregation",
      "paper_url": "http://arxiv.org/abs/2401.04330v2",
      "paper_abstract": "The purpose of remote sensing image change detection (RSCD) is to detect differences between bi-temporal images taken at the same place. Deep learning has been extensively used to RSCD tasks, yielding significant results in terms of result recognition. However, due to the shooting angle of the satellite, the impacts of thin clouds, and certain lighting conditions, the problem of fuzzy edges in the change region in some remote sensing photographs cannot be properly handled using current RSCD algorithms. To solve this issue, we proposed a Body Decouple Multi-Scale by fearure Aggregation change detection (BD-MSA), a novel model that collects both global and local feature map information in the channel and space dimensions of the feature map during the training and prediction phases. This approach allows us to successfully extract the change region's boundary information while also divorcing the change region's main body from its boundary. Numerous studies have shown that the assessment metrics and evaluation effects of the model described in this paper on the publicly available datasets DSIFN-CD, S2Looking and WHU-CD are the best when compared to other models.",
      "paper_authors": [
        "Yonghui Tan",
        "Xiaolong Li",
        "Yishu Chen",
        "Jinquan Ai"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-01-09",
      "update_time": "2024-03-03",
      "comments": null,
      "repo_url": "#"
    },
    "2401.01107": {
      "paper_id": "2401.01107v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2401.01107v2",
      "paper_key": "2401.01107",
      "paper_title": "CityPulse: Fine-Grained Assessment of Urban Change with Street View Time Series",
      "paper_url": "http://arxiv.org/abs/2401.01107v2",
      "paper_abstract": "Urban transformations have profound societal impact on both individuals and communities at large. Accurately assessing these shifts is essential for understanding their underlying causes and ensuring sustainable urban planning. Traditional measurements often encounter constraints in spatial and temporal granularity, failing to capture real-time physical changes. While street view imagery, capturing the heartbeat of urban spaces from a pedestrian point of view, can add as a high-definition, up-to-date, and on-the-ground visual proxy of urban change. We curate the largest street view time series dataset to date, and propose an end-to-end change detection model to effectively capture physical alterations in the built environment at scale. We demonstrate the effectiveness of our proposed method by benchmark comparisons with previous literature and implementing it at the city-wide level. Our approach has the potential to supplement existing dataset and serve as a fine-grained and accurate assessment of urban change.",
      "paper_authors": [
        "Tianyuan Huang",
        "Zejia Wu",
        "Jiajun Wu",
        "Jackelyn Hwang",
        "Ram Rajagopal"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-01-02",
      "update_time": "2024-01-03",
      "comments": "Accepted by AAAI 2024",
      "repo_url": "#"
    },
    "2312.17428": {
      "paper_id": "2312.17428v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.17428v2",
      "paper_key": "2312.17428",
      "paper_title": "ChangeNet: Multi-Temporal Asymmetric Change Detection Dataset",
      "paper_url": "http://arxiv.org/abs/2312.17428v2",
      "paper_abstract": "Change Detection (CD) has been attracting extensive interests with the availability of bi-temporal datasets. However, due to the huge cost of multi-temporal images acquisition and labeling, existing change detection datasets are small in quantity, short in temporal, and low in practicability. Therefore, a large-scale practical-oriented dataset covering wide temporal phases is urgently needed to facilitate the community. To this end, the ChangeNet dataset is presented especially for multi-temporal change detection, along with the new task of \"Asymmetric Change Detection\". Specifically, ChangeNet consists of 31,000 multi-temporal images pairs, a wide range of complex scenes from 100 cities, and 6 pixel-level annotated categories, which is far superior to all the existing change detection datasets including LEVIR-CD, WHU Building CD, etc.. In addition, ChangeNet contains amounts of real-world perspective distortions in different temporal phases on the same areas, which is able to promote the practical application of change detection algorithms. The ChangeNet dataset is suitable for both binary change detection (BCD) and semantic change detection (SCD) tasks. Accordingly, we benchmark the ChangeNet dataset on six BCD methods and two SCD methods, and extensive experiments demonstrate its challenges and great significance. The dataset is available at https://github.com/jankyee/ChangeNet.",
      "paper_authors": [
        "Deyi Ji",
        "Siqi Gao",
        "Mingyuan Tao",
        "Hongtao Lu",
        "Feng Zhao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-12-29",
      "update_time": "2024-04-12",
      "comments": "Accepted to ICASSP 2024 Oral/Lecture",
      "repo_url": "#"
    },
    "2312.16965": {
      "paper_id": "2312.16965v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.16965v1",
      "paper_key": "2312.16965",
      "paper_title": "Reinforcement-based Display-size Selection for Frugal Satellite Image Change Detection",
      "paper_url": "http://arxiv.org/abs/2312.16965v1",
      "paper_abstract": "We introduce a novel interactive satellite image change detection algorithm based on active learning. The proposed method is iterative and consists in frugally probing the user (oracle) about the labels of the most critical images, and according to the oracle's annotations, it updates change detection results. First, we consider a probabilistic framework which assigns to each unlabeled sample a relevance measure modeling how critical is that sample when training change detection functions. We obtain these relevance measures by minimizing an objective function mixing diversity, representativity and uncertainty. These criteria when combined allow exploring different data modes and also refining change detections. Then, we further explore the potential of this objective function, by considering a reinforcement learning approach that finds the best combination of diversity, representativity and uncertainty as well as display-sizes through active learning iterations, leading to better generalization as shown through experiments in interactive satellite image change detection.",
      "paper_authors": [
        "Hichem Sahbi"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-12-28",
      "update_time": "2023-12-28",
      "comments": null,
      "repo_url": "#"
    },
    "2312.16875": {
      "paper_id": "2312.16875v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.16875v2",
      "paper_key": "2312.16875",
      "paper_title": "Cellular forgetting, desensitisation, stress and aging in signalling networks. When do cells refuse to learn more?",
      "paper_url": "http://arxiv.org/abs/2312.16875v2",
      "paper_abstract": "Recent findings show that single, non-neuronal cells are also able to learn signalling responses developing cellular memory. In cellular learning nodes of signalling networks strengthen their interactions e.g. by the conformational memory of intrinsically disordered proteins, protein translocation, miRNAs, lncRNAs, chromatin memory and signalling cascades. This can be described by a generalized, unicellular Hebbian learning process, where those signalling connections, which participate in learning, become stronger. Here we review those scenarios, where cellular signalling is not only repeated in a few times (when learning occurs), but becomes too frequent, too large, or too complex and overloads the cell. This leads to desensitisation of signalling networks by decoupling signalling components, receptor internalization, and consequent downregulation. These molecular processes are examples of anti-Hebbian learning and forgetting of signalling networks. Stress can be perceived as signalling overload inducing the desensitisation of signalling pathways. Aging occurs by the summative effects of cumulative stress downregulating signalling. We propose that cellular learning desensitisation, stress and aging may be placed along the same axis of more and more intensive (prolonged or repeated) signalling. We discuss how cells might discriminate between repeated and unexpected signals, and highlight the Hebbian and anti-Hebbian mechanisms behind the fold-change detection in the NF-\\k{appa}B signalling pathway. We list drug design methods using Hebbian learning (such as chemically-induced proximity) and clinical treatment modalities inducing (cancer, drug allergies) desensitisation or avoiding drug-induced desensitisation. A better discrimination between cellular learning, desensitisation and stress may open novel directions in drug design, e.g., helping to overcome drug-resistance.",
      "paper_authors": [
        "Tamas Veres",
        "Mark Kerestely",
        "Borbala M. Kovacs",
        "David Keresztes",
        "Klara Schulc",
        "Erik Seitz",
        "Zsolt Vassy",
        "Daniel V. Veres",
        "Peter Csermely"
      ],
      "primary_category": "q-bio.MN",
      "publish_time": "2023-12-28",
      "update_time": "2024-02-20",
      "comments": "19 pages, 4 figures",
      "repo_url": "#"
    },
    "2312.16410": {
      "paper_id": "2312.16410v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.16410v2",
      "paper_key": "2312.16410",
      "paper_title": "Segment Change Model (SCM) for Unsupervised Change detection in VHR Remote Sensing Images: a Case Study of Buildings",
      "paper_url": "http://arxiv.org/abs/2312.16410v2",
      "paper_abstract": "The field of Remote Sensing (RS) widely employs Change Detection (CD) on very-high-resolution (VHR) images. A majority of extant deep-learning-based methods hinge on annotated samples to complete the CD process. Recently, the emergence of Vision Foundation Model (VFM) enables zero-shot predictions in particular vision tasks. In this work, we propose an unsupervised CD method named Segment Change Model (SCM), built upon the Segment Anything Model (SAM) and Contrastive Language-Image Pre-training (CLIP). Our method recalibrates features extracted at different scales and integrates them in a top-down manner to enhance discriminative change edges. We further design an innovative Piecewise Semantic Attention (PSA) scheme, which can offer semantic representation without training, thereby minimize pseudo change phenomenon. Through conducting experiments on two public datasets, the proposed SCM increases the mIoU from 46.09% to 53.67% on the LEVIR-CD dataset, and from 47.56% to 52.14% on the WHU-CD dataset. Our codes are available at https://github.com/StephenApX/UCD-SCM.",
      "paper_authors": [
        "Xiaoliang Tan",
        "Guanzhou Chen",
        "Tong Wang",
        "Jiaqi Wang",
        "Xiaodong Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-12-27",
      "update_time": "2024-09-06",
      "comments": "Published in: IGARSS 2024 - 2024 IEEE International Geoscience and\n  Remote Sensing Symposium",
      "repo_url": "https://github.com/stephenapx/ucd-scm"
    },
    "2312.15311": {
      "paper_id": "2312.15311v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.15311v2",
      "paper_key": "2312.15311",
      "paper_title": "Pixel-Level Change Detection Pseudo-Label Learning for Remote Sensing Change Captioning",
      "paper_url": "http://arxiv.org/abs/2312.15311v2",
      "paper_abstract": "The existing methods for Remote Sensing Image Change Captioning (RSICC) perform well in simple scenes but exhibit poorer performance in complex scenes. This limitation is primarily attributed to the model's constrained visual ability to distinguish and locate changes. Acknowledging the inherent correlation between change detection (CD) and RSICC tasks, we believe pixel-level CD is significant for describing the differences between images through language. Regrettably, the current RSICC dataset lacks readily available pixel-level CD labels. To address this deficiency, we leverage a model trained on existing CD datasets to derive CD pseudo-labels. We propose an innovative network with an auxiliary CD branch, supervised by pseudo-labels. Furthermore, a semantic fusion augment (SFA) module is proposed to fuse the feature information extracted by the CD branch, thereby facilitating the nuanced description of changes. Experiments demonstrate that our method achieves state-of-the-art performance and validate that learning pixel-level CD pseudo-labels significantly contributes to change captioning. Our code will be available at: https://github.com/Chen-Yang-Liu/Pix4Cap",
      "paper_authors": [
        "Chenyang Liu",
        "Keyan Chen",
        "Zipeng Qi",
        "Haotian Zhang",
        "Zhengxia Zou",
        "Zhenwei Shi"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-12-23",
      "update_time": "2024-05-21",
      "comments": null,
      "repo_url": "#"
    },
    "2312.16202": {
      "paper_id": "2312.16202v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.16202v1",
      "paper_key": "2312.16202",
      "paper_title": "Time Travelling Pixels: Bitemporal Features Integration with Foundation Model for Remote Sensing Image Change Detection",
      "paper_url": "http://arxiv.org/abs/2312.16202v1",
      "paper_abstract": "Change detection, a prominent research area in remote sensing, is pivotal in observing and analyzing surface transformations. Despite significant advancements achieved through deep learning-based methods, executing high-precision change detection in spatio-temporally complex remote sensing scenarios still presents a substantial challenge. The recent emergence of foundation models, with their powerful universality and generalization capabilities, offers potential solutions. However, bridging the gap of data and tasks remains a significant obstacle. In this paper, we introduce Time Travelling Pixels (TTP), a novel approach that integrates the latent knowledge of the SAM foundation model into change detection. This method effectively addresses the domain shift in general knowledge transfer and the challenge of expressing homogeneous and heterogeneous characteristics of multi-temporal images. The state-of-the-art results obtained on the LEVIR-CD underscore the efficacy of the TTP. The Code is available at \\url{https://kychen.me/TTP}.",
      "paper_authors": [
        "Keyan Chen",
        "Chengyang Liu",
        "Wenyuan Li",
        "Zili Liu",
        "Hao Chen",
        "Haotian Zhang",
        "Zhengxia Zou",
        "Zhenwei Shi"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-12-23",
      "update_time": "2023-12-23",
      "comments": null,
      "repo_url": "https://github.com/KyanChen/TTP"
    },
    "2312.15090": {
      "paper_id": "2312.15090v5",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.15090v5",
      "paper_key": "2312.15090",
      "paper_title": "Change Point Detection of Events in Molecular Simulations using dupin",
      "paper_url": "http://arxiv.org/abs/2312.15090v5",
      "paper_abstract": "Particle tracking is commonly used to study time-dependent behavior in many different types of physical and chemical systems involving constituents that span many length scales, including atoms, molecules, nanoparticles, granular particles, etc. Behaviors of interest studied using particle tracking information include disorder-order transitions, thermodynamic phase transitions, structural transitions, protein folding, crystallization, gelation, swarming, avalanches and fracture. A common challenge in studies of these systems involves change detection. Change point detection discerns when a temporal signal undergoes a change in distribution. These changes can be local or global, instantaneous or prolonged, obvious or subtle. Moreover, system-wide changes marking an interesting physical or chemical phenomenon (e.g. crystallization) are often preceded by events (e.g. pre-nucleation clusters) that are localized and can occur anywhere at anytime in the system. For these reasons, detecting events in particle trajectories generated by molecular simulation is challenging and typically accomplished via ad hoc solutions unique to the behavior and system under study. Consequently, methods for event detection lack generality, and those used in one field are not easily used by scientists in other fields. Here we present a new Python-based tool, dupin, that allows for event detection from particle trajectory data irrespective of the system details. dupin works by creating a signal representing the simulation and partitioning the signal based on events (changes within the trajectory). This approach allows for studies where manual annotating of event boundaries would require a prohibitive amount of time. Furthermore, dupin can serve as a tool in automated and reproducible workflows. We demonstrate the application of dupin using two examples and discuss its applicability to a wider class of problems.",
      "paper_authors": [
        "Brandon L. Butler",
        "Domagoj Fijan",
        "Sharon C. Glotzer"
      ],
      "primary_category": "physics.comp-ph",
      "publish_time": "2023-12-22",
      "update_time": "2024-08-05",
      "comments": null,
      "repo_url": "https://github.com/glotzerlab/dupin"
    },
    "2312.06002": {
      "paper_id": "2312.06002v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.06002v1",
      "paper_key": "2312.06002",
      "paper_title": "Large Language Models on Lexical Semantic Change Detection: An Evaluation",
      "paper_url": "http://arxiv.org/abs/2312.06002v1",
      "paper_abstract": "Lexical Semantic Change Detection stands out as one of the few areas where Large Language Models (LLMs) have not been extensively involved. Traditional methods like PPMI, and SGNS remain prevalent in research, alongside newer BERT-based approaches. Despite the comprehensive coverage of various natural language processing domains by LLMs, there is a notable scarcity of literature concerning their application in this specific realm. In this work, we seek to bridge this gap by introducing LLMs into the domain of Lexical Semantic Change Detection. Our work presents novel prompting solutions and a comprehensive evaluation that spans all three generations of language models, contributing to the exploration of LLMs in this research area.",
      "paper_authors": [
        "Ruiyu Wang",
        "Matthew Choi"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2023-12-10",
      "update_time": "2023-12-10",
      "comments": null,
      "repo_url": "#"
    },
    "2312.04869": {
      "paper_id": "2312.04869v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.04869v1",
      "paper_key": "2312.04869",
      "paper_title": "Adapting Vision Transformer for Efficient Change Detection",
      "paper_url": "http://arxiv.org/abs/2312.04869v1",
      "paper_abstract": "Most change detection models based on vision transformers currently follow a \"pretraining then fine-tuning\" strategy. This involves initializing the model weights using large scale classification datasets, which can be either natural images or remote sensing images. However, fully tuning such a model requires significant time and resources. In this paper, we propose an efficient tuning approach that involves freezing the parameters of the pretrained image encoder and introducing additional training parameters. Through this approach, we have achieved competitive or even better results while maintaining extremely low resource consumption across six change detection benchmarks. For example, training time on LEVIR-CD, a change detection benchmark, is only half an hour with 9 GB memory usage, which could be very convenient for most researchers. Additionally, the decoupled tuning framework can be extended to any pretrained model for semantic change detection and multi temporal change detection as well. We hope that our proposed approach will serve as a part of foundational model to inspire more unified training approaches on change detection in the future.",
      "paper_authors": [
        "Yang Zhao",
        "Yuxiang Zhang",
        "Yanni Dong",
        "Bo Du"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-12-08",
      "update_time": "2023-12-08",
      "comments": null,
      "repo_url": "#"
    },
    "2312.03176": {
      "paper_id": "2312.03176v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.03176v1",
      "paper_key": "2312.03176",
      "paper_title": "Active Learning for Abrupt Shifts Change-point Detection via Derivative-Aware Gaussian Processes",
      "paper_url": "http://arxiv.org/abs/2312.03176v1",
      "paper_abstract": "Change-point detection (CPD) is crucial for identifying abrupt shifts in data, which influence decision-making and efficient resource allocation across various domains. To address the challenges posed by the costly and time-intensive data acquisition in CPD, we introduce the Derivative-Aware Change Detection (DACD) method. It leverages the derivative process of a Gaussian process (GP) for Active Learning (AL), aiming to pinpoint change-point locations effectively. DACD balances the exploitation and exploration of derivative processes through multiple data acquisition functions (AFs). By utilizing GP derivative mean and variance as criteria, DACD sequentially selects the next sampling data point, thus enhancing algorithmic efficiency and ensuring reliable and accurate results. We investigate the effectiveness of DACD method in diverse scenarios and show it outperforms other active learning change-point detection approaches.",
      "paper_authors": [
        "Hao Zhao",
        "Rong Pan"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2023-12-05",
      "update_time": "2023-12-05",
      "comments": null,
      "repo_url": "#"
    },
    "2312.02807": {
      "paper_id": "2312.02807v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.02807v1",
      "paper_key": "2312.02807",
      "paper_title": "Online Change Detection in SAR Time-Series with Kronecker Product Structured Scaled Gaussian Models",
      "paper_url": "http://arxiv.org/abs/2312.02807v1",
      "paper_abstract": "We develop the information geometry of scaled Gaussian distributions for which the covariance matrix exhibits a Kronecker product structure. This model and its geometry are then used to propose an online change detection (CD) algorithm for multivariate image times series (MITS). The proposed approach relies mainly on the online estimation of the structured covariance matrix under the null hypothesis, which is performed through a recursive (natural) Riemannian gradient descent. This approach exhibits a practical interest compared to the corresponding offline version, as its computational cost remains constant for each new image added in the time series. Simulations show that the proposed recursive estimators reach the Intrinsic Cram\\'er-Rao bound. The interest of the proposed online CD approach is demonstrated on both simulated and real data.",
      "paper_authors": [
        "Ammar Mian",
        "Guillaume Ginolhac",
        "Florent Bouchard",
        "Arnaud Breloy"
      ],
      "primary_category": "stat.AP",
      "publish_time": "2023-12-05",
      "update_time": "2023-12-05",
      "comments": null,
      "repo_url": "#"
    },
    "2312.02751": {
      "paper_id": "2312.02751v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.02751v2",
      "paper_key": "2312.02751",
      "paper_title": "C-NERF: Representing Scene Changes as Directional Consistency Difference-based NeRF",
      "paper_url": "http://arxiv.org/abs/2312.02751v2",
      "paper_abstract": "In this work, we aim to detect the changes caused by object variations in a scene represented by the neural radiance fields (NeRFs). Given an arbitrary view and two sets of scene images captured at different timestamps, we can predict the scene changes in that view, which has significant potential applications in scene monitoring and measuring. We conducted preliminary studies and found that such an exciting task cannot be easily achieved by utilizing existing NeRFs and 2D change detection methods with many false or missing detections. The main reason is that the 2D change detection is based on the pixel appearance difference between spatial-aligned image pairs and neglects the stereo information in the NeRF. To address the limitations, we propose the C-NERF to represent scene changes as directional consistency difference-based NeRF, which mainly contains three modules. We first perform the spatial alignment of two NeRFs captured before and after changes. Then, we identify the change points based on the direction-consistent constraint; that is, real change points have similar change representations across view directions, but fake change points do not. Finally, we design the change map rendering process based on the built NeRFs and can generate the change map of an arbitrarily specified view direction. To validate the effectiveness, we build a new dataset containing ten scenes covering diverse scenarios with different changing objects. Our approach surpasses state-of-the-art 2D change detection and NeRF-based methods by a significant margin.",
      "paper_authors": [
        "Rui Huang",
        "Binbin Jiang",
        "Qingyi Zhao",
        "William Wang",
        "Yuxiang Zhang",
        "Qing Guo"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-12-05",
      "update_time": "2023-12-23",
      "comments": null,
      "repo_url": "https://github.com/c-nerf/c-nerf"
    },
    "2312.02545": {
      "paper_id": "2312.02545v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.02545v2",
      "paper_key": "2312.02545",
      "paper_title": "Graph Information Bottleneck for Remote Sensing Segmentation",
      "paper_url": "http://arxiv.org/abs/2312.02545v2",
      "paper_abstract": "Remote sensing segmentation has a wide range of applications in environmental protection, and urban change detection, etc. Despite the success of deep learning-based remote sensing segmentation methods (e.g., CNN and Transformer), they are not flexible enough to model irregular objects. In addition, existing graph contrastive learning methods usually adopt the way of maximizing mutual information to keep the node representations consistent between different graph views, which may cause the model to learn task-independent redundant information. To tackle the above problems, this paper treats images as graph structures and introduces a simple contrastive vision GNN (SC-ViG) architecture for remote sensing segmentation. Specifically, we construct a node-masked and edge-masked graph view to obtain an optimal graph structure representation, which can adaptively learn whether to mask nodes and edges. Furthermore, this paper innovatively introduces information bottleneck theory into graph contrastive learning to maximize task-related information while minimizing task-independent redundant information. Finally, we replace the convolutional module in UNet with the SC-ViG module to complete the segmentation and classification tasks of remote sensing images. Extensive experiments on publicly available real datasets demonstrate that our method outperforms state-of-the-art remote sensing image segmentation methods.",
      "paper_authors": [
        "Yuntao Shou",
        "Wei Ai",
        "Tao Meng",
        "Nan Yin"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-12-05",
      "update_time": "2024-08-31",
      "comments": "13 pages, 6 figures",
      "repo_url": "#"
    },
    "2312.02396": {
      "paper_id": "2312.02396v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.02396v3",
      "paper_key": "2312.02396",
      "paper_title": "Unsupervised Change Detection for Space Habitats Using 3D Point Clouds",
      "paper_url": "http://arxiv.org/abs/2312.02396v3",
      "paper_abstract": "This work presents an algorithm for scene change detection from point clouds to enable autonomous robotic caretaking in future space habitats. Autonomous robotic systems will help maintain future deep-space habitats, such as the Gateway space station, which will be uncrewed for extended periods. Existing scene analysis software used on the International Space Station (ISS) relies on manually-labeled images for detecting changes. In contrast, the algorithm presented in this work uses raw, unlabeled point clouds as inputs. The algorithm first applies modified Expectation-Maximization Gaussian Mixture Model (GMM) clustering to two input point clouds. It then performs change detection by comparing the GMMs using the Earth Mover's Distance. The algorithm is validated quantitatively and qualitatively using a test dataset collected by an Astrobee robot in the NASA Ames Granite Lab comprising single frame depth images taken directly by Astrobee and full-scene reconstructed maps built with RGB-D and pose data from Astrobee. The runtimes of the approach are also analyzed in depth. The source code is publicly released to promote further development.",
      "paper_authors": [
        "Jamie Santos",
        "Holly Dinkel",
        "Julia Di",
        "Paulo V. K. Borges",
        "Marina Moreira",
        "Oleg Alexandrov",
        "Brian Coltin",
        "Trey Smith"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2023-12-04",
      "update_time": "2024-08-05",
      "comments": "15 pages, 7 figures, Manuscript was presented at the AIAA SciTech\n  Forum in Orlando, FL, USA, 8 - 12 January 2024. Video presentation:\n  [https://www.youtube.com/watch?v=7WHp0dQYG4Y]. Code:\n  [https://github.com/nasa/isaac/tree/master/anomaly/gmm-change-detection]",
      "repo_url": "https://github.com/nasa/isaac"
    },
    "2312.01163": {
      "paper_id": "2312.01163v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.01163v2",
      "paper_key": "2312.01163",
      "paper_title": "A New Learning Paradigm for Foundation Model-based Remote Sensing Change Detection",
      "paper_url": "http://arxiv.org/abs/2312.01163v2",
      "paper_abstract": "Change detection (CD) is a critical task to observe and analyze dynamic processes of land cover. Although numerous deep learning-based CD models have performed excellently, their further performance improvements are constrained by the limited knowledge extracted from the given labelled data. On the other hand, the foundation models that emerged recently contain a huge amount of knowledge by scaling up across data modalities and proxy tasks. In this paper, we propose a Bi-Temporal Adapter Network (BAN), which is a universal foundation model-based CD adaptation framework aiming to extract the knowledge of foundation models for CD. The proposed BAN contains three parts, i.e. frozen foundation model (e.g., CLIP), bi-temporal adapter branch (Bi-TAB), and bridging modules between them. Specifically, BAN extracts general features through a frozen foundation model, which are then selected, aligned, and injected into Bi-TAB via the bridging modules. Bi-TAB is designed as a model-agnostic concept to extract task/domain-specific features, which can be either an existing arbitrary CD model or some hand-crafted stacked blocks. Beyond current customized models, BAN is the first extensive attempt to adapt the foundation model to the CD task. Experimental results show the effectiveness of our BAN in improving the performance of existing CD methods (e.g., up to 4.08\\% IoU improvement) with only a few additional learnable parameters. More importantly, these successful practices show us the potential of foundation models for remote sensing CD. The code is available at \\url{https://github.com/likyoo/BAN} and will be supported in our Open-CD.",
      "paper_authors": [
        "Kaiyu Li",
        "Xiangyong Cao",
        "Deyu Meng"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-12-02",
      "update_time": "2024-02-11",
      "comments": null,
      "repo_url": "https://github.com/likyoo/ban"
    },
    "2312.01148": {
      "paper_id": "2312.01148v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2312.01148v1",
      "paper_key": "2312.01148",
      "paper_title": "Has Anything Changed? 3D Change Detection by 2D Segmentation Masks",
      "paper_url": "http://arxiv.org/abs/2312.01148v1",
      "paper_abstract": "As capturing devices become common, 3D scans of interior spaces are acquired on a daily basis. Through scene comparison over time, information about objects in the scene and their changes is inferred. This information is important for robots and AR and VR devices, in order to operate in an immersive virtual experience. We thus propose an unsupervised object discovery method that identifies added, moved, or removed objects without any prior knowledge of what objects exist in the scene. We model this problem as a combination of a 3D change detection and a 2D segmentation task. Our algorithm leverages generic 2D segmentation masks to refine an initial but incomplete set of 3D change detections. The initial changes, acquired through render-and-compare likely correspond to movable objects. The incomplete detections are refined through graph optimization, distilling the information of the 2D segmentation masks in the 3D space. Experiments on the 3Rscan dataset prove that our method outperforms competitive baselines, with SoTA results.",
      "paper_authors": [
        "Aikaterini Adam",
        "Konstantinos Karantzalos",
        "Lazaros Grammatikopoulos",
        "Torsten Sattler"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-12-02",
      "update_time": "2023-12-02",
      "comments": null,
      "repo_url": "#"
    },
    "2311.18694": {
      "paper_id": "2311.18694v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.18694v2",
      "paper_key": "2311.18694",
      "paper_title": "Balancing Summarization and Change Detection in Graph Streams",
      "paper_url": "http://arxiv.org/abs/2311.18694v2",
      "paper_abstract": "This study addresses the issue of balancing graph summarization and graph change detection. Graph summarization compresses large-scale graphs into a smaller scale. However, the question remains: To what extent should the original graph be compressed? This problem is solved from the perspective of graph change detection, aiming to detect statistically significant changes using a stream of summary graphs. If the compression rate is extremely high, important changes can be ignored, whereas if the compression rate is extremely low, false alarms may increase with more memory. This implies that there is a trade-off between compression rate in graph summarization and accuracy in change detection. We propose a novel quantitative methodology to balance this trade-off to simultaneously realize reliable graph summarization and change detection. We introduce a probabilistic structure of hierarchical latent variable model into a graph, thereby designing a parameterized summary graph on the basis of the minimum description length principle. The parameter specifying the summary graph is then optimized so that the accuracy of change detection is guaranteed to suppress Type I error probability (probability of raising false alarms) to be less than a given confidence level. First, we provide a theoretical framework for connecting graph summarization with change detection. Then, we empirically demonstrate its effectiveness on synthetic and real datasets.",
      "paper_authors": [
        "Shintaro Fukushima",
        "Kenji Yamanishi"
      ],
      "primary_category": "stat.ML",
      "publish_time": "2023-11-30",
      "update_time": "2023-12-12",
      "comments": "6 pages, Accepted to 23rd IEEE International Conference on Data\n  Mining (ICDM2023)",
      "repo_url": "https://github.com/s-fuku/bsc"
    },
    "2311.15128": {
      "paper_id": "2311.15128v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.15128v1",
      "paper_key": "2311.15128",
      "paper_title": "Quickest Change Detection with Post-Change Density Estimation",
      "paper_url": "http://arxiv.org/abs/2311.15128v1",
      "paper_abstract": "The problem of quickest change detection in a sequence of independent observations is considered. The pre-change distribution is assumed to be known, while the post-change distribution is unknown. Two tests based on post-change density estimation are developed for this problem, the window-limited non-parametric generalized likelihood ratio (NGLR) CuSum test and the non-parametric window-limited adaptive (NWLA) CuSum test. Both tests do not assume any knowledge of the post-change distribution, except that the post-change density satisfies certain smoothness conditions that allows for efficient non-parametric estimation. Also, they do not require any pre-collected post-change training samples. Under certain convergence conditions on the density estimator, it is shown that both tests are first-order asymptotically optimal, as the false alarm rate goes to zero. The analysis is validated through numerical results, where both tests are compared with baseline tests that have distributional knowledge.",
      "paper_authors": [
        "Yuchen Liang",
        "Venugopal V. Veeravalli"
      ],
      "primary_category": "math.ST",
      "publish_time": "2023-11-25",
      "update_time": "2023-11-25",
      "comments": "arXiv admin note: text overlap with arXiv:2211.00223",
      "repo_url": "#"
    },
    "2311.12083": {
      "paper_id": "2311.12083v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.12083v1",
      "paper_key": "2311.12083",
      "paper_title": "PanBench: Towards High-Resolution and High-Performance Pansharpening",
      "paper_url": "http://arxiv.org/abs/2311.12083v1",
      "paper_abstract": "Pansharpening, a pivotal task in remote sensing, involves integrating low-resolution multispectral images with high-resolution panchromatic images to synthesize an image that is both high-resolution and retains multispectral information. These pansharpened images enhance precision in land cover classification, change detection, and environmental monitoring within remote sensing data analysis. While deep learning techniques have shown significant success in pansharpening, existing methods often face limitations in their evaluation, focusing on restricted satellite data sources, single scene types, and low-resolution images. This paper addresses this gap by introducing PanBench, a high-resolution multi-scene dataset containing all mainstream satellites and comprising 5,898 pairs of samples. Each pair includes a four-channel (RGB + near-infrared) multispectral image of 256x256 pixels and a mono-channel panchromatic image of 1,024x1,024 pixels. To achieve high-fidelity synthesis, we propose a Cascaded Multiscale Fusion Network (CMFNet) for Pansharpening. Extensive experiments validate the effectiveness of CMFNet. We have released the dataset, source code, and pre-trained models in the supplementary, fostering further research in remote sensing.",
      "paper_authors": [
        "Shiying Wang",
        "Xuechao Zou",
        "Kai Li",
        "Junliang Xing",
        "Pin Tao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-20",
      "update_time": "2023-11-20",
      "comments": "10 pages, 5 figures",
      "repo_url": "#"
    },
    "2311.11580": {
      "paper_id": "2311.11580v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.11580v1",
      "paper_key": "2311.11580",
      "paper_title": "SeaDSC: A video-based unsupervised method for dynamic scene change detection in unmanned surface vehicles",
      "paper_url": "http://arxiv.org/abs/2311.11580v1",
      "paper_abstract": "Recently, there has been an upsurge in the research on maritime vision, where a lot of works are influenced by the application of computer vision for Unmanned Surface Vehicles (USVs). Various sensor modalities such as camera, radar, and lidar have been used to perform tasks such as object detection, segmentation, object tracking, and motion planning. A large subset of this research is focused on the video analysis, since most of the current vessel fleets contain the camera's onboard for various surveillance tasks. Due to the vast abundance of the video data, video scene change detection is an initial and crucial stage for scene understanding of USVs. This paper outlines our approach to detect dynamic scene changes in USVs. To the best of our understanding, this work represents the first investigation of scene change detection in the maritime vision application. Our objective is to identify significant changes in the dynamic scenes of maritime video data, particularly those scenes that exhibit a high degree of resemblance. In our system for dynamic scene change detection, we propose completely unsupervised learning method. In contrast to earlier studies, we utilize a modified cutting-edge generative picture model called VQ-VAE-2 to train on multiple marine datasets, aiming to enhance the feature extraction. Next, we introduce our innovative similarity scoring technique for directly calculating the level of similarity in a sequence of consecutive frames by utilizing grid calculation on retrieved features. The experiments were conducted using a nautical video dataset called RoboWhaler to showcase the efficient performance of our technique.",
      "paper_authors": [
        "Linh Trinh",
        "Ali Anwar",
        "Siegfried Mercelis"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-20",
      "update_time": "2023-11-20",
      "comments": "WACV 2024 conference",
      "repo_url": "#"
    },
    "2311.11302": {
      "paper_id": "2311.11302v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.11302v1",
      "paper_key": "2311.11302",
      "paper_title": "Exchanging Dual Encoder-Decoder: A New Strategy for Change Detection with Semantic Guidance and Spatial Localization",
      "paper_url": "http://arxiv.org/abs/2311.11302v1",
      "paper_abstract": "Change detection is a critical task in earth observation applications. Recently, deep learning-based methods have shown promising performance and are quickly adopted in change detection. However, the widely used multiple encoder and single decoder (MESD) as well as dual encoder-decoder (DED) architectures still struggle to effectively handle change detection well. The former has problems of bitemporal feature interference in the feature-level fusion, while the latter is inapplicable to intraclass change detection and multiview building change detection. To solve these problems, we propose a new strategy with an exchanging dual encoder-decoder structure for binary change detection with semantic guidance and spatial localization. The proposed strategy solves the problems of bitemporal feature inference in MESD by fusing bitemporal features in the decision level and the inapplicability in DED by determining changed areas using bitemporal semantic features. We build a binary change detection model based on this strategy, and then validate and compare it with 18 state-of-the-art change detection methods on six datasets in three scenarios, including intraclass change detection datasets (CDD, SYSU), single-view building change detection datasets (WHU, LEVIR-CD, LEVIR-CD+) and a multiview building change detection dataset (NJDS). The experimental results demonstrate that our model achieves superior performance with high efficiency and outperforms all benchmark methods with F1-scores of 97.77%, 83.07%, 94.86%, 92.33%, 91.39%, 74.35% on CDD, SYSU, WHU, LEVIR-CD, LEVIR- CD+, and NJDS datasets, respectively. The code of this work will be available at https://github.com/NJU-LHRS/official-SGSLN.",
      "paper_authors": [
        "Sijie Zhao",
        "Xueliang Zhang",
        "Pengfeng Xiao",
        "Guangjun He"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-19",
      "update_time": "2023-11-19",
      "comments": null,
      "repo_url": "https://github.com/walking-shadow/Semantic-guidance-and-spatial-localization-network"
    },
    "2311.09726": {
      "paper_id": "2311.09726v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.09726v1",
      "paper_key": "2311.09726",
      "paper_title": "MS-Former: Memory-Supported Transformer for Weakly Supervised Change Detection with Patch-Level Annotations",
      "paper_url": "http://arxiv.org/abs/2311.09726v1",
      "paper_abstract": "Fully supervised change detection methods have achieved significant advancements in performance, yet they depend severely on acquiring costly pixel-level labels. Considering that the patch-level annotations also contain abundant information corresponding to both changed and unchanged objects in bi-temporal images, an intuitive solution is to segment the changes with patch-level annotations. How to capture the semantic variations associated with the changed and unchanged regions from the patch-level annotations to obtain promising change results is the critical challenge for the weakly supervised change detection task. In this paper, we propose a memory-supported transformer (MS-Former), a novel framework consisting of a bi-directional attention block (BAB) and a patch-level supervision scheme (PSS) tailored for weakly supervised change detection with patch-level annotations. More specifically, the BAM captures contexts associated with the changed and unchanged regions from the temporal difference features to construct informative prototypes stored in the memory bank. On the other hand, the BAM extracts useful information from the prototypes as supplementary contexts to enhance the temporal difference features, thereby better distinguishing changed and unchanged regions. After that, the PSS guides the network learning valuable knowledge from the patch-level annotations, thus further elevating the performance. Experimental results on three benchmark datasets demonstrate the effectiveness of our proposed method in the change detection task. The demo code for our work will be publicly available at \\url{https://github.com/guanyuezhen/MS-Former}.",
      "paper_authors": [
        "Zhenglai Li",
        "Chang Tang",
        "Xinwang Liu",
        "Changdong Li",
        "Xianju Li",
        "Wei Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-16",
      "update_time": "2023-11-16",
      "comments": "11 pages, 6 figures",
      "repo_url": "https://github.com/guanyuezhen/ms-former"
    },
    "2311.07993": {
      "paper_id": "2311.07993v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.07993v1",
      "paper_key": "2311.07993",
      "paper_title": "Explicit Change Relation Learning for Change Detection in VHR Remote Sensing Images",
      "paper_url": "http://arxiv.org/abs/2311.07993v1",
      "paper_abstract": "Change detection has always been a concerned task in the interpretation of remote sensing images. It is essentially a unique binary classification task with two inputs, and there is a change relationship between these two inputs. At present, the mining of change relationship features is usually implicit in the network architectures that contain single-branch or two-branch encoders. However, due to the lack of artificial prior design for change relationship features, these networks cannot learn enough change semantic information and lose more accurate change detection performance. So we propose a network architecture NAME for the explicit mining of change relation features. In our opinion, the change features of change detection should be divided into pre-changed image features, post-changed image features and change relation features. In order to fully mine these three kinds of change features, we propose the triple branch network combining the transformer and convolutional neural network (CNN) to extract and fuse these change features from two perspectives of global information and local information, respectively. In addition, we design the continuous change relation (CCR) branch to further obtain the continuous and detail change relation features to improve the change discrimination capability of the model. The experimental results show that our network performs better, in terms of F1, IoU, and OA, than those of the existing advanced networks for change detection on four public very high-resolution (VHR) remote sensing datasets. Our source code is available at https://github.com/DalongZ/NAME.",
      "paper_authors": [
        "Dalong Zheng",
        "Zebin Wu",
        "Jia Liu",
        "Chih-Cheng Hung",
        "Zhihui Wei"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-14",
      "update_time": "2023-11-14",
      "comments": null,
      "repo_url": "#"
    },
    "2311.07113": {
      "paper_id": "2311.07113v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.07113v3",
      "paper_key": "2311.07113",
      "paper_title": "SpectralGPT: Spectral Remote Sensing Foundation Model",
      "paper_url": "http://arxiv.org/abs/2311.07113v3",
      "paper_abstract": "The foundation model has recently garnered significant attention due to its potential to revolutionize the field of visual representation learning in a self-supervised manner. While most foundation models are tailored to effectively process RGB images for various visual tasks, there is a noticeable gap in research focused on spectral data, which offers valuable information for scene understanding, especially in remote sensing (RS) applications. To fill this gap, we created for the first time a universal RS foundation model, named SpectralGPT, which is purpose-built to handle spectral RS images using a novel 3D generative pretrained transformer (GPT). Compared to existing foundation models, SpectralGPT 1) accommodates input images with varying sizes, resolutions, time series, and regions in a progressive training fashion, enabling full utilization of extensive RS big data; 2) leverages 3D token generation for spatial-spectral coupling; 3) captures spectrally sequential patterns via multi-target reconstruction; 4) trains on one million spectral RS images, yielding models with over 600 million parameters. Our evaluation highlights significant performance improvements with pretrained SpectralGPT models, signifying substantial potential in advancing spectral RS big data applications within the field of geoscience across four downstream tasks: single/multi-label scene classification, semantic segmentation, and change detection.",
      "paper_authors": [
        "Danfeng Hong",
        "Bing Zhang",
        "Xuyang Li",
        "Yuxuan Li",
        "Chenyu Li",
        "Jing Yao",
        "Naoto Yokoya",
        "Hao Li",
        "Pedram Ghamisi",
        "Xiuping Jia",
        "Antonio Plaza",
        "Paolo Gamba",
        "Jon Atli Benediktsson",
        "Jocelyn Chanussot"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-13",
      "update_time": "2024-02-12",
      "comments": "Accepted by IEEE TPAMI",
      "repo_url": "#"
    },
    "2311.06222": {
      "paper_id": "2311.06222v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.06222v1",
      "paper_key": "2311.06222",
      "paper_title": "Diffusion Models for Earth Observation Use-cases: from cloud removal to urban change detection",
      "paper_url": "http://arxiv.org/abs/2311.06222v1",
      "paper_abstract": "The advancements in the state of the art of generative Artificial Intelligence (AI) brought by diffusion models can be highly beneficial in novel contexts involving Earth observation data. After introducing this new family of generative models, this work proposes and analyses three use cases which demonstrate the potential of diffusion-based approaches for satellite image data. Namely, we tackle cloud removal and inpainting, dataset generation for change-detection tasks, and urban replanning.",
      "paper_authors": [
        "Fulvio Sanguigni",
        "Mikolaj Czerkawski",
        "Lorenzo Papa",
        "Irene Amerini",
        "Bertrand Le Saux"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-10",
      "update_time": "2023-11-10",
      "comments": "Presented at Big Data from Space 2023 (BiDS)",
      "repo_url": "#"
    },
    "2311.04991": {
      "paper_id": "2311.04991v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.04991v1",
      "paper_key": "2311.04991",
      "paper_title": "Effective Restoration of Source Knowledge in Continual Test Time Adaptation",
      "paper_url": "http://arxiv.org/abs/2311.04991v1",
      "paper_abstract": "Traditional test-time adaptation (TTA) methods face significant challenges in adapting to dynamic environments characterized by continuously changing long-term target distributions. These challenges primarily stem from two factors: catastrophic forgetting of previously learned valuable source knowledge and gradual error accumulation caused by miscalibrated pseudo labels. To address these issues, this paper introduces an unsupervised domain change detection method that is capable of identifying domain shifts in dynamic environments and subsequently resets the model parameters to the original source pre-trained values. By restoring the knowledge from the source, it effectively corrects the negative consequences arising from the gradual deterioration of model parameters caused by ongoing shifts in the domain. Our method involves progressive estimation of global batch-norm statistics specific to each domain, while keeping track of changes in the statistics triggered by domain shifts. Importantly, our method is agnostic to the specific adaptation technique employed and thus, can be incorporated to existing TTA methods to enhance their performance in dynamic environments. We perform extensive experiments on benchmark datasets to demonstrate the superior performance of our method compared to state-of-the-art adaptation methods.",
      "paper_authors": [
        "Fahim Faisal Niloy",
        "Sk Miraj Ahmed",
        "Dripta S. Raychaudhuri",
        "Samet Oymak",
        "Amit K. Roy-Chowdhury"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2023-11-08",
      "update_time": "2023-11-08",
      "comments": "WACV 2024",
      "repo_url": "#"
    },
    "2311.03889": {
      "paper_id": "2311.03889v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.03889v1",
      "paper_key": "2311.03889",
      "paper_title": "Efficiently Detecting Performance Changes in FaaS Application Releases",
      "paper_url": "http://arxiv.org/abs/2311.03889v1",
      "paper_abstract": "The source code of Function as a Service (FaaS) applications is constantly being refined. To detect if a source code change introduces a significant performance regression, the traditional benchmarking approach evaluates both the old and new function version separately using numerous artificial requests.   In this paper, we describe a wrapper approach that enables the Randomized Multiple Interleaved Trials (RMIT) benchmark execution methodology in FaaS environments and use bootstrapping percentile intervals to derive more accurate confidence intervals of detected performance changes. We evaluate our approach using two public FaaS providers, an artificial performance issue, and several benchmark configuration parameters. We conclude that RMIT can shrink the width of confidence intervals in the results from 10.65% using the traditional approach to 0.37% using RMIT and thus enables a more fine-grained performance change detection.",
      "paper_authors": [
        "Martin Grambow",
        "Tim Dockenfu\u00df",
        "Trever Schirmer",
        "Nils Japke",
        "David Bermbach"
      ],
      "primary_category": "cs.DC",
      "publish_time": "2023-11-07",
      "update_time": "2023-11-07",
      "comments": "Accepted for publication in 9th International Workshop on Serverless\n  Computing (WoSC '23), ACM 2023",
      "repo_url": "#"
    },
    "2311.03762": {
      "paper_id": "2311.03762v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.03762v1",
      "paper_key": "2311.03762",
      "paper_title": "Image change detection with only a few samples",
      "paper_url": "http://arxiv.org/abs/2311.03762v1",
      "paper_abstract": "This paper considers image change detection with only a small number of samples, which is a significant problem in terms of a few annotations available. A major impediment of image change detection task is the lack of large annotated datasets covering a wide variety of scenes. Change detection models trained on insufficient datasets have shown poor generalization capability. To address the poor generalization issue, we propose using simple image processing methods for generating synthetic but informative datasets, and design an early fusion network based on object detection which could outperform the siamese neural network. Our key insight is that the synthetic data enables the trained model to have good generalization ability for various scenarios. We compare the model trained on the synthetic data with that on the real-world data captured from a challenging dataset, CDNet, using six different test sets. The results demonstrate that the synthetic data is informative enough to achieve higher generalization ability than the insufficient real-world data. Besides, the experiment shows that utilizing a few (often tens of) samples to fine-tune the model trained on the synthetic data will achieve excellent results.",
      "paper_authors": [
        "Ke Liu",
        "Zhaoyi Song",
        "Haoyue Bai"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-07",
      "update_time": "2023-11-07",
      "comments": "8 pages, 7 figures",
      "repo_url": "#"
    },
    "2311.03679": {
      "paper_id": "2311.03679v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.03679v1",
      "paper_key": "2311.03679",
      "paper_title": "Unsupervised convolutional neural network fusion approach for change detection in remote sensing images",
      "paper_url": "http://arxiv.org/abs/2311.03679v1",
      "paper_abstract": "With the rapid development of deep learning, a variety of change detection methods based on deep learning have emerged in recent years. However, these methods usually require a large number of training samples to train the network model, so it is very expensive. In this paper, we introduce a completely unsupervised shallow convolutional neural network (USCNN) fusion approach for change detection. Firstly, the bi-temporal images are transformed into different feature spaces by using convolution kernels of different sizes to extract multi-scale information of the images. Secondly, the output features of bi-temporal images at the same convolution kernels are subtracted to obtain the corresponding difference images, and the difference feature images at the same scale are fused into one feature image by using 1 * 1 convolution layer. Finally, the output features of different scales are concatenated and a 1 * 1 convolution layer is used to fuse the multi-scale information of the image. The model parameters are obtained by a redesigned sparse function. Our model has three features: the entire training process is conducted in an unsupervised manner, the network architecture is shallow, and the objective function is sparse. Thus, it can be seen as a kind of lightweight network model. Experimental results on four real remote sensing datasets indicate the feasibility and effectiveness of the proposed approach.",
      "paper_authors": [
        "Weidong Yan",
        "Pei Yan",
        "Li Cao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-07",
      "update_time": "2023-11-07",
      "comments": null,
      "repo_url": "#"
    },
    "2311.03339": {
      "paper_id": "2311.03339v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.03339v1",
      "paper_key": "2311.03339",
      "paper_title": "FLOGA: A machine learning ready dataset, a benchmark and a novel deep learning model for burnt area mapping with Sentinel-2",
      "paper_url": "http://arxiv.org/abs/2311.03339v1",
      "paper_abstract": "Over the last decade there has been an increasing frequency and intensity of wildfires across the globe, posing significant threats to human and animal lives, ecosystems, and socio-economic stability. Therefore urgent action is required to mitigate their devastating impact and safeguard Earth's natural resources. Robust Machine Learning methods combined with the abundance of high-resolution satellite imagery can provide accurate and timely mappings of the affected area in order to assess the scale of the event, identify the impacted assets and prioritize and allocate resources effectively for the proper restoration of the damaged region. In this work, we create and introduce a machine-learning ready dataset we name FLOGA (Forest wiLdfire Observations for the Greek Area). This dataset is unique as it comprises of satellite imagery acquired before and after a wildfire event, it contains information from Sentinel-2 and MODIS modalities with variable spatial and spectral resolution, and contains a large number of events where the corresponding burnt area ground truth has been annotated by domain experts. FLOGA covers the wider region of Greece, which is characterized by a Mediterranean landscape and climatic conditions. We use FLOGA to provide a thorough comparison of multiple Machine Learning and Deep Learning algorithms for the automatic extraction of burnt areas, approached as a change detection task. We also compare the results to those obtained using standard specialized spectral indices for burnt area mapping. Finally, we propose a novel Deep Learning model, namely BAM-CD. Our benchmark results demonstrate the efficacy of the proposed technique in the automatic extraction of burnt areas, outperforming all other methods in terms of accuracy and robustness. Our dataset and code are publicly available at: https://github.com/Orion-AI-Lab/FLOGA.",
      "paper_authors": [
        "Maria Sdraka",
        "Alkinoos Dimakos",
        "Alexandros Malounis",
        "Zisoula Ntasiou",
        "Konstantinos Karantzalos",
        "Dimitrios Michail",
        "Ioannis Papoutsis"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-06",
      "update_time": "2023-11-06",
      "comments": null,
      "repo_url": "https://github.com/orion-ai-lab/floga"
    },
    "2311.03124": {
      "paper_id": "2311.03124v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.03124v1",
      "paper_key": "2311.03124",
      "paper_title": "TAMPAR: Visual Tampering Detection for Parcel Logistics in Postal Supply Chains",
      "paper_url": "http://arxiv.org/abs/2311.03124v1",
      "paper_abstract": "Due to the steadily rising amount of valuable goods in supply chains, tampering detection for parcels is becoming increasingly important. In this work, we focus on the use-case last-mile delivery, where only a single RGB image is taken and compared against a reference from an existing database to detect potential appearance changes that indicate tampering. We propose a tampering detection pipeline that utilizes keypoint detection to identify the eight corner points of a parcel. This permits applying a perspective transformation to create normalized fronto-parallel views for each visible parcel side surface. These viewpoint-invariant parcel side surface representations facilitate the identification of signs of tampering on parcels within the supply chain, since they reduce the problem to parcel side surface matching with pair-wise appearance change detection. Experiments with multiple classical and deep learning-based change detection approaches are performed on our newly collected TAMpering detection dataset for PARcels, called TAMPAR. We evaluate keypoint and change detection separately, as well as in a unified system for tampering detection. Our evaluation shows promising results for keypoint (Keypoint AP 75.76) and tampering detection (81% accuracy, F1-Score 0.83) on real images. Furthermore, a sensitivity analysis for tampering types, lens distortion and viewing angles is presented. Code and dataset are available at https://a-nau.github.io/tampar.",
      "paper_authors": [
        "Alexander Naumann",
        "Felix Hertlein",
        "Laura D\u00f6rr",
        "Kai Furmans"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-06",
      "update_time": "2023-11-06",
      "comments": "Accepted at WACV 2024",
      "repo_url": "https://github.com/a-nau/tampar"
    },
    "2311.02558": {
      "paper_id": "2311.02558v4",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.02558v4",
      "paper_key": "2311.02558",
      "paper_title": "Multi-Agent 3D Map Reconstruction and Change Detection in Microgravity with Free-Flying Robots",
      "paper_url": "http://arxiv.org/abs/2311.02558v4",
      "paper_abstract": "Assistive free-flyer robots autonomously caring for future crewed outposts -- such as NASA's Astrobee robots on the International Space Station (ISS) -- must be able to detect day-to-day interior changes to track inventory, detect and diagnose faults, and monitor the outpost status. This work presents a framework for multi-agent cooperative mapping and change detection to enable robotic maintenance of space outposts. One agent is used to reconstruct a 3D model of the environment from sequences of images and corresponding depth information. Another agent is used to periodically scan the environment for inconsistencies against the 3D model. Change detection is validated after completing the surveys using real image and pose data collected by Astrobee robots in a ground testing environment and from microgravity aboard the ISS. This work outlines the objectives, requirements, and algorithmic modules for the multi-agent reconstruction system, including recommendations for its use by assistive free-flyers aboard future microgravity outposts.",
      "paper_authors": [
        "Holly Dinkel",
        "Julia Di",
        "Jamie Santos",
        "Keenan Albee",
        "Paulo Borges",
        "Marina Moreira",
        "Oleg Alexandrov",
        "Brian Coltin",
        "Trey Smith"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2023-11-05",
      "update_time": "2024-09-14",
      "comments": "11 pages, 8 figures, Manuscript presented at the 74th International\n  Astronautical Congress, IAC 2023, Baku, Azerbaijan, 2 - 6 October 2023. Video\n  presentation: [https://www.youtube.com/watch?v=VfjV-zwFEtU]. Code:\n  [https://github.com/hollydinkel/astrobeecd]",
      "repo_url": "https://github.com/hollydinkel/astrobeecd"
    },
    "2311.01659": {
      "paper_id": "2311.01659v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.01659v1",
      "paper_key": "2311.01659",
      "paper_title": "Efficient Cloud Pipelines for Neural Radiance Fields",
      "paper_url": "http://arxiv.org/abs/2311.01659v1",
      "paper_abstract": "Since their introduction in 2020, Neural Radiance Fields (NeRFs) have taken the computer vision community by storm. They provide a multi-view representation of a scene or object that is ideal for eXtended Reality (XR) applications and for creative endeavors such as virtual production, as well as change detection operations in geospatial analytics. The computational cost of these generative AI models is quite high, however, and the construction of cloud pipelines to generate NeRFs is neccesary to realize their potential in client applications. In this paper, we present pipelines on a high performance academic computing cluster and compare it with a pipeline implemented on Microsoft Azure. Along the way, we describe some uses of NeRFs in enabling novel user interaction scenarios.",
      "paper_authors": [
        "Derek Jacoby",
        "Donglin Xu",
        "Weder Ribas",
        "Minyi Xu",
        "Ting Liu",
        "Vishwanath Jayaraman",
        "Mengdi Wei",
        "Emma De Blois",
        "Yvonne Coady"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-11-03",
      "update_time": "2023-11-03",
      "comments": null,
      "repo_url": "#"
    },
    "2310.19951": {
      "paper_id": "2310.19951v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.19951v2",
      "paper_key": "2310.19951",
      "paper_title": "Measuring Behavior Change with Observational Studies: a Review",
      "paper_url": "http://arxiv.org/abs/2310.19951v2",
      "paper_abstract": "Exploring behavioral change in the digital age is imperative for societal progress in the context of 21st-century challenges. We analyzed 148 articles (2000-2023) and built a map that categorizes behaviors and change detection methodologies, platforms of reference, and theoretical frameworks that characterize online behavior change. Our findings uncover a focus on sentiment shifts, an emphasis on API-restricted platforms, and limited theory integration. We call for methodologies able to capture a wider range of behavioral types, diverse data sources, and stronger theory-practice alignment in the study of online behavioral change.",
      "paper_authors": [
        "Arianna Pera",
        "Gianmarco de Francisci Morales",
        "Luca Maria Aiello"
      ],
      "primary_category": "cs.CY",
      "publish_time": "2023-10-30",
      "update_time": "2023-11-02",
      "comments": null,
      "repo_url": "#"
    },
    "2310.17223": {
      "paper_id": "2310.17223v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.17223v1",
      "paper_key": "2310.17223",
      "paper_title": "Quickest Change Detection with Controlled Sensing",
      "paper_url": "http://arxiv.org/abs/2310.17223v1",
      "paper_abstract": "In the problem of quickest change detection, a change occurs at some unknown time in the distribution of a sequence of random vectors that are monitored in real time, and the goal is to detect this change as quickly as possible subject to a certain false alarm constraint. In this work we consider this problem in the presence of parametric uncertainty in the post-change regime and controlled sensing. That is, the post-change distribution contains an unknown parameter, and the distribution of each observation, before and after the change, is affected by a control action. In this context, in addition to a stopping rule that determines the time at which it is declared that the change has occurred, one also needs to determine a sequential control policy, which chooses the control action at each time based on the already collected observations. We formulate this problem mathematically using Lorden's minimax criterion, and assuming that there are finitely many possible actions and post-change parameter values. We then propose a specific procedure for this problem that employs an adaptive CuSum statistic in which (i) the estimate of the parameter is based on a fixed number of the more recent observations, and (ii) each action is selected to maximize the Kullback-Leibler divergence of the next observation based on the current parameter estimate, apart from a small number of exploration times. We show that this procedure, which we call the Windowed Chernoff-CuSum (WCC), is first-order asymptotically optimal under Lorden's minimax criterion, for every possible possible value of the unknown post-change parameter, as the mean time to false alarm goes to infinity. We also provide simulation results to illustrate the performance of the WCC procedure.",
      "paper_authors": [
        "Venugopal V. Veeravalli",
        "Georgios Fellouris",
        "George V. Moustakides"
      ],
      "primary_category": "cs.IT",
      "publish_time": "2023-10-26",
      "update_time": "2023-10-26",
      "comments": null,
      "repo_url": "#"
    },
    "2310.14214": {
      "paper_id": "2310.14214v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.14214v1",
      "paper_key": "2310.14214",
      "paper_title": "TransY-Net:Learning Fully Transformer Networks for Change Detection of Remote Sensing Images",
      "paper_url": "http://arxiv.org/abs/2310.14214v1",
      "paper_abstract": "In the remote sensing field, Change Detection (CD) aims to identify and localize the changed regions from dual-phase images over the same places. Recently, it has achieved great progress with the advances of deep learning. However, current methods generally deliver incomplete CD regions and irregular CD boundaries due to the limited representation ability of the extracted visual features. To relieve these issues, in this work we propose a novel Transformer-based learning framework named TransY-Net for remote sensing image CD, which improves the feature extraction from a global view and combines multi-level visual features in a pyramid manner. More specifically, the proposed framework first utilizes the advantages of Transformers in long-range dependency modeling. It can help to learn more discriminative global-level features and obtain complete CD regions. Then, we introduce a novel pyramid structure to aggregate multi-level visual features from Transformers for feature enhancement. The pyramid structure grafted with a Progressive Attention Module (PAM) can improve the feature representation ability with additional inter-dependencies through spatial and channel attentions. Finally, to better train the whole framework, we utilize the deeply-supervised learning with multiple boundary-aware loss functions. Extensive experiments demonstrate that our proposed method achieves a new state-of-the-art performance on four optical and two SAR image CD benchmarks. The source code is released at https://github.com/Drchip61/TransYNet.",
      "paper_authors": [
        "Tianyu Yan",
        "Zifu Wan",
        "Pingping Zhang",
        "Gong Cheng",
        "Huchuan Lu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-10-22",
      "update_time": "2023-10-22",
      "comments": "This work is accepted by TGRS2023. It is an extension of our ACCV2022\n  paper and arXiv:2210.00757",
      "repo_url": "https://github.com/drchip61/transynet"
    },
    "2310.13627": {
      "paper_id": "2310.13627v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.13627v1",
      "paper_key": "2310.13627",
      "paper_title": "Deep-Learning-based Change Detection with Spaceborne Hyperspectral PRISMA data",
      "paper_url": "http://arxiv.org/abs/2310.13627v1",
      "paper_abstract": "Change detection (CD) methods have been applied to optical data for decades, while the use of hyperspectral data with a fine spectral resolution has been rarely explored. CD is applied in several sectors, such as environmental monitoring and disaster management. Thanks to the PRecursore IperSpettrale della Missione operativA (PRISMA), hyperspectral-from-space CD is now possible. In this work, we apply standard and deep-learning (DL) CD methods to different targets, from natural to urban areas. We propose a pipeline starting from coregistration, followed by CD with a full-spectrum algorithm and by a DL network developed for optical data. We find that changes in vegetation and built environments are well captured. The spectral information is valuable to identify subtle changes and the DL methods are less affected by noise compared to the statistical method, but atmospheric effects and the lack of reliable ground truth represent a major challenge to hyperspectral CD.",
      "paper_authors": [
        "J. F. Amieva",
        "A. Austoni",
        "M. A. Brovelli",
        "L. Ansalone",
        "P. Naylor",
        "F. Serva",
        "B. Le Saux"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-10-20",
      "update_time": "2023-10-20",
      "comments": "Accepted at Big Data from Space 2023 (BiDS); 4 pages, 4 figures",
      "repo_url": "#"
    },
    "2310.11417": {
      "paper_id": "2310.11417v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.11417v1",
      "paper_key": "2310.11417",
      "paper_title": "VcT: Visual change Transformer for Remote Sensing Image Change Detection",
      "paper_url": "http://arxiv.org/abs/2310.11417v1",
      "paper_abstract": "Existing visual change detectors usually adopt CNNs or Transformers for feature representation learning and focus on learning effective representation for the changed regions between images. Although good performance can be obtained by enhancing the features of the change regions, however, these works are still limited mainly due to the ignorance of mining the unchanged background context information. It is known that one main challenge for change detection is how to obtain the consistent representations for two images involving different variations, such as spatial variation, sunlight intensity, etc. In this work, we demonstrate that carefully mining the common background information provides an important cue to learn the consistent representations for the two images which thus obviously facilitates the visual change detection problem. Based on this observation, we propose a novel Visual change Transformer (VcT) model for visual change detection problem. To be specific, a shared backbone network is first used to extract the feature maps for the given image pair. Then, each pixel of feature map is regarded as a graph node and the graph neural network is proposed to model the structured information for coarse change map prediction. Top-K reliable tokens can be mined from the map and refined by using the clustering algorithm. Then, these reliable tokens are enhanced by first utilizing self/cross-attention schemes and then interacting with original features via an anchor-primary attention learning module. Finally, the prediction head is proposed to get a more accurate change map. Extensive experiments on multiple benchmark datasets validated the effectiveness of our proposed VcT model.",
      "paper_authors": [
        "Bo Jiang",
        "Zitian Wang",
        "Xixi Wang",
        "Ziyan Zhang",
        "Lan Chen",
        "Xiao Wang",
        "Bin Luo"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-10-17",
      "update_time": "2023-10-17",
      "comments": "Accepted by IEEE Transactions on Geoscience and Remote Sensing (TGRS)\n  2023",
      "repo_url": "https://github.com/event-ahu/vct_remote_sensing_change_detection"
    },
    "2310.10400": {
      "paper_id": "2310.10400v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.10400v1",
      "paper_key": "2310.10400",
      "paper_title": "Can Word Sense Distribution Detect Semantic Changes of Words?",
      "paper_url": "http://arxiv.org/abs/2310.10400v1",
      "paper_abstract": "Semantic Change Detection (SCD) of words is an important task for various NLP applications that must make time-sensitive predictions. Some words are used over time in novel ways to express new meanings, and these new meanings establish themselves as novel senses of existing words. On the other hand, Word Sense Disambiguation (WSD) methods associate ambiguous words with sense ids, depending on the context in which they occur. Given this relationship between WSD and SCD, we explore the possibility of predicting whether a target word has its meaning changed between two corpora collected at different time steps, by comparing the distributions of senses of that word in each corpora. For this purpose, we use pretrained static sense embeddings to automatically annotate each occurrence of the target word in a corpus with a sense id. Next, we compute the distribution of sense ids of a target word in a given corpus. Finally, we use different divergence or distance measures to quantify the semantic change of the target word across the two given corpora. Our experimental results on SemEval 2020 Task 1 dataset show that word sense distributions can be accurately used to predict semantic changes of words in English, German, Swedish and Latin.",
      "paper_authors": [
        "Xiaohang Tang",
        "Yi Zhou",
        "Taichi Aida",
        "Procheta Sen",
        "Danushka Bollegala"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2023-10-16",
      "update_time": "2023-10-16",
      "comments": "Accepted to Findings of EMNLP 2023",
      "repo_url": "https://github.com/LivNLP/Sense-based-Semantic-Change-Prediction"
    },
    "2310.10166": {
      "paper_id": "2310.10166v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.10166v1",
      "paper_key": "2310.10166",
      "paper_title": "The Road to On-board Change Detection: A Lightweight Patch-Level Change Detection Network via Exploring the Potential of Pruning and Pooling",
      "paper_url": "http://arxiv.org/abs/2310.10166v1",
      "paper_abstract": "Existing satellite remote sensing change detection (CD) methods often crop original large-scale bi-temporal image pairs into small patch pairs and then use pixel-level CD methods to fairly process all the patch pairs. However, due to the sparsity of change in large-scale satellite remote sensing images, existing pixel-level CD methods suffer from a waste of computational cost and memory resources on lots of unchanged areas, which reduces the processing efficiency of on-board platform with extremely limited computation and memory resources. To address this issue, we propose a lightweight patch-level CD network (LPCDNet) to rapidly remove lots of unchanged patch pairs in large-scale bi-temporal image pairs. This is helpful to accelerate the subsequent pixel-level CD processing stage and reduce its memory costs. In our LPCDNet, a sensitivity-guided channel pruning method is proposed to remove unimportant channels and construct the lightweight backbone network on basis of ResNet18 network. Then, the multi-layer feature compression (MLFC) module is designed to compress and fuse the multi-level feature information of bi-temporal image patch. The output of MLFC module is fed into the fully-connected decision network to generate the predicted binary label. Finally, a weighted cross-entropy loss is utilized in the training process of network to tackle the change/unchange class imbalance problem. Experiments on two CD datasets demonstrate that our LPCDNet achieves more than 1000 frames per second on an edge computation platform, i.e., NVIDIA Jetson AGX Orin, which is more than 3 times that of the existing methods without noticeable CD performance loss. In addition, our method reduces more than 60% memory costs of the subsequent pixel-level CD processing stage.",
      "paper_authors": [
        "Lihui Xue",
        "Zhihao Wang",
        "Xueqian Wang",
        "Gang Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-10-16",
      "update_time": "2023-10-16",
      "comments": null,
      "repo_url": "#"
    },
    "2310.09759": {
      "paper_id": "2310.09759v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.09759v2",
      "paper_key": "2310.09759",
      "paper_title": "Prototype-oriented Unsupervised Change Detection for Disaster Management",
      "paper_url": "http://arxiv.org/abs/2310.09759v2",
      "paper_abstract": "Climate change has led to an increased frequency of natural disasters such as floods and cyclones. This emphasizes the importance of effective disaster monitoring. In response, the remote sensing community has explored change detection methods. These methods are primarily categorized into supervised techniques, which yield precise results but come with high labeling costs, and unsupervised techniques, which eliminate the need for labeling but involve intricate hyperparameter tuning. To address these challenges, we propose a novel unsupervised change detection method named Prototype-oriented Unsupervised Change Detection for Disaster Management (PUCD). PUCD captures changes by comparing features from pre-event, post-event, and prototype-oriented change synthesis images via a foundational model, and refines results using the Segment Anything Model (SAM). Although PUCD is an unsupervised change detection, it does not require complex hyperparameter tuning. We evaluate PUCD framework on the LEVIR-Extension dataset and the disaster dataset and it achieves state-of-the-art performance compared to other methods on the LEVIR-Extension dataset.",
      "paper_authors": [
        "Youngtack Oh",
        "Minseok Seo",
        "Doyi Kim",
        "Junghoon Seo"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-10-15",
      "update_time": "2023-10-17",
      "comments": "4page, 2 figures",
      "repo_url": "#"
    },
    "2310.09673": {
      "paper_id": "2310.09673v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.09673v1",
      "paper_key": "2310.09673",
      "paper_title": "Robust Quickest Change Detection in Non-Stationary Processes",
      "paper_url": "http://arxiv.org/abs/2310.09673v1",
      "paper_abstract": "Optimal algorithms are developed for robust detection of changes in non-stationary processes. These are processes in which the distribution of the data after change varies with time. The decision-maker does not have access to precise information on the post-change distribution. It is shown that if the post-change non-stationary family has a distribution that is least favorable in a well-defined sense, then the algorithms designed using the least favorable distributions are robust and optimal. Non-stationary processes are encountered in public health monitoring and space and military applications. The robust algorithms are applied to real and simulated data to show their effectiveness.",
      "paper_authors": [
        "Yingze Hou",
        "Yousef Oleyaeimotlagh",
        "Rahul Mishra",
        "Hoda Bidkhori",
        "Taposh Banerjee"
      ],
      "primary_category": "stat.ME",
      "publish_time": "2023-10-14",
      "update_time": "2023-10-14",
      "comments": null,
      "repo_url": "#"
    },
    "2310.09276": {
      "paper_id": "2310.09276v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.09276v3",
      "paper_key": "2310.09276",
      "paper_title": "Transformer-based Multimodal Change Detection with Multitask Consistency Constraints",
      "paper_url": "http://arxiv.org/abs/2310.09276v3",
      "paper_abstract": "Change detection plays a fundamental role in Earth observation for analyzing temporal iterations over time. However, recent studies have largely neglected the utilization of multimodal data that presents significant practical and technical advantages compared to single-modal approaches. This research focuses on leveraging {pre-event} digital surface model (DSM) data and {post-event} digital aerial images captured at different times for detecting change beyond 2D. We observe that the current change detection methods struggle with the multitask conflicts between semantic and height change detection tasks. To address this challenge, we propose an efficient Transformer-based network that learns shared representation between cross-dimensional inputs through cross-attention. {It adopts a consistency constraint to establish the multimodal relationship. Initially, pseudo-changes are derived by employing height change thresholding. Subsequently, the $L2$ distance between semantic and pseudo-changes within their overlapping regions is minimized. This explicitly endows the height change detection (regression task) and semantic change detection (classification task) with representation consistency.} A DSM-to-image multimodal dataset encompassing three cities in the Netherlands was constructed. It lays a new foundation for beyond-2D change detection from cross-dimensional inputs. Compared to five state-of-the-art change detection methods, our model demonstrates consistent multitask superiority in terms of semantic and height change detection. Furthermore, the consistency strategy can be seamlessly adapted to the other methods, yielding promising improvements.",
      "paper_authors": [
        "Biyuan Liu",
        "Huaixin Chen",
        "Kun Li",
        "Michael Ying Yang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-10-13",
      "update_time": "2024-04-17",
      "comments": null,
      "repo_url": "https://github.com/qaz670756/mmcd"
    },
    "2310.08789": {
      "paper_id": "2310.08789v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.08789v1",
      "paper_key": "2310.08789",
      "paper_title": "Quickest Change Detection in Autoregressive Models",
      "paper_url": "http://arxiv.org/abs/2310.08789v1",
      "paper_abstract": "The problem of quickest change detection (QCD) in autoregressive (AR) models is investigated. A system is being monitored with sequentially observed samples. At some unknown time, a disturbance signal occurs and changes the distribution of the observations. The disturbance signal follows an AR model, which is dependent over time. Before the change, observations only consist of measurement noise, and are independent and identically distributed (i.i.d.). After the change, observations consist of the disturbance signal and the measurement noise, are dependent over time, which essentially follow a continuous-state hidden Markov model (HMM). The goal is to design a stopping time to detect the disturbance signal as quickly as possible subject to false alarm constraints. Existing approaches for general non-i.i.d. settings and discrete-state HMMs cannot be applied due to their high computational complexity and memory consumption, and they usually assume some asymptotic stability condition. In this paper, the asymptotic stability condition is firstly theoretically proved for the AR model by a novel design of forward variable and auxiliary Markov chain. A computationally efficient Ergodic CuSum algorithm that can be updated recursively is then constructed and is further shown to be asymptotically optimal. The data-driven setting where the disturbance signal parameters are unknown is further investigated, and an online and computationally efficient gradient ascent CuSum algorithm is designed. The algorithm is constructed by iteratively updating the estimate of the unknown parameters based on the maximum likelihood principle and the gradient ascent approach. The lower bound on its average running length to false alarm is also derived for practical false alarm control. Simulation results are provided to demonstrate the performance of the proposed algorithms.",
      "paper_authors": [
        "Zhongchang Sun",
        "Shaofeng Zou"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2023-10-13",
      "update_time": "2023-10-13",
      "comments": null,
      "repo_url": "#"
    },
    "2310.08671": {
      "paper_id": "2310.08671v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.08671v1",
      "paper_key": "2310.08671",
      "paper_title": "SSG2: A new modelling paradigm for semantic segmentation",
      "paper_url": "http://arxiv.org/abs/2310.08671v1",
      "paper_abstract": "State-of-the-art models in semantic segmentation primarily operate on single, static images, generating corresponding segmentation masks. This one-shot approach leaves little room for error correction, as the models lack the capability to integrate multiple observations for enhanced accuracy. Inspired by work on semantic change detection, we address this limitation by introducing a methodology that leverages a sequence of observables generated for each static input image. By adding this \"temporal\" dimension, we exploit strong signal correlations between successive observations in the sequence to reduce error rates. Our framework, dubbed SSG2 (Semantic Segmentation Generation 2), employs a dual-encoder, single-decoder base network augmented with a sequence model. The base model learns to predict the set intersection, union, and difference of labels from dual-input images. Given a fixed target input image and a set of support images, the sequence model builds the predicted mask of the target by synthesizing the partial views from each sequence step and filtering out noise. We evaluate SSG2 across three diverse datasets: UrbanMonitor, featuring orthoimage tiles from Darwin, Australia with five spectral bands and 0.2m spatial resolution; ISPRS Potsdam, which includes true orthophoto images with multiple spectral bands and a 5cm ground sampling distance; and ISIC2018, a medical dataset focused on skin lesion segmentation, particularly melanoma. The SSG2 model demonstrates rapid convergence within the first few tens of epochs and significantly outperforms UNet-like baseline models with the same number of gradient updates. However, the addition of the temporal dimension results in an increased memory footprint. While this could be a limitation, it is offset by the advent of higher-memory GPUs and coding optimizations.",
      "paper_authors": [
        "Foivos I. Diakogiannis",
        "Suzanne Furby",
        "Peter Caccetta",
        "Xiaoliang Wu",
        "Rodrigo Ibata",
        "Ondrej Hlinka",
        "John Taylor"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-10-12",
      "update_time": "2023-10-12",
      "comments": "19 pages, Under review",
      "repo_url": "https://github.com/feevos/ssg2"
    },
    "2310.07886": {
      "paper_id": "2310.07886v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.07886v1",
      "paper_key": "2310.07886",
      "paper_title": "A Survey of Feature Types and Their Contributions for Camera Tampering Detection",
      "paper_url": "http://arxiv.org/abs/2310.07886v1",
      "paper_abstract": "Camera tamper detection is the ability to detect unauthorized and unintentional alterations in surveillance cameras by analyzing the video. Camera tampering can occur due to natural events or it can be caused intentionally to disrupt surveillance. We cast tampering detection as a change detection problem, and perform a review of the existing literature with emphasis on feature types. We formulate tampering detection as a time series analysis problem, and design experiments to study the robustness and capability of various feature types. We compute ten features on real-world surveillance video and apply time series analysis to ascertain their predictability, and their capability to detect tampering. Finally, we quantify the performance of various time series models using each feature type to detect tampering.",
      "paper_authors": [
        "Pranav Mantini",
        "Shishir K. Shah"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-10-11",
      "update_time": "2023-10-11",
      "comments": null,
      "repo_url": "#"
    },
    "2310.03891": {
      "paper_id": "2310.03891v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.03891v1",
      "paper_key": "2310.03891",
      "paper_title": "HDNA: A graph-based change detection in HTML pages(Deface Attack Detection)",
      "paper_url": "http://arxiv.org/abs/2310.03891v1",
      "paper_abstract": "In this paper, a new approach called HDNA (HTML DNA) is introduced for analyzing and comparing Document Object Model (DOM) trees in order to detect differences in HTML pages. This method assigns an identifier to each HTML page based on its structure, which proves to be particularly useful for detecting variations caused by server-side updates, user interactions or potential security risks. The process involves preprocessing the HTML content generating a DOM tree and calculating the disparities between two or more trees. By assigning weights to the nodes valuable insights about their hierarchical importance are obtained. The effectiveness of the HDNA approach has been demonstrated in identifying changes in DOM trees even when dynamically generated content is involved. Not does this method benefit web developers, testers, and security analysts by offering a deeper understanding of how web pages evolve. It also helps ensure the functionality and performance of web applications. Additionally, it enables detection and response to vulnerabilities that may arise from modifications in DOM structures. As the web ecosystem continues to evolve HDNA proves to be a tool, for individuals engaged in web development, testing, or security analysis.",
      "paper_authors": [
        "Mahdi Akhi",
        "Nona Ghazizadeh"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2023-10-05",
      "update_time": "2023-10-05",
      "comments": "6 pages, 3 figures",
      "repo_url": "#"
    },
    "2310.02674": {
      "paper_id": "2310.02674v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.02674v3",
      "paper_key": "2310.02674",
      "paper_title": "ObjFormer: Learning Land-Cover Changes From Paired OSM Data and Optical High-Resolution Imagery via Object-Guided Transformer",
      "paper_url": "http://arxiv.org/abs/2310.02674v3",
      "paper_abstract": "Optical high-resolution imagery and OSM data are two important data sources of change detection (CD). Previous related studies focus on utilizing the information in OSM data to aid the CD on optical high-resolution images. This paper pioneers the direct detection of land-cover changes utilizing paired OSM data and optical imagery, thereby expanding the scope of CD tasks. To this end, we propose an object-guided Transformer (ObjFormer) by naturally combining the object-based image analysis (OBIA) technique with the advanced vision Transformer architecture. This combination can significantly reduce the computational overhead in the self-attention module without adding extra parameters or layers. ObjFormer has a hierarchical pseudo-siamese encoder consisting of object-guided self-attention modules that extracts multi-level heterogeneous features from OSM data and optical images; a decoder consisting of object-guided cross-attention modules can recover land-cover changes from the extracted heterogeneous features. Beyond basic binary change detection, this paper raises a new semi-supervised semantic change detection task that does not require any manually annotated land-cover labels to train semantic change detectors. Two lightweight semantic decoders are added to ObjFormer to accomplish this task efficiently. A converse cross-entropy loss is designed to fully utilize negative samples, contributing to the great performance improvement in this task. A large-scale benchmark dataset called OpenMapCD containing 1,287 samples covering 40 regions on six continents is constructed to conduct detailed experiments. The results show the effectiveness of our methods in this new kind of CD task. Additionally, case studies in Japanese cities demonstrate the framework's generalizability and practical potential. The OpenMapCD and source code are available in https://github.com/ChenHongruixuan/ObjFormer",
      "paper_authors": [
        "Hongruixuan Chen",
        "Cuiling Lan",
        "Jian Song",
        "Clifford Broni-Bediako",
        "Junshi Xia",
        "Naoto Yokoya"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-10-04",
      "update_time": "2024-06-26",
      "comments": "Accepted by IEEE TGRS",
      "repo_url": "https://github.com/chenhongruixuan/objformer"
    },
    "2310.01876": {
      "paper_id": "2310.01876v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.01876v1",
      "paper_key": "2310.01876",
      "paper_title": "A Dual Attentive Generative Adversarial Network for Remote Sensing Image Change Detection",
      "paper_url": "http://arxiv.org/abs/2310.01876v1",
      "paper_abstract": "Remote sensing change detection between bi-temporal images receives growing concentration from researchers. However, comparing two bi-temporal images for detecting changes is challenging, as they demonstrate different appearances. In this paper, we propose a dual attentive generative adversarial network for achieving very high-resolution remote sensing image change detection tasks, which regards the detection model as a generator and attains the optimal weights of the detection model without increasing the parameters of the detection model through generative-adversarial strategy, boosting the spatial contiguity of predictions. Moreover, We design a multi-level feature extractor for effectively fusing multi-level features, which adopts the pre-trained model to extract multi-level features from bi-temporal images and introduces aggregate connections to fuse them. To strengthen the identification of multi-scale objects, we propose a multi-scale adaptive fusion module to adaptively fuse multi-scale features through various receptive fields and design a context refinement module to explore contextual dependencies. Moreover, the DAGAN framework utilizes the 4-layer convolution network as a discriminator to identify whether the synthetic image is fake or real. Extensive experiments represent that the DAGAN framework has better performance with 85.01% mean IoU and 91.48% mean F1 score than advanced methods on the LEVIR dataset.",
      "paper_authors": [
        "Luyi Qiu",
        "Xiaofeng Zhang",
        "ChaoChen Gu",
        "and ShanYing Zhu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-10-03",
      "update_time": "2023-10-03",
      "comments": null,
      "repo_url": "#"
    },
    "2310.00689": {
      "paper_id": "2310.00689v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2310.00689v1",
      "paper_key": "2310.00689",
      "paper_title": "Exchange means change: an unsupervised single-temporal change detection framework based on intra- and inter-image patch exchange",
      "paper_url": "http://arxiv.org/abs/2310.00689v1",
      "paper_abstract": "Change detection (CD) is a critical task in studying the dynamics of ecosystems and human activities using multi-temporal remote sensing images. While deep learning has shown promising results in CD tasks, it requires a large number of labeled and paired multi-temporal images to achieve high performance. Pairing and annotating large-scale multi-temporal remote sensing images is both expensive and time-consuming. To make deep learning-based CD techniques more practical and cost-effective, we propose an unsupervised single-temporal CD framework based on intra- and inter-image patch exchange (I3PE). The I3PE framework allows for training deep change detectors on unpaired and unlabeled single-temporal remote sensing images that are readily available in real-world applications. The I3PE framework comprises four steps: 1) intra-image patch exchange method is based on an object-based image analysis method and adaptive clustering algorithm, which generates pseudo-bi-temporal image pairs and corresponding change labels from single-temporal images by exchanging patches within the image; 2) inter-image patch exchange method can generate more types of land-cover changes by exchanging patches between images; 3) a simulation pipeline consisting of several image enhancement methods is proposed to simulate the radiometric difference between pre- and post-event images caused by different imaging conditions in real situations; 4) self-supervised learning based on pseudo-labels is applied to further improve the performance of the change detectors in both unsupervised and semi-supervised cases. Extensive experiments on two large-scale datasets demonstrate that I3PE outperforms representative unsupervised approaches and achieves F1 value improvements of 10.65% and 6.99% to the SOTA method. Moreover, I3PE can improve the performance of the ... (see the original article for full abstract)",
      "paper_authors": [
        "Hongruixuan Chen",
        "Jian Song",
        "Chen Wu",
        "Bo Du",
        "Naoto Yokoya"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-10-01",
      "update_time": "2023-10-01",
      "comments": null,
      "repo_url": "https://github.com/chenhongruixuan/i3pe"
    },
    "2309.16171": {
      "paper_id": "2309.16171v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.16171v1",
      "paper_key": "2309.16171",
      "paper_title": "Distributionally Robust Quickest Change Detection using Wasserstein Uncertainty Sets",
      "paper_url": "http://arxiv.org/abs/2309.16171v1",
      "paper_abstract": "The problem of quickest detection of a change in the distribution of a sequence of independent observations is considered. It is assumed that the pre-change distribution is known (accurately estimated), while the only information about the post-change distribution is through a (small) set of labeled data. This post-change data is used in a data-driven minimax robust framework, where an uncertainty set for the post-change distribution is constructed using the Wasserstein distance from the empirical distribution of the data. The robust change detection problem is studied in an asymptotic setting where the mean time to false alarm goes to infinity, for which the least favorable post-change distribution within the uncertainty set is the one that minimizes the Kullback-Leibler divergence between the post- and the pre-change distributions. It is shown that the density corresponding to the least favorable distribution is an exponentially tilted version of the pre-change density and can be calculated efficiently. A Cumulative Sum (CuSum) test based on the least favorable distribution, which is referred to as the distributionally robust (DR) CuSum test, is then shown to be asymptotically robust. The results are extended to the case where the post-change uncertainty set is a finite union of multiple Wasserstein uncertainty sets, corresponding to multiple post-change scenarios, each with its own labeled data. The proposed method is validated using synthetic and real data examples.",
      "paper_authors": [
        "Liyan Xie",
        "Yuchen Liang",
        "Venugopal V. Veeravalli"
      ],
      "primary_category": "math.ST",
      "publish_time": "2023-09-28",
      "update_time": "2023-09-28",
      "comments": null,
      "repo_url": "#"
    },
    "2311.09225": {
      "paper_id": "2311.09225v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2311.09225v1",
      "paper_key": "2311.09225",
      "paper_title": "Autonomous Driving using Spiking Neural Networks on Dynamic Vision Sensor Data: A Case Study of Traffic Light Change Detection",
      "paper_url": "http://arxiv.org/abs/2311.09225v1",
      "paper_abstract": "Autonomous driving is a challenging task that has gained broad attention from both academia and industry. Current solutions using convolutional neural networks require large amounts of computational resources, leading to high power consumption. Spiking neural networks (SNNs) provide an alternative computation model to process information and make decisions. This biologically plausible model has the advantage of low latency and energy efficiency. Recent work using SNNs for autonomous driving mostly focused on simple tasks like lane keeping in simplified simulation environments. This project studies SNNs on photo-realistic driving scenes in the CARLA simulator, which is an important step toward using SNNs on real vehicles. The efficacy and generalizability of the method will be investigated.",
      "paper_authors": [
        "Xuelei Chen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-09-27",
      "update_time": "2023-09-27",
      "comments": null,
      "repo_url": "https://github.com/xueleichen/snn-dvs-carla"
    },
    "2309.15383": {
      "paper_id": "2309.15383v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.15383v1",
      "paper_key": "2309.15383",
      "paper_title": "Intelligent trading strategy based on improved directional change and regime change detection",
      "paper_url": "http://arxiv.org/abs/2309.15383v1",
      "paper_abstract": "Previous research primarily characterized price movements according to time intervals, resulting in temporal discontinuity and overlooking crucial activities in financial markets. Directional Change (DC) is an alternative approach to sampling price data, highlighting significant points while blurring out noise details in price movements. However, traditional DC treated the thresholds of upward and downward trends with distinct intrinsic patterns as equivalent and preset them as fixed values, which are dependent on the subjective judgment of traders. To enhance the generalization performance of this methodology, we improved DC by introducing a modified threshold selection technique. Specifically, we addressed upward and downward trends distinctly by incorporating a decay coefficient. Further, we simultaneously optimized the threshold and decay coefficient using the Bayesian Optimization Algorithm (BOA). Additionally, we recognized the abnormal market state by regime change detection based on the Hidden Markov Model (RCD-HMM) to reduce the risk. Our Intelligent Trading Algorithm (ITA) was constructed based on above methods and the experiments were carried out on tick data from diverse currency pairs in the forex market. The experimental results showed a significant increase in profit and reduction in risk of DC-based trading strategies, which demonstrated the effectiveness of our proposed methods.",
      "paper_authors": [
        "Bing Wu",
        "Xiangzu Han"
      ],
      "primary_category": "cs.CE",
      "publish_time": "2023-09-27",
      "update_time": "2023-09-27",
      "comments": "22 pages, 5 figures",
      "repo_url": "#"
    },
    "2309.14781": {
      "paper_id": "2309.14781v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.14781v1",
      "paper_key": "2309.14781",
      "paper_title": "Frugal Satellite Image Change Detection with Deep-Net Inversion",
      "paper_url": "http://arxiv.org/abs/2309.14781v1",
      "paper_abstract": "Change detection in satellite imagery seeks to find occurrences of targeted changes in a given scene taken at different instants. This task has several applications ranging from land-cover mapping, to anthropogenic activity monitory as well as climate change and natural hazard damage assessment. However, change detection is highly challenging due to the acquisition conditions and also to the subjectivity of changes. In this paper, we devise a novel algorithm for change detection based on active learning. The proposed method is based on a question and answer model that probes an oracle (user) about the relevance of changes only on a small set of critical images (referred to as virtual exemplars), and according to oracle's responses updates deep neural network (DNN) classifiers. The main contribution resides in a novel adversarial model that allows learning the most representative, diverse and uncertain virtual exemplars (as inverted preimages of the trained DNNs) that challenge (the most) the trained DNNs, and this leads to a better re-estimate of these networks in the subsequent iterations of active learning. Experiments show the out-performance of our proposed deep-net inversion against the related work.",
      "paper_authors": [
        "Hichem Sahbi",
        "Sebastien Deschamps"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-09-26",
      "update_time": "2023-09-26",
      "comments": "arXiv admin note: text overlap with arXiv:2212.13974",
      "repo_url": "#"
    },
    "2309.14379": {
      "paper_id": "2309.14379v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.14379v1",
      "paper_key": "2309.14379",
      "paper_title": "Machine-assisted mixed methods: augmenting humanities and social sciences with artificial intelligence",
      "paper_url": "http://arxiv.org/abs/2309.14379v1",
      "paper_abstract": "The increasing capacities of large language models (LLMs) present an unprecedented opportunity to scale up data analytics in the humanities and social sciences, augmenting and automating qualitative analytic tasks previously typically allocated to human labor. This contribution proposes a systematic mixed methods framework to harness qualitative analytic expertise, machine scalability, and rigorous quantification, with attention to transparency and replicability. 16 machine-assisted case studies are showcased as proof of concept. Tasks include linguistic and discourse analysis, lexical semantic change detection, interview analysis, historical event cause inference and text mining, detection of political stance, text and idea reuse, genre composition in literature and film; social network inference, automated lexicography, missing metadata augmentation, and multimodal visual cultural analytics. In contrast to the focus on English in the emerging LLM applicability literature, many examples here deal with scenarios involving smaller languages and historical texts prone to digitization distortions. In all but the most difficult tasks requiring expert knowledge, generative LLMs can demonstrably serve as viable research instruments. LLM (and human) annotations may contain errors and variation, but the agreement rate can and should be accounted for in subsequent statistical modeling; a bootstrapping approach is discussed. The replications among the case studies illustrate how tasks previously requiring potentially months of team effort and complex computational pipelines, can now be accomplished by an LLM-assisted scholar in a fraction of the time. Importantly, this approach is not intended to replace, but to augment researcher knowledge and skills. With these opportunities in sight, qualitative expertise and the ability to pose insightful questions have arguably never been more critical.",
      "paper_authors": [
        "Andres Karjus"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2023-09-24",
      "update_time": "2023-09-24",
      "comments": null,
      "repo_url": "https://github.com/andreskarjus/machineassistedmixedmethods"
    },
    "2309.13619": {
      "paper_id": "2309.13619v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.13619v1",
      "paper_key": "2309.13619",
      "paper_title": "Changes-Aware Transformer: Learning Generalized Changes Representation",
      "paper_url": "http://arxiv.org/abs/2309.13619v1",
      "paper_abstract": "Difference features obtained by comparing the images of two periods play an indispensable role in the change detection (CD) task. However, a pair of bi-temporal images can exhibit diverse changes, which may cause various difference features. Identifying changed pixels with differ difference features to be the same category is thus a challenge for CD. Most nowadays' methods acquire distinctive difference features in implicit ways like enhancing image representation or supervision information. Nevertheless, informative image features only guarantee object semantics are modeled and can not guarantee that changed pixels have similar semantics in the difference feature space and are distinct from those unchanged ones. In this work, the generalized representation of various changes is learned straightforwardly in the difference feature space, and a novel Changes-Aware Transformer (CAT) for refining difference features is proposed. This generalized representation can perceive which pixels are changed and which are unchanged and further guide the update of pixels' difference features. CAT effectively accomplishes this refinement process through the stacked cosine cross-attention layer and self-attention layer. After refinement, the changed pixels in the difference feature space are closer to each other, which facilitates change detection. In addition, CAT is compatible with various backbone networks and existing CD methods. Experiments on remote sensing CD data set and street scene CD data set show that our method achieves state-of-the-art performance and has excellent generalization.",
      "paper_authors": [
        "Dan Wang",
        "Licheng Jiao",
        "Jie Chen",
        "Shuyuan Yang",
        "Fang Liu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2023-09-24",
      "update_time": "2023-09-24",
      "comments": null,
      "repo_url": "#"
    },
    "2309.12010": {
      "paper_id": "2309.12010v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2309.12010v1",
      "paper_key": "2309.12010",
      "paper_title": "Convolution and Attention Mixer for Synthetic Aperture Radar Image Change Detection",
      "paper_url": "http://arxiv.org/abs/2309.12010v1",
      "paper_abstract": "Synthetic aperture radar (SAR) image change detection is a critical task and has received increasing attentions in the remote sensing community. However, existing SAR change detection methods are mainly based on convolutional neural networks (CNNs), with limited consideration of global attention mechanism. In this letter, we explore Transformer-like architecture for SAR change detection to incorporate global attention. To this end, we propose a convolution and attention mixer (CAMixer). First, to compensate the inductive bias for Transformer, we combine self-attention with shift convolution in a parallel way. The parallel design effectively captures the global semantic information via the self-attention and performs local feature extraction through shift convolution simultaneously. Second, we adopt a gating mechanism in the feed-forward network to enhance the non-linear feature transformation. The gating mechanism is formulated as the element-wise multiplication of two parallel linear layers. Important features can be highlighted, leading to high-quality representations against speckle noise. Extensive experiments conducted on three SAR datasets verify the superior performance of the proposed CAMixer. The source codes will be publicly available at https://github.com/summitgao/CAMixer .",
      "paper_authors": [
        "Haopeng Zhang",
        "Zijing Lin",
        "Feng Gao",
        "Junyu Dong",
        "Qian Du",
        "Heng-Chao Li"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2023-09-21",
      "update_time": "2023-09-21",
      "comments": "Accepted by IEEE GRSL",
      "repo_url": "https://github.com/summitgao/camixer"
    },
    "2409.02618": {
      "paper_id": "2409.02618v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.02618v1",
      "paper_key": "2409.02618",
      "paper_title": "Neuromorphic Heart Rate Monitors: Neural State Machines for Monotonic Change Detection",
      "paper_url": "http://arxiv.org/abs/2409.02618v1",
      "paper_abstract": "Detecting monotonic changes in heart rate (HR) is crucial for early identification of cardiac conditions and health management. This is particularly important for dementia patients, where HR trends can signal stress or agitation. Developing wearable technologies that can perform always-on monitoring of HRs is essential to effectively detect slow changes over extended periods of time. However, designing compact electronic circuits that can monitor and process bio-signals continuously, and that can operate in a low-power regime to ensure long-lasting performance, is still an open challenge. Neuromorphic technology offers an energy-efficient solution for real-time health monitoring. We propose a neuromorphic implementation of a Neural State Machine (NSM) network to encode different health states and switch between them based on the input stimuli. Our focus is on detecting monotonic state switches in electrocardiogram data to identify progressive HR increases. This innovative approach promises significant advancements in continuous health monitoring and management.",
      "paper_authors": [
        "Alessio Carpegna",
        "Chiara De Luca",
        "Federico Emanuele Pozzi",
        "Alessandro Savino",
        "Stefano Di Carlo",
        "Giacomo Indiveri",
        "Elisa Donati"
      ],
      "primary_category": "cs.ET",
      "publish_time": "2024-09-04",
      "update_time": "2024-09-04",
      "comments": null,
      "repo_url": "#"
    },
    "2409.00589": {
      "paper_id": "2409.00589v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.00589v1",
      "paper_key": "2409.00589",
      "paper_title": "Change-Aware Siamese Network for Surface Defects Segmentation under Complex Background",
      "paper_url": "http://arxiv.org/abs/2409.00589v1",
      "paper_abstract": "Despite the eye-catching breakthroughs achieved by deep visual networks in detecting region-level surface defects, the challenge of high-quality pixel-wise defect detection remains due to diverse defect appearances and data scarcity. To avoid over-reliance on defect appearance and achieve accurate defect segmentation, we proposed a change-aware Siamese network that solves the defect segmentation in a change detection framework. A novel multi-class balanced contrastive loss is introduced to guide the Transformer-based encoder, which enables encoding diverse categories of defects as the unified class-agnostic difference between defect and defect-free images. The difference presented by a distance map is then skip-connected to the change-aware decoder to assist in the location of both inter-class and out-of-class pixel-wise defects. In addition, we proposed a synthetic dataset with multi-class liquid crystal display (LCD) defects under a complex and disjointed background context, to demonstrate the advantages of change-based modeling over appearance-based modeling for defect segmentation. In our proposed dataset and two public datasets, our model achieves superior performances than the leading semantic segmentation methods, while maintaining a relatively small model size. Moreover, our model achieves a new state-of-the-art performance compared to the semi-supervised approaches in various supervision settings.",
      "paper_authors": [
        "Biyuan Liu",
        "Huaixin Chen",
        "Huiyao Zhan",
        "Sijie Luo",
        "Zhou Huang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-01",
      "update_time": "2024-09-01",
      "comments": null,
      "repo_url": "https://github.com/qaz670756/CADNet"
    },
    "2409.05679": {
      "paper_id": "2409.05679v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05679v1",
      "paper_key": "2409.05679",
      "paper_title": "AnomalyCD: A benchmark for Earth anomaly change detection with high-resolution and time-series observations",
      "paper_url": "http://arxiv.org/abs/2409.05679v1",
      "paper_abstract": "Various Earth anomalies have destroyed the stable, balanced state, resulting in fatalities and serious destruction of property. With the advantages of large-scale and precise observation, high-resolution remote sensing images have been widely used for anomaly monitoring and localization. Powered by the deep representation, the existing methods have achieved remarkable advances, primarily in classification and change detection techniques. However, labeled samples are difficult to acquire due to the low probability of anomaly occurrence, and the trained models are limited to fixed anomaly categories, which hinders the application for anomalies with few samples or unknown anomalies. In this paper, to tackle this problem, we propose the anomaly change detection (AnomalyCD) technique, which accepts time-series observations and learns to identify anomalous changes by learning from the historical normal change pattern. Compared to the existing techniques, AnomalyCD processes an unfixed number of time steps and can localize the various anomalies in a unified manner, without human supervision. To benchmark AnomalyCD, we constructed a high-resolution dataset with time-series images dedicated to various Earth anomalies (the AnomalyCDD dataset). AnomalyCDD contains high-resolution (from 0.15 to 2.39 m/pixel), time-series (from 3 to 7 time steps), and large-scale images (1927.93 km2 in total) collected globally Furthermore, we developed a zero-shot baseline model (AnomalyCDM), which implements the AnomalyCD technique by extracting a general representation from the segment anything model (SAM) and conducting temporal comparison to distinguish the anomalous changes from normal changes. AnomalyCDM is designed as a two-stage workflow to enhance the efficiency, and has the ability to process the unseen images directly, without retraining for each scene.",
      "paper_authors": [
        "Jingtao Li",
        "Qian Zhu",
        "Xinyu Wang",
        "Hengwei Zhao",
        "Yanfei Zhong"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "remote sensing benchmark",
      "repo_url": "#"
    },
    "2409.06214": {
      "paper_id": "2409.06214v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06214v1",
      "paper_key": "2409.06214",
      "paper_title": "Towards Generalizable Scene Change Detection",
      "paper_url": "http://arxiv.org/abs/2409.06214v1",
      "paper_abstract": "Scene Change Detection (SCD) is vital for applications such as visual surveillance and mobile robotics. However, current SCD methods exhibit a bias to the temporal order of training datasets and limited performance on unseen domains; coventional SCD benchmarks are not able to evaluate generalization or temporal consistency. To tackle these limitations, we introduce a Generalizable Scene Change Detection Framework (GeSCF) in this work. The proposed GeSCF leverages localized semantics of a foundation model without any re-training or fine-tuning -- for generalization over unseen domains. Specifically, we design an adaptive thresholding of the similarity distribution derived from facets of the pre-trained foundation model to generate initial pseudo-change mask. We further utilize Segment Anything Model's (SAM) class-agnostic masks to refine pseudo-masks. Moreover, our proposed framework maintains commutative operations in all settings to ensure complete temporal consistency. Finally, we define new metrics, evaluation dataset, and evaluation protocol for Generalizable Scene Change Detection (GeSCD). Extensive experiments demonstrate that GeSCF excels across diverse and challenging environments -- establishing a new benchmark for SCD performance.",
      "paper_authors": [
        "Jaewoo Kim",
        "Uehwan Kim"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": "7 pages, 5 figures",
      "repo_url": "#"
    },
    "2409.07948": {
      "paper_id": "2409.07948v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07948v1",
      "paper_key": "2409.07948",
      "paper_title": "Quickest Change Detection Using Mismatched CUSUM",
      "paper_url": "http://arxiv.org/abs/2409.07948v1",
      "paper_abstract": "The field of quickest change detection (QCD) concerns design and analysis of algorithms to estimate in real time the time at which an important event takes place and identify properties of the post-change behavior. The goal is to devise a stopping time adapted to the observations that minimizes an $L_1$ loss.   Approximately optimal solutions are well known under a variety of assumptions. In the work surveyed here we consider the CUSUM statistic, which is defined as a one-dimensional reflected random walk driven by a functional of the observations. It is known that the optimal functional is a log likelihood ratio subject to special statical assumptions.   The paper concerns model free approaches to detection design, considering the following questions:   1. What is the performance for a given functional of the observations?   2. How do the conclusions change when there is dependency between pre- and post-change behavior?   3. How can techniques from statistics and machine learning be adapted to approximate the best functional in a given class?",
      "paper_authors": [
        "Austin Cooper",
        "Sean Meyn"
      ],
      "primary_category": "math.ST",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "Extended version of extended abstract for the Allerton Conference on\n  Communication, Control, and Computing, September 2024",
      "repo_url": "#"
    },
    "2409.08582": {
      "paper_id": "2409.08582v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08582v1",
      "paper_key": "2409.08582",
      "paper_title": "ChangeChat: An Interactive Model for Remote Sensing Change Analysis via Multimodal Instruction Tuning",
      "paper_url": "http://arxiv.org/abs/2409.08582v1",
      "paper_abstract": "Remote sensing (RS) change analysis is vital for monitoring Earth's dynamic processes by detecting alterations in images over time. Traditional change detection excels at identifying pixel-level changes but lacks the ability to contextualize these alterations. While recent advancements in change captioning offer natural language descriptions of changes, they do not support interactive, user-specific queries. To address these limitations, we introduce ChangeChat, the first bitemporal vision-language model (VLM) designed specifically for RS change analysis. ChangeChat utilizes multimodal instruction tuning, allowing it to handle complex queries such as change captioning, category-specific quantification, and change localization. To enhance the model's performance, we developed the ChangeChat-87k dataset, which was generated using a combination of rule-based methods and GPT-assisted techniques. Experiments show that ChangeChat offers a comprehensive, interactive solution for RS change analysis, achieving performance comparable to or even better than state-of-the-art (SOTA) methods on specific tasks, and significantly surpassing the latest general-domain model, GPT-4. Code and pre-trained weights are available at https://github.com/hanlinwu/ChangeChat.",
      "paper_authors": [
        "Pei Deng",
        "Wenqian Zhou",
        "Hanlin Wu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "5 pages, 2 figures",
      "repo_url": "#"
    },
    "2409.10178": {
      "paper_id": "2409.10178v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10178v1",
      "paper_key": "2409.10178",
      "paper_title": "ExelMap: Explainable Element-based HD-Map Change Detection and Update",
      "paper_url": "http://arxiv.org/abs/2409.10178v1",
      "paper_abstract": "Acquisition and maintenance are central problems in deploying high-definition (HD) maps for autonomous driving, with two lines of research prevalent in current literature: Online HD map generation and HD map change detection. However, the generated map's quality is currently insufficient for safe deployment, and many change detection approaches fail to precisely localize and extract the changed map elements, hence lacking explainability and hindering a potential fleet-based cooperative HD map update. In this paper, we propose the novel task of explainable element-based HD map change detection and update. In extending recent approaches that use online mapping techniques informed with an outdated map prior for HD map updating, we present ExelMap, an explainable element-based map updating strategy that specifically identifies changed map elements. In this context, we discuss how currently used metrics fail to capture change detection performance, while allowing for unfair comparison between prior-less and prior-informed map generation methods. Finally, we present an experimental study on real-world changes related to pedestrian crossings of the Argoverse 2 Map Change Dataset. To the best of our knowledge, this is the first comprehensive problem investigation of real-world end-to-end element-based HD map change detection and update, and ExelMap the first proposed solution.",
      "paper_authors": [
        "Lena Wild",
        "Ludvig Ericson",
        "Rafael Valencia",
        "Patric Jensfelt"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "17 pages, 3 figures",
      "repo_url": "#"
    },
    "2409.09726": {
      "paper_id": "2409.09726v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09726v1",
      "paper_key": "2409.09726",
      "paper_title": "High Definition Map Mapping and Update: A General Overview and Future Directions",
      "paper_url": "http://arxiv.org/abs/2409.09726v1",
      "paper_abstract": "Along with the rapid growth of autonomous vehicles (AVs), more and more demands are required for environment perception technology. Among others, HD mapping has become one of the more prominent roles in helping the vehicle realize essential tasks such as localization and path planning. While increasing research efforts have been directed toward HD Map development. However, a comprehensive overview of the overall HD map mapping and update framework is still lacking. This article introduces the development and current state of the algorithm involved in creating HD map mapping and its maintenance. As part of this study, the primary data preprocessing approach of processing raw data to information ready to feed for mapping and update purposes, semantic segmentation, and localization are also briefly reviewed. Moreover, the map taxonomy, ontology, and quality assessment are extensively discussed, the map data's general representation method is presented, and the mapping algorithm ranging from SLAM to transformers learning-based approaches are also discussed. The development of the HD map update algorithm, from change detection to the update methods, is also presented. Finally, the authors discuss possible future developments and the remaining challenges in HD map mapping and update technology. This paper simultaneously serves as a position paper and tutorial to those new to HD map mapping and update domains.",
      "paper_authors": [
        "Benny Wijaya",
        "Kun Jiang",
        "Mengmeng Yang",
        "Tuopu Wen",
        "Yunlong Wang",
        "Xuewei Tang",
        "Zheng Fu",
        "Taohua Zhou",
        "Diange Yang"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": "30 Pages, 13 figures",
      "repo_url": "#"
    }
  },
  "Rethinking": {
    "2409.08141": {
      "paper_id": "2409.08141v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08141v1",
      "paper_key": "2409.08141",
      "paper_title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent Interconnects",
      "paper_url": "http://arxiv.org/abs/2409.08141v1",
      "paper_abstract": "Conventional wisdom holds that an efficient interface between an OS running on a CPU and a high-bandwidth I/O device should be based on Direct Memory Access (DMA), descriptor rings, and interrupts: DMA offloads transfers from the CPU, descriptor rings provide buffering and queuing, and interrupts facilitate asynchronous interaction between cores and device with a lightweight notification mechanism. In this paper we question this wisdom in the light of modern hardware and workloads, particularly in cloud servers. We argue that the assumptions that led to this model are obsolete, and in many use-cases use of programmed I/O, where the CPU explicitly transfers data and control information to and from a device via loads and stores, actually results in a more efficient system. We quantitatively demonstrate these advantages using three use-cases: fine-grained RPC-style invocation of functions on an accelerator, offloading of operators in a streaming dataflow engine, and a network interface targeting for serverless functions. Moreover, we show that while these advantages are significant over a modern PCIe peripheral bus, a truly cache-coherent interconnect offers significant additional efficiency gains.",
      "paper_authors": [
        "Anastasiia Ruzhanskaia",
        "Pengcheng Xu",
        "David Cock",
        "Timothy Roscoe"
      ],
      "primary_category": "cs.AR",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": null,
      "repo_url": "#"
    },
    "2409.07273": {
      "paper_id": "2409.07273v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07273v1",
      "paper_key": "2409.07273",
      "paper_title": "Rethinking Mamba in Speech Processing by Self-Supervised Models",
      "paper_url": "http://arxiv.org/abs/2409.07273v1",
      "paper_abstract": "The Mamba-based model has demonstrated outstanding performance across tasks in computer vision, natural language processing, and speech processing. However, in the realm of speech processing, the Mamba-based model's performance varies across different tasks. For instance, in tasks such as speech enhancement and spectrum reconstruction, the Mamba model performs well when used independently. However, for tasks like speech recognition, additional modules are required to surpass the performance of attention-based models. We propose the hypothesis that the Mamba-based model excels in \"reconstruction\" tasks within speech processing. However, for \"classification tasks\" such as Speech Recognition, additional modules are necessary to accomplish the \"reconstruction\" step. To validate our hypothesis, we analyze the previous Mamba-based Speech Models from an information theory perspective. Furthermore, we leveraged the properties of HuBERT in our study. We trained a Mamba-based HuBERT model, and the mutual information patterns, along with the model's performance metrics, confirmed our assumptions.",
      "paper_authors": [
        "Xiangyu Zhang",
        "Jianbo Ma",
        "Mostafa Shahin",
        "Beena Ahmed",
        "Julien Epps"
      ],
      "primary_category": "eess.AS",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": null,
      "repo_url": "#"
    },
    "2409.06923": {
      "paper_id": "2409.06923v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06923v1",
      "paper_key": "2409.06923",
      "paper_title": "Rethinking Directional Parameterization in Neural Implicit Surface Reconstruction",
      "paper_url": "http://arxiv.org/abs/2409.06923v1",
      "paper_abstract": "Multi-view 3D surface reconstruction using neural implicit representations has made notable progress by modeling the geometry and view-dependent radiance fields within a unified framework. However, their effectiveness in reconstructing objects with specular or complex surfaces is typically biased by the directional parameterization used in their view-dependent radiance network. {\\it Viewing direction} and {\\it reflection direction} are the two most commonly used directional parameterizations but have their own limitations. Typically, utilizing the viewing direction usually struggles to correctly decouple the geometry and appearance of objects with highly specular surfaces, while using the reflection direction tends to yield overly smooth reconstructions for concave or complex structures. In this paper, we analyze their failed cases in detail and propose a novel hybrid directional parameterization to address their limitations in a unified form. Extensive experiments demonstrate the proposed hybrid directional parameterization consistently delivered satisfactory results in reconstructing objects with a wide variety of materials, geometry and appearance, whereas using other directional parameterizations faces challenges in reconstructing certain objects. Moreover, the proposed hybrid directional parameterization is nearly parameter-free and can be effortlessly applied in any existing neural surface reconstruction method.",
      "paper_authors": [
        "Zijie Jiang",
        "Tianhan Xu",
        "Hiroharu Kato"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "Accepted to ECCV 2024",
      "repo_url": "#"
    },
    "2409.05573": {
      "paper_id": "2409.05573v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05573v1",
      "paper_key": "2409.05573",
      "paper_title": "Learning to Model Graph Structural Information on MLPs via Graph Structure Self-Contrasting",
      "paper_url": "http://arxiv.org/abs/2409.05573v1",
      "paper_abstract": "Recent years have witnessed great success in handling graph-related tasks with Graph Neural Networks (GNNs). However, most existing GNNs are based on message passing to perform feature aggregation and transformation, where the structural information is explicitly involved in the forward propagation by coupling with node features through graph convolution at each layer. As a result, subtle feature noise or structure perturbation may cause severe error propagation, resulting in extremely poor robustness. In this paper, we rethink the roles played by graph structural information in graph data training and identify that message passing is not the only path to modeling structural information. Inspired by this, we propose a simple but effective Graph Structure Self-Contrasting (GSSC) framework that learns graph structural information without message passing. The proposed framework is based purely on Multi-Layer Perceptrons (MLPs), where the structural information is only implicitly incorporated as prior knowledge to guide the computation of supervision signals, substituting the explicit message propagation as in GNNs. Specifically, it first applies structural sparsification to remove potentially uninformative or noisy edges in the neighborhood, and then performs structural self-contrasting in the sparsified neighborhood to learn robust node representations. Finally, structural sparsification and self-contrasting are formulated as a bi-level optimization problem and solved in a unified framework. Extensive experiments have qualitatively and quantitatively demonstrated that the GSSC framework can produce truly encouraging performance with better generalization and robustness than other leading competitors.",
      "paper_authors": [
        "Lirong Wu",
        "Haitao Lin",
        "Guojiang Zhao",
        "Cheng Tan",
        "Stan Z. Li"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": null,
      "repo_url": "#"
    },
    "2409.05558": {
      "paper_id": "2409.05558v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05558v1",
      "paper_key": "2409.05558",
      "paper_title": "Seeing Through the Mask: Rethinking Adversarial Examples for CAPTCHAs",
      "paper_url": "http://arxiv.org/abs/2409.05558v1",
      "paper_abstract": "Modern CAPTCHAs rely heavily on vision tasks that are supposedly hard for computers but easy for humans. However, advances in image recognition models pose a significant threat to such CAPTCHAs. These models can easily be fooled by generating some well-hidden \"random\" noise and adding it to the image, or hiding objects in the image. However, these methods are model-specific and thus can not aid CAPTCHAs in fooling all models. We show in this work that by allowing for more significant changes to the images while preserving the semantic information and keeping it solvable by humans, we can fool many state-of-the-art models. Specifically, we demonstrate that by adding masks of various intensities the Accuracy @ 1 (Acc@1) drops by more than 50%-points for all models, and supposedly robust models such as vision transformers see an Acc@1 drop of 80%-points.   These masks can therefore effectively fool modern image classifiers, thus showing that machines have not caught up with humans -- yet.",
      "paper_authors": [
        "Yahya Jabary",
        "Andreas Plesner",
        "Turlan Kuzhagaliyev",
        "Roger Wattenhofer"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "Under review",
      "repo_url": "https://github.com/ETH-DISCO/advx-bench"
    },
    "2409.05274": {
      "paper_id": "2409.05274v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05274v1",
      "paper_key": "2409.05274",
      "paper_title": "Rethinking the Atmospheric Scattering-driven Attention via Channel and Gamma Correction Priors for Low-Light Image Enhancement",
      "paper_url": "http://arxiv.org/abs/2409.05274v1",
      "paper_abstract": "Low-light image enhancement remains a critical challenge in computer vision, as does the lightweight design for edge devices with the computational burden for deep learning models. In this article, we introduce an extended version of Channel-Prior and Gamma-Estimation Network (CPGA-Net), termed CPGA-Net+, which incorporates an attention mechanism driven by a reformulated Atmospheric Scattering Model and effectively addresses both global and local image processing through Plug-in Attention with gamma correction. These innovations enable CPGA-Net+ to achieve superior performance on image enhancement tasks, surpassing lightweight state-of-the-art methods with high efficiency. Our results demonstrate the model's effectiveness and show the potential applications in resource-constrained environments.",
      "paper_authors": [
        "Shyang-En Weng",
        "Cheng-Yen Hsiao",
        "Shaou-Gang Miaou"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": null,
      "repo_url": "https://github.com/Shyandram/CPGA-Net_Plus"
    },
    "2409.04878": {
      "paper_id": "2409.04878v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.04878v1",
      "paper_key": "2409.04878",
      "paper_title": "Plug-and-Hide: Provable and Adjustable Diffusion Generative Steganography",
      "paper_url": "http://arxiv.org/abs/2409.04878v1",
      "paper_abstract": "Generative Steganography (GS) is a novel technique that utilizes generative models to conceal messages without relying on cover images. Contemporary GS algorithms leverage the powerful generative capabilities of Diffusion Models (DMs) to create high-fidelity stego images. However, these algorithms, while yielding relatively satisfactory generation outcomes and message extraction accuracy, significantly alter modifications to the initial Gaussian noise of DMs, thereby compromising steganographic security. In this paper, we rethink the trade-off among image quality, steganographic security, and message extraction accuracy within Diffusion Generative Steganography (DGS) settings. Our findings reveal that the normality of initial noise of DMs is crucial to these factors and can offer theoretically grounded guidance for DGS design. Based on this insight, we propose a Provable and Adjustable Message Mapping (PA-B2G) approach. It can, on one hand, theoretically guarantee reversible encoding of bit messages from arbitrary distributions into standard Gaussian noise for DMs. On the other hand, its adjustability provides a more natural and fine-grained way to trade off image quality, steganographic security, and message extraction accuracy. By integrating PA-B2G with a probability flow ordinary differential equation, we establish an invertible mapping between secret messages and stego images. PA-B2G can be seamlessly incorporated with most mainstream DMs, such as the Stable Diffusion, without necessitating additional training or fine-tuning. Comprehensive experiments corroborate our theoretical insights regarding the trade-off in DGS settings and demonstrate the effectiveness of our DGS algorithm in producing high-quality stego images while preserving desired levels of steganographic security and extraction accuracy.",
      "paper_authors": [
        "Jiahao Zhu",
        "Zixuan Chen",
        "Lingxiao Yang",
        "Xiaohua Xie",
        "Yi Zhou"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-09-07",
      "update_time": "2024-09-07",
      "comments": null,
      "repo_url": "#"
    },
    "2409.04847": {
      "paper_id": "2409.04847v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.04847v1",
      "paper_key": "2409.04847",
      "paper_title": "Rethinking The Training And Evaluation of Rich-Context Layout-to-Image Generation",
      "paper_url": "http://arxiv.org/abs/2409.04847v1",
      "paper_abstract": "Recent advancements in generative models have significantly enhanced their capacity for image generation, enabling a wide range of applications such as image editing, completion and video editing. A specialized area within generative modeling is layout-to-image (L2I) generation, where predefined layouts of objects guide the generative process. In this study, we introduce a novel regional cross-attention module tailored to enrich layout-to-image generation. This module notably improves the representation of layout regions, particularly in scenarios where existing methods struggle with highly complex and detailed textual descriptions. Moreover, while current open-vocabulary L2I methods are trained in an open-set setting, their evaluations often occur in closed-set environments. To bridge this gap, we propose two metrics to assess L2I performance in open-vocabulary scenarios. Additionally, we conduct a comprehensive user study to validate the consistency of these metrics with human preferences.",
      "paper_authors": [
        "Jiaxin Cheng",
        "Zixu Zhao",
        "Tong He",
        "Tianjun Xiao",
        "Yicong Zhou",
        "Zheng Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-07",
      "update_time": "2024-09-07",
      "comments": null,
      "repo_url": "#"
    },
    "2409.03344": {
      "paper_id": "2409.03344v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.03344v1",
      "paper_key": "2409.03344",
      "paper_title": "Rethinking Improved Privacy-Utility Trade-off with Pre-existing Knowledge for DP Training",
      "paper_url": "http://arxiv.org/abs/2409.03344v1",
      "paper_abstract": "Differential privacy (DP) provides a provable framework for protecting individuals by customizing a random mechanism over a privacy-sensitive dataset. Deep learning models have demonstrated privacy risks in model exposure as an established learning model unintentionally records membership-level privacy leakage. Differentially private stochastic gradient descent (DP- SGD) has been proposed to safeguard training individuals by adding random Gaussian noise to gradient updates in the backpropagation. Researchers identify that DP-SGD typically causes utility loss since the injected homogeneous noise alters the gradient updates calculated at each iteration. Namely, all elements in the gradient are contaminated regardless of their importance in updating model parameters. In this work, we argue that the utility loss mainly results from the homogeneity of injected noise. Consequently, we propose a generic differential privacy framework with heterogeneous noise (DP-Hero) by defining a heterogeneous random mechanism to abstract its property. The insight of DP-Hero is to leverage the knowledge encoded in the previously trained model to guide the subsequent allocation of noise heterogeneity, thereby leveraging the statistical perturbation and achieving enhanced utility. Atop DP-Hero, we instantiate a heterogeneous version of DP-SGD, where the noise injected into gradients is heterogeneous and guided by prior-established model parameters. We conduct comprehensive experiments to verify and explain the effectiveness of the proposed DP-Hero, showing improved training accuracy compared with state-of-the-art works. Broadly, we shed light on improving the privacy-utility space by learning the noise guidance from the pre-existing leaked knowledge encoded in the previously trained model, showing a different perspective of understanding the utility-improved DP training.",
      "paper_authors": [
        "Yu Zheng",
        "Wenchao Zhang",
        "Yonggang Zhang",
        "Wei Song",
        "Kai Zhou",
        "Bo Han"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-09-05",
      "update_time": "2024-09-05",
      "comments": "13 pages",
      "repo_url": "#"
    },
    "2409.03206": {
      "paper_id": "2409.03206v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.03206v1",
      "paper_key": "2409.03206",
      "paper_title": "TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with Temporal Considerations",
      "paper_url": "http://arxiv.org/abs/2409.03206v1",
      "paper_abstract": "Multimodal Large Language Models (MLLMs) have significantly improved performance across various image-language applications. Recently, there has been a growing interest in adapting image pre-trained MLLMs for video-related tasks. However, most efforts concentrate on enhancing the vision encoder and projector components, while the core part, Large Language Models (LLMs), remains comparatively under-explored. In this paper, we propose two strategies to enhance the model's capability in video understanding tasks by improving inter-layer attention computation in LLMs. Specifically, the first approach focuses on the enhancement of Rotary Position Embedding (RoPE) with Temporal-Aware Dual RoPE, which introduces temporal position information to strengthen the MLLM's temporal modeling capabilities while preserving the relative position relationships of both visual and text tokens. The second approach involves enhancing the Attention Mask with the Frame-wise Block Causal Attention Mask, a simple yet effective method that broadens visual token interactions within and across video frames while maintaining the causal inference mechanism. Based on these proposed methods, we adapt LLaVA for video understanding tasks, naming it Temporal-Considered LLaVA (TC-LLaVA). Our TC-LLaVA achieves new state-of-the-art performance across various video understanding benchmarks with only supervised fine-tuning (SFT) on video-related datasets.",
      "paper_authors": [
        "Mingze Gao",
        "Jingyu Liu",
        "Mingda Li",
        "Jiangtao Xie",
        "Qingbin Liu",
        "Bo Zhao",
        "Xi Chen",
        "Hui Xiong"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-05",
      "update_time": "2024-09-05",
      "comments": null,
      "repo_url": "#"
    },
    "2409.02683": {
      "paper_id": "2409.02683v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.02683v1",
      "paper_key": "2409.02683",
      "paper_title": "Rethinking HTG Evaluation: Bridging Generation and Recognition",
      "paper_url": "http://arxiv.org/abs/2409.02683v1",
      "paper_abstract": "The evaluation of generative models for natural image tasks has been extensively studied. Similar protocols and metrics are used in cases with unique particularities, such as Handwriting Generation, even if they might not be completely appropriate. In this work, we introduce three measures tailored for HTG evaluation, $ \\text{HTG}_{\\text{HTR}} $, $ \\text{HTG}_{\\text{style}} $, and $ \\text{HTG}_{\\text{OOV}} $, and argue that they are more expedient to evaluate the quality of generated handwritten images. The metrics rely on the recognition error/accuracy of Handwriting Text Recognition and Writer Identification models and emphasize writing style, textual content, and diversity as the main aspects that adhere to the content of handwritten images. We conduct comprehensive experiments on the IAM handwriting database, showcasing that widely used metrics such as FID fail to properly quantify the diversity and the practical utility of generated handwriting samples. Our findings show that our metrics are richer in information and underscore the necessity of standardized evaluation protocols in HTG. The proposed metrics provide a more robust and informative protocol for assessing HTG quality, contributing to improved performance in HTR. Code for the evaluation protocol is available at: https://github.com/koninik/HTG_evaluation.",
      "paper_authors": [
        "Konstantina Nikolaidou",
        "George Retsinas",
        "Giorgos Sfikas",
        "Marcus Liwicki"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-04",
      "update_time": "2024-09-04",
      "comments": null,
      "repo_url": "https://github.com/koninik/htg_evaluation"
    },
    "2409.02492": {
      "paper_id": "2409.02492v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.02492v1",
      "paper_key": "2409.02492",
      "paper_title": "Reliable Deep Diffusion Tensor Estimation: Rethinking the Power of Data-Driven Optimization Routine",
      "paper_url": "http://arxiv.org/abs/2409.02492v1",
      "paper_abstract": "Diffusion tensor imaging (DTI) holds significant importance in clinical diagnosis and neuroscience research. However, conventional model-based fitting methods often suffer from sensitivity to noise, leading to decreased accuracy in estimating DTI parameters. While traditional data-driven deep learning methods have shown potential in terms of accuracy and efficiency, their limited generalization to out-of-training-distribution data impedes their broader application due to the diverse scan protocols used across centers, scanners, and studies. This work aims to tackle these challenges and promote the use of DTI by introducing a data-driven optimization-based method termed DoDTI. DoDTI combines the weighted linear least squares fitting algorithm and regularization by denoising technique. The former fits DW images from diverse acquisition settings into diffusion tensor field, while the latter applies a deep learning-based denoiser to regularize the diffusion tensor field instead of the DW images, which is free from the limitation of fixed-channel assignment of the network. The optimization object is solved using the alternating direction method of multipliers and then unrolled to construct a deep neural network, leveraging a data-driven strategy to learn network parameters. Extensive validation experiments are conducted utilizing both internally simulated datasets and externally obtained in-vivo datasets. The results, encompassing both qualitative and quantitative analyses, showcase that the proposed method attains state-of-the-art performance in DTI parameter estimation. Notably, it demonstrates superior generalization, accuracy, and efficiency, rendering it highly reliable for widespread application in the field.",
      "paper_authors": [
        "Jialong Li",
        "Zhicheng Zhang",
        "Yunwei Chen",
        "Qiqi Lu",
        "Ye Wu",
        "Xiaoming Liu",
        "QianJin Feng",
        "Yanqiu Feng",
        "Xinyuan Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-04",
      "update_time": "2024-09-04",
      "comments": null,
      "repo_url": "#"
    },
    "2409.00867": {
      "paper_id": "2409.00867v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.00867v1",
      "paper_key": "2409.00867",
      "paper_title": "Kinematics & Dynamics Library for Baxter Arm",
      "paper_url": "http://arxiv.org/abs/2409.00867v1",
      "paper_abstract": "The Baxter robot is a standard research platform used widely in research tasks, supported with an SDK provided by the developers, Rethink Robotics. Despite the ubiquitous use of the robot, the official software support is sub-standard. Especially, the native IK service has a low success rate and is often inconsistent. This unreliable behavior makes Baxter difficult to use for experiments and the research community is in need of a more reliable software support to control the robot. We present our work towards creating a Python based software library supporting the kinematics and dynamics of the Baxter robot. Our toolbox contains implementation of pose and velocity kinematics with support for Jacobian operations for redundancy resolution. We present the implementation and performance of our library, along with a comparison with PyKDL. Keywords- Baxter Research Robot, Manipulator Kinematics, Iterative IK, Dynamical Model, Redundant Manipulator",
      "paper_authors": [
        "Akshay Kumar",
        "Ashwin Sahasrabudhe",
        "Chaitanya Perugu",
        "Sanjuksha Nirgude",
        "Aakash Murugan"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-09-01",
      "update_time": "2024-09-01",
      "comments": null,
      "repo_url": "#"
    },
    "2409.00768": {
      "paper_id": "2409.00768v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.00768v1",
      "paper_key": "2409.00768",
      "paper_title": "Rethinking Image Super-Resolution from Training Data Perspectives",
      "paper_url": "http://arxiv.org/abs/2409.00768v1",
      "paper_abstract": "In this work, we investigate the understudied effect of the training data used for image super-resolution (SR). Most commonly, novel SR methods are developed and benchmarked on common training datasets such as DIV2K and DF2K. However, we investigate and rethink the training data from the perspectives of diversity and quality, {thereby addressing the question of ``How important is SR training for SR models?''}. To this end, we propose an automated image evaluation pipeline. With this, we stratify existing high-resolution image datasets and larger-scale image datasets such as ImageNet and PASS to compare their performances. We find that datasets with (i) low compression artifacts, (ii) high within-image diversity as judged by the number of different objects, and (iii) a large number of images from ImageNet or PASS all positively affect SR performance. We hope that the proposed simple-yet-effective dataset curation pipeline will inform the construction of SR datasets in the future and yield overall better models.",
      "paper_authors": [
        "Go Ohtani",
        "Ryu Tadokoro",
        "Ryosuke Yamada",
        "Yuki M. Asano",
        "Iro Laina",
        "Christian Rupprecht",
        "Nakamasa Inoue",
        "Rio Yokota",
        "Hirokatsu Kataoka",
        "Yoshimitsu Aoki"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-01",
      "update_time": "2024-09-01",
      "comments": "Accepted to ECCV2024",
      "repo_url": "https://github.com/gohtanii/DiverSeg-dataset"
    },
    "2409.00764": {
      "paper_id": "2409.00764v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.00764v1",
      "paper_key": "2409.00764",
      "paper_title": "tl;dr: Chill, y'all: AI Will Not Devour SE",
      "paper_url": "http://arxiv.org/abs/2409.00764v1",
      "paper_abstract": "Social media provide a steady diet of dire warnings that artificial intelligence (AI) will make software engineering (SE) irrelevant or obsolete. To the contrary, the engineering discipline of software is rich and robust; it encompasses the full scope of software design, development, deployment, and practical use; and it has regularly assimilated radical new offerings from AI. Current AI innovations such as machine learning, large language models (LLMs) and generative AI will offer new opportunities to extend the models and methods of SE. They may automate some routine development processes, and they will bring new kinds of components and architectures. If we're fortunate they may force SE to rethink what we mean by correctness and reliability. They will not, however, render SE irrelevant.",
      "paper_authors": [
        "Eunsuk Kang",
        "Mary Shaw"
      ],
      "primary_category": "cs.SE",
      "publish_time": "2024-09-01",
      "update_time": "2024-09-01",
      "comments": "13 pages, accepted at Onward! Essays at SPLASH 2024",
      "repo_url": "#"
    },
    "2409.00399": {
      "paper_id": "2409.00399v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.00399v1",
      "paper_key": "2409.00399",
      "paper_title": "Rethinking Backdoor Detection Evaluation for Language Models",
      "paper_url": "http://arxiv.org/abs/2409.00399v1",
      "paper_abstract": "Backdoor attacks, in which a model behaves maliciously when given an attacker-specified trigger, pose a major security risk for practitioners who depend on publicly released language models. Backdoor detection methods aim to detect whether a released model contains a backdoor, so that practitioners can avoid such vulnerabilities. While existing backdoor detection methods have high accuracy in detecting backdoored models on standard benchmarks, it is unclear whether they can robustly identify backdoors in the wild. In this paper, we examine the robustness of backdoor detectors by manipulating different factors during backdoor planting. We find that the success of existing methods highly depends on how intensely the model is trained on poisoned data during backdoor planting. Specifically, backdoors planted with either more aggressive or more conservative training are significantly more difficult to detect than the default ones. Our results highlight a lack of robustness of existing backdoor detectors and the limitations in current benchmark construction.",
      "paper_authors": [
        "Jun Yan",
        "Wenjie Jacky Mo",
        "Xiang Ren",
        "Robin Jia"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-08-31",
      "update_time": "2024-08-31",
      "comments": null,
      "repo_url": "#"
    },
    "2409.00250": {
      "paper_id": "2409.00250v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.00250v1",
      "paper_key": "2409.00250",
      "paper_title": "Medical Report Generation Is A Multi-label Classification Problem",
      "paper_url": "http://arxiv.org/abs/2409.00250v1",
      "paper_abstract": "Medical report generation is a critical task in healthcare that involves the automatic creation of detailed and accurate descriptions from medical images. Traditionally, this task has been approached as a sequence generation problem, relying on vision-and-language techniques to generate coherent and contextually relevant reports. However, in this paper, we propose a novel perspective: rethinking medical report generation as a multi-label classification problem. By framing the task this way, we leverage the radiology nodes from the commonly used knowledge graph, which can be better captured through classification techniques. To verify our argument, we introduce a novel report generation framework based on BLIP integrated with classified key nodes, which allows for effective report generation with accurate classification of multiple key aspects within the medical images. This approach not only simplifies the report generation process but also significantly enhances performance metrics. Our extensive experiments demonstrate that leveraging key nodes can achieve state-of-the-art (SOTA) performance, surpassing existing approaches across two benchmark datasets. The results underscore the potential of re-envisioning traditional tasks with innovative methodologies, paving the way for more efficient and accurate medical report generation.",
      "paper_authors": [
        "Yijian Fan",
        "Zhenbang Yang",
        "Rui Liu",
        "Mingjie Li",
        "Xiaojun Chang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-30",
      "update_time": "2024-08-30",
      "comments": "Accepted to 2024 IEEE International Conference on Medical Artificial\n  Intelligence",
      "repo_url": "#"
    },
    "2408.17052": {
      "paper_id": "2408.17052v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.17052v1",
      "paper_key": "2408.17052",
      "paper_title": "Can We Leave Deepfake Data Behind in Training Deepfake Detector?",
      "paper_url": "http://arxiv.org/abs/2408.17052v1",
      "paper_abstract": "The generalization ability of deepfake detectors is vital for their applications in real-world scenarios. One effective solution to enhance this ability is to train the models with manually-blended data, which we termed \"blendfake\", encouraging models to learn generic forgery artifacts like blending boundary. Interestingly, current SoTA methods utilize blendfake without incorporating any deepfake data in their training process. This is likely because previous empirical observations suggest that vanilla hybrid training (VHT), which combines deepfake and blendfake data, results in inferior performance to methods using only blendfake data (so-called \"1+1<2\"). Therefore, a critical question arises: Can we leave deepfake behind and rely solely on blendfake data to train an effective deepfake detector? Intuitively, as deepfakes also contain additional informative forgery clues (e.g., deep generative artifacts), excluding all deepfake data in training deepfake detectors seems counter-intuitive. In this paper, we rethink the role of blendfake in detecting deepfakes and formulate the process from \"real to blendfake to deepfake\" to be a progressive transition. Specifically, blendfake and deepfake can be explicitly delineated as the oriented pivot anchors between \"real-to-fake\" transitions. The accumulation of forgery information should be oriented and progressively increasing during this transition process. To this end, we propose an Oriented Progressive Regularizor (OPR) to establish the constraints that compel the distribution of anchors to be discretely arranged. Furthermore, we introduce feature bridging to facilitate the smooth transition between adjacent anchors. Extensive experiments confirm that our design allows leveraging forgery information from both blendfake and deepfake effectively and comprehensively.",
      "paper_authors": [
        "Jikang Cheng",
        "Zhiyuan Yan",
        "Ying Zhang",
        "Yuhao Luo",
        "Zhongyuan Wang",
        "Chen Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-30",
      "update_time": "2024-08-30",
      "comments": null,
      "repo_url": "#"
    },
    "2408.16296": {
      "paper_id": "2408.16296v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.16296v1",
      "paper_key": "2408.16296",
      "paper_title": "Rethinking Sparse Lexical Representations for Image Retrieval in the Age of Rising Multi-Modal Large Language Models",
      "paper_url": "http://arxiv.org/abs/2408.16296v1",
      "paper_abstract": "In this paper, we rethink sparse lexical representations for image retrieval. By utilizing multi-modal large language models (M-LLMs) that support visual prompting, we can extract image features and convert them into textual data, enabling us to utilize efficient sparse retrieval algorithms employed in natural language processing for image retrieval tasks. To assist the LLM in extracting image features, we apply data augmentation techniques for key expansion and analyze the impact with a metric for relevance between images and textual data. We empirically show the superior precision and recall performance of our image retrieval method compared to conventional vision-language model-based methods on the MS-COCO, PASCAL VOC, and NUS-WIDE datasets in a keyword-based image retrieval scenario, where keywords serve as search queries. We also demonstrate that the retrieval performance can be improved by iteratively incorporating keywords into search queries.",
      "paper_authors": [
        "Kengo Nakata",
        "Daisuke Miyashita",
        "Youyang Ng",
        "Yasuto Hoshi",
        "Jun Deguchi"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-29",
      "update_time": "2024-08-29",
      "comments": "Accepted to ECCV 2024 Workshops: 2nd Workshop on Traditional Computer\n  Vision in the Age of Deep Learning (TradiCV)",
      "repo_url": "#"
    },
    "2408.15966": {
      "paper_id": "2408.15966v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.15966v2",
      "paper_key": "2408.15966",
      "paper_title": "More Text, Less Point: Towards 3D Data-Efficient Point-Language Understanding",
      "paper_url": "http://arxiv.org/abs/2408.15966v2",
      "paper_abstract": "Enabling Large Language Models (LLMs) to comprehend the 3D physical world remains a significant challenge. Due to the lack of large-scale 3D-text pair datasets, the success of LLMs has yet to be replicated in 3D understanding. In this paper, we rethink this issue and propose a new task: 3D Data-Efficient Point-Language Understanding. The goal is to enable LLMs to achieve robust 3D object understanding with minimal 3D point cloud and text data pairs. To address this task, we introduce GreenPLM, which leverages more text data to compensate for the lack of 3D data. First, inspired by using CLIP to align images and text, we utilize a pre-trained point cloud-text encoder to map the 3D point cloud space to the text space. This mapping leaves us to seamlessly connect the text space with LLMs. Once the point-text-LLM connection is established, we further enhance text-LLM alignment by expanding the intermediate text space, thereby reducing the reliance on 3D point cloud data. Specifically, we generate 6M free-text descriptions of 3D objects, and design a three-stage training strategy to help LLMs better explore the intrinsic connections between different modalities. To achieve efficient modality alignment, we design a zero-parameter cross-attention module for token pooling. Extensive experimental results show that GreenPLM requires only 12% of the 3D training data used by existing state-of-the-art models to achieve superior 3D understanding. Remarkably, GreenPLM also achieves competitive performance using text-only data. The code and weights are available at: https://github.com/TangYuan96/GreenPLM.",
      "paper_authors": [
        "Yuan Tang",
        "Xu Han",
        "Xianzhi Li",
        "Qiao Yu",
        "Jinfeng Xu",
        "Yixue Hao",
        "Long Hu",
        "Min Chen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-28",
      "update_time": "2024-09-05",
      "comments": null,
      "repo_url": "https://github.com/tangyuan96/greenplm"
    },
    "2408.15091": {
      "paper_id": "2408.15091v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.15091v1",
      "paper_key": "2408.15091",
      "paper_title": "Relation Also Knows: Rethinking the Recall and Editing of Factual Associations in Auto-Regressive Transformer Language Models",
      "paper_url": "http://arxiv.org/abs/2408.15091v1",
      "paper_abstract": "The storage and recall of factual associations in auto-regressive transformer language models (LMs) have drawn a great deal of attention, inspiring knowledge editing by directly modifying the located model weights. Most editing works achieve knowledge editing under the guidance of existing interpretations of knowledge recall that mainly focus on subject knowledge. However, these interpretations are seriously flawed, neglecting relation information and leading to the over-generalizing problem for editing. In this work, we discover a novel relation-focused perspective to interpret the knowledge recall of transformer LMs during inference and apply it on knowledge editing to avoid over-generalizing. Experimental results on the dataset supplemented with a new R-Specificity criterion demonstrate that our editing approach significantly alleviates over-generalizing while remaining competitive on other criteria, breaking the domination of subject-focused editing for future research.",
      "paper_authors": [
        "Xiyu Liu",
        "Zhengxiao Liu",
        "Naibin Gu",
        "Zheng Lin",
        "Wanli Ma",
        "Ji Xiang",
        "Weiping Wang"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-08-27",
      "update_time": "2024-08-27",
      "comments": null,
      "repo_url": "#"
    },
    "2408.14319": {
      "paper_id": "2408.14319v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.14319v1",
      "paper_key": "2408.14319",
      "paper_title": "Rethinking Knowledge Transfer in Learning Using Privileged Information",
      "paper_url": "http://arxiv.org/abs/2408.14319v1",
      "paper_abstract": "In supervised machine learning, privileged information (PI) is information that is unavailable at inference, but is accessible during training time. Research on learning using privileged information (LUPI) aims to transfer the knowledge captured in PI onto a model that can perform inference without PI. It seems that this extra bit of information ought to make the resulting model better. However, finding conclusive theoretical or empirical evidence that supports the ability to transfer knowledge using PI has been challenging. In this paper, we critically examine the assumptions underlying existing theoretical analyses and argue that there is little theoretical justification for when LUPI should work. We analyze LUPI methods and reveal that apparent improvements in empirical risk of existing research may not directly result from PI. Instead, these improvements often stem from dataset anomalies or modifications in model design misguidedly attributed to PI. Our experiments for a wide variety of application domains further demonstrate that state-of-the-art LUPI approaches fail to effectively transfer knowledge from PI. Thus, we advocate for practitioners to exercise caution when working with PI to avoid unintended inductive biases.",
      "paper_authors": [
        "Danil Provodin",
        "Bram van den Akker",
        "Christina Katsimerou",
        "Maurits Kaptein",
        "Mykola Pechenizkiy"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-08-26",
      "update_time": "2024-08-26",
      "comments": null,
      "repo_url": "https://github.com/danilprov/rethinking_lupi"
    },
    "2408.13498": {
      "paper_id": "2408.13498v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.13498v1",
      "paper_key": "2408.13498",
      "paper_title": "Rethinking State Disentanglement in Causal Reinforcement Learning",
      "paper_url": "http://arxiv.org/abs/2408.13498v1",
      "paper_abstract": "One of the significant challenges in reinforcement learning (RL) when dealing with noise is estimating latent states from observations. Causality provides rigorous theoretical support for ensuring that the underlying states can be uniquely recovered through identifiability. Consequently, some existing work focuses on establishing identifiability from a causal perspective to aid in the design of algorithms. However, these results are often derived from a purely causal viewpoint, which may overlook the specific RL context. We revisit this research line and find that incorporating RL-specific context can reduce unnecessary assumptions in previous identifiability analyses for latent states. More importantly, removing these assumptions allows algorithm design to go beyond the earlier boundaries constrained by them. Leveraging these insights, we propose a novel approach for general partially observable Markov Decision Processes (POMDPs) by replacing the complicated structural constraints in previous methods with two simple constraints for transition and reward preservation. With the two constraints, the proposed algorithm is guaranteed to disentangle state and noise that is faithful to the underlying dynamics. Empirical evidence from extensive benchmark control tasks demonstrates the superiority of our approach over existing counterparts in effectively disentangling state belief from noise.",
      "paper_authors": [
        "Haiyao Cao",
        "Zhen Zhang",
        "Panpan Cai",
        "Yuhang Liu",
        "Jinan Zou",
        "Ehsan Abbasnejad",
        "Biwei Huang",
        "Mingming Gong",
        "Anton van den Hengel",
        "Javen Qinfeng Shi"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-08-24",
      "update_time": "2024-08-24",
      "comments": null,
      "repo_url": "#"
    },
    "2408.13459": {
      "paper_id": "2408.13459v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.13459v1",
      "paper_key": "2408.13459",
      "paper_title": "Rethinking Video Deblurring with Wavelet-Aware Dynamic Transformer and Diffusion Model",
      "paper_url": "http://arxiv.org/abs/2408.13459v1",
      "paper_abstract": "Current video deblurring methods have limitations in recovering high-frequency information since the regression losses are conservative with high-frequency details. Since Diffusion Models (DMs) have strong capabilities in generating high-frequency details, we consider introducing DMs into the video deblurring task. However, we found that directly applying DMs to the video deblurring task has the following problems: (1) DMs require many iteration steps to generate videos from Gaussian noise, which consumes many computational resources. (2) DMs are easily misled by the blurry artifacts in the video, resulting in irrational content and distortion of the deblurred video. To address the above issues, we propose a novel video deblurring framework VD-Diff that integrates the diffusion model into the Wavelet-Aware Dynamic Transformer (WADT). Specifically, we perform the diffusion model in a highly compact latent space to generate prior features containing high-frequency information that conforms to the ground truth distribution. We design the WADT to preserve and recover the low-frequency information in the video while utilizing the high-frequency information generated by the diffusion model. Extensive experiments show that our proposed VD-Diff outperforms SOTA methods on GoPro, DVD, BSD, and Real-World Video datasets.",
      "paper_authors": [
        "Chen Rao",
        "Guangyuan Li",
        "Zehua Lan",
        "Jiakai Sun",
        "Junsheng Luan",
        "Wei Xing",
        "Lei Zhao",
        "Huaizhong Lin",
        "Jianfeng Dong",
        "Dalong Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-24",
      "update_time": "2024-08-24",
      "comments": "accepted by ECCV2024",
      "repo_url": "#"
    },
    "2409.05872": {
      "paper_id": "2409.05872v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05872v1",
      "paper_key": "2409.05872",
      "paper_title": "CSRec: Rethinking Sequential Recommendation from A Causal Perspective",
      "paper_url": "http://arxiv.org/abs/2409.05872v1",
      "paper_abstract": "The essence of sequential recommender systems (RecSys) lies in understanding how users make decisions. Most existing approaches frame the task as sequential prediction based on users' historical purchase records. While effective in capturing users' natural preferences, this formulation falls short in accurately modeling actual recommendation scenarios, particularly in accounting for how unsuccessful recommendations influence future purchases. Furthermore, the impact of the RecSys itself on users' decisions has not been appropriately isolated and quantitatively analyzed. To address these challenges, we propose a novel formulation of sequential recommendation, termed Causal Sequential Recommendation (CSRec). Instead of predicting the next item in the sequence, CSRec aims to predict the probability of a recommended item's acceptance within a sequential context and backtrack how current decisions are made. Critically, CSRec facilitates the isolation of various factors that affect users' final decisions, especially the influence of the recommender system itself, thereby opening new avenues for the design of recommender systems. CSRec can be seamlessly integrated into existing methodologies. Experimental evaluations on both synthetic and real-world datasets demonstrate that the proposed implementation significantly improves upon state-of-the-art baselines.",
      "paper_authors": [
        "Xiaoyu Liu",
        "Jiaxin Yuan",
        "Yuhang Zhou",
        "Jingling Li",
        "Furong Huang",
        "Wei Ai"
      ],
      "primary_category": "cs.IR",
      "publish_time": "2024-08-23",
      "update_time": "2024-08-23",
      "comments": null,
      "repo_url": "https://github.com/margotyjx/CSRec_repo"
    },
    "2408.13385": {
      "paper_id": "2408.13385v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.13385v1",
      "paper_key": "2408.13385",
      "paper_title": "MICM: Rethinking Unsupervised Pretraining for Enhanced Few-shot Learning",
      "paper_url": "http://arxiv.org/abs/2408.13385v1",
      "paper_abstract": "Humans exhibit a remarkable ability to learn quickly from a limited number of labeled samples, a capability that starkly contrasts with that of current machine learning systems. Unsupervised Few-Shot Learning (U-FSL) seeks to bridge this divide by reducing reliance on annotated datasets during initial training phases. In this work, we first quantitatively assess the impacts of Masked Image Modeling (MIM) and Contrastive Learning (CL) on few-shot learning tasks. Our findings highlight the respective limitations of MIM and CL in terms of discriminative and generalization abilities, which contribute to their underperformance in U-FSL contexts. To address these trade-offs between generalization and discriminability in unsupervised pretraining, we introduce a novel paradigm named Masked Image Contrastive Modeling (MICM). MICM creatively combines the targeted object learning strength of CL with the generalized visual feature learning capability of MIM, significantly enhancing its efficacy in downstream few-shot learning inference. Extensive experimental analyses confirm the advantages of MICM, demonstrating significant improvements in both generalization and discrimination capabilities for few-shot learning. Our comprehensive quantitative evaluations further substantiate the superiority of MICM, showing that our two-stage U-FSL framework based on MICM markedly outperforms existing leading baselines.",
      "paper_authors": [
        "Zhenyu Zhang",
        "Guangyao Chen",
        "Yixiong Zou",
        "Zhimeng Huang",
        "Yuhua Li",
        "Ruixuan Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-23",
      "update_time": "2024-08-23",
      "comments": "ACMMM 2024 (Oral)",
      "repo_url": "https://github.com/icgy96/micm"
    },
    "2408.11785": {
      "paper_id": "2408.11785v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.11785v1",
      "paper_key": "2408.11785",
      "paper_title": "Timeline and Boundary Guided Diffusion Network for Video Shadow Detection",
      "paper_url": "http://arxiv.org/abs/2408.11785v1",
      "paper_abstract": "Video Shadow Detection (VSD) aims to detect the shadow masks with frame sequence. Existing works suffer from inefficient temporal learning. Moreover, few works address the VSD problem by considering the characteristic (i.e., boundary) of shadow. Motivated by this, we propose a Timeline and Boundary Guided Diffusion (TBGDiff) network for VSD where we take account of the past-future temporal guidance and boundary information jointly. In detail, we design a Dual Scale Aggregation (DSA) module for better temporal understanding by rethinking the affinity of the long-term and short-term frames for the clipped video. Next, we introduce Shadow Boundary Aware Attention (SBAA) to utilize the edge contexts for capturing the characteristics of shadows. Moreover, we are the first to introduce the Diffusion model for VSD in which we explore a Space-Time Encoded Embedding (STEE) to inject the temporal guidance for Diffusion to conduct shadow detection. Benefiting from these designs, our model can not only capture the temporal information but also the shadow property. Extensive experiments show that the performance of our approach overtakes the state-of-the-art methods, verifying the effectiveness of our components. We release the codes, weights, and results at \\url{https://github.com/haipengzhou856/TBGDiff}.",
      "paper_authors": [
        "Haipeng Zhou",
        "Honqiu Wang",
        "Tian Ye",
        "Zhaohu Xing",
        "Jun Ma",
        "Ping Li",
        "Qiong Wang",
        "Lei Zhu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-21",
      "update_time": "2024-08-21",
      "comments": "ACM MM2024",
      "repo_url": "https://github.com/haipengzhou856/tbgdiff"
    },
    "2408.11535": {
      "paper_id": "2408.11535v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.11535v2",
      "paper_key": "2408.11535",
      "paper_title": "SAM-REF: Rethinking Image-Prompt Synergy for Refinement in Segment Anything",
      "paper_url": "http://arxiv.org/abs/2408.11535v2",
      "paper_abstract": "The advent of the Segment Anything Model (SAM) marks a significant milestone for interactive segmentation using generalist models. As a late fusion model, SAM extracts image embeddings once and merges them with prompts in later interactions. This strategy limits the models ability to extract detailed information from the prompted target zone. Current specialist models utilize the early fusion strategy that encodes the combination of images and prompts to target the prompted objects, yet repetitive complex computations on the images result in high latency. The key to these issues is efficiently synergizing the images and prompts. We propose SAM-REF, a two-stage refinement framework that fully integrates images and prompts globally and locally while maintaining the accuracy of early fusion and the efficiency of late fusion. The first-stage GlobalDiff Refiner is a lightweight early fusion network that combines the whole image and prompts, focusing on capturing detailed information for the entire object. The second-stage PatchDiff Refiner locates the object detail window according to the mask and prompts, then refines the local details of the object. Experimentally, we demonstrated the high effectiveness and efficiency of our method in tackling complex cases with multiple interactions. Our SAM-REF model outperforms the current state-of-the-art method in most metrics on segmentation quality without compromising efficiency.",
      "paper_authors": [
        "Chongkai Yu",
        "Anqi Li",
        "Xiaochao Qu",
        "Luoqi Liu",
        "Ting Liu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-21",
      "update_time": "2024-08-22",
      "comments": null,
      "repo_url": "#"
    },
    "2408.11875": {
      "paper_id": "2408.11875v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.11875v1",
      "paper_key": "2408.11875",
      "paper_title": "Hierarchical Retrieval-Augmented Generation Model with Rethink for Multi-hop Question Answering",
      "paper_url": "http://arxiv.org/abs/2408.11875v1",
      "paper_abstract": "Multi-hop Question Answering (QA) necessitates complex reasoning by integrating multiple pieces of information to resolve intricate questions. However, existing QA systems encounter challenges such as outdated information, context window length limitations, and an accuracy-quantity trade-off. To address these issues, we propose a novel framework, the Hierarchical Retrieval-Augmented Generation Model with Rethink (HiRAG), comprising Decomposer, Definer, Retriever, Filter, and Summarizer five key modules. We introduce a new hierarchical retrieval strategy that incorporates both sparse retrieval at the document level and dense retrieval at the chunk level, effectively integrating their strengths. Additionally, we propose a single-candidate retrieval method to mitigate the limitations of multi-candidate retrieval. We also construct two new corpora, Indexed Wikicorpus and Profile Wikicorpus, to address the issues of outdated and insufficient knowledge.   Our experimental results on four datasets demonstrate that HiRAG outperforms state-of-the-art models across most metrics, and our Indexed Wikicorpus is effective. The code for HiRAG is available at https://github.com/2282588541a/HiRAG",
      "paper_authors": [
        "Xiaoming Zhang",
        "Ming Wang",
        "Xiaocui Yang",
        "Daling Wang",
        "Shi Feng",
        "Yifei Zhang"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-08-20",
      "update_time": "2024-08-20",
      "comments": "undereview",
      "repo_url": "https://github.com/2282588541a/hirag"
    },
    "2408.10627": {
      "paper_id": "2408.10627v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.10627v1",
      "paper_key": "2408.10627",
      "paper_title": "Rethinking Video Segmentation with Masked Video Consistency: Did the Model Learn as Intended?",
      "paper_url": "http://arxiv.org/abs/2408.10627v1",
      "paper_abstract": "Video segmentation aims at partitioning video sequences into meaningful segments based on objects or regions of interest within frames. Current video segmentation models are often derived from image segmentation techniques, which struggle to cope with small-scale or class-imbalanced video datasets. This leads to inconsistent segmentation results across frames. To address these issues, we propose a training strategy Masked Video Consistency, which enhances spatial and temporal feature aggregation. MVC introduces a training strategy that randomly masks image patches, compelling the network to predict the entire semantic segmentation, thus improving contextual information integration. Additionally, we introduce Object Masked Attention (OMA) to optimize the cross-attention mechanism by reducing the impact of irrelevant queries, thereby enhancing temporal modeling capabilities. Our approach, integrated into the latest decoupled universal video segmentation framework, achieves state-of-the-art performance across five datasets for three video segmentation tasks, demonstrating significant improvements over previous methods without increasing model parameters.",
      "paper_authors": [
        "Chen Liang",
        "Qiang Guo",
        "Xiaochao Qu",
        "Luoqi Liu",
        "Ting Liu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-20",
      "update_time": "2024-08-20",
      "comments": null,
      "repo_url": "#"
    },
    "2409.00046": {
      "paper_id": "2409.00046v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.00046v3",
      "paper_key": "2409.00046",
      "paper_title": "Rethinking Molecular Design: Integrating Latent Variable and Auto-Regressive Models for Goal Directed Generation",
      "paper_url": "http://arxiv.org/abs/2409.00046v3",
      "paper_abstract": "De novo molecule design has become a highly active research area, advanced significantly through the use of state-of-the-art generative models. Despite these advances, several fundamental questions remain unanswered as the field increasingly focuses on more complex generative models and sophisticated molecular representations as an answer to the challenges of drug design. In this paper, we return to the simplest representation of molecules, and investigate overlooked limitations of classical generative approaches, particularly Variational Autoencoders (VAEs) and auto-regressive models. We propose a hybrid model in the form of a novel regularizer that leverages the strengths of both to improve validity, conditional generation, and style transfer of molecular sequences. Additionally, we provide an in depth discussion of overlooked assumptions of these models' behaviour.",
      "paper_authors": [
        "Heath Arthur-Loui",
        "Amina Mollaysa",
        "Michael Krauthammer"
      ],
      "primary_category": "q-bio.BM",
      "publish_time": "2024-08-19",
      "update_time": "2024-09-06",
      "comments": null,
      "repo_url": "https://github.com/HeathArhturLouis/Rethinking-Molecular-Design-Integrating-Latent-Variable-and-Autoregressive-Models-for-Enhanced-Goal"
    },
    "2408.09469": {
      "paper_id": "2408.09469v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.09469v2",
      "paper_key": "2408.09469",
      "paper_title": "Enhancing Adversarial Transferability with Adversarial Weight Tuning",
      "paper_url": "http://arxiv.org/abs/2408.09469v2",
      "paper_abstract": "Deep neural networks (DNNs) are vulnerable to adversarial examples (AEs) that mislead the model while appearing benign to human observers. A critical concern is the transferability of AEs, which enables black-box attacks without direct access to the target model. However, many previous attacks have failed to explain the intrinsic mechanism of adversarial transferability. In this paper, we rethink the property of transferable AEs and reformalize the formulation of transferability. Building on insights from this mechanism, we analyze the generalization of AEs across models with different architectures and prove that we can find a local perturbation to mitigate the gap between surrogate and target models. We further establish the inner connections between model smoothness and flat local maxima, both of which contribute to the transferability of AEs. Further, we propose a new adversarial attack algorithm, \\textbf{A}dversarial \\textbf{W}eight \\textbf{T}uning (AWT), which adaptively adjusts the parameters of the surrogate model using generated AEs to optimize the flat local maxima and model smoothness simultaneously, without the need for extra data. AWT is a data-free tuning method that combines gradient-based and model-based attack methods to enhance the transferability of AEs. Extensive experiments on a variety of models with different architectures on ImageNet demonstrate that AWT yields superior performance over other attacks, with an average increase of nearly 5\\% and 10\\% attack success rates on CNN-based and Transformer-based models, respectively, compared to state-of-the-art attacks.",
      "paper_authors": [
        "Jiahao Chen",
        "Zhou Feng",
        "Rui Zeng",
        "Yuwen Pu",
        "Chunyi Zhou",
        "Yi Jiang",
        "Yuyou Gan",
        "Jinbao Li",
        "Shouling Ji"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-08-18",
      "update_time": "2024-08-20",
      "comments": "13 pages",
      "repo_url": "#"
    },
    "2409.03760": {
      "paper_id": "2409.03760v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.03760v1",
      "paper_key": "2409.03760",
      "paper_title": "Rethinking Deep Learning: Propagating Information in Neural Networks without Backpropagation and Statistical Optimization",
      "paper_url": "http://arxiv.org/abs/2409.03760v1",
      "paper_abstract": "Developing strong AI signifies the arrival of technological singularity, contributing greatly to advancing human civilization and resolving social issues. Neural networks (NNs) and deep learning, which utilize NNs, are expected to lead to strong AI due to their biological neural system-mimicking structures. However, the statistical weight optimization techniques commonly used, such as error backpropagation and loss functions, may hinder the mimicry of neural systems. This study discusses the information propagation capabilities and potential practical applications of NNs as neural system mimicking structures by solving the handwritten character recognition problem in the Modified National Institute of Standards and Technology (MNIST) database without using statistical weight optimization techniques like error backpropagation. In this study, the NNs architecture comprises fully connected layers using step functions as activation functions, with 0-15 hidden layers, and no weight updates. The accuracy is calculated by comparing the average output vectors of the training data for each label with the output vectors of the test data, based on vector similarity. The results showed that the maximum accuracy achieved is around 80%. This indicates that NNs can propagate information correctly without using statistical weight optimization. Additionally, the accuracy decreased with an increasing number of hidden layers. This is attributed to the decrease in the variance of the output vectors as the number of hidden layers increases, suggesting that the output data becomes smooth. This study's NNs and accuracy calculation methods are simple and have room for various improvements. Moreover, creating a feedforward NNs that repeatedly cycles through 'input -> processing -> output -> environmental response -> input -> ...' could pave the way for practical software applications.",
      "paper_authors": [
        "Kei Itoh"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-08-18",
      "update_time": "2024-08-18",
      "comments": "9 pages, 3 figures",
      "repo_url": "#"
    },
    "2408.11071": {
      "paper_id": "2408.11071v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.11071v1",
      "paper_key": "2408.11071",
      "paper_title": "DiffZOO: A Purely Query-Based Black-Box Attack for Red-teaming Text-to-Image Generative Model via Zeroth Order Optimization",
      "paper_url": "http://arxiv.org/abs/2408.11071v1",
      "paper_abstract": "Current text-to-image (T2I) synthesis diffusion models raise misuse concerns, particularly in creating prohibited or not-safe-for-work (NSFW) images. To address this, various safety mechanisms and red teaming attack methods are proposed to enhance or expose the T2I model's capability to generate unsuitable content. However, many red teaming attack methods assume knowledge of the text encoders, limiting their practical usage. In this work, we rethink the case of \\textit{purely black-box} attacks without prior knowledge of the T2l model. To overcome the unavailability of gradients and the inability to optimize attacks within a discrete prompt space, we propose DiffZOO which applies Zeroth Order Optimization to procure gradient approximations and harnesses both C-PRV and D-PRV to enhance attack prompts within the discrete prompt domain. We evaluated our method across multiple safety mechanisms of the T2I diffusion model and online servers. Experiments on multiple state-of-the-art safety mechanisms show that DiffZOO attains an 8.5% higher average attack success rate than previous works, hence its promise as a practical red teaming tool for T2l models.",
      "paper_authors": [
        "Pucheng Dang",
        "Xing Hu",
        "Dong Li",
        "Rui Zhang",
        "Qi Guo",
        "Kaidi Xu"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-08-18",
      "update_time": "2024-08-18",
      "comments": null,
      "repo_url": "#"
    },
    "2408.08765": {
      "paper_id": "2408.08765v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.08765v1",
      "paper_key": "2408.08765",
      "paper_title": "Rethinking Generative Semantic Communication for Multi-User Systems with Multi-Modal LLM",
      "paper_url": "http://arxiv.org/abs/2408.08765v1",
      "paper_abstract": "The surge in connected devices in 6G with typical massive access scenarios, such as smart agriculture, and smart cities, poses significant challenges to unsustainable traditional communication with limited radio resources and already high system complexity. Fortunately, the booming artificial intelligence technology and the growing computational power of devices offer a promising 6G enabler: semantic communication (SemCom). However, existing deep learning-based SemCom paradigms struggle to extend to multi-user scenarios due to their rigid end-to-end training approach. Consequently, to truly empower 6G networks with this critical technology, this article rethinks generative SemCom for multi-user system with multi-modal large language model (MLLM), and propose a novel framework called \"M2GSC\". In this framework, the MLLM, which serves as shared knowledge base (SKB), plays three critical roles for complex tasks, spawning a series of benefits such as semantic encoding standardization and semantic decoding personalization. Meanwhile, to enhance the performance of M2GSC framework and to advance its implementation in 6G, we highlight three research directions on M2GSC framework, namely, upgrading SKB to closed loop agent, adaptive semantic encoding offloading, and streamlined semantic decoding offloading. Finally, a case study is conducted to demonstrate the preliminary validation on the effectiveness of the M2GSC framework in terms of streamlined decoding offloading.",
      "paper_authors": [
        "Wanting Yang",
        "Zehui Xiong",
        "Shiwen Mao",
        "Tony Q. S. Quek",
        "Ping Zhang",
        "Merouane Debbah",
        "Rahim Tafazolli"
      ],
      "primary_category": "cs.NI",
      "publish_time": "2024-08-16",
      "update_time": "2024-08-16",
      "comments": null,
      "repo_url": "#"
    },
    "2408.08499": {
      "paper_id": "2408.08499v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.08499v1",
      "paper_key": "2408.08499",
      "paper_title": "The Limitations of Model Retraining in the Face of Performativity",
      "paper_url": "http://arxiv.org/abs/2408.08499v1",
      "paper_abstract": "We study stochastic optimization in the context of performative shifts, where the data distribution changes in response to the deployed model. We demonstrate that naive retraining can be provably suboptimal even for simple distribution shifts. The issue worsens when models are retrained given a finite number of samples at each retraining step. We show that adding regularization to retraining corrects both of these issues, attaining provably optimal models in the face of distribution shifts. Our work advocates rethinking how machine learning models are retrained in the presence of performative effects.",
      "paper_authors": [
        "Anmol Kabra",
        "Kumar Kshitij Patel"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-08-16",
      "update_time": "2024-08-16",
      "comments": "Accepted to 2024 ICML Workshop on Humans, Algorithmic Decision-Making\n  and Society",
      "repo_url": "#"
    },
    "2408.08228": {
      "paper_id": "2408.08228v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.08228v1",
      "paper_key": "2408.08228",
      "paper_title": "Rethinking Medical Anomaly Detection in Brain MRI: An Image Quality Assessment Perspective",
      "paper_url": "http://arxiv.org/abs/2408.08228v1",
      "paper_abstract": "Reconstruction-based methods, particularly those leveraging autoencoders, have been widely adopted to perform anomaly detection in brain MRI. While most existing works try to improve detection accuracy by proposing new model structures or algorithms, we tackle the problem through image quality assessment, an underexplored perspective in the field. We propose a fusion quality loss function that combines Structural Similarity Index Measure loss with l1 loss, offering a more comprehensive evaluation of reconstruction quality. Additionally, we introduce a data pre-processing strategy that enhances the average intensity ratio (AIR) between normal and abnormal regions, further improving the distinction of anomalies. By fusing the aforementioned two methods, we devise the image quality assessment (IQA) approach. The proposed IQA approach achieves significant improvements (>10%) in terms of Dice coefficient (DICE) and Area Under the Precision-Recall Curve (AUPRC) on the BraTS21 (T2, FLAIR) and MSULB datasets when compared with state-of-the-art methods. These results highlight the importance of invoking the comprehensive image quality assessment in medical anomaly detection and provide a new perspective for future research in this field.",
      "paper_authors": [
        "Zixuan Pan",
        "Jun Xia",
        "Zheyu Yan",
        "Guoyue Xu",
        "Yawen Wu",
        "Zhenge Jia",
        "Jianxu Chen",
        "Yiyu Shi"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2024-08-15",
      "update_time": "2024-08-15",
      "comments": null,
      "repo_url": "https://github.com/zx-pan/medanomalydetection-iqa"
    },
    "2408.07613": {
      "paper_id": "2408.07613v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.07613v1",
      "paper_key": "2408.07613",
      "paper_title": "Rethinking the Key Factors for the Generalization of Remote Sensing Stereo Matching Networks",
      "paper_url": "http://arxiv.org/abs/2408.07613v1",
      "paper_abstract": "Stereo matching, a critical step of 3D reconstruction, has fully shifted towards deep learning due to its strong feature representation of remote sensing images. However, ground truth for stereo matching task relies on expensive airborne LiDAR data, thus making it difficult to obtain enough samples for supervised learning. To improve the generalization ability of stereo matching networks on cross-domain data from different sensors and scenarios, in this paper, we dedicate to study key training factors from three perspectives. (1) For the selection of training dataset, it is important to select data with similar regional target distribution as the test set instead of utilizing data from the same sensor. (2) For model structure, cascaded structure that flexibly adapts to different sizes of features is preferred. (3) For training manner, unsupervised methods generalize better than supervised methods, and we design an unsupervised early-stop strategy to help retain the best model with pre-trained weights as the basis. Extensive experiments are conducted to support the previous findings, on the basis of which we present an unsupervised stereo matching network with good generalization performance. We release the source code and the datasets at https://github.com/Elenairene/RKF_RSSM to reproduce the results and encourage future work.",
      "paper_authors": [
        "Liting Jiang",
        "Feng Wang",
        "Wenyi Zhang",
        "Peifeng Li",
        "Hongjian You",
        "Yuming Xiang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-14",
      "update_time": "2024-08-14",
      "comments": "submitted to IEEE jstars",
      "repo_url": "#"
    },
    "2408.07416": {
      "paper_id": "2408.07416v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.07416v2",
      "paper_key": "2408.07416",
      "paper_title": "Rethinking Open-Vocabulary Segmentation of Radiance Fields in 3D Space",
      "paper_url": "http://arxiv.org/abs/2408.07416v2",
      "paper_abstract": "Understanding the 3D semantics of a scene is a fundamental problem for various scenarios such as embodied agents. While NeRFs and 3DGS excel at novel-view synthesis, previous methods for understanding their semantics have been limited to incomplete 3D understanding: their segmentation results are 2D masks and their supervision is anchored at 2D pixels. This paper revisits the problem set to pursue a better 3D understanding of a scene modeled by NeRFs and 3DGS as follows. 1) We directly supervise the 3D points to train the language embedding field. It achieves state-of-the-art accuracy without relying on multi-scale language embeddings. 2) We transfer the pre-trained language field to 3DGS, achieving the first real-time rendering speed without sacrificing training time or accuracy. 3) We introduce a 3D querying and evaluation protocol for assessing the reconstructed geometry and semantics together. Code, checkpoints, and annotations will be available online. Project page: https://hyunji12.github.io/Open3DRF",
      "paper_authors": [
        "Hyunjee Lee",
        "Youngsik Yun",
        "Jeongmin Bae",
        "Seoha Kim",
        "Youngjung Uh"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-14",
      "update_time": "2024-08-18",
      "comments": "Project page: https://hyunji12.github.io/Open3DRF",
      "repo_url": "#"
    },
    "2408.06248": {
      "paper_id": "2408.06248v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.06248v1",
      "paper_key": "2408.06248",
      "paper_title": "Rethinking Video with a Universal Event-Based Representation",
      "paper_url": "http://arxiv.org/abs/2408.06248v1",
      "paper_abstract": "Traditionally, video is structured as a sequence of discrete image frames. Recently, however, a novel video sensing paradigm has emerged which eschews video frames entirely. These \"event\" sensors aim to mimic the human vision system with asynchronous sensing, where each pixel has an independent, sparse data stream. While these cameras enable high-speed and high-dynamic-range sensing, researchers often revert to a framed representation of the event data for existing applications, or build bespoke applications for a particular camera's event data type. At the same time, classical video systems have significant computational redundancy at the application layer, since pixel samples are repeated across frames in the uncompressed domain.   To address the shortcomings of existing systems, I introduce Address, Decimation, {\\Delta}t Event Representation (AD{\\Delta}ER, pronounced \"adder\"), a novel intermediate video representation and system framework. The framework transcodes a variety of framed and event camera sources into a single event-based representation, which supports source-modeled lossy compression and backward compatibility with traditional frame-based applications. I demonstrate that AD{\\Delta}ER achieves state-of-the-art application speed and compression performance for scenes with high temporal redundancy. Crucially, I describe how AD{\\Delta}ER unlocks an entirely new control mechanism for computer vision: application speed can correlate with both the scene content and the level of lossy compression. Finally, I discuss the implications for event-based video on large-scale video surveillance and resource-constrained sensing.",
      "paper_authors": [
        "Andrew Freeman"
      ],
      "primary_category": "cs.MM",
      "publish_time": "2024-08-12",
      "update_time": "2024-08-12",
      "comments": "137 pages. PhD dissertation at the University of North Carolina,\n  Chapel Hill",
      "repo_url": "#"
    },
    "2408.05098": {
      "paper_id": "2408.05098v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.05098v1",
      "paper_key": "2408.05098",
      "paper_title": "Overcoming the Limitations of Layer Synchronization in Spiking Neural Networks",
      "paper_url": "http://arxiv.org/abs/2408.05098v1",
      "paper_abstract": "Currently, neural-network processing in machine learning applications relies on layer synchronization, whereby neurons in a layer aggregate incoming currents from all neurons in the preceding layer, before evaluating their activation function. This is practiced even in artificial Spiking Neural Networks (SNNs), which are touted as consistent with neurobiology, in spite of processing in the brain being, in fact asynchronous. A truly asynchronous system however would allow all neurons to evaluate concurrently their threshold and emit spikes upon receiving any presynaptic current. Omitting layer synchronization is potentially beneficial, for latency and energy efficiency, but asynchronous execution of models previously trained with layer synchronization may entail a mismatch in network dynamics and performance. We present a study that documents and quantifies this problem in three datasets on our simulation environment that implements network asynchrony, and we show that models trained with layer synchronization either perform sub-optimally in absence of the synchronization, or they will fail to benefit from any energy and latency reduction, when such a mechanism is in place. We then \"make ends meet\" and address the problem with unlayered backprop, a novel backpropagation-based training method, for learning models suitable for asynchronous processing. We train with it models that use different neuron execution scheduling strategies, and we show that although their neurons are more reactive, these models consistently exhibit lower overall spike density (up to 50%), reach a correct decision faster (up to 2x) without integrating all spikes, and achieve superior accuracy (up to 10% higher). Our findings suggest that asynchronous event-based (neuromorphic) AI computing is indeed more efficient, but we need to seriously rethink how we train our SNN models, to benefit from it.",
      "paper_authors": [
        "Roel Koopman",
        "Amirreza Yousefzadeh",
        "Mahyar Shahsavari",
        "Guangzhi Tang",
        "Manolis Sifalakis"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-08-09",
      "update_time": "2024-08-09",
      "comments": null,
      "repo_url": "#"
    },
    "2408.04813": {
      "paper_id": "2408.04813v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.04813v1",
      "paper_key": "2408.04813",
      "paper_title": "Rethinking Multiple Instance Learning: Developing an Instance-Level Classifier via Weakly-Supervised Self-Training",
      "paper_url": "http://arxiv.org/abs/2408.04813v1",
      "paper_abstract": "Multiple instance learning (MIL) problem is currently solved from either bag-classification or instance-classification perspective, both of which ignore important information contained in some instances and result in limited performance. For example, existing methods often face difficulty in learning hard positive instances. In this paper, we formulate MIL as a semi-supervised instance classification problem, so that all the labeled and unlabeled instances can be fully utilized to train a better classifier. The difficulty in this formulation is that all the labeled instances are negative in MIL, and traditional self-training techniques used in semi-supervised learning tend to degenerate in generating pseudo labels for the unlabeled instances in this scenario. To resolve this problem, we propose a weakly-supervised self-training method, in which we utilize the positive bag labels to construct a global constraint and a local constraint on the pseudo labels to prevent them from degenerating and force the classifier to learn hard positive instances. It is worth noting that easy positive instances are instances are far from the decision boundary in the classification process, while hard positive instances are those close to the decision boundary. Through iterative optimization, the pseudo labels can gradually approach the true labels. Extensive experiments on two MNIST synthetic datasets, five traditional MIL benchmark datasets and two histopathology whole slide image datasets show that our method achieved new SOTA performance on all of them. The code will be publicly available.",
      "paper_authors": [
        "Yingfan Ma",
        "Xiaoyuan Luo",
        "Mingzhi Yuan",
        "Xinrong Chen",
        "Manning Wang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-09",
      "update_time": "2024-08-09",
      "comments": null,
      "repo_url": "#"
    },
    "2408.04810": {
      "paper_id": "2408.04810v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.04810v1",
      "paper_key": "2408.04810",
      "paper_title": "UniBench: Visual Reasoning Requires Rethinking Vision-Language Beyond Scaling",
      "paper_url": "http://arxiv.org/abs/2408.04810v1",
      "paper_abstract": "Significant research efforts have been made to scale and improve vision-language model (VLM) training approaches. Yet, with an ever-growing number of benchmarks, researchers are tasked with the heavy burden of implementing each protocol, bearing a non-trivial computational cost, and making sense of how all these benchmarks translate into meaningful axes of progress. To facilitate a systematic evaluation of VLM progress, we introduce UniBench: a unified implementation of 50+ VLM benchmarks spanning a comprehensive range of carefully categorized capabilities from object recognition to spatial awareness, counting, and much more. We showcase the utility of UniBench for measuring progress by evaluating nearly 60 publicly available vision-language models, trained on scales of up to 12.8B samples. We find that while scaling training data or model size can boost many vision-language model capabilities, scaling offers little benefit for reasoning or relations. Surprisingly, we also discover today's best VLMs struggle on simple digit recognition and counting tasks, e.g. MNIST, which much simpler networks can solve. Where scale falls short, we find that more precise interventions, such as data quality or tailored-learning objectives offer more promise. For practitioners, we also offer guidance on selecting a suitable VLM for a given application. Finally, we release an easy-to-run UniBench code-base with the full set of 50+ benchmarks and comparisons across 59 models as well as a distilled, representative set of benchmarks that runs in 5 minutes on a single GPU.",
      "paper_authors": [
        "Haider Al-Tahan",
        "Quentin Garrido",
        "Randall Balestriero",
        "Diane Bouchacourt",
        "Caner Hazirbas",
        "Mark Ibrahim"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-09",
      "update_time": "2024-08-09",
      "comments": null,
      "repo_url": "https://github.com/facebookresearch/unibench"
    },
    "2408.02164": {
      "paper_id": "2408.02164v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.02164v2",
      "paper_key": "2408.02164",
      "paper_title": "Rethinking Affect Analysis: A Protocol for Ensuring Fairness and Consistency",
      "paper_url": "http://arxiv.org/abs/2408.02164v2",
      "paper_abstract": "Evaluating affect analysis methods presents challenges due to inconsistencies in database partitioning and evaluation protocols, leading to unfair and biased results. Previous studies claim continuous performance improvements, but our findings challenge such assertions. Using these insights, we propose a unified protocol for database partitioning that ensures fairness and comparability. We provide detailed demographic annotations (in terms of race, gender and age), evaluation metrics, and a common framework for expression recognition, action unit detection and valence-arousal estimation. We also rerun the methods with the new protocol and introduce a new leaderboards to encourage future research in affect recognition with a fairer comparison. Our annotations, code, and pre-trained models are available on \\hyperlink{https://github.com/dkollias/Fair-Consistent-Affect-Analysis}{Github}.",
      "paper_authors": [
        "Guanyu Hu",
        "Dimitrios Kollias",
        "Eleni Papadopoulou",
        "Paraskevi Tzouveli",
        "Jie Wei",
        "Xinyu Yang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-04",
      "update_time": "2024-08-07",
      "comments": "arXiv admin note: text overlap with arXiv:2405.06841",
      "repo_url": "https://github.com/dkollias/fair-consistent-affect-analysis"
    },
    "2408.01167": {
      "paper_id": "2408.01167v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.01167v1",
      "paper_key": "2408.01167",
      "paper_title": "Rethinking Pre-trained Feature Extractor Selection in Multiple Instance Learning for Whole Slide Image Classification",
      "paper_url": "http://arxiv.org/abs/2408.01167v1",
      "paper_abstract": "Multiple instance learning (MIL) has become a preferred method for classifying gigapixel whole slide images (WSIs), without requiring patch label annotation. The focus of the current MIL research stream is on the embedding-based MIL approach, which involves extracting feature vectors from patches using a pre-trained feature extractor. These feature vectors are then fed into an MIL aggregator for slide-level prediction. Despite prior research suggestions on enhancing the most commonly used ResNet50 supervised model pre-trained on ImageNet-1K, there remains a lack of clear guidance on selecting the optimal feature extractor to maximize WSI performance. This study aims at addressing this gap by examining MIL feature extractors across three dimensions: pre-training dataset, backbone model, and pre-training method. Extensive experiments were carried out on the two public WSI datasets (TCGA-NSCLC and Camelyon16) using four SOTA MIL models. The main findings indicate the following: 1) Performance significantly improves with larger and more varied pre-training datasets in both CNN and Transformer backbones. 2) `Modern and deeper' backbones greatly outperform `standard' backbones (ResNet and ViT), with performance improvements more guaranteed in Transformer-based backbones. 3) The choice of self-supervised learning (SSL) method is crucial, with the most significant benefits observed when applied to the Transformer (ViT) backbone. The study findings have practical implications, including designing more effective pathological foundation models. Our code is available at: https://anonymous.4open.science/r/MIL-Feature-Extractor-Selection",
      "paper_authors": [
        "Bryan Wong",
        "Mun Yong Yi"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-08-02",
      "update_time": "2024-08-02",
      "comments": "12 pages",
      "repo_url": "#"
    },
    "2408.00165": {
      "paper_id": "2408.00165v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.00165v2",
      "paper_key": "2408.00165",
      "paper_title": "Non-convolutional Graph Neural Networks",
      "paper_url": "http://arxiv.org/abs/2408.00165v2",
      "paper_abstract": "Rethink convolution-based graph neural networks (GNN) -- they characteristically suffer from limited expressiveness, over-smoothing, and over-squashing, and require specialized sparse kernels for efficient computation. Here, we design a simple graph learning module entirely free of convolution operators, coined random walk with unifying memory (RUM) neural network, where an RNN merges the topological and semantic graph features along the random walks terminating at each node. Relating the rich literature on RNN behavior and graph topology, we theoretically show and experimentally verify that RUM attenuates the aforementioned symptoms and is more expressive than the Weisfeiler-Lehman (WL) isomorphism test. On a variety of node- and graph-level classification and regression tasks, RUM not only achieves competitive performance, but is also robust, memory-efficient, scalable, and faster than the simplest convolutional GNNs.",
      "paper_authors": [
        "Yuanqing Wang",
        "Kyunghyun Cho"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-07-31",
      "update_time": "2024-08-04",
      "comments": null,
      "repo_url": "#"
    },
    "2408.00114": {
      "paper_id": "2408.00114v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2408.00114v2",
      "paper_key": "2408.00114",
      "paper_title": "Inductive or Deductive? Rethinking the Fundamental Reasoning Abilities of LLMs",
      "paper_url": "http://arxiv.org/abs/2408.00114v2",
      "paper_abstract": "Reasoning encompasses two typical types: deductive reasoning and inductive reasoning. Despite extensive research into the reasoning capabilities of Large Language Models (LLMs), most studies have failed to rigorously differentiate between inductive and deductive reasoning, leading to a blending of the two. This raises an essential question: In LLM reasoning, which poses a greater challenge - deductive or inductive reasoning? While the deductive reasoning capabilities of LLMs, (i.e. their capacity to follow instructions in reasoning tasks), have received considerable attention, their abilities in true inductive reasoning remain largely unexplored. To investigate into the true inductive reasoning capabilities of LLMs, we propose a novel framework, SolverLearner. This framework enables LLMs to learn the underlying function (i.e., $y = f_w(x)$), that maps input data points $(x)$ to their corresponding output values $(y)$, using only in-context examples. By focusing on inductive reasoning and separating it from LLM-based deductive reasoning, we can isolate and investigate inductive reasoning of LLMs in its pure form via SolverLearner. Our observations reveal that LLMs demonstrate remarkable inductive reasoning capabilities through SolverLearner, achieving near-perfect performance with ACC of 1 in most cases. Surprisingly, despite their strong inductive reasoning abilities, LLMs tend to relatively lack deductive reasoning capabilities, particularly in tasks involving ``counterfactual'' reasoning.",
      "paper_authors": [
        "Kewei Cheng",
        "Jingfeng Yang",
        "Haoming Jiang",
        "Zhengyang Wang",
        "Binxuan Huang",
        "Ruirui Li",
        "Shiyang Li",
        "Zheng Li",
        "Yifan Gao",
        "Xian Li",
        "Bing Yin",
        "Yizhou Sun"
      ],
      "primary_category": "cs.AI",
      "publish_time": "2024-07-31",
      "update_time": "2024-08-07",
      "comments": null,
      "repo_url": "#"
    },
    "2407.20667": {
      "paper_id": "2407.20667v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.20667v1",
      "paper_key": "2407.20667",
      "paper_title": "Rethinking the Function of Neurons in KANs",
      "paper_url": "http://arxiv.org/abs/2407.20667v1",
      "paper_abstract": "The neurons of Kolmogorov-Arnold Networks (KANs) perform a simple summation motivated by the Kolmogorov-Arnold representation theorem, which asserts that sum is the only fundamental multivariate function. In this work, we investigate the potential for identifying an alternative multivariate function for KAN neurons that may offer increased practical utility. Our empirical research involves testing various multivariate functions in KAN neurons across a range of benchmark Machine Learning tasks.   Our findings indicate that substituting the sum with the average function in KAN neurons results in significant performance enhancements compared to traditional KANs. Our study demonstrates that this minor modification contributes to the stability of training by confining the input to the spline within the effective range of the activation function. Our implementation and experiments are available at: \\url{https://github.com/Ghaith81/dropkan}",
      "paper_authors": [
        "Mohammed Ghaith Altarabichi"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-07-30",
      "update_time": "2024-07-30",
      "comments": null,
      "repo_url": "https://github.com/ghaith81/dropkan"
    },
    "2407.19714": {
      "paper_id": "2407.19714v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.19714v1",
      "paper_key": "2407.19714",
      "paper_title": "Rethinking RGB-D Fusion for Semantic Segmentation in Surgical Datasets",
      "paper_url": "http://arxiv.org/abs/2407.19714v1",
      "paper_abstract": "Surgical scene understanding is a key technical component for enabling intelligent and context aware systems that can transform various aspects of surgical interventions. In this work, we focus on the semantic segmentation task, propose a simple yet effective multi-modal (RGB and depth) training framework called SurgDepth, and show state-of-the-art (SOTA) results on all publicly available datasets applicable for this task. Unlike previous approaches, which either fine-tune SOTA segmentation models trained on natural images, or encode RGB or RGB-D information using RGB only pre-trained backbones, SurgDepth, which is built on top of Vision Transformers (ViTs), is designed to encode both RGB and depth information through a simple fusion mechanism. We conduct extensive experiments on benchmark datasets including EndoVis2022, AutoLapro, LapI2I and EndoVis2017 to verify the efficacy of SurgDepth. Specifically, SurgDepth achieves a new SOTA IoU of 0.86 on EndoVis 2022 SAR-RARP50 challenge and outperforms the current best method by at least 4%, using a shallow and compute efficient decoder consisting of ConvNeXt blocks.",
      "paper_authors": [
        "Muhammad Abdullah Jamal",
        "Omid Mohareri"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-29",
      "update_time": "2024-07-29",
      "comments": null,
      "repo_url": "#"
    },
    "2407.19666": {
      "paper_id": "2407.19666v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.19666v1",
      "paper_key": "2407.19666",
      "paper_title": "Take A Step Back: Rethinking the Two Stages in Visual Reasoning",
      "paper_url": "http://arxiv.org/abs/2407.19666v1",
      "paper_abstract": "Visual reasoning, as a prominent research area, plays a crucial role in AI by facilitating concept formation and interaction with the world. However, current works are usually carried out separately on small datasets thus lacking generalization ability. Through rigorous evaluation of diverse benchmarks, we demonstrate the shortcomings of existing ad-hoc methods in achieving cross-domain reasoning and their tendency to data bias fitting. In this paper, we revisit visual reasoning with a two-stage perspective: (1) symbolization and (2) logical reasoning given symbols or their representations. We find that the reasoning stage is better at generalization than symbolization. Thus, it is more efficient to implement symbolization via separated encoders for different data domains while using a shared reasoner. Given our findings, we establish design principles for visual reasoning frameworks following the separated symbolization and shared reasoning. The proposed two-stage framework achieves impressive generalization ability on various visual reasoning tasks, including puzzles, physical prediction, and visual question answering (VQA), encompassing both 2D and 3D modalities. We believe our insights will pave the way for generalizable visual reasoning.",
      "paper_authors": [
        "Mingyu Zhang",
        "Jiting Cai",
        "Mingyu Liu",
        "Yue Xu",
        "Cewu Lu",
        "Yong-Lu Li"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-29",
      "update_time": "2024-07-29",
      "comments": "ECCV 2024, Project page:\n  https://mybearyzhang.github.io/projects/TwoStageReason/",
      "repo_url": "#"
    },
    "2407.19294": {
      "paper_id": "2407.19294v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.19294v1",
      "paper_key": "2407.19294",
      "paper_title": "Rethinking Attention Module Design for Point Cloud Analysis",
      "paper_url": "http://arxiv.org/abs/2407.19294v1",
      "paper_abstract": "In recent years, there have been significant advancements in applying attention mechanisms to point cloud analysis. However, attention module variants featured in various research papers often operate under diverse settings and tasks, incorporating potential training strategies. This heterogeneity poses challenges in establishing a fair comparison among these attention module variants. In this paper, we address this issue by rethinking and exploring attention module design within a consistent base framework and settings. Both global-based and local-based attention methods are studied, with a focus on the selection basis and scales of neighbors for local-based attention. Different combinations of aggregated local features and computation methods for attention scores are evaluated, ranging from the initial addition/concatenation-based approach to the widely adopted dot product-based method and the recently proposed vector attention technique. Various position encoding methods are also investigated. Our extensive experimental analysis reveals that there is no universally optimal design across diverse point cloud tasks. Instead, drawing from best practices, we propose tailored attention modules for specific tasks, leading to superior performance on point cloud classification and segmentation benchmarks.",
      "paper_authors": [
        "Chengzhi Wu",
        "Kaige Wang",
        "Zeyun Zhong",
        "Hao Fu",
        "Junwei Zheng",
        "Jiaming Zhang",
        "Julius Pfrommer",
        "J\u00fcrgen Beyerer"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-27",
      "update_time": "2024-07-27",
      "comments": null,
      "repo_url": "#"
    },
    "2407.19274": {
      "paper_id": "2407.19274v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.19274v1",
      "paper_key": "2407.19274",
      "paper_title": "Mamba? Catch The Hype Or Rethink What Really Helps for Image Registration",
      "paper_url": "http://arxiv.org/abs/2407.19274v1",
      "paper_abstract": "Our findings indicate that adopting \"advanced\" computational elements fails to significantly improve registration accuracy. Instead, well-established registration-specific designs offer fair improvements, enhancing results by a marginal 1.5\\% over the baseline. Our findings emphasize the importance of rigorous, unbiased evaluation and contribution disentanglement of all low- and high-level registration components, rather than simply following the computer vision trends with \"more advanced\" computational blocks. We advocate for simpler yet effective solutions and novel evaluation metrics that go beyond conventional registration accuracy, warranting further research across diverse organs and modalities. The code is available at \\url{https://github.com/BailiangJ/rethink-reg}.",
      "paper_authors": [
        "Bailiang Jian",
        "Jiazhen Pan",
        "Morteza Ghahremani",
        "Daniel Rueckert",
        "Christian Wachinger",
        "Benedikt Wiestler"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-27",
      "update_time": "2024-07-27",
      "comments": "WBIR 2024 Workshop on Biomedical Imaging Registration",
      "repo_url": "https://github.com/bailiangj/rethink-reg"
    },
    "2407.17843": {
      "paper_id": "2407.17843v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.17843v1",
      "paper_key": "2407.17843",
      "paper_title": "DragText: Rethinking Text Embedding in Point-based Image Editing",
      "paper_url": "http://arxiv.org/abs/2407.17843v1",
      "paper_abstract": "Point-based image editing enables accurate and flexible control through content dragging. However, the role of text embedding in the editing process has not been thoroughly investigated. A significant aspect that remains unexplored is the interaction between text and image embeddings. In this study, we show that during the progressive editing of an input image in a diffusion model, the text embedding remains constant. As the image embedding increasingly diverges from its initial state, the discrepancy between the image and text embeddings presents a significant challenge. Moreover, we found that the text prompt significantly influences the dragging process, particularly in maintaining content integrity and achieving the desired manipulation. To utilize these insights, we propose DragText, which optimizes text embedding in conjunction with the dragging process to pair with the modified image embedding. Simultaneously, we regularize the text optimization process to preserve the integrity of the original text prompt. Our approach can be seamlessly integrated with existing diffusion-based drag methods with only a few lines of code.",
      "paper_authors": [
        "Gayoon Choi",
        "Taejin Jeong",
        "Sujung Hong",
        "Jaehoon Joo",
        "Seong Jae Hwang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-25",
      "update_time": "2024-07-25",
      "comments": "22 pages, 18 figures",
      "repo_url": "https://github.com/MICV-yonsei/DragText"
    },
    "2407.17596": {
      "paper_id": "2407.17596v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.17596v2",
      "paper_key": "2407.17596",
      "paper_title": "Quality Assured: Rethinking Annotation Strategies in Imaging AI",
      "paper_url": "http://arxiv.org/abs/2407.17596v2",
      "paper_abstract": "This paper does not describe a novel method. Instead, it studies an essential foundation for reliable benchmarking and ultimately real-world application of AI-based image analysis: generating high-quality reference annotations. Previous research has focused on crowdsourcing as a means of outsourcing annotations. However, little attention has so far been given to annotation companies, specifically regarding their internal quality assurance (QA) processes. Therefore, our aim is to evaluate the influence of QA employed by annotation companies on annotation quality and devise methodologies for maximizing data annotation efficacy. Based on a total of 57,648 instance segmented images obtained from a total of 924 annotators and 34 QA workers from four annotation companies and Amazon Mechanical Turk (MTurk), we derived the following insights: (1) Annotation companies perform better both in terms of quantity and quality compared to the widely used platform MTurk. (2) Annotation companies' internal QA only provides marginal improvements, if any. However, improving labeling instructions instead of investing in QA can substantially boost annotation performance. (3) The benefit of internal QA depends on specific image characteristics. Our work could enable researchers to derive substantially more value from a fixed annotation budget and change the way annotation companies conduct internal QA.",
      "paper_authors": [
        "Tim R\u00e4dsch",
        "Annika Reinke",
        "Vivienn Weru",
        "Minu D. Tizabi",
        "Nicholas Heller",
        "Fabian Isensee",
        "Annette Kopp-Schneider",
        "Lena Maier-Hein"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-24",
      "update_time": "2024-07-26",
      "comments": "Accepted at ECCV 2024, preprint, Computer Vision, Data Annotation",
      "repo_url": "#"
    },
    "2407.16430": {
      "paper_id": "2407.16430v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.16430v1",
      "paper_key": "2407.16430",
      "paper_title": "Rethinking Out-of-Distribution Detection on Imbalanced Data Distribution",
      "paper_url": "http://arxiv.org/abs/2407.16430v1",
      "paper_abstract": "Detecting and rejecting unknown out-of-distribution (OOD) samples is critical for deployed neural networks to void unreliable predictions. In real-world scenarios, however, the efficacy of existing OOD detection methods is often impeded by the inherent imbalance of in-distribution (ID) data, which causes significant performance decline. Through statistical observations, we have identified two common challenges faced by different OOD detectors: misidentifying tail class ID samples as OOD, while erroneously predicting OOD samples as head class from ID. To explain this phenomenon, we introduce a generalized statistical framework, termed ImOOD, to formulate the OOD detection problem on imbalanced data distribution. Consequently, the theoretical analysis reveals that there exists a class-aware bias item between balanced and imbalanced OOD detection, which contributes to the performance gap. Building upon this finding, we present a unified training-time regularization technique to mitigate the bias and boost imbalanced OOD detectors across architecture designs. Our theoretically grounded method translates into consistent improvements on the representative CIFAR10-LT, CIFAR100-LT, and ImageNet-LT benchmarks against several state-of-the-art OOD detection approaches. Code will be made public soon.",
      "paper_authors": [
        "Kai Liu",
        "Zhihang Fu",
        "Sheng Jin",
        "Chao Chen",
        "Ze Chen",
        "Rongxin Jiang",
        "Fan Zhou",
        "Yaowu Chen",
        "Jieping Ye"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-23",
      "update_time": "2024-07-23",
      "comments": "N/A",
      "repo_url": "#"
    },
    "2407.15266": {
      "paper_id": "2407.15266v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.15266v2",
      "paper_key": "2407.15266",
      "paper_title": "STrack: A Reliable Multipath Transport for AI/ML Clusters",
      "paper_url": "http://arxiv.org/abs/2407.15266v2",
      "paper_abstract": "Emerging artificial intelligence (AI) and machine learning (ML) workloads present new challenges of managing the collective communication used in distributed training across hundreds or even thousands of GPUs. This paper presents STrack, a novel hardware-offloaded reliable transport protocol aimed at improving the performance of AI /ML workloads by rethinking key aspects of the transport layer. STrack optimizes congestion control and load balancing in tandem: it incorporates an adaptive load balancing algorithm leveraging ECN, while adopts RTT as multi-bit congestion indicators for precise congestion window adjustment. Additionally, STrack facilitates out-of-order delivery, selective retransmission, and swift loss recovery in hardware for multipath environment. The extensive simulation comparing STrack and RoCEv2 demonstrates that STrack outperforms RoCEv2 by up to 6X with synthetic workloads and by 27.4% with collective workloads, even with the optimized RoCEv2 system setup.",
      "paper_authors": [
        "Yanfang Le",
        "Rong Pan",
        "Peter Newman",
        "Jeremias Blendin",
        "Abdul Kabbani",
        "Vipin Jain",
        "Raghava Sivaramu",
        "Francis Matus"
      ],
      "primary_category": "cs.NI",
      "publish_time": "2024-07-21",
      "update_time": "2024-07-23",
      "comments": null,
      "repo_url": "#"
    },
    "2407.15173": {
      "paper_id": "2407.15173v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.15173v1",
      "paper_key": "2407.15173",
      "paper_title": "Rethinking Domain Adaptation and Generalization in the Era of CLIP",
      "paper_url": "http://arxiv.org/abs/2407.15173v1",
      "paper_abstract": "In recent studies on domain adaptation, significant emphasis has been placed on the advancement of learning shared knowledge from a source domain to a target domain. Recently, the large vision-language pre-trained model, i.e., CLIP has shown strong ability on zero-shot recognition, and parameter efficient tuning can further improve its performance on specific tasks. This work demonstrates that a simple domain prior boosts CLIP's zero-shot recognition in a specific domain. Besides, CLIP's adaptation relies less on source domain data due to its diverse pre-training dataset. Furthermore, we create a benchmark for zero-shot adaptation and pseudo-labeling based self-training with CLIP. Last but not least, we propose to improve the task generalization ability of CLIP from multiple unlabeled domains, which is a more practical and unique scenario. We believe our findings motivate a rethinking of domain adaptation benchmarks and the associated role of related algorithms in the era of CLIP.",
      "paper_authors": [
        "Ruoyu Feng",
        "Tao Yu",
        "Xin Jin",
        "Xiaoyuan Yu",
        "Lei Xiao",
        "Zhibo Chen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-21",
      "update_time": "2024-07-21",
      "comments": null,
      "repo_url": "#"
    },
    "2407.15143": {
      "paper_id": "2407.15143v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.15143v2",
      "paper_key": "2407.15143",
      "paper_title": "Rethinking Feature Backbone Fine-tuning for Remote Sensing Object Detection",
      "paper_url": "http://arxiv.org/abs/2407.15143v2",
      "paper_abstract": "Recently, numerous methods have achieved impressive performance in remote sensing object detection, relying on convolution or transformer architectures. Such detectors typically have a feature backbone to extract useful features from raw input images. For the remote sensing domain, a common practice among current detectors is to initialize the backbone with pre-training on ImageNet consisting of natural scenes. Fine-tuning the backbone is then typically required to generate features suitable for remote-sensing images. However, this could hinder the extraction of basic visual features in long-term training, thus restricting performance improvement. To mitigate this issue, we propose a novel method named DBF (Dynamic Backbone Freezing) for feature backbone fine-tuning on remote sensing object detection. Our method aims to handle the dilemma of whether the backbone should extract low-level generic features or possess specific knowledge of the remote sensing domain, by introducing a module called 'Freezing Scheduler' to dynamically manage the update of backbone features during training. Extensive experiments on DOTA and DIOR-R show that our approach enables more accurate model learning while substantially reducing computational costs. Our method can be seamlessly adopted without additional effort due to its straightforward design.",
      "paper_authors": [
        "Yechan Kim",
        "JongHyun Park",
        "SooYeon Kim",
        "Moongu Jeon"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-21",
      "update_time": "2024-08-08",
      "comments": "Under Review",
      "repo_url": "#"
    },
    "2407.15886": {
      "paper_id": "2407.15886v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.15886v1",
      "paper_key": "2407.15886",
      "paper_title": "CatVTON: Concatenation Is All You Need for Virtual Try-On with Diffusion Models",
      "paper_url": "http://arxiv.org/abs/2407.15886v1",
      "paper_abstract": "Virtual try-on methods based on diffusion models achieve realistic try-on effects but often replicate the backbone network as a ReferenceNet or use additional image encoders to process condition inputs, leading to high training and inference costs. In this work, we rethink the necessity of ReferenceNet and image encoders and innovate the interaction between garment and person by proposing CatVTON, a simple and efficient virtual try-on diffusion model. CatVTON facilitates the seamless transfer of in-shop or worn garments of any category to target persons by simply concatenating them in spatial dimensions as inputs. The efficiency of our model is demonstrated in three aspects: (1) Lightweight network: Only the original diffusion modules are used, without additional network modules. The text encoder and cross-attentions for text injection in the backbone are removed, reducing the parameters by 167.02M. (2) Parameter-efficient training: We identified the try-on relevant modules through experiments and achieved high-quality try-on effects by training only 49.57M parameters, approximately 5.51 percent of the backbone network's parameters. (3) Simplified inference: CatVTON eliminates all unnecessary conditions and preprocessing steps, including pose estimation, human parsing, and text input, requiring only a garment reference, target person image, and mask for the virtual try-on process. Extensive experiments demonstrate that CatVTON achieves superior qualitative and quantitative results with fewer prerequisites and trainable parameters than baseline methods. Furthermore, CatVTON shows good generalization in in-the-wild scenarios despite using open-source datasets with only 73K samples.",
      "paper_authors": [
        "Zheng Chong",
        "Xiao Dong",
        "Haoxiang Li",
        "Shiyue Zhang",
        "Wenqing Zhang",
        "Xujie Zhang",
        "Hanqing Zhao",
        "Xiaodan Liang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-21",
      "update_time": "2024-07-21",
      "comments": "10 pages, 9 figures, 4 tables",
      "repo_url": "https://github.com/zheng-chong/catvton"
    },
    "2407.14768": {
      "paper_id": "2407.14768v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.14768v1",
      "paper_key": "2407.14768",
      "paper_title": "Teach Harder, Learn Poorer: Rethinking Hard Sample Distillation for GNN-to-MLP Knowledge Distillation",
      "paper_url": "http://arxiv.org/abs/2407.14768v1",
      "paper_abstract": "To bridge the gaps between powerful Graph Neural Networks (GNNs) and lightweight Multi-Layer Perceptron (MLPs), GNN-to-MLP Knowledge Distillation (KD) proposes to distill knowledge from a well-trained teacher GNN into a student MLP. In this paper, we revisit the knowledge samples (nodes) in teacher GNNs from the perspective of hardness, and identify that hard sample distillation may be a major performance bottleneck of existing graph KD algorithms. The GNN-to-MLP KD involves two different types of hardness, one student-free knowledge hardness describing the inherent complexity of GNN knowledge, and the other student-dependent distillation hardness describing the difficulty of teacher-to-student distillation. However, most of the existing work focuses on only one of these aspects or regards them as one thing. This paper proposes a simple yet effective Hardness-aware GNN-to-MLP Distillation (HGMD) framework, which decouples the two hardnesses and estimates them using a non-parametric approach. Finally, two hardness-aware distillation schemes (i.e., HGMD-weight and HGMD-mixup) are further proposed to distill hardness-aware knowledge from teacher GNNs into the corresponding nodes of student MLPs. As non-parametric distillation, HGMD does not involve any additional learnable parameters beyond the student MLPs, but it still outperforms most of the state-of-the-art competitors. HGMD-mixup improves over the vanilla MLPs by 12.95% and outperforms its teacher GNNs by 2.48% averaged over seven real-world datasets.",
      "paper_authors": [
        "Lirong Wu",
        "Yunfan Liu",
        "Haitao Lin",
        "Yufei Huang",
        "Stan Z. Li"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-07-20",
      "update_time": "2024-07-20",
      "comments": null,
      "repo_url": "https://github.com/lirongwu/hgmd"
    },
    "2407.14117": {
      "paper_id": "2407.14117v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.14117v1",
      "paper_key": "2407.14117",
      "paper_title": "Rethinking Visual Content Refinement in Low-Shot CLIP Adaptation",
      "paper_url": "http://arxiv.org/abs/2407.14117v1",
      "paper_abstract": "Recent adaptations can boost the low-shot capability of Contrastive Vision-Language Pre-training (CLIP) by effectively facilitating knowledge transfer. However, these adaptation methods are usually operated on the global view of an input image, and thus biased perception of partial local details of the image. To solve this problem, we propose a Visual Content Refinement (VCR) before the adaptation calculation during the test stage. Specifically, we first decompose the test image into different scales to shift the feature extractor's attention to the details of the image. Then, we select the image view with the max prediction margin in each scale to filter out the noisy image views, where the prediction margins are calculated from the pre-trained CLIP model. Finally, we merge the content of the aforementioned selected image views based on their scales to construct a new robust representation. Thus, the merged content can be directly used to help the adapter focus on both global and local parts without any extra training parameters. We apply our method to 3 popular low-shot benchmark tasks with 13 datasets and achieve a significant improvement over state-of-the-art methods. For example, compared to the baseline (Tip-Adapter) on the few-shot classification task, our method achieves about 2\\% average improvement for both training-free and training-need settings.",
      "paper_authors": [
        "Jinda Lu",
        "Shuo Wang",
        "Yanbin Hao",
        "Haifeng Liu",
        "Xiang Wang",
        "Meng Wang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-19",
      "update_time": "2024-07-19",
      "comments": null,
      "repo_url": "https://github.com/injadlu/VCR"
    },
    "2407.14029": {
      "paper_id": "2407.14029v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.14029v1",
      "paper_key": "2407.14029",
      "paper_title": "PASS++: A Dual Bias Reduction Framework for Non-Exemplar Class-Incremental Learning",
      "paper_url": "http://arxiv.org/abs/2407.14029v1",
      "paper_abstract": "Class-incremental learning (CIL) aims to recognize new classes incrementally while maintaining the discriminability of old classes. Most existing CIL methods are exemplar-based, i.e., storing a part of old data for retraining. Without relearning old data, those methods suffer from catastrophic forgetting. In this paper, we figure out two inherent problems in CIL, i.e., representation bias and classifier bias, that cause catastrophic forgetting of old knowledge. To address these two biases, we present a simple and novel dual bias reduction framework that employs self-supervised transformation (SST) in input space and prototype augmentation (protoAug) in deep feature space. On the one hand, SST alleviates the representation bias by learning generic and diverse representations that can transfer across different tasks. On the other hand, protoAug overcomes the classifier bias by explicitly or implicitly augmenting prototypes of old classes in the deep feature space, which poses tighter constraints to maintain previously learned decision boundaries. We further propose hardness-aware prototype augmentation and multi-view ensemble strategies, leading to significant improvements. The proposed framework can be easily integrated with pre-trained models. Without storing any samples of old classes, our method can perform comparably with state-of-the-art exemplar-based approaches which store plenty of old data. We hope to draw the attention of researchers back to non-exemplar CIL by rethinking the necessity of storing old samples in CIL.",
      "paper_authors": [
        "Fei Zhu",
        "Xu-Yao Zhang",
        "Zhen Cheng",
        "Cheng-Lin Liu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-19",
      "update_time": "2024-07-19",
      "comments": null,
      "repo_url": "#"
    },
    "2407.13185": {
      "paper_id": "2407.13185v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.13185v1",
      "paper_key": "2407.13185",
      "paper_title": "KFD-NeRF: Rethinking Dynamic NeRF with Kalman Filter",
      "paper_url": "http://arxiv.org/abs/2407.13185v1",
      "paper_abstract": "We introduce KFD-NeRF, a novel dynamic neural radiance field integrated with an efficient and high-quality motion reconstruction framework based on Kalman filtering. Our key idea is to model the dynamic radiance field as a dynamic system whose temporally varying states are estimated based on two sources of knowledge: observations and predictions. We introduce a novel plug-in Kalman filter guided deformation field that enables accurate deformation estimation from scene observations and predictions. We use a shallow Multi-Layer Perceptron (MLP) for observations and model the motion as locally linear to calculate predictions with motion equations. To further enhance the performance of the observation MLP, we introduce regularization in the canonical space to facilitate the network's ability to learn warping for different frames. Additionally, we employ an efficient tri-plane representation for encoding the canonical space, which has been experimentally demonstrated to converge quickly with high quality. This enables us to use a shallower observation MLP, consisting of just two layers in our implementation. We conduct experiments on synthetic and real data and compare with past dynamic NeRF methods. Our KFD-NeRF demonstrates similar or even superior rendering performance within comparable computational time and achieves state-of-the-art view synthesis performance with thorough training.",
      "paper_authors": [
        "Yifan Zhan",
        "Zhuoxiao Li",
        "Muyao Niu",
        "Zhihang Zhong",
        "Shohei Nobuhara",
        "Ko Nishino",
        "Yinqiang Zheng"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-18",
      "update_time": "2024-07-18",
      "comments": "accepted to eccv2024",
      "repo_url": "#"
    },
    "2407.13094": {
      "paper_id": "2407.13094v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.13094v1",
      "paper_key": "2407.13094",
      "paper_title": "Rethinking Video-Text Understanding: Retrieval from Counterfactually Augmented Data",
      "paper_url": "http://arxiv.org/abs/2407.13094v1",
      "paper_abstract": "Recent video-text foundation models have demonstrated strong performance on a wide variety of downstream video understanding tasks. Can these video-text models genuinely understand the contents of natural videos? Standard video-text evaluations could be misleading as many questions can be inferred merely from the objects and contexts in a single frame or biases inherent in the datasets. In this paper, we aim to better assess the capabilities of current video-text models and understand their limitations. We propose a novel evaluation task for video-text understanding, namely retrieval from counterfactually augmented data (RCAD), and a new Feint6K dataset. To succeed on our new evaluation task, models must derive a comprehensive understanding of the video from cross-frame reasoning. Analyses show that previous video-text foundation models can be easily fooled by counterfactually augmented data and are far behind human-level performance. In order to narrow the gap between video-text models and human performance on RCAD, we identify a key limitation of current contrastive approaches on video-text data and introduce LLM-teacher, a more effective approach to learn action semantics by leveraging knowledge obtained from a pretrained large language model. Experiments and analyses show that our approach successfully learn more discriminative action embeddings and improves results on Feint6K when applied to multiple video-text models. Our Feint6K dataset and project page is available at https://feint6k.github.io.",
      "paper_authors": [
        "Wufei Ma",
        "Kai Li",
        "Zhongshi Jiang",
        "Moustafa Meshry",
        "Qihao Liu",
        "Huiyu Wang",
        "Christian H\u00e4ne",
        "Alan Yuille"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-18",
      "update_time": "2024-07-18",
      "comments": "ECCV 2024. Project page: https://feint6k.github.io",
      "repo_url": "#"
    },
    "2407.12993": {
      "paper_id": "2407.12993v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.12993v1",
      "paper_key": "2407.12993",
      "paper_title": "Improving SAM Requires Rethinking its Optimization Formulation",
      "paper_url": "http://arxiv.org/abs/2407.12993v1",
      "paper_abstract": "This paper rethinks Sharpness-Aware Minimization (SAM), which is originally formulated as a zero-sum game where the weights of a network and a bounded perturbation try to minimize/maximize, respectively, the same differentiable loss. To fundamentally improve this design, we argue that SAM should instead be reformulated using the 0-1 loss. As a continuous relaxation, we follow the simple conventional approach where the minimizing (maximizing) player uses an upper bound (lower bound) surrogate to the 0-1 loss. This leads to a novel formulation of SAM as a bilevel optimization problem, dubbed as BiSAM. BiSAM with newly designed lower-bound surrogate loss indeed constructs stronger perturbation. Through numerical evidence, we show that BiSAM consistently results in improved performance when compared to the original SAM and variants, while enjoying similar computational complexity. Our code is available at https://github.com/LIONS-EPFL/BiSAM.",
      "paper_authors": [
        "Wanyun Xie",
        "Fabian Latorre",
        "Kimon Antonakopoulos",
        "Thomas Pethick",
        "Volkan Cevher"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-07-17",
      "update_time": "2024-07-17",
      "comments": "International Conference on Machine Learning (ICML), 2024",
      "repo_url": "https://github.com/LIONS-EPFL/BiSAM"
    },
    "2407.12622": {
      "paper_id": "2407.12622v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.12622v1",
      "paper_key": "2407.12622",
      "paper_title": "Rethinking the Architecture Design for Efficient Generic Event Boundary Detection",
      "paper_url": "http://arxiv.org/abs/2407.12622v1",
      "paper_abstract": "Generic event boundary detection (GEBD), inspired by human visual cognitive behaviors of consistently segmenting videos into meaningful temporal chunks, finds utility in various applications such as video editing and. In this paper, we demonstrate that SOTA GEBD models often prioritize final performance over model complexity, resulting in low inference speed and hindering efficient deployment in real-world scenarios. We contribute to addressing this challenge by experimentally reexamining the architecture of GEBD models and uncovering several surprising findings. Firstly, we reveal that a concise GEBD baseline model already achieves promising performance without any sophisticated design. Secondly, we find that the widely applied image-domain backbones in GEBD models can contain plenty of architecture redundancy, motivating us to gradually ``modernize'' each component to enhance efficiency. Thirdly, we show that the GEBD models using image-domain backbones conducting the spatiotemporal learning in a spatial-then-temporal greedy manner can suffer from a distraction issue, which might be the inefficient villain for GEBD. Using a video-domain backbone to jointly conduct spatiotemporal modeling is an effective solution for this issue. The outcome of our exploration is a family of GEBD models, named EfficientGEBD, significantly outperforms the previous SOTA methods by up to 1.7\\% performance gain and 280\\% speedup under the same backbone. Our research prompts the community to design modern GEBD methods with the consideration of model complexity, particularly in resource-aware applications. The code is available at \\url{https://github.com/Ziwei-Zheng/EfficientGEBD}.",
      "paper_authors": [
        "Ziwei Zheng",
        "Zechuan Zhang",
        "Yulin Wang",
        "Shiji Song",
        "Gao Huang",
        "Le Yang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-17",
      "update_time": "2024-07-17",
      "comments": "ACM MM 2024",
      "repo_url": "https://github.com/ziwei-zheng/efficientgebd"
    },
    "2407.12488": {
      "paper_id": "2407.12488v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.12488v1",
      "paper_key": "2407.12488",
      "paper_title": "What's Distributive Justice Got to Do with It? Rethinking Algorithmic Fairness from the Perspective of Approximate Justice",
      "paper_url": "http://arxiv.org/abs/2407.12488v1",
      "paper_abstract": "In the field of algorithmic fairness, many fairness criteria have been proposed. Oftentimes, their proposal is only accompanied by a loose link to ideas from moral philosophy -- which makes it difficult to understand when the proposed criteria should be used to evaluate the fairness of a decision-making system. More recently, researchers have thus retroactively tried to tie existing fairness criteria to philosophical concepts. Group fairness criteria have typically been linked to egalitarianism, a theory of distributive justice. This makes it tempting to believe that fairness criteria mathematically represent ideals of distributive justice and this is indeed how they are typically portrayed. In this paper, we will discuss why the current approach of linking algorithmic fairness and distributive justice is too simplistic and, hence, insufficient. We argue that in the context of imperfect decision-making systems -- which is what we deal with in algorithmic fairness -- we should not only care about what the ideal distribution of benefits/harms among individuals would look like but also about how deviations from said ideal are distributed. Our claim is that algorithmic fairness is concerned with unfairness in these deviations. This requires us to rethink the way in which we, as algorithmic fairness researchers, view distributive justice and use fairness criteria.",
      "paper_authors": [
        "Corinna Hertweck",
        "Christoph Heitz",
        "Michele Loi"
      ],
      "primary_category": "cs.CY",
      "publish_time": "2024-07-17",
      "update_time": "2024-07-17",
      "comments": null,
      "repo_url": "#"
    },
    "2407.12239": {
      "paper_id": "2407.12239v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.12239v2",
      "paper_key": "2407.12239",
      "paper_title": "Motion and Structure from Event-based Normal Flow",
      "paper_url": "http://arxiv.org/abs/2407.12239v2",
      "paper_abstract": "Recovering the camera motion and scene geometry from visual data is a fundamental problem in the field of computer vision. Its success in standard vision is attributed to the maturity of feature extraction, data association and multi-view geometry. The recent emergence of neuromorphic event-based cameras places great demands on approaches that use raw event data as input to solve this fundamental problem.Existing state-of-the-art solutions typically infer implicitly data association by iteratively reversing the event data generation process. However, the nonlinear nature of these methods limits their applicability in real-time tasks, and the constant-motion assumption leads to unstable results under agile motion. To this end, we rethink the problem formulation in a way that aligns better with the differential working principle of event cameras.We show that the event-based normal flow can be used, via the proposed geometric error term, as an alternative to the full flow in solving a family of geometric problems that involve instantaneous first-order kinematics and scene geometry. Furthermore, we develop a fast linear solver and a continuous-time nonlinear solver on top of the proposed geometric error term.Experiments on both synthetic and real data show the superiority of our linear solver in terms of accuracy and efficiency, and indicate its complementary feature as an initialization method for existing nonlinear solvers. Besides, our continuous-time non-linear solver exhibits exceptional capability in accommodating sudden variations in motion since it does not rely on the constant-motion assumption.",
      "paper_authors": [
        "Zhongyang Ren",
        "Bangyan Liao",
        "Delei Kong",
        "Jinghang Li",
        "Peidong Liu",
        "Laurent Kneip",
        "Guillermo Gallego",
        "Yi Zhou"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-17",
      "update_time": "2024-07-19",
      "comments": "This paper has been accepted by ECCV 2024",
      "repo_url": "#"
    },
    "2407.11948": {
      "paper_id": "2407.11948v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.11948v1",
      "paper_key": "2407.11948",
      "paper_title": "Rethinking Transformer-based Multi-document Summarization: An Empirical Investigation",
      "paper_url": "http://arxiv.org/abs/2407.11948v1",
      "paper_abstract": "The utilization of Transformer-based models prospers the growth of multi-document summarization (MDS). Given the huge impact and widespread adoption of Transformer-based models in various natural language processing tasks, investigating their performance and behaviors in the context of MDS becomes crucial for advancing the field and enhancing the quality of summary. To thoroughly examine the behaviours of Transformer-based MDS models, this paper presents five empirical studies on (1) measuring the impact of document boundary separators quantitatively; (2) exploring the effectiveness of different mainstream Transformer structures; (3) examining the sensitivity of the encoder and decoder; (4) discussing different training strategies; and (5) discovering the repetition in a summary generation. The experimental results on prevalent MDS datasets and eleven evaluation metrics show the influence of document boundary separators, the granularity of different level features and different model training strategies. The results also reveal that the decoder exhibits greater sensitivity to noises compared to the encoder. This underscores the important role played by the decoder, suggesting a potential direction for future research in MDS. Furthermore, the experimental results indicate that the repetition problem in the generated summaries has correlations with the high uncertainty scores.",
      "paper_authors": [
        "Congbo Ma",
        "Wei Emma Zhang",
        "Dileepa Pitawela",
        "Haojie Zhuang",
        "Yanfeng Shu"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-07-16",
      "update_time": "2024-07-16",
      "comments": null,
      "repo_url": "#"
    },
    "2407.11624": {
      "paper_id": "2407.11624v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.11624v1",
      "paper_key": "2407.11624",
      "paper_title": "Rethinking Fair Graph Neural Networks from Re-balancing",
      "paper_url": "http://arxiv.org/abs/2407.11624v1",
      "paper_abstract": "Driven by the powerful representation ability of Graph Neural Networks (GNNs), plentiful GNN models have been widely deployed in many real-world applications. Nevertheless, due to distribution disparities between different demographic groups, fairness in high-stake decision-making systems is receiving increasing attention. Although lots of recent works devoted to improving the fairness of GNNs and achieved considerable success, they all require significant architectural changes or additional loss functions requiring more hyper-parameter tuning. Surprisingly, we find that simple re-balancing methods can easily match or surpass existing fair GNN methods. We claim that the imbalance across different demographic groups is a significant source of unfairness, resulting in imbalanced contributions from each group to the parameters updating. However, these simple re-balancing methods have their own shortcomings during training. In this paper, we propose FairGB, Fair Graph Neural Network via re-Balancing, which mitigates the unfairness of GNNs by group balancing. Technically, FairGB consists of two modules: counterfactual node mixup and contribution alignment loss. Firstly, we select counterfactual pairs across inter-domain and inter-class, and interpolate the ego-networks to generate new samples. Guided by analysis, we can reveal the debiasing mechanism of our model by the causal view and prove that our strategy can make sensitive attributes statistically independent from target labels. Secondly, we reweigh the contribution of each group according to gradients. By combining these two modules, they can mutually promote each other. Experimental results on benchmark datasets show that our method can achieve state-of-the-art results concerning both utility and fairness metrics. Code is available at https://github.com/ZhixunLEE/FairGB.",
      "paper_authors": [
        "Zhixun Li",
        "Yushun Dong",
        "Qiang Liu",
        "Jeffrey Xu Yu"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-07-16",
      "update_time": "2024-07-16",
      "comments": "Accepted by SIGKDD 2024, research track",
      "repo_url": "https://github.com/zhixunlee/fairgb"
    },
    "2407.11590": {
      "paper_id": "2407.11590v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.11590v3",
      "paper_key": "2407.11590",
      "paper_title": "Rethinking Learned Image Compression: Context is All You Need",
      "paper_url": "http://arxiv.org/abs/2407.11590v3",
      "paper_abstract": "Since LIC has made rapid progress recently compared to traditional methods, this paper attempts to discuss the question about 'Where is the boundary of Learned Image Compression(LIC)?'. Thus this paper splits the above problem into two sub-problems:1)Where is the boundary of rate-distortion performance of PSNR? 2)How to further improve the compression gain and achieve the boundary? Therefore this paper analyzes the effectiveness of scaling parameters for encoder, decoder and context model, which are the three components of LIC. Then we conclude that scaling for LIC is to scale for context model and decoder within LIC. Extensive experiments demonstrate that overfitting can actually serve as an effective context. By optimizing the context, this paper further improves PSNR and achieves state-of-the-art performance, showing a performance gain of 14.39% with BD-RATE over VVC.",
      "paper_authors": [
        "Jixiang Luo"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2024-07-16",
      "update_time": "2024-08-02",
      "comments": null,
      "repo_url": "#"
    },
    "2407.11503": {
      "paper_id": "2407.11503v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.11503v1",
      "paper_key": "2407.11503",
      "paper_title": "Beyond Mask: Rethinking Guidance Types in Few-shot Segmentation",
      "paper_url": "http://arxiv.org/abs/2407.11503v1",
      "paper_abstract": "Existing few-shot segmentation (FSS) methods mainly focus on prototype feature generation and the query-support matching mechanism. As a crucial prompt for generating prototype features, the pair of image-mask types in the support set has become the default setting. However, various types such as image, text, box, and mask all can provide valuable information regarding the objects in context, class, localization, and shape appearance. Existing work focuses on specific combinations of guidance, leading FSS into different research branches. Rethinking guidance types in FSS is expected to explore the efficient joint representation of the coupling between the support set and query set, giving rise to research trends in the weakly or strongly annotated guidance to meet the customized requirements of practical users. In this work, we provide the generalized FSS with seven guidance paradigms and develop a universal vision-language framework (UniFSS) to integrate prompts from text, mask, box, and image. Leveraging the advantages of large-scale pre-training vision-language models in textual and visual embeddings, UniFSS proposes high-level spatial correction and embedding interactive units to overcome the semantic ambiguity drawbacks typically encountered by pure visual matching methods when facing intra-class appearance diversities. Extensive experiments show that UniFSS significantly outperforms the state-of-the-art methods. Notably, the weakly annotated class-aware box paradigm even surpasses the finely annotated mask paradigm.",
      "paper_authors": [
        "Shijie Chang",
        "Youwei Pang",
        "Xiaoqi Zhao",
        "Lihe Zhang",
        "Huchuan Lu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-16",
      "update_time": "2024-07-16",
      "comments": "Preprint under review",
      "repo_url": "#"
    },
    "2407.10233": {
      "paper_id": "2407.10233v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.10233v1",
      "paper_key": "2407.10233",
      "paper_title": "Visual Prompt Selection for In-Context Learning Segmentation",
      "paper_url": "http://arxiv.org/abs/2407.10233v1",
      "paper_abstract": "As a fundamental and extensively studied task in computer vision, image segmentation aims to locate and identify different semantic concepts at the pixel level. Recently, inspired by In-Context Learning (ICL), several generalist segmentation frameworks have been proposed, providing a promising paradigm for segmenting specific objects. However, existing works mostly ignore the value of visual prompts or simply apply similarity sorting to select contextual examples. In this paper, we focus on rethinking and improving the example selection strategy. By comprehensive comparisons, we first demonstrate that ICL-based segmentation models are sensitive to different contexts. Furthermore, empirical evidence indicates that the diversity of contextual prompts plays a crucial role in guiding segmentation. Based on the above insights, we propose a new stepwise context search method. Different from previous works, we construct a small yet rich candidate pool and adaptively search the well-matched contexts. More importantly, this method effectively reduces the annotation cost by compacting the search space. Extensive experiments show that our method is an effective strategy for selecting examples and enhancing segmentation performance.",
      "paper_authors": [
        "Wei Suo",
        "Lanqing Lai",
        "Mengyang Sun",
        "Hanwang Zhang",
        "Peng Wang",
        "Yanning Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-14",
      "update_time": "2024-07-14",
      "comments": "Accept by ECCV2024",
      "repo_url": "https://github.com/lanqingl/scs"
    },
    "2407.09431": {
      "paper_id": "2407.09431v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.09431v1",
      "paper_key": "2407.09431",
      "paper_title": "Rethinking temporal self-similarity for repetitive action counting",
      "paper_url": "http://arxiv.org/abs/2407.09431v1",
      "paper_abstract": "Counting repetitive actions in long untrimmed videos is a challenging task that has many applications such as rehabilitation. State-of-the-art methods predict action counts by first generating a temporal self-similarity matrix (TSM) from the sampled frames and then feeding the matrix to a predictor network. The self-similarity matrix, however, is not an optimal input to a network since it discards too much information from the frame-wise embeddings. We thus rethink how a TSM can be utilized for counting repetitive actions and propose a framework that learns embeddings and predicts action start probabilities at full temporal resolution. The number of repeated actions is then inferred from the action start probabilities. In contrast to current approaches that have the TSM as an intermediate representation, we propose a novel loss based on a generated reference TSM, which enforces that the self-similarity of the learned frame-wise embeddings is consistent with the self-similarity of repeated actions. The proposed framework achieves state-of-the-art results on three datasets, i.e., RepCount, UCFRep, and Countix.",
      "paper_authors": [
        "Yanan Luo",
        "Jinhui Yi",
        "Yazan Abu Farha",
        "Moritz Wolter",
        "Juergen Gall"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-12",
      "update_time": "2024-07-12",
      "comments": "Accepted to ICIP 2024",
      "repo_url": "#"
    },
    "2407.08514": {
      "paper_id": "2407.08514v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.08514v1",
      "paper_key": "2407.08514",
      "paper_title": "Rethinking the Threat and Accessibility of Adversarial Attacks against Face Recognition Systems",
      "paper_url": "http://arxiv.org/abs/2407.08514v1",
      "paper_abstract": "Face recognition pipelines have been widely deployed in various mission-critical systems in trust, equitable and responsible AI applications. However, the emergence of adversarial attacks has threatened the security of the entire recognition pipeline. Despite the sheer number of attack methods proposed for crafting adversarial examples in both digital and physical forms, it is never an easy task to assess the real threat level of different attacks and obtain useful insight into the key risks confronted by face recognition systems. Traditional attacks view imperceptibility as the most important measurement to keep perturbations stealthy, while we suspect that industry professionals may possess a different opinion. In this paper, we delve into measuring the threat brought about by adversarial attacks from the perspectives of the industry and the applications of face recognition. In contrast to widely studied sophisticated attacks in the field, we propose an effective yet easy-to-launch physical adversarial attack, named AdvColor, against black-box face recognition pipelines in the physical world. AdvColor fools models in the recognition pipeline via directly supplying printed photos of human faces to the system under adversarial illuminations. Experimental results show that physical AdvColor examples can achieve a fooling rate of more than 96% against the anti-spoofing model and an overall attack success rate of 88% against the face recognition pipeline. We also conduct a survey on the threats of prevailing adversarial attacks, including AdvColor, to understand the gap between the machine-measured and human-assessed threat levels of different forms of adversarial attacks. The survey results surprisingly indicate that, compared to deliberately launched imperceptible attacks, perceptible but accessible attacks pose more lethal threats to real-world commercial systems of face recognition.",
      "paper_authors": [
        "Yuxin Cao",
        "Yumeng Zhu",
        "Derui Wang",
        "Sheng Wen",
        "Minhui Xue",
        "Jin Lu",
        "Hao Ge"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-11",
      "update_time": "2024-07-11",
      "comments": "19 pages, 12 figures",
      "repo_url": "https://github.com/advcolor123/advcolor"
    },
    "2407.07468": {
      "paper_id": "2407.07468v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.07468v1",
      "paper_key": "2407.07468",
      "paper_title": "Rethinking Few-shot Class-incremental Learning: Learning from Yourself",
      "paper_url": "http://arxiv.org/abs/2407.07468v1",
      "paper_abstract": "Few-shot class-incremental learning (FSCIL) aims to learn sequential classes with limited samples in a few-shot fashion. Inherited from the classical class-incremental learning setting, the popular benchmark of FSCIL uses averaged accuracy (aAcc) and last-task averaged accuracy (lAcc) as the evaluation metrics. However, we reveal that such evaluation metrics may not provide adequate emphasis on the novel class performance, and the continual learning ability of FSCIL methods could be ignored under this benchmark. In this work, as a complement to existing metrics, we offer a new metric called generalized average accuracy (gAcc) which is designed to provide an extra equitable evaluation by incorporating different perspectives of the performance under the guidance of a parameter $\\alpha$. We also present an overall metric in the form of the area under the curve (AUC) along the $\\alpha$. Under the guidance of gAcc, we release the potential of intermediate features of the vision transformers to boost the novel-class performance. Taking information from intermediate layers which are less class-specific and more generalizable, we manage to rectify the final features, leading to a more generalizable transformer-based FSCIL framework. Without complex network designs or cumbersome training procedures, our method outperforms existing FSCIL methods at aAcc and gAcc on three datasets. See codes at https://github.com/iSEE-Laboratory/Revisting_FSCIL",
      "paper_authors": [
        "Yu-Ming Tang",
        "Yi-Xing Peng",
        "Jingke Meng",
        "Wei-Shi Zheng"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-10",
      "update_time": "2024-07-10",
      "comments": "Accepted to ECCV 2024",
      "repo_url": "https://github.com/isee-laboratory/revisting_fscil"
    },
    "2407.06871": {
      "paper_id": "2407.06871v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.06871v1",
      "paper_key": "2407.06871",
      "paper_title": "Rethinking Image-to-Video Adaptation: An Object-centric Perspective",
      "paper_url": "http://arxiv.org/abs/2407.06871v1",
      "paper_abstract": "Image-to-video adaptation seeks to efficiently adapt image models for use in the video domain. Instead of finetuning the entire image backbone, many image-to-video adaptation paradigms use lightweight adapters for temporal modeling on top of the spatial module. However, these attempts are subject to limitations in efficiency and interpretability. In this paper, we propose a novel and efficient image-to-video adaptation strategy from the object-centric perspective. Inspired by human perception, which identifies objects as key components for video understanding, we integrate a proxy task of object discovery into image-to-video transfer learning. Specifically, we adopt slot attention with learnable queries to distill each frame into a compact set of object tokens. These object-centric tokens are then processed through object-time interaction layers to model object state changes across time. Integrated with two novel object-level losses, we demonstrate the feasibility of performing efficient temporal reasoning solely on the compressed object-centric representations for video downstream tasks. Our method achieves state-of-the-art performance with fewer tunable parameters, only 5\\% of fully finetuned models and 50\\% of efficient tuning methods, on action recognition benchmarks. In addition, our model performs favorably in zero-shot video object segmentation without further retraining or object annotations, proving the effectiveness of object-centric video understanding.",
      "paper_authors": [
        "Rui Qian",
        "Shuangrui Ding",
        "Dahua Lin"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-09",
      "update_time": "2024-07-09",
      "comments": "ECCV 2024",
      "repo_url": "#"
    },
    "2407.05527": {
      "paper_id": "2407.05527v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.05527v1",
      "paper_key": "2407.05527",
      "paper_title": "Rethinking Image Skip Connections in StyleGAN2",
      "paper_url": "http://arxiv.org/abs/2407.05527v1",
      "paper_abstract": "Various models based on StyleGAN have gained significant traction in the field of image synthesis, attributed to their robust training stability and superior performances. Within the StyleGAN framework, the adoption of image skip connection is favored over the traditional residual connection. However, this preference is just based on empirical observations; there has not been any in-depth mathematical analysis on it yet. To rectify this situation, this brief aims to elucidate the mathematical meaning of the image skip connection and introduce a groundbreaking methodology, termed the image squeeze connection, which significantly improves the quality of image synthesis. Specifically, we analyze the image skip connection technique to reveal its problem and introduce the proposed method which not only effectively boosts the GAN performance but also reduces the required number of network parameters. Extensive experiments on various datasets demonstrate that the proposed method consistently enhances the performance of state-of-the-art models based on StyleGAN. We believe that our findings represent a vital advancement in the field of image synthesis, suggesting a novel direction for future research and applications.",
      "paper_authors": [
        "Seung Park",
        "Yong-Goo Shin"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-08",
      "update_time": "2024-07-08",
      "comments": null,
      "repo_url": "#"
    },
    "2407.05441": {
      "paper_id": "2407.05441v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.05441v1",
      "paper_key": "2407.05441",
      "paper_title": "Language Models Encode Collaborative Signals in Recommendation",
      "paper_url": "http://arxiv.org/abs/2407.05441v1",
      "paper_abstract": "Recent studies empirically indicate that language models (LMs) encode rich world knowledge beyond mere semantics, attracting significant attention across various fields. However, in the recommendation domain, it remains uncertain whether LMs implicitly encode user preference information. Contrary to the prevailing understanding that LMs and traditional recommender models learn two distinct representation spaces due to a huge gap in language and behavior modeling objectives, this work rethinks such understanding and explores extracting a recommendation space directly from the language representation space. Surprisingly, our findings demonstrate that item representations, when linearly mapped from advanced LM representations, yield superior recommendation performance. This outcome suggests the homomorphism between the language representation space and an effective recommendation space, implying that collaborative signals may indeed be encoded within advanced LMs. Motivated by these findings, we propose a simple yet effective collaborative filtering (CF) model named AlphaRec, which utilizes language representations of item textual metadata (e.g., titles) instead of traditional ID-based embeddings. Specifically, AlphaRec is comprised of three main components: a multilayer perceptron (MLP), graph convolution, and contrastive learning (CL) loss function, making it extremely easy to implement and train. Our empirical results show that AlphaRec outperforms leading ID-based CF models on multiple datasets, marking the first instance of such a recommender with text embeddings achieving this level of performance. Moreover, AlphaRec introduces a new language-representation-based CF paradigm with several desirable advantages: being easy to implement, lightweight, rapid convergence, superior zero-shot recommendation abilities in new domains, and being aware of user intention.",
      "paper_authors": [
        "Leheng Sheng",
        "An Zhang",
        "Yi Zhang",
        "Yuxin Chen",
        "Xiang Wang",
        "Tat-Seng Chua"
      ],
      "primary_category": "cs.IR",
      "publish_time": "2024-07-07",
      "update_time": "2024-07-07",
      "comments": "Codes are available at https://github.com/LehengTHU/AlphaRec",
      "repo_url": "https://github.com/lehengthu/alpharec"
    },
    "2407.05382": {
      "paper_id": "2407.05382v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.05382v2",
      "paper_key": "2407.05382",
      "paper_title": "Rethinking Unsupervised Outlier Detection via Multiple Thresholding",
      "paper_url": "http://arxiv.org/abs/2407.05382v2",
      "paper_abstract": "In the realm of unsupervised image outlier detection, assigning outlier scores holds greater significance than its subsequent task: thresholding for predicting labels. This is because determining the optimal threshold on non-separable outlier score functions is an ill-posed problem. However, the lack of predicted labels not only hiders some real applications of current outlier detectors but also causes these methods not to be enhanced by leveraging the dataset's self-supervision. To advance existing scoring methods, we propose a multiple thresholding (Multi-T) module. It generates two thresholds that isolate inliers and outliers from the unlabelled target dataset, whereas outliers are employed to obtain better feature representation while inliers provide an uncontaminated manifold. Extensive experiments verify that Multi-T can significantly improve proposed outlier scoring methods. Moreover, Multi-T contributes to a naive distance-based method being state-of-the-art.",
      "paper_authors": [
        "Zhonghang Liu",
        "Panzhong Lu",
        "Guoyang Xie",
        "Zhichao Lu",
        "Wen-Yan Lin"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-07",
      "update_time": "2024-07-14",
      "comments": null,
      "repo_url": "https://github.com/zhliu-uod/multi-t"
    },
    "2407.05376": {
      "paper_id": "2407.05376v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.05376v1",
      "paper_key": "2407.05376",
      "paper_title": "Rethinking Closed-loop Planning Framework for Imitation-based Model Integrating Prediction and Planning",
      "paper_url": "http://arxiv.org/abs/2407.05376v1",
      "paper_abstract": "In recent years, the integration of prediction and planning through neural networks has received substantial attention. Despite extensive studies on it, there is a noticeable gap in understanding the operation of such models within a closed-loop planning setting. To bridge this gap, we propose a novel closed-loop planning framework compatible with neural networks engaged in joint prediction and planning. The framework contains two running modes, namely planning and safety monitoring, wherein the neural network performs Motion Prediction and Planning (MPP) and Conditional Motion Prediction (CMP) correspondingly without altering architecture. We evaluate the efficacy of our framework using the nuPlan dataset and its simulator, conducting closed-loop experiments across diverse scenarios. The results demonstrate that the proposed framework ensures the feasibility and local stability of the planning process while maintaining safety with CMP safety monitoring. Compared to other learning-based methods, our approach achieves substantial improvement.",
      "paper_authors": [
        "Jiayu Guo",
        "Mingyue Feng",
        "Pengfei Zhu",
        "Chengjun Li",
        "Jian Pu"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-07-07",
      "update_time": "2024-07-07",
      "comments": "7 pages,5 figures",
      "repo_url": "#"
    },
    "2407.05319": {
      "paper_id": "2407.05319v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.05319v1",
      "paper_key": "2407.05319",
      "paper_title": "Rethinking Targeted Adversarial Attacks For Neural Machine Translation",
      "paper_url": "http://arxiv.org/abs/2407.05319v1",
      "paper_abstract": "Targeted adversarial attacks are widely used to evaluate the robustness of neural machine translation systems. Unfortunately, this paper first identifies a critical issue in the existing settings of NMT targeted adversarial attacks, where their attacking results are largely overestimated. To this end, this paper presents a new setting for NMT targeted adversarial attacks that could lead to reliable attacking results. Under the new setting, it then proposes a Targeted Word Gradient adversarial Attack (TWGA) method to craft adversarial examples. Experimental results demonstrate that our proposed setting could provide faithful attacking results for targeted adversarial attacks on NMT systems, and the proposed TWGA method can effectively attack such victim NMT systems. In-depth analyses on a large-scale dataset further illustrate some valuable findings. 1 Our code and data are available at https://github.com/wujunjie1998/TWGA.",
      "paper_authors": [
        "Junjie Wu",
        "Lemao Liu",
        "Wei Bi",
        "Dit-Yan Yeung"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-07-07",
      "update_time": "2024-07-07",
      "comments": "5 pages, 2 figures, accepted by ICASSP 2024",
      "repo_url": "https://github.com/wujunjie1998/twga"
    },
    "2407.05203": {
      "paper_id": "2407.05203v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.05203v1",
      "paper_key": "2407.05203",
      "paper_title": "Toward a Unified Metadata Schema for Ecological Momentary Assessment with Voice-First Virtual Assistants",
      "paper_url": "http://arxiv.org/abs/2407.05203v1",
      "paper_abstract": "Ecological momentary assessment (EMA) is used to evaluate subjects' behaviors and moods in their natural environments, yet collecting real-time and self-report data with EMA is challenging due to user burden. Integrating voice into EMA data collection platforms through today's intelligent virtual assistants (IVAs) is promising due to hands-free and eye-free nature. However, efficiently managing conversations and EMAs is non-trivial and time consuming due to the ambiguity of the voice input. We approach this problem by rethinking the data modeling of EMA questions and what is needed to deploy them on voice-first user interfaces. We propose a unified metadata schema that models EMA questions and the necessary attributes to effectively and efficiently integrate voice as a new EMA modality. Our schema allows user experience researchers to write simple rules that can be rendered at run-time, instead of having to edit the source code. We showcase an example EMA survey implemented with our schema, which can run on multiple voice-only and voice-first devices. We believe that our work will accelerate the iterative prototyping and design process of real-world voice-based EMA data collection platforms.",
      "paper_authors": [
        "Chen Chen",
        "Khalil Mrini",
        "Kemeberly Charles",
        "Ella T. Lifset",
        "Michael Hogarth",
        "Alison A. Moore",
        "Nadir Weibel",
        "Emilia Farcas"
      ],
      "primary_category": "cs.HC",
      "publish_time": "2024-07-06",
      "update_time": "2024-07-06",
      "comments": "6 pages, In Proceedings of the 3rd Conference on Conversational User\n  Interfaces (CUI '21). Association for Computing Machinery, New York, NY, USA,\n  Article 31, 1-6",
      "repo_url": "#"
    },
    "2407.04999": {
      "paper_id": "2407.04999v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.04999v1",
      "paper_key": "2407.04999",
      "paper_title": "Rethinking the Effectiveness of Graph Classification Datasets in Benchmarks for Assessing GNNs",
      "paper_url": "http://arxiv.org/abs/2407.04999v1",
      "paper_abstract": "Graph classification benchmarks, vital for assessing and developing graph neural networks (GNNs), have recently been scrutinized, as simple methods like MLPs have demonstrated comparable performance. This leads to an important question: Do these benchmarks effectively distinguish the advancements of GNNs over other methodologies? If so, how do we quantitatively measure this effectiveness? In response, we first propose an empirical protocol based on a fair benchmarking framework to investigate the performance discrepancy between simple methods and GNNs. We further propose a novel metric to quantify the dataset effectiveness by considering both dataset complexity and model performance. To the best of our knowledge, our work is the first to thoroughly study and provide an explicit definition for dataset effectiveness in the graph learning area. Through testing across 16 real-world datasets, we found our metric to align with existing studies and intuitive assumptions. Finally, we explore the causes behind the low effectiveness of certain datasets by investigating the correlation between intrinsic graph properties and class labels, and we developed a novel technique supporting the correlation-controllable synthetic dataset generation. Our findings shed light on the current understanding of benchmark datasets, and our new platform could fuel the future evolution of graph classification benchmarks.",
      "paper_authors": [
        "Zhengdao Li",
        "Yong Cao",
        "Kefan Shuai",
        "Yiming Miao",
        "Kai Hwang"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-07-06",
      "update_time": "2024-07-06",
      "comments": null,
      "repo_url": "https://github.com/ICLab4DL/GNNBenchEffectiveness"
    },
    "2407.04681": {
      "paper_id": "2407.04681v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.04681v1",
      "paper_key": "2407.04681",
      "paper_title": "Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge",
      "paper_url": "http://arxiv.org/abs/2407.04681v1",
      "paper_abstract": "In recent years, multimodal large language models (MLLMs) have made significant strides by training on vast high-quality image-text datasets, enabling them to generally understand images well. However, the inherent difficulty in explicitly conveying fine-grained or spatially dense information in text, such as masks, poses a challenge for MLLMs, limiting their ability to answer questions requiring an understanding of detailed or localized visual elements. Drawing inspiration from the Retrieval-Augmented Generation (RAG) concept, this paper proposes a new visual prompt approach to integrate fine-grained external knowledge, gleaned from specialized vision models (e.g., instance segmentation/OCR models), into MLLMs. This is a promising yet underexplored direction for enhancing MLLMs' performance. Our approach diverges from concurrent works, which transform external knowledge into additional text prompts, necessitating the model to indirectly learn the correspondence between visual content and text coordinates. Instead, we propose embedding fine-grained knowledge information directly into a spatial embedding map as a visual prompt. This design can be effortlessly incorporated into various MLLMs, such as LLaVA and Mipha, considerably improving their visual understanding performance. Through rigorous experiments, we demonstrate that our method can enhance MLLM performance across nine benchmarks, amplifying their fine-grained context-aware capabilities.",
      "paper_authors": [
        "Yuanze Lin",
        "Yunsheng Li",
        "Dongdong Chen",
        "Weijian Xu",
        "Ronald Clark",
        "Philip Torr",
        "Lu Yuan"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-05",
      "update_time": "2024-07-05",
      "comments": null,
      "repo_url": "#"
    },
    "2407.04597": {
      "paper_id": "2407.04597v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.04597v1",
      "paper_key": "2407.04597",
      "paper_title": "Feature Attenuation of Defective Representation Can Resolve Incomplete Masking on Anomaly Detection",
      "paper_url": "http://arxiv.org/abs/2407.04597v1",
      "paper_abstract": "In unsupervised anomaly detection (UAD) research, while state-of-the-art models have reached a saturation point with extensive studies on public benchmark datasets, they adopt large-scale tailor-made neural networks (NN) for detection performance or pursued unified models for various tasks. Towards edge computing, it is necessary to develop a computationally efficient and scalable solution that avoids large-scale complex NNs. Motivated by this, we aim to optimize the UAD performance with minimal changes to NN settings. Thus, we revisit the reconstruction-by-inpainting approach and rethink to improve it by analyzing strengths and weaknesses. The strength of the SOTA methods is a single deterministic masking approach that addresses the challenges of random multiple masking that is inference latency and output inconsistency. Nevertheless, the issue of failure to provide a mask to completely cover anomalous regions is a remaining weakness. To mitigate this issue, we propose Feature Attenuation of Defective Representation (FADeR) that only employs two MLP layers which attenuates feature information of anomaly reconstruction during decoding. By leveraging FADeR, features of unseen anomaly patterns are reconstructed into seen normal patterns, reducing false alarms. Experimental results demonstrate that FADeR achieves enhanced performance compared to similar-scale NNs. Furthermore, our approach exhibits scalability in performance enhancement when integrated with other single deterministic masking methods in a plug-and-play manner.",
      "paper_authors": [
        "YeongHyeon Park",
        "Sungho Kang",
        "Myung Jin Kim",
        "Hyeong Seok Kim",
        "Juneho Yi"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-05",
      "update_time": "2024-07-05",
      "comments": "11 pages, 6 figures, 5 tables",
      "repo_url": "#"
    },
    "2407.04573": {
      "paper_id": "2407.04573v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.04573v1",
      "paper_key": "2407.04573",
      "paper_title": "VRSD: Rethinking Similarity and Diversity for Retrieval in Large Language Models",
      "paper_url": "http://arxiv.org/abs/2407.04573v1",
      "paper_abstract": "Vector retrieval algorithms are vital for semantic queries in the evolving landscape of Large Language Models (LLMs). Retrieving vectors that simultaneously meet criteria for both similarity and diversity significantly enhances the capabilities of LLM-based agents. Despite the widespread use of the Maximal Marginal Relevance (MMR) in retrieval scenarios with relevance and diversity requirements, fluctuations caused by variations in the parameter $ \\lambda $ within the MMR complicate the determination of the optimization trajectory in vector spaces, thus obscuring the direction of enhancement. Moreover, there is a lack of a robust theoretical analysis for the constraints of similarity and diversity in retrieval processes. This paper introduces a novel approach to characterizing both constraints through the relationship between the sum vector and the query vector. The proximity of these vectors addresses the similarity constraint, while necessitating that individual vectors within the sum vector divergently align with the query vector to satisfy the diversity constraint. We also formulate a new combinatorial optimization challenge, taking a selection of $k$ vectors from a set of candidates such that their sum vector maximally aligns with the query vector, a problem we demonstrate to be NP-complete. This establishes the profound difficulty of pursuing similarity and diversity simultaneously in vector retrieval and lays a theoretical groundwork for further research. Additionally, we present the heuristic algorithm Vectors Retrieval with Similarity and Diversity (VRSD) which not only has a definitive optimization goal and eschews the need for preset parameters but also offers a modest reduction in time complexity compared to MMR. Empirical validation further confirm that VRSD significantly surpasses MMR across various datasets.",
      "paper_authors": [
        "Hang Gao",
        "Yongfeng Zhang"
      ],
      "primary_category": "cs.IR",
      "publish_time": "2024-07-05",
      "update_time": "2024-07-05",
      "comments": null,
      "repo_url": "#"
    },
    "2407.04542": {
      "paper_id": "2407.04542v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.04542v1",
      "paper_key": "2407.04542",
      "paper_title": "Rethinking Image Compression on the Web with Generative AI",
      "paper_url": "http://arxiv.org/abs/2407.04542v1",
      "paper_abstract": "The rapid growth of the Internet, driven by social media, web browsing, and video streaming, has made images central to the Web experience, resulting in significant data transfer and increased webpage sizes. Traditional image compression methods, while reducing bandwidth, often degrade image quality. This paper explores a novel approach using generative AI to reconstruct images at the edge or client-side. We develop a framework that leverages text prompts and provides additional conditioning inputs like Canny edges and color palettes to a text-to-image model, achieving up to 99.8% bandwidth savings in the best cases and 92.6% on average, while maintaining high perceptual similarity. Empirical analysis and a user study show that our method preserves image meaning and structure more effectively than traditional compression methods, offering a promising solution for reducing bandwidth usage and improving Internet affordability with minimal degradation in image quality.",
      "paper_authors": [
        "Shayan Ali Hassan",
        "Danish Humair",
        "Ihsan Ayyub Qazi",
        "Zafar Ayyub Qazi"
      ],
      "primary_category": "cs.NI",
      "publish_time": "2024-07-05",
      "update_time": "2024-07-05",
      "comments": null,
      "repo_url": "#"
    },
    "2407.04538": {
      "paper_id": "2407.04538v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.04538v3",
      "paper_key": "2407.04538",
      "paper_title": "PDiscoFormer: Relaxing Part Discovery Constraints with Vision Transformers",
      "paper_url": "http://arxiv.org/abs/2407.04538v3",
      "paper_abstract": "Computer vision methods that explicitly detect object parts and reason on them are a step towards inherently interpretable models. Existing approaches that perform part discovery driven by a fine-grained classification task make very restrictive assumptions on the geometric properties of the discovered parts; they should be small and compact. Although this prior is useful in some cases, in this paper we show that pre-trained transformer-based vision models, such as self-supervised DINOv2 ViT, enable the relaxation of these constraints. In particular, we find that a total variation (TV) prior, which allows for multiple connected components of any size, substantially outperforms previous work. We test our approach on three fine-grained classification benchmarks: CUB, PartImageNet and Oxford Flowers, and compare our results to previously published methods as well as a re-implementation of the state-of-the-art method PDiscoNet with a transformer-based backbone. We consistently obtain substantial improvements across the board, both on part discovery metrics and the downstream classification task, showing that the strong inductive biases in self-supervised ViT models require to rethink the geometric priors that can be used for unsupervised part discovery.",
      "paper_authors": [
        "Ananthu Aniraj",
        "Cassio F. Dantas",
        "Dino Ienco",
        "Diego Marcos"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-05",
      "update_time": "2024-07-22",
      "comments": "Accepted as a main conference paper at the European Conference of\n  Computer Vision (ECCV) 2024",
      "repo_url": "https://github.com/ananthu-aniraj/pdiscoformer"
    },
    "2407.04476": {
      "paper_id": "2407.04476v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.04476v1",
      "paper_key": "2407.04476",
      "paper_title": "Rethinking Data Input for Point Cloud Upsampling",
      "paper_url": "http://arxiv.org/abs/2407.04476v1",
      "paper_abstract": "In recent years, point cloud upsampling has been widely applied in fields such as 3D reconstruction and surface generation. However, existing point cloud upsampling inputs are all patch based, and there is no research discussing the differences and principles between point cloud model full input and patch based input. In order to compare with patch based point cloud input, this article proposes a new data input method, which divides the full point cloud model to ensure shape integrity while training PU-GCN. This article was validated on the PU1K and ABC datasets, but the results showed that Patch based performance is better than model based full input i.e. Average Segment input. Therefore, this article explores the data input factors and model modules that affect the upsampling results of point clouds.",
      "paper_authors": [
        "Tongxu Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-05",
      "update_time": "2024-07-05",
      "comments": "16 pages, 6 figures",
      "repo_url": "#"
    },
    "2407.04263": {
      "paper_id": "2407.04263v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.04263v1",
      "paper_key": "2407.04263",
      "paper_title": "Drop it All or Pick it Up? How Developers Responded to the Log4JShell Vulnerability",
      "paper_url": "http://arxiv.org/abs/2407.04263v1",
      "paper_abstract": "Although using third-party libraries has become prevalent in contemporary software development, developers often struggle to update their dependencies. Prior works acknowledge that due to the migration effort, priority and other issues cause lags in the migration process. The common assumption is that developers should drop all other activities and prioritize fixing the vulnerability. Our objective is to understand developer behavior when facing high-risk vulnerabilities in their code. We explore the prolific, and possibly one of the cases of the Log4JShell, a vulnerability that has the highest severity rating ever, which received widespread media attention. Using a mixed-method approach, we analyze 219 GitHub Pull Requests (PR) and 354 issues belonging to 53 Maven projects affected by the Log4JShell vulnerability. Our study confirms that developers show a quick response taking from 5 to 6 days. However, instead of dropping everything, surprisingly developer activities tend to increase for all pending issues and PRs. Developer discussions involved either giving information (29.3\\%) and seeking information (20.6\\%), which is missing in existing support tools. Leveraging this possibly-one of a kind event, insights opens up a new line of research, causing us to rethink best practices and what developers need in order to efficiently fix vulnerabilities.",
      "paper_authors": [
        "Vittunyuta Maeprasart",
        "Ali Ouni",
        "Raula Gaikovina Kula"
      ],
      "primary_category": "cs.SE",
      "publish_time": "2024-07-05",
      "update_time": "2024-07-05",
      "comments": "Accepted to SERA24. arXiv admin note: text overlap with\n  arXiv:2406.11362",
      "repo_url": "#"
    },
    "2407.03926": {
      "paper_id": "2407.03926v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.03926v1",
      "paper_key": "2407.03926",
      "paper_title": "Rethinking the fundamental performance limits of integrated sensing and communication systems",
      "paper_url": "http://arxiv.org/abs/2407.03926v1",
      "paper_abstract": "Integrated sensing and communication (ISAC) has been recognized as a key enabler and feature of future wireless networks. In the existing works analyzing the performances of ISAC, discrete-time systems were commonly assumed, which, however, overlooked the impacts of temporal, spectral, and spatial properties. To address this issue, we establish a unified information model for the band-limited continuous-time ISAC systems. In the established information model, we employ a novel sensing performance metric, called the sensing mutual information (SMI). Through analysis, we show how the SMI can be utilized as a bridge between the mutual information domain and the mean squared error (MSE) domain. In addition, we illustrate the communication mutual information (CMI)-SMI and CMI-MSE regions to identify the performance bounds of ISAC systems in practical settings and reveal the trade-off between communication and sensing performances. Moreover, via analysis and numerical results, we provide two valuable insights into the design of novel ISAC-enabled systems: i) communication prefers the waveforms of random amplitude, sensing prefers the waveforms of constant amplitude, both communication and sensing favor the waveforms of low correlations with random phases; ii) There exists a linear positive proportional relationship between the allocated time-frequency resource and the achieved communication rate/sensing MSE.",
      "paper_authors": [
        "Zhouyuan Yu",
        "Xiaoling Hu",
        "Chenxi Liu",
        "Mugen Peng"
      ],
      "primary_category": "cs.IT",
      "publish_time": "2024-07-04",
      "update_time": "2024-07-04",
      "comments": null,
      "repo_url": "#"
    },
    "2407.02687": {
      "paper_id": "2407.02687v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.02687v1",
      "paper_key": "2407.02687",
      "paper_title": "No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models",
      "paper_url": "http://arxiv.org/abs/2407.02687v1",
      "paper_abstract": "Classifier-free guidance (CFG) has become the standard method for enhancing the quality of conditional diffusion models. However, employing CFG requires either training an unconditional model alongside the main diffusion model or modifying the training procedure by periodically inserting a null condition. There is also no clear extension of CFG to unconditional models. In this paper, we revisit the core principles of CFG and introduce a new method, independent condition guidance (ICG), which provides the benefits of CFG without the need for any special training procedures. Our approach streamlines the training process of conditional diffusion models and can also be applied during inference on any pre-trained conditional model. Additionally, by leveraging the time-step information encoded in all diffusion networks, we propose an extension of CFG, called time-step guidance (TSG), which can be applied to any diffusion model, including unconditional ones. Our guidance techniques are easy to implement and have the same sampling cost as CFG. Through extensive experiments, we demonstrate that ICG matches the performance of standard CFG across various conditional diffusion models. Moreover, we show that TSG improves generation quality in a manner similar to CFG, without relying on any conditional information.",
      "paper_authors": [
        "Seyedmorteza Sadat",
        "Manuel Kansy",
        "Otmar Hilliges",
        "Romann M. Weber"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-07-02",
      "update_time": "2024-07-02",
      "comments": null,
      "repo_url": "#"
    },
    "2407.02286": {
      "paper_id": "2407.02286v4",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.02286v4",
      "paper_key": "2407.02286",
      "paper_title": "Rethinking Data Augmentation for Robust LiDAR Semantic Segmentation in Adverse Weather",
      "paper_url": "http://arxiv.org/abs/2407.02286v4",
      "paper_abstract": "Existing LiDAR semantic segmentation methods often struggle with performance declines in adverse weather conditions. Previous work has addressed this issue by simulating adverse weather or employing universal data augmentation during training. However, these methods lack a detailed analysis and understanding of how adverse weather negatively affects LiDAR semantic segmentation performance. Motivated by this issue, we identified key factors of adverse weather and conducted a toy experiment to pinpoint the main causes of performance degradation: (1) Geometric perturbation due to refraction caused by fog or droplets in the air and (2) Point drop due to energy absorption and occlusions. Based on these findings, we propose new strategic data augmentation techniques. First, we introduced a Selective Jittering (SJ) that jitters points in the random range of depth (or angle) to mimic geometric perturbation. Additionally, we developed a Learnable Point Drop (LPD) to learn vulnerable erase patterns with a Deep Q-Learning Network to approximate the point drop phenomenon from adverse weather conditions. Without precise weather simulation, these techniques strengthen the LiDAR semantic segmentation model by exposing it to vulnerable conditions identified by our data-centric analysis. Experimental results confirmed the suitability of the proposed data augmentation methods for enhancing robustness against adverse weather conditions. Our method achieves a notable 39.5 mIoU on the SemanticKITTI-to-SemanticSTF benchmark, improving the baseline by 8.1\\%p and establishing a new state-of-the-art. Our code will be released at \\url{https://github.com/engineerJPark/LiDARWeather}.",
      "paper_authors": [
        "Junsung Park",
        "Kyungmin Kim",
        "Hyunjung Shim"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-02",
      "update_time": "2024-07-17",
      "comments": "29 pages, 11 figures, accpeted in ECCV 2024",
      "repo_url": "https://github.com/engineerjpark/lidarweather"
    },
    "2407.02176": {
      "paper_id": "2407.02176v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.02176v1",
      "paper_key": "2407.02176",
      "paper_title": "Spacetime-topological events",
      "paper_url": "http://arxiv.org/abs/2407.02176v1",
      "paper_abstract": "Time is, figuratively and literally, becoming the new dimension for crystalline matter. As such, rapid recent progress on time-varying media gave rise to the notion of temporal and spatiotemporal crystals. Fundamentally rethinking the role of time, which, in contrast to space exhibits a unique unidirectionality often referred to as the arrow of time, promises a new dimension also for topological physics. Here, we enter the new realm of time and spacetime topology: Firstly, we implement a time-topological time interface state. Secondly, we propose and observe a spacetime-topological event and demonstrate unique features like its limited collapse under disorder and causality-suppressed coupling. The new paradigms of time and spacetime topology unveil a distinctive role of causality and non-Hermiticity in topology and pave the way towards topological spatiotemporal wave control with unique robustness.",
      "paper_authors": [
        "Joshua Feis",
        "Sebastian Weidemann",
        "Tom Sheppard",
        "Hannah M. Price",
        "Alexander Szameit"
      ],
      "primary_category": "physics.optics",
      "publish_time": "2024-07-02",
      "update_time": "2024-07-02",
      "comments": null,
      "repo_url": "#"
    },
    "2407.01925": {
      "paper_id": "2407.01925v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.01925v1",
      "paper_key": "2407.01925",
      "paper_title": "Looking From the Future: Multi-order Iterations Can Enhance Adversarial Attack Transferability",
      "paper_url": "http://arxiv.org/abs/2407.01925v1",
      "paper_abstract": "Various methods try to enhance adversarial transferability by improving the generalization from different perspectives. In this paper, we rethink the optimization process and propose a novel sequence optimization concept, which is named Looking From the Future (LFF). LFF makes use of the original optimization process to refine the very first local optimization choice. Adapting the LFF concept to the adversarial attack task, we further propose an LFF attack as well as an MLFF attack with better generalization ability. Furthermore, guiding with the LFF concept, we propose an $LLF^{\\mathcal{N}}$ attack which entends the LFF attack to a multi-order attack, further enhancing the transfer attack ability. All our proposed methods can be directly applied to the iteration-based attack methods. We evaluate our proposed method on the ImageNet1k dataset by applying several SOTA adversarial attack methods under four kinds of tasks. Experimental results show that our proposed method can greatly enhance the attack transferability. Ablation experiments are also applied to verify the effectiveness of each component. The source code will be released after this paper is accepted.",
      "paper_authors": [
        "Zijian Ying",
        "Qianmu Li",
        "Tao Wang",
        "Zhichao Lian",
        "Shunmei Meng",
        "Xuyun Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-07-02",
      "update_time": "2024-07-02",
      "comments": null,
      "repo_url": "#"
    },
    "2407.01919": {
      "paper_id": "2407.01919v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.01919v1",
      "paper_key": "2407.01919",
      "paper_title": "A Method to Facilitate Membership Inference Attacks in Deep Learning Models",
      "paper_url": "http://arxiv.org/abs/2407.01919v1",
      "paper_abstract": "Modern machine learning (ML) ecosystems offer a surging number of ML frameworks and code repositories that can greatly facilitate the development of ML models. Today, even ordinary data holders who are not ML experts can apply off-the-shelf codebase to build high-performance ML models on their data, many of which are sensitive in nature (e.g., clinical records).   In this work, we consider a malicious ML provider who supplies model-training code to the data holders, does not have access to the training process, and has only black-box query access to the resulting model. In this setting, we demonstrate a new form of membership inference attack that is strictly more powerful than prior art. Our attack empowers the adversary to reliably de-identify all the training samples (average >99% attack TPR@0.1% FPR), and the compromised models still maintain competitive performance as their uncorrupted counterparts (average <1% accuracy drop). Moreover, we show that the poisoned models can effectively disguise the amplified membership leakage under common membership privacy auditing, which can only be revealed by a set of secret samples known by the adversary.   Overall, our study not only points to the worst-case membership privacy leakage, but also unveils a common pitfall underlying existing privacy auditing methods, which calls for future efforts to rethink the current practice of auditing membership privacy in machine learning models.",
      "paper_authors": [
        "Zitao Chen",
        "Karthik Pattabiraman"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-07-02",
      "update_time": "2024-07-02",
      "comments": "NDSS'25 (a shorter version of this paper will appear in the\n  conference proceeding)",
      "repo_url": "https://github.com/DependableSystemsLab/code_poison_MIA"
    },
    "2407.01085": {
      "paper_id": "2407.01085v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.01085v2",
      "paper_key": "2407.01085",
      "paper_title": "Rethinking LLM-based Preference Evaluation",
      "paper_url": "http://arxiv.org/abs/2407.01085v2",
      "paper_abstract": "The use of large language model (LLM)-based preference evaluations has become widespread for comparing model responses, but it has revealed a notable bias towards longer responses, questioning the reliability of such evaluations. This paper explores the length bias in LLM evaluations from a data-centric perspective, analyzing 14 commonly used preference datasets and 10 reward models. Our findings indicate that human preference labeling favors longer responses and this spurious correlation is learned by the reward model and subsequently propagated to the aligned model during training. We decompose the preference evaluation metric, i.e., win rate, from the perspective of human to identify the deeper factors and conclude that the win rate is affected by two axes of model response: desirability and information mass, where the former is length-independent and related to trustworthiness, and the latter is length-dependent and can be represented by conditional entropy. Controlled experiments demonstrate that response length impacts evaluations by influencing information mass. To ensure reliable evaluation metrics that assess content quality without being confounded by response length, we propose AdapAlpaca, a simple yet effective adjustment to win rate measurement. Specifically, by adjusting the lengths of reference answers to match the test model's answers within the same interval, we debias information mass relative to length, ensuring a fair model evaluation. Furthermore, we investigate length bias in DPO using AlpacaEval and AdapAlpaca. By testing Tulu2 and Tulu2-dpo at 7B, 13B, and 70B scales, we found that DPO leads to higher human preference, but this gain is amplified by response length, with AlpacaEval showing higher win rates gain than AdapAlpaca.",
      "paper_authors": [
        "Zhengyu Hu",
        "Linxin Song",
        "Jieyu Zhang",
        "Zheyuan Xiao",
        "Jingang Wang",
        "Zhenyu Chen",
        "Hui Xiong"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-07-01",
      "update_time": "2024-08-08",
      "comments": null,
      "repo_url": "#"
    },
    "2406.19552": {
      "paper_id": "2406.19552v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.19552v1",
      "paper_key": "2406.19552",
      "paper_title": "Rethinking harmless refusals when fine-tuning foundation models",
      "paper_url": "http://arxiv.org/abs/2406.19552v1",
      "paper_abstract": "In this paper, we investigate the degree to which fine-tuning in Large Language Models (LLMs) effectively mitigates versus merely conceals undesirable behavior. Through the lens of semi-realistic role-playing exercises designed to elicit such behaviors, we explore the response dynamics of LLMs post fine-tuning interventions. Our methodology involves prompting models for Chain-of-Thought (CoT) reasoning and analyzing the coherence between the reasoning traces and the resultant outputs. Notably, we identify a pervasive phenomenon we term \\emph{reason-based deception}, where models either stop producing reasoning traces or produce seemingly ethical reasoning traces that belie the unethical nature of their final outputs. We further examine the efficacy of response strategies (polite refusal versus explicit rebuttal) in curbing the occurrence of undesired behavior in subsequent outputs of multi-turn interactions. Our findings reveal that explicit rebuttals significantly outperform polite refusals in preventing the continuation of undesired outputs and nearly eliminate reason-based deception, challenging current practices in model fine-tuning. Accordingly, the two key contributions of this paper are (1) defining and studying reason-based deception, a new type of hidden behavior, and (2) demonstrating that rebuttals provide a more robust response model to harmful requests than refusals, thereby highlighting the need to reconsider the response strategies in fine-tuning approaches.",
      "paper_authors": [
        "Florin Pop",
        "Judd Rosenblatt",
        "Diogo Schwerz de Lucena",
        "Michael Vaiana"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-06-27",
      "update_time": "2024-06-27",
      "comments": "ICLR 2024 AGI Workshop Poster",
      "repo_url": "#"
    },
    "2406.18967": {
      "paper_id": "2406.18967v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.18967v2",
      "paper_key": "2406.18967",
      "paper_title": "Structural Attention: Rethinking Transformer for Unpaired Medical Image Synthesis",
      "paper_url": "http://arxiv.org/abs/2406.18967v2",
      "paper_abstract": "Unpaired medical image synthesis aims to provide complementary information for an accurate clinical diagnostics, and address challenges in obtaining aligned multi-modal medical scans. Transformer-based models excel in imaging translation tasks thanks to their ability to capture long-range dependencies. Although effective in supervised training settings, their performance falters in unpaired image synthesis, particularly in synthesizing structural details. This paper empirically demonstrates that, lacking strong inductive biases, Transformer can converge to non-optimal solutions in the absence of paired data. To address this, we introduce UNet Structured Transformer (UNest), a novel architecture incorporating structural inductive biases for unpaired medical image synthesis. We leverage the foundational Segment-Anything Model to precisely extract the foreground structure and perform structural attention within the main anatomy. This guides the model to learn key anatomical regions, thus improving structural synthesis under the lack of supervision in unpaired training. Evaluated on two public datasets, spanning three modalities, i.e., MR, CT, and PET, UNest improves recent methods by up to 19.30% across six medical image synthesis tasks. Our code is released at https://github.com/HieuPhan33/MICCAI2024-UNest.",
      "paper_authors": [
        "Vu Minh Hieu Phan",
        "Yutong Xie",
        "Bowen Zhang",
        "Yuankai Qi",
        "Zhibin Liao",
        "Antonios Perperidis",
        "Son Lam Phung",
        "Johan W. Verjans",
        "Minh-Son To"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-27",
      "update_time": "2024-08-28",
      "comments": "MICCAI version before camera ready",
      "repo_url": "https://github.com/hieuphan33/miccai2024-unest"
    },
    "2406.17642": {
      "paper_id": "2406.17642v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.17642v1",
      "paper_key": "2406.17642",
      "paper_title": "Banishing LLM Hallucinations Requires Rethinking Generalization",
      "paper_url": "http://arxiv.org/abs/2406.17642v1",
      "paper_abstract": "Despite their powerful chat, coding, and reasoning abilities, Large Language Models (LLMs) frequently hallucinate. Conventional wisdom suggests that hallucinations are a consequence of a balance between creativity and factuality, which can be mitigated, but not eliminated, by grounding the LLM in external knowledge sources. Through extensive systematic experiments, we show that these traditional approaches fail to explain why LLMs hallucinate in practice. Specifically, we show that LLMs augmented with a massive Mixture of Memory Experts (MoME) can easily memorize large datasets of random numbers. We corroborate these experimental findings with a theoretical construction showing that simple neural networks trained to predict the next token hallucinate when the training loss is above a threshold as it usually does in practice when training on internet scale data. We interpret our findings by comparing against traditional retrieval methods for mitigating hallucinations. We use our findings to design a first generation model for removing hallucinations -- Lamini-1 -- that stores facts in a massive mixture of millions of memory experts that are retrieved dynamically.",
      "paper_authors": [
        "Johnny Li",
        "Saksham Consul",
        "Eda Zhou",
        "James Wong",
        "Naila Farooqui",
        "Yuxin Ye",
        "Nithyashree Manohar",
        "Zhuxiaona Wei",
        "Tian Wu",
        "Ben Echols",
        "Sharon Zhou",
        "Gregory Diamos"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-06-25",
      "update_time": "2024-06-25",
      "comments": null,
      "repo_url": "#"
    },
    "2406.15796": {
      "paper_id": "2406.15796v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.15796v1",
      "paper_key": "2406.15796",
      "paper_title": "Rethinking Entity-level Unlearning for Large Language Models",
      "paper_url": "http://arxiv.org/abs/2406.15796v1",
      "paper_abstract": "Large language model unlearning has gained increasing attention due to its potential to mitigate security and privacy concerns. Current research predominantly focuses on Instance-level unlearning, specifically aiming at forgetting predefined instances of sensitive content. However, a notable gap still exists in exploring the deletion of complete entity-related information, which is crucial in many real-world scenarios, such as copyright protection. To this end, we propose a novel task of Entity-level unlearning, where the entity-related knowledge within the target model is supposed to be entirely erased. Given the challenge of practically accessing all entity-related knowledge within a model, we begin by simulating entity-level unlearning scenarios through fine-tuning models to introduce pseudo entities. Following this, we develop baseline methods inspired by trending unlearning techniques and conduct a detailed comparison of their effectiveness in this task. Extensive experiments reveal that current unlearning algorithms struggle to achieve effective entity-level unlearning. Additionally, our analyses further indicate that entity-related knowledge injected through fine-tuning is more susceptible than original entities from pre-training during unlearning, highlighting the necessity for more thorough pseudo-entity injection methods to make them closer to pre-trained knowledge.",
      "paper_authors": [
        "Weitao Ma",
        "Xiaocheng Feng",
        "Weihong Zhong",
        "Lei Huang",
        "Yangfan Ye",
        "Bing Qin"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-06-22",
      "update_time": "2024-06-22",
      "comments": "Work in progress",
      "repo_url": "#"
    },
    "2406.15762": {
      "paper_id": "2406.15762v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.15762v1",
      "paper_key": "2406.15762",
      "paper_title": "Rethinking the Diffusion Models for Numerical Tabular Data Imputation from the Perspective of Wasserstein Gradient Flow",
      "paper_url": "http://arxiv.org/abs/2406.15762v1",
      "paper_abstract": "Diffusion models (DMs) have gained attention in Missing Data Imputation (MDI), but there remain two long-neglected issues to be addressed: (1). Inaccurate Imputation, which arises from inherently sample-diversification-pursuing generative process of DMs. (2). Difficult Training, which stems from intricate design required for the mask matrix in model training stage. To address these concerns within the realm of numerical tabular datasets, we introduce a novel principled approach termed Kernelized Negative Entropy-regularized Wasserstein gradient flow Imputation (KnewImp). Specifically, based on Wasserstein gradient flow (WGF) framework, we first prove that issue (1) stems from the cost functionals implicitly maximized in DM-based MDI are equivalent to the MDI's objective plus diversification-promoting non-negative terms. Based on this, we then design a novel cost functional with diversification-discouraging negative entropy and derive our KnewImp approach within WGF framework and reproducing kernel Hilbert space. After that, we prove that the imputation procedure of KnewImp can be derived from another cost functional related to the joint distribution, eliminating the need for the mask matrix and hence naturally addressing issue (2). Extensive experiments demonstrate that our proposed KnewImp approach significantly outperforms existing state-of-the-art methods.",
      "paper_authors": [
        "Zhichao Chen",
        "Haoxuan Li",
        "Fangyikang Wang",
        "Odin Zhang",
        "Hu Xu",
        "Xiaoyu Jiang",
        "Zhihuan Song",
        "Eric H. Wang"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-06-22",
      "update_time": "2024-06-22",
      "comments": null,
      "repo_url": "#"
    },
    "2406.15320": {
      "paper_id": "2406.15320v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.15320v1",
      "paper_key": "2406.15320",
      "paper_title": "Rethinking Remote Sensing Change Detection With A Mask View",
      "paper_url": "http://arxiv.org/abs/2406.15320v1",
      "paper_abstract": "Remote sensing change detection aims to compare two or more images recorded for the same area but taken at different time stamps to quantitatively and qualitatively assess changes in geographical entities and environmental factors. Mainstream models usually built on pixel-by-pixel change detection paradigms, which cannot tolerate the diversity of changes due to complex scenes and variation in imaging conditions. To address this shortcoming, this paper rethinks the change detection with the mask view, and further proposes the corresponding: 1) meta-architecture CDMask and 2) instance network CDMaskFormer. Components of CDMask include Siamese backbone, change extractor, pixel decoder, transformer decoder and normalized detector, which ensures the proper functioning of the mask detection paradigm. Since the change query can be adaptively updated based on the bi-temporal feature content, the proposed CDMask can adapt to different latent data distributions, thus accurately identifying regions of interest changes in complex scenarios. Consequently, we further propose the instance network CDMaskFormer customized for the change detection task, which includes: (i) a Spatial-temporal convolutional attention-based instantiated change extractor to capture spatio-temporal context simultaneously with lightweight operations; and (ii) a scene-guided axial attention-instantiated transformer decoder to extract more spatial details. State-of-the-art performance of CDMaskFormer is achieved on five benchmark datasets with a satisfactory efficiency-accuracy trade-off. Code is available at https://github.com/xwmaxwma/rschange.",
      "paper_authors": [
        "Xiaowen Ma",
        "Zhenkai Wu",
        "Rongrong Lian",
        "Wei Zhang",
        "Siyang Song"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-21",
      "update_time": "2024-06-21",
      "comments": "Under review",
      "repo_url": "https://github.com/xwmaxwma/rschange"
    },
    "2406.15524": {
      "paper_id": "2406.15524v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.15524v1",
      "paper_key": "2406.15524",
      "paper_title": "Rethinking Pruning Large Language Models: Benefits and Pitfalls of Reconstruction Error Minimization",
      "paper_url": "http://arxiv.org/abs/2406.15524v1",
      "paper_abstract": "This work suggests fundamentally rethinking the current practice of pruning large language models (LLMs). The way it is done is by divide and conquer: split the model into submodels, sequentially prune them, and reconstruct predictions of the dense counterparts on small calibration data one at a time; the final model is obtained simply by putting the resulting sparse submodels together. While this approach enables pruning under memory constraints, it generates high reconstruction errors. In this work, we first present an array of reconstruction techniques that can significantly reduce this error by more than $90\\%$. Unwittingly, however, we discover that minimizing reconstruction error is not always ideal and can overfit the given calibration data, resulting in rather increased language perplexity and poor performance at downstream tasks. We find out that a strategy of self-generating calibration data can mitigate this trade-off between reconstruction and generalization, suggesting new directions in the presence of both benefits and pitfalls of reconstruction for pruning LLMs.",
      "paper_authors": [
        "Sungbin Shin",
        "Wonpyo Park",
        "Jaeho Lee",
        "Namhoon Lee"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-06-21",
      "update_time": "2024-06-21",
      "comments": null,
      "repo_url": "#"
    },
    "2406.14115": {
      "paper_id": "2406.14115v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.14115v1",
      "paper_key": "2406.14115",
      "paper_title": "Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models",
      "paper_url": "http://arxiv.org/abs/2406.14115v1",
      "paper_abstract": "Data selection for fine-tuning Large Language Models (LLMs) aims to select a high-quality subset from a given candidate dataset to train a Pending Fine-tune Model (PFM) into a Selective-Enhanced Model (SEM). It can improve the model performance and accelerate the training process. Although a few surveys have investigated related works of data selection, there is a lack of comprehensive comparison between existing methods due to their various experimental settings. To address this issue, we first propose a three-stage scheme for data selection and comprehensively review existing works according to this scheme. Then, we design a unified comparing method with ratio-based efficiency indicators and ranking-based feasibility indicators to overcome the difficulty of comparing various models with diverse experimental settings. After an in-depth comparative analysis, we find that the more targeted method with data-specific and model-specific quality labels has higher efficiency, but the introduction of additional noise information should be avoided when designing selection algorithms. Finally, we summarize the trends in data selection and highlight the short-term and long-term challenges to guide future research.",
      "paper_authors": [
        "Ziche Liu",
        "Rui Ke",
        "Feng Jiang",
        "Haizhou Li"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-06-20",
      "update_time": "2024-06-20",
      "comments": null,
      "repo_url": "#"
    },
    "2406.13733": {
      "paper_id": "2406.13733v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.13733v1",
      "paper_key": "2406.13733",
      "paper_title": "You can't handle the (dirty) truth: Data-centric insights improve pseudo-labeling",
      "paper_url": "http://arxiv.org/abs/2406.13733v1",
      "paper_abstract": "Pseudo-labeling is a popular semi-supervised learning technique to leverage unlabeled data when labeled samples are scarce. The generation and selection of pseudo-labels heavily rely on labeled data. Existing approaches implicitly assume that the labeled data is gold standard and 'perfect'. However, this can be violated in reality with issues such as mislabeling or ambiguity. We address this overlooked aspect and show the importance of investigating labeled data quality to improve any pseudo-labeling method. Specifically, we introduce a novel data characterization and selection framework called DIPS to extend pseudo-labeling. We select useful labeled and pseudo-labeled samples via analysis of learning dynamics. We demonstrate the applicability and impact of DIPS for various pseudo-labeling methods across an extensive range of real-world tabular and image datasets. Additionally, DIPS improves data efficiency and reduces the performance distinctions between different pseudo-labelers. Overall, we highlight the significant benefits of a data-centric rethinking of pseudo-labeling in real-world settings.",
      "paper_authors": [
        "Nabeel Seedat",
        "Nicolas Huynh",
        "Fergus Imrie",
        "Mihaela van der Schaar"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-06-19",
      "update_time": "2024-06-19",
      "comments": "Published in the Journal of Data-centric Machine Learning Research\n  (DMLR) *Seedat & Huynh contributed equally",
      "repo_url": "https://github.com/seedatnabeel/dips"
    },
    "2406.13674": {
      "paper_id": "2406.13674v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.13674v1",
      "paper_key": "2406.13674",
      "paper_title": "Rethinking Abdominal Organ Segmentation (RAOS) in the clinical scenario: A robustness evaluation benchmark with challenging cases",
      "paper_url": "http://arxiv.org/abs/2406.13674v1",
      "paper_abstract": "Deep learning has enabled great strides in abdominal multi-organ segmentation, even surpassing junior oncologists on common cases or organs. However, robustness on corner cases and complex organs remains a challenging open problem for clinical adoption. To investigate model robustness, we collected and annotated the RAOS dataset comprising 413 CT scans ($\\sim$80k 2D images, $\\sim$8k 3D organ annotations) from 413 patients each with 17 (female) or 19 (male) labelled organs, manually delineated by oncologists. We grouped scans based on clinical information into 1) diagnosis/radiotherapy (317 volumes), 2) partial excision without the whole organ missing (22 volumes), and 3) excision with the whole organ missing (74 volumes). RAOS provides a potential benchmark for evaluating model robustness including organ hallucination. It also includes some organs that can be very hard to access on public datasets like the rectum, colon, intestine, prostate and seminal vesicles. We benchmarked several state-of-the-art methods in these three clinical groups to evaluate performance and robustness. We also assessed cross-generalization between RAOS and three public datasets. This dataset and comprehensive analysis establish a potential baseline for future robustness research: \\url{https://github.com/Luoxd1996/RAOS}.",
      "paper_authors": [
        "Xiangde Luo",
        "Zihan Li",
        "Shaoting Zhang",
        "Wenjun Liao",
        "Guotai Wang"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2024-06-19",
      "update_time": "2024-06-19",
      "comments": "10 pages, 1 figure, 6 tables, Early Accept to MICCAI 2024",
      "repo_url": "https://github.com/luoxd1996/raos"
    },
    "2407.09515": {
      "paper_id": "2407.09515v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.09515v1",
      "paper_key": "2407.09515",
      "paper_title": "Rethinking Knee Osteoarthritis Severity Grading: A Few Shot Self-Supervised Contrastive Learning Approach",
      "paper_url": "http://arxiv.org/abs/2407.09515v1",
      "paper_abstract": "Knee Osteoarthritis (OA) is a debilitating disease affecting over 250 million people worldwide. Currently, radiologists grade the severity of OA on an ordinal scale from zero to four using the Kellgren-Lawrence (KL) system. Recent studies have raised concern in relation to the subjectivity of the KL grading system, highlighting the requirement for an automated system, while also indicating that five ordinal classes may not be the most appropriate approach for assessing OA severity. This work presents preliminary results of an automated system with a continuous grading scale. This system, namely SS-FewSOME, uses self-supervised pre-training to learn robust representations of the features of healthy knee X-rays. It then assesses the OA severity by the X-rays' distance to the normal representation space. SS-FewSOME initially trains on only 'few' examples of healthy knee X-rays, thus reducing the barriers to clinical implementation by eliminating the need for large training sets and costly expert annotations that existing automated systems require. The work reports promising initial results, obtaining a positive Spearman Rank Correlation Coefficient of 0.43, having had access to only 30 ground truth labels at training time.",
      "paper_authors": [
        "Niamh Belton",
        "Misgina Tsighe Hagos",
        "Aonghus Lawlor",
        "Kathleen M. Curran"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2024-06-18",
      "update_time": "2024-06-18",
      "comments": null,
      "repo_url": "#"
    },
    "2406.11661": {
      "paper_id": "2406.11661v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.11661v2",
      "paper_key": "2406.11661",
      "paper_title": "Cultural Conditioning or Placebo? On the Effectiveness of Socio-Demographic Prompting",
      "paper_url": "http://arxiv.org/abs/2406.11661v2",
      "paper_abstract": "Socio-demographic prompting is a commonly employed approach to study cultural biases in LLMs as well as for aligning models to certain cultures. In this paper, we systematically probe four LLMs (Llama 3, Mistral v0.2, GPT-3.5 Turbo and GPT-4) with prompts that are conditioned on culturally sensitive and non-sensitive cues, on datasets that are supposed to be culturally sensitive (EtiCor and CALI) or neutral (MMLU and ETHICS). We observe that all models except GPT-4 show significant variations in their responses on both kinds of datasets for both kinds of prompts, casting doubt on the robustness of the culturally-conditioned prompting as a method for eliciting cultural bias in models or as an alignment strategy. The work also calls rethinking the control experiment design to tease apart the cultural conditioning of responses from \"placebo effect\", i.e., random perturbations of model responses due to arbitrary tokens in the prompt.",
      "paper_authors": [
        "Sagnik Mukherjee",
        "Muhammad Farid Adilazuarda",
        "Sunayana Sitaram",
        "Kalika Bali",
        "Alham Fikri Aji",
        "Monojit Choudhury"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-06-17",
      "update_time": "2024-06-20",
      "comments": null,
      "repo_url": "#"
    },
    "2406.11921": {
      "paper_id": "2406.11921v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.11921v1",
      "paper_key": "2406.11921",
      "paper_title": "Rethinking Spatio-Temporal Transformer for Traffic Prediction:Multi-level Multi-view Augmented Learning Framework",
      "paper_url": "http://arxiv.org/abs/2406.11921v1",
      "paper_abstract": "Traffic prediction is a challenging spatio-temporal forecasting problem that involves highly complex spatio-temporal correlations. This paper proposes a Multi-level Multi-view Augmented Spatio-temporal Transformer (LVSTformer) for traffic prediction. The model aims to capture spatial dependencies from three different levels: local geographic, global semantic, and pivotal nodes, along with long- and short-term temporal dependencies. Specifically, we design three spatial augmented views to delve into the spatial information from the perspectives of local, global, and pivotal nodes. By combining three spatial augmented views with three parallel spatial self-attention mechanisms, the model can comprehensively captures spatial dependencies at different levels. We design a gated temporal self-attention mechanism to effectively capture long- and short-term temporal dependencies. Furthermore, a spatio-temporal context broadcasting module is introduced between two spatio-temporal layers to ensure a well-distributed allocation of attention scores, alleviating overfitting and information loss, and enhancing the generalization ability and robustness of the model. A comprehensive set of experiments is conducted on six well-known traffic benchmarks, the experimental results demonstrate that LVSTformer achieves state-of-the-art performance compared to competing baselines, with the maximum improvement reaching up to 4.32%.",
      "paper_authors": [
        "Jiaqi Lin",
        "Qianqian Ren"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-06-17",
      "update_time": "2024-06-17",
      "comments": null,
      "repo_url": "#"
    },
    "2406.11214": {
      "paper_id": "2406.11214v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.11214v2",
      "paper_key": "2406.11214",
      "paper_title": "Large Language Model Tokenizer Bias: A Case Study and Solution on GPT-4o",
      "paper_url": "http://arxiv.org/abs/2406.11214v2",
      "paper_abstract": "Recent advancements in large language models (LLMs), such as GPT-4 and GPT-4o, have shown exceptional performance, especially in languages with abundant resources like English, thanks to extensive datasets that ensure robust training. Conversely, these models exhibit limitations when processing under-resourced languages such as Chinese and Korean, where issues including hallucinatory responses remain prevalent. This paper traces the roots of these disparities to the tokenization process inherent to these models. Specifically, it explores how the tokenizer vocabulary, often used to speed up the tokenization process and reduce tokens but constructed independently of the actual model training data, inadequately represents non-English languages. This misrepresentation results in the propagation of 'under-trained' or 'untrained' tokens, which perpetuate biases and pose serious concerns related to data security and ethical standards. We aim to dissect the tokenization mechanics of GPT-4o, illustrating how its simplified token-handling methods amplify these risks and offer strategic solutions to mitigate associated security and ethical issues. Through this study, we emphasize the critical need to rethink tokenization frameworks to foster more equitable and secure AI technologies.",
      "paper_authors": [
        "Jin Yang",
        "Zhiqiang Wang",
        "Yanbin Lin",
        "Zunduo Zhao"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-06-17",
      "update_time": "2024-08-11",
      "comments": "6 pages, 3 figures, and 5 tables",
      "repo_url": "https://github.com/yeyimilk/llmgpt4o"
    },
    "2407.15016": {
      "paper_id": "2407.15016v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.15016v1",
      "paper_key": "2407.15016",
      "paper_title": "Rethinking Digitalization and Climate: Don't Predict, Mitigate",
      "paper_url": "http://arxiv.org/abs/2407.15016v1",
      "paper_abstract": "Digitalization is a core component of the green transition. Today's focus is on quantifying and pre-dicting the climate effects of digitalization through various life-cycle assessments and baseline sce-nario methodologies. Here we argue that this is a mistake. Most attempts at prediction are based on three implicit assumptions: (a) the digital carbon footprint can be quantified, (b) business-as-usual with episodic change leading to a new era of stability, and (c) investments in digitalization will be delivered within the cost, timeframe, and benefits described in their business cases. We problema-tize each assumption within the context of digitalization and argue that the digital carbon footprint is inherently unpredictable. We build on uncertainty literature to show that even if you cannot predict, you can still mitigate. On that basis, we propose to rethink practice on the digital carbon footprint from prediction to mitigation.",
      "paper_authors": [
        "Daria Gritsenko",
        "Jon Aaen",
        "Bent Flyvbjerg"
      ],
      "primary_category": "cs.CY",
      "publish_time": "2024-06-16",
      "update_time": "2024-06-16",
      "comments": null,
      "repo_url": "#"
    },
    "2406.11036": {
      "paper_id": "2406.11036v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.11036v1",
      "paper_key": "2406.11036",
      "paper_title": "garak: A Framework for Security Probing Large Language Models",
      "paper_url": "http://arxiv.org/abs/2406.11036v1",
      "paper_abstract": "As Large Language Models (LLMs) are deployed and integrated into thousands of applications, the need for scalable evaluation of how models respond to adversarial attacks grows rapidly. However, LLM security is a moving target: models produce unpredictable output, are constantly updated, and the potential adversary is highly diverse: anyone with access to the internet and a decent command of natural language. Further, what constitutes a security weak in one context may not be an issue in a different context; one-fits-all guardrails remain theoretical. In this paper, we argue that it is time to rethink what constitutes ``LLM security'', and pursue a holistic approach to LLM security evaluation, where exploration and discovery of issues are central. To this end, this paper introduces garak (Generative AI Red-teaming and Assessment Kit), a framework which can be used to discover and identify vulnerabilities in a target LLM or dialog system. garak probes an LLM in a structured fashion to discover potential vulnerabilities. The outputs of the framework describe a target model's weaknesses, contribute to an informed discussion of what composes vulnerabilities in unique contexts, and can inform alignment and policy discussions for LLM deployment.",
      "paper_authors": [
        "Leon Derczynski",
        "Erick Galinkin",
        "Jeffrey Martin",
        "Subho Majumdar",
        "Nanna Inie"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-06-16",
      "update_time": "2024-06-16",
      "comments": "https://garak.ai",
      "repo_url": "https://github.com/leondz/garak"
    },
    "2406.10828": {
      "paper_id": "2406.10828v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.10828v1",
      "paper_key": "2406.10828",
      "paper_title": "PyramidMamba: Rethinking Pyramid Feature Fusion with Selective Space State Model for Semantic Segmentation of Remote Sensing Imagery",
      "paper_url": "http://arxiv.org/abs/2406.10828v1",
      "paper_abstract": "Semantic segmentation, as a basic tool for intelligent interpretation of remote sensing images, plays a vital role in many Earth Observation (EO) applications. Nowadays, accurate semantic segmentation of remote sensing images remains a challenge due to the complex spatial-temporal scenes and multi-scale geo-objects. Driven by the wave of deep learning (DL), CNN- and Transformer-based semantic segmentation methods have been explored widely, and these two architectures both revealed the importance of multi-scale feature representation for strengthening semantic information of geo-objects. However, the actual multi-scale feature fusion often comes with the semantic redundancy issue due to homogeneous semantic contents in pyramid features. To handle this issue, we propose a novel Mamba-based segmentation network, namely PyramidMamba. Specifically, we design a plug-and-play decoder, which develops a dense spatial pyramid pooling (DSPP) to encode rich multi-scale semantic features and a pyramid fusion Mamba (PFM) to reduce semantic redundancy in multi-scale feature fusion. Comprehensive ablation experiments illustrate the effectiveness and superiority of the proposed method in enhancing multi-scale feature representation as well as the great potential for real-time semantic segmentation. Moreover, our PyramidMamba yields state-of-the-art performance on three publicly available datasets, i.e. the OpenEarthMap (70.8% mIoU), ISPRS Vaihingen (84.8% mIoU) and Potsdam (88.0% mIoU) datasets. The code will be available at https://github.com/WangLibo1995/GeoSeg.",
      "paper_authors": [
        "Libo Wang",
        "Dongxu Li",
        "Sijun Dong",
        "Xiaoliang Meng",
        "Xiaokang Zhang",
        "Danfeng Hong"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-16",
      "update_time": "2024-06-16",
      "comments": null,
      "repo_url": "https://github.com/WangLibo1995/GeoSeg"
    },
    "2407.12005": {
      "paper_id": "2407.12005v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2407.12005v1",
      "paper_key": "2407.12005",
      "paper_title": "VCEval: Rethinking What is a Good Educational Video and How to Automatically Evaluate It",
      "paper_url": "http://arxiv.org/abs/2407.12005v1",
      "paper_abstract": "Online courses have significantly lowered the barrier to accessing education, yet the varying content quality of these videos poses challenges. In this work, we focus on the task of automatically evaluating the quality of video course content. We have constructed a dataset with a substantial collection of video courses and teaching materials. We propose three evaluation principles and design a new evaluation framework, \\textit{VCEval}, based on these principles. The task is modeled as a multiple-choice question-answering task, with a language model serving as the evaluator. Our method effectively distinguishes video courses of different content quality and produces a range of interpretable results.",
      "paper_authors": [
        "Xiaoxuan Zhu",
        "Zhouhong Gu",
        "Sihang Jiang",
        "Zhixu Li",
        "Hongwei Feng",
        "Yanghua Xiao"
      ],
      "primary_category": "cs.MM",
      "publish_time": "2024-06-15",
      "update_time": "2024-06-15",
      "comments": null,
      "repo_url": "#"
    },
    "2406.09867": {
      "paper_id": "2406.09867v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.09867v1",
      "paper_key": "2406.09867",
      "paper_title": "Rethinking the Evaluation of Out-of-Distribution Detection: A Sorites Paradox",
      "paper_url": "http://arxiv.org/abs/2406.09867v1",
      "paper_abstract": "Most existing out-of-distribution (OOD) detection benchmarks classify samples with novel labels as the OOD data. However, some marginal OOD samples actually have close semantic contents to the in-distribution (ID) sample, which makes determining the OOD sample a Sorites Paradox. In this paper, we construct a benchmark named Incremental Shift OOD (IS-OOD) to address the issue, in which we divide the test samples into subsets with different semantic and covariate shift degrees relative to the ID dataset. The data division is achieved through a shift measuring method based on our proposed Language Aligned Image feature Decomposition (LAID). Moreover, we construct a Synthetic Incremental Shift (Syn-IS) dataset that contains high-quality generated images with more diverse covariate contents to complement the IS-OOD benchmark. We evaluate current OOD detection methods on our benchmark and find several important insights: (1) The performance of most OOD detection methods significantly improves as the semantic shift increases; (2) Some methods like GradNorm may have different OOD detection mechanisms as they rely less on semantic shifts to make decisions; (3) Excessive covariate shifts in the image are also likely to be considered as OOD for some methods. Our code and data are released in https://github.com/qqwsad5/IS-OOD.",
      "paper_authors": [
        "Xingming Long",
        "Jie Zhang",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-14",
      "update_time": "2024-06-14",
      "comments": "v1",
      "repo_url": "https://github.com/qqwsad5/is-ood"
    },
    "2406.09417": {
      "paper_id": "2406.09417v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.09417v1",
      "paper_key": "2406.09417",
      "paper_title": "Rethinking Score Distillation as a Bridge Between Image Distributions",
      "paper_url": "http://arxiv.org/abs/2406.09417v1",
      "paper_abstract": "Score distillation sampling (SDS) has proven to be an important tool, enabling the use of large-scale diffusion priors for tasks operating in data-poor domains. Unfortunately, SDS has a number of characteristic artifacts that limit its usefulness in general-purpose applications. In this paper, we make progress toward understanding the behavior of SDS and its variants by viewing them as solving an optimal-cost transport path from a source distribution to a target distribution. Under this new interpretation, these methods seek to transport corrupted images (source) to the natural image distribution (target). We argue that current methods' characteristic artifacts are caused by (1) linear approximation of the optimal path and (2) poor estimates of the source distribution. We show that calibrating the text conditioning of the source distribution can produce high-quality generation and translation results with little extra overhead. Our method can be easily applied across many domains, matching or beating the performance of specialized methods. We demonstrate its utility in text-to-2D, text-based NeRF optimization, translating paintings to real images, optical illusion generation, and 3D sketch-to-real. We compare our method to existing approaches for score distillation sampling and show that it can produce high-frequency details with realistic colors.",
      "paper_authors": [
        "David McAllister",
        "Songwei Ge",
        "Jia-Bin Huang",
        "David W. Jacobs",
        "Alexei A. Efros",
        "Aleksander Holynski",
        "Angjoo Kanazawa"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-13",
      "update_time": "2024-06-13",
      "comments": "Project webpage: https://sds-bridge.github.io/",
      "repo_url": "#"
    },
    "2406.09190": {
      "paper_id": "2406.09190v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.09190v1",
      "paper_key": "2406.09190",
      "paper_title": "Rethinking Waveform for 6G: Harnessing Delay-Doppler Alignment Modulation",
      "paper_url": "http://arxiv.org/abs/2406.09190v1",
      "paper_abstract": "Waveform design has served as a cornerstone for each generation of mobile communication systems. The future sixth-generation (6G) mobile communication networks are expected to employ larger-scale antenna arrays and exploit higher-frequency bands for further boosting data transmission rate and providing ubiquitous wireless sensing. This brings new opportunities and challenges for 6G waveform design. In this article, by leveraging the super spatial resolution of large antenna arrays and the multi-path spatial sparsity of highfrequency wireless channels, we introduce a new approach for waveform design based on the recently proposed delay-Doppler alignment modulation (DDAM). In particular, DDAM makes a paradigm shift of waveform design from the conventional manner of tolerating channel delay and Doppler spreads to actively manipulating them. First, we review the fundamental constraints and performance limitations of orthogonal frequency division multiplexing (OFDM) and introduce new opportunities for 6G waveform design. Next, the motivations and basic principles of DDAM are presented, followed by its various extensions to different wireless system setups. Finally, the main design considerations for DDAM are discussed and the new opportunities for future research are highlighted.",
      "paper_authors": [
        "Zhiqiang Xiao",
        "Xianda Liu",
        "Yong Zeng",
        "J. Andrew Zhang",
        "Shi Jin",
        "Rui Zhang"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2024-06-13",
      "update_time": "2024-06-13",
      "comments": null,
      "repo_url": "#"
    },
    "2406.08845": {
      "paper_id": "2406.08845v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.08845v1",
      "paper_key": "2406.08845",
      "paper_title": "Rethinking Human Evaluation Protocol for Text-to-Video Models: Enhancing Reliability,Reproducibility, and Practicality",
      "paper_url": "http://arxiv.org/abs/2406.08845v1",
      "paper_abstract": "Recent text-to-video (T2V) technology advancements, as demonstrated by models such as Gen2, Pika, and Sora, have significantly broadened its applicability and popularity. Despite these strides, evaluating these models poses substantial challenges. Primarily, due to the limitations inherent in automatic metrics, manual evaluation is often considered a superior method for assessing T2V generation. However, existing manual evaluation protocols face reproducibility, reliability, and practicality issues. To address these challenges, this paper introduces the Text-to-Video Human Evaluation (T2VHE) protocol, a comprehensive and standardized protocol for T2V models. The T2VHE protocol includes well-defined metrics, thorough annotator training, and an effective dynamic evaluation module. Experimental results demonstrate that this protocol not only ensures high-quality annotations but can also reduce evaluation costs by nearly 50%. We will open-source the entire setup of the T2VHE protocol, including the complete protocol workflow, the dynamic evaluation component details, and the annotation interface code. This will help communities establish more sophisticated human assessment protocols.",
      "paper_authors": [
        "Tianle Zhang",
        "Langtian Ma",
        "Yuchen Yan",
        "Yuchen Zhang",
        "Kai Wang",
        "Yue Yang",
        "Ziyao Guo",
        "Wenqi Shao",
        "Yang You",
        "Yu Qiao",
        "Ping Luo",
        "Kaipeng Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-13",
      "update_time": "2024-06-13",
      "comments": null,
      "repo_url": "https://github.com/ztlmememe/T2VHE"
    },
    "2406.17797": {
      "paper_id": "2406.17797v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.17797v1",
      "paper_key": "2406.17797",
      "paper_title": "MoleculeCLA: Rethinking Molecular Benchmark via Computational Ligand-Target Binding Analysis",
      "paper_url": "http://arxiv.org/abs/2406.17797v1",
      "paper_abstract": "Molecular representation learning is pivotal for various molecular property prediction tasks related to drug discovery. Robust and accurate benchmarks are essential for refining and validating current methods. Existing molecular property benchmarks derived from wet experiments, however, face limitations such as data volume constraints, unbalanced label distribution, and noisy labels. To address these issues, we construct a large-scale and precise molecular representation dataset of approximately 140,000 small molecules, meticulously designed to capture an extensive array of chemical, physical, and biological properties, derived through a robust computational ligand-target binding analysis pipeline. We conduct extensive experiments on various deep learning models, demonstrating that our dataset offers significant physicochemical interpretability to guide model development and design. Notably, the dataset's properties are linked to binding affinity metrics, providing additional insights into model performance in drug-target interaction tasks. We believe this dataset will serve as a more accurate and reliable benchmark for molecular representation learning, thereby expediting progress in the field of artificial intelligence-driven drug discovery.",
      "paper_authors": [
        "Shikun Feng",
        "Jiaxin Zheng",
        "Yinjun Jia",
        "Yanwen Huang",
        "Fengfeng Zhou",
        "Wei-Ying Ma",
        "Yanyan Lan"
      ],
      "primary_category": "physics.chem-ph",
      "publish_time": "2024-06-13",
      "update_time": "2024-06-13",
      "comments": null,
      "repo_url": "https://github.com/Zhenger959/MoleculeCLA"
    },
    "2406.08164": {
      "paper_id": "2406.08164v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.08164v1",
      "paper_key": "2406.08164",
      "paper_title": "ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs",
      "paper_url": "http://arxiv.org/abs/2406.08164v1",
      "paper_abstract": "Compositional Reasoning (CR) entails grasping the significance of attributes, relations, and word order. Recent Vision-Language Models (VLMs), comprising a visual encoder and a Large Language Model (LLM) decoder, have demonstrated remarkable proficiency in such reasoning tasks. This prompts a crucial question: have VLMs effectively tackled the CR challenge? We conjecture that existing CR benchmarks may not adequately push the boundaries of modern VLMs due to the reliance on an LLM-only negative text generation pipeline. Consequently, the negatives produced either appear as outliers from the natural language distribution learned by VLMs' LLM decoders or as improbable within the corresponding image context. To address these limitations, we introduce ConMe -- a compositional reasoning benchmark and a novel data generation pipeline leveraging VLMs to produce `hard CR Q&A'. Through a new concept of VLMs conversing with each other to collaboratively expose their weaknesses, our pipeline autonomously generates, evaluates, and selects challenging compositional reasoning questions, establishing a robust CR benchmark, also subsequently validated manually. Our benchmark provokes a noteworthy, up to 33%, decrease in CR performance compared to preceding benchmarks, reinstating the CR challenge even for state-of-the-art VLMs.",
      "paper_authors": [
        "Irene Huang",
        "Wei Lin",
        "M. Jehanzeb Mirza",
        "Jacob A. Hansen",
        "Sivan Doveh",
        "Victor Ion Butoi",
        "Roei Herzig",
        "Assaf Arbelle",
        "Hilde Kuhene",
        "Trevor Darrel",
        "Chuang Gan",
        "Aude Oliva",
        "Rogerio Feris",
        "Leonid Karlinsky"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-12",
      "update_time": "2024-06-12",
      "comments": "The first three authors contributed equally",
      "repo_url": "https://github.com/jmiemirza/conme"
    },
    "2406.07879": {
      "paper_id": "2406.07879v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.07879v1",
      "paper_key": "2406.07879",
      "paper_title": "KernelWarehouse: Rethinking the Design of Dynamic Convolution",
      "paper_url": "http://arxiv.org/abs/2406.07879v1",
      "paper_abstract": "Dynamic convolution learns a linear mixture of n static kernels weighted with their input-dependent attentions, demonstrating superior performance than normal convolution. However, it increases the number of convolutional parameters by n times, and thus is not parameter efficient. This leads to no research progress that can allow researchers to explore the setting n>100 (an order of magnitude larger than the typical setting n<10) for pushing forward the performance boundary of dynamic convolution while enjoying parameter efficiency. To fill this gap, in this paper, we propose KernelWarehouse, a more general form of dynamic convolution, which redefines the basic concepts of ``kernels\", ``assembling kernels\" and ``attention function\" through the lens of exploiting convolutional parameter dependencies within the same layer and across neighboring layers of a ConvNet. We testify the effectiveness of KernelWarehouse on ImageNet and MS-COCO datasets using various ConvNet architectures. Intriguingly, KernelWarehouse is also applicable to Vision Transformers, and it can even reduce the model size of a backbone while improving the model accuracy. For instance, KernelWarehouse (n=4) achieves 5.61%|3.90%|4.38% absolute top-1 accuracy gain on the ResNet18|MobileNetV2|DeiT-Tiny backbone, and KernelWarehouse (n=1/4) with 65.10% model size reduction still achieves 2.29% gain on the ResNet18 backbone. The code and models are available at https://github.com/OSVAI/KernelWarehouse.",
      "paper_authors": [
        "Chao Li",
        "Anbang Yao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-12",
      "update_time": "2024-06-12",
      "comments": "This work is accepted to ICML 2024. The project page:\n  https://github.com/OSVAI/KernelWarehouse. arXiv admin note: substantial text\n  overlap with arXiv:2308.08361",
      "repo_url": "https://github.com/osvai/kernelwarehouse"
    },
    "2406.09441": {
      "paper_id": "2406.09441v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.09441v1",
      "paper_key": "2406.09441",
      "paper_title": "Comment on paper: Position: Rethinking Post-Hoc Search-Based Neural Approaches for Solving Large-Scale Traveling Salesman Problems",
      "paper_url": "http://arxiv.org/abs/2406.09441v1",
      "paper_abstract": "We identify two major issues in the SoftDist paper (Xia et al.): (1) the failure to run all steps of different baselines on the same hardware environment, and (2) the use of inconsistent time measurements when comparing to other baselines. These issues lead to flawed conclusions. When all steps are executed in the same hardware environment, the primary claim made in SoftDist is no longer supported.",
      "paper_authors": [
        "Yimeng Min"
      ],
      "primary_category": "cs.PF",
      "publish_time": "2024-06-11",
      "update_time": "2024-06-11",
      "comments": "comment on arXiv:2406.03503, 4 pages, 1 figure and 1 table",
      "repo_url": "#"
    },
    "2406.07314": {
      "paper_id": "2406.07314v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.07314v1",
      "paper_key": "2406.07314",
      "paper_title": "Rethinking the impact of noisy labels in graph classification: A utility and privacy perspective",
      "paper_url": "http://arxiv.org/abs/2406.07314v1",
      "paper_abstract": "Graph neural networks based on message-passing mechanisms have achieved advanced results in graph classification tasks. However, their generalization performance degrades when noisy labels are present in the training data. Most existing noisy labeling approaches focus on the visual domain or graph node classification tasks and analyze the impact of noisy labels only from a utility perspective. Unlike existing work, in this paper, we measure the effects of noise labels on graph classification from data privacy and model utility perspectives. We find that noise labels degrade the model's generalization performance and enhance the ability of membership inference attacks on graph data privacy. To this end, we propose the robust graph neural network approach with noisy labeled graph classification. Specifically, we first accurately filter the noisy samples by high-confidence samples and the first feature principal component vector of each class. Then, the robust principal component vectors and the model output under data augmentation are utilized to achieve noise label correction guided by dual spatial information. Finally, supervised graph contrastive learning is introduced to enhance the embedding quality of the model and protect the privacy of the training graph data. The utility and privacy of the proposed method are validated by comparing twelve different methods on eight real graph classification datasets. Compared with the state-of-the-art methods, the RGLC method achieves at most and at least 7.8% and 0.8% performance gain at 30% noisy labeling rate, respectively, and reduces the accuracy of privacy attacks to below 60%.",
      "paper_authors": [
        "De Li",
        "Xianxian Li",
        "Zeming Gan",
        "Qiyu Li",
        "Bin Qu",
        "Jinyan Wang"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-06-11",
      "update_time": "2024-06-11",
      "comments": null,
      "repo_url": "#"
    },
    "2406.06673": {
      "paper_id": "2406.06673v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.06673v1",
      "paper_key": "2406.06673",
      "paper_title": "Instanton Density Operator in Lattice QCD from Higher Category Theory",
      "paper_url": "http://arxiv.org/abs/2406.06673v1",
      "paper_abstract": "A natural definition for instanton density operator in lattice QCD has been long desired. We show this problem is, and has to be, resolved by higher category theory. The problem is resolved by refining at a conceptual level the Yang-Mills theory on lattice, in order to recover the homotopy information in the continuum, which would have been lost if we put the theory on lattice in the traditional way.   The refinement needed is a generalization -- through the lens of higher category theory -- of the familiar process of Villainization that captures winding in lattice XY model and Dirac quantization in lattice Maxwell theory. The apparent difference is that Villainization is in the end described by principal bundles, hence familiar, but more general topological operators can only be captured on the lattice by more flexible structures beyond the usual group theory and fibre bundles, hence the language of categories becomes natural and necessary. The key structure we need for our particular problem is called multiplicative bundle gerbe, based upon which we can construct suitable structures to naturally define the 2d Wess-Zumino-Witten term, 3d skyrmion density operator and 4d hedgehog defect for lattice $S^3$ (pion vacua) non-linear sigma model, and the 3d Chern-Simons term, 4d instanton density operator and 5d Yang monopole defect for lattice $SU(N)$ Yang-Mills theory.   In a broader perspective, higher category theory enables us to rethink more systematically the relation between continuum quantum field theory and lattice quantum field theory. We sketch a proposal towards a general machinery that constructs the suitably refined lattice degrees of freedom for a given non-linear sigma model or gauge theory in the continuum, realizing the desired topological operators on the lattice.",
      "paper_authors": [
        "Jing-Yuan Chen"
      ],
      "primary_category": "hep-lat",
      "publish_time": "2024-06-10",
      "update_time": "2024-06-10",
      "comments": "126 pages",
      "repo_url": "#"
    },
    "2406.06087": {
      "paper_id": "2406.06087v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.06087v1",
      "paper_key": "2406.06087",
      "paper_title": "GAIA: Rethinking Action Quality Assessment for AI-Generated Videos",
      "paper_url": "http://arxiv.org/abs/2406.06087v1",
      "paper_abstract": "Assessing action quality is both imperative and challenging due to its significant impact on the quality of AI-generated videos, further complicated by the inherently ambiguous nature of actions within AI-generated video (AIGV). Current action quality assessment (AQA) algorithms predominantly focus on actions from real specific scenarios and are pre-trained with normative action features, thus rendering them inapplicable in AIGVs. To address these problems, we construct GAIA, a Generic AI-generated Action dataset, by conducting a large-scale subjective evaluation from a novel causal reasoning-based perspective, resulting in 971,244 ratings among 9,180 video-action pairs. Based on GAIA, we evaluate a suite of popular text-to-video (T2V) models on their ability to generate visually rational actions, revealing their pros and cons on different categories of actions. We also extend GAIA as a testbed to benchmark the AQA capacity of existing automatic evaluation methods. Results show that traditional AQA methods, action-related metrics in recent T2V benchmarks, and mainstream video quality methods correlate poorly with human opinions, indicating a sizable gap between current models and human action perception patterns in AIGVs. Our findings underscore the significance of action quality as a unique perspective for studying AIGVs and can catalyze progress towards methods with enhanced capacities for AQA in AIGVs.",
      "paper_authors": [
        "Zijian Chen",
        "Wei Sun",
        "Yuan Tian",
        "Jun Jia",
        "Zicheng Zhang",
        "Jiarui Wang",
        "Ru Huang",
        "Xiongkuo Min",
        "Guangtao Zhai",
        "Wenjun Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-10",
      "update_time": "2024-06-10",
      "comments": "28 pages, 13 figures",
      "repo_url": "https://github.com/zijianchen98/gaia"
    },
    "2406.05814": {
      "paper_id": "2406.05814v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.05814v1",
      "paper_key": "2406.05814",
      "paper_title": "Unified Text-to-Image Generation and Retrieval",
      "paper_url": "http://arxiv.org/abs/2406.05814v1",
      "paper_abstract": "How humans can efficiently and effectively acquire images has always been a perennial question. A typical solution is text-to-image retrieval from an existing database given the text query; however, the limited database typically lacks creativity. By contrast, recent breakthroughs in text-to-image generation have made it possible to produce fancy and diverse visual content, but it faces challenges in synthesizing knowledge-intensive images. In this work, we rethink the relationship between text-to-image generation and retrieval and propose a unified framework in the context of Multimodal Large Language Models (MLLMs). Specifically, we first explore the intrinsic discriminative abilities of MLLMs and introduce a generative retrieval method to perform retrieval in a training-free manner. Subsequently, we unify generation and retrieval in an autoregressive generation way and propose an autonomous decision module to choose the best-matched one between generated and retrieved images as the response to the text query. Additionally, we construct a benchmark called TIGeR-Bench, including creative and knowledge-intensive domains, to standardize the evaluation of unified text-to-image generation and retrieval. Extensive experimental results on TIGeR-Bench and two retrieval benchmarks, i.e., Flickr30K and MS-COCO, demonstrate the superiority and effectiveness of our proposed method.",
      "paper_authors": [
        "Leigang Qu",
        "Haochuan Li",
        "Tan Wang",
        "Wenjie Wang",
        "Yongqi Li",
        "Liqiang Nie",
        "Tat-Seng Chua"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-09",
      "update_time": "2024-06-09",
      "comments": null,
      "repo_url": "#"
    },
    "2406.05678": {
      "paper_id": "2406.05678v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.05678v1",
      "paper_key": "2406.05678",
      "paper_title": "SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models",
      "paper_url": "http://arxiv.org/abs/2406.05678v1",
      "paper_abstract": "Extending the functionality of the Transformer model to accommodate longer sequence lengths has become a critical challenge. This extension is crucial not only for improving tasks such as language translation and long-context processing but also for enabling novel applications like chatbots, code generation, and multimedia content creation. The primary obstacle is the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention. However, LongLoRA is still not as efficient as vanilla attention, reaching only 39\\% of the perplexity improvement compared to full attention. This inefficiency is due to the cyclic shift applied within different attention head patterns, causing either chaos in the attention head structure or unnecessary information exchange between token groups. To address these issues, We propose \\textbf{SinkLoRA}, which features better work partitioning. Specifically, (1) we developed SF-Attn with a segmentation and reassembly algorithm to proportionally return cyclically shifted groups of attention heads to their un-shifted state together with global attention of \"sink attention tokens\", achieving 92\\% of the perplexity improvement compared to full attention after fine tuning, and (2) applied a SOTA KV cache compression algorithm H$_2$O to accelerate inference. Furthermore, We conducted supervised fine-tuning with SinkLoRA using a self collected LongAlpaca-plus dataset. All our code, models, datasets, and demos are available at \\url{https://github.com/Dexter-GT-86/SinkLoRA}.",
      "paper_authors": [
        "Hengyu Zhang"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-06-09",
      "update_time": "2024-06-09",
      "comments": "A rethinking of Short Shifted Attention",
      "repo_url": "https://github.com/dexter-gt-86/sinklora"
    },
    "2406.03725": {
      "paper_id": "2406.03725v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.03725v1",
      "paper_key": "2406.03725",
      "paper_title": "LLMEmbed: Rethinking Lightweight LLM's Genuine Function in Text Classification",
      "paper_url": "http://arxiv.org/abs/2406.03725v1",
      "paper_abstract": "With the booming of Large Language Models (LLMs), prompt-learning has become a promising method mainly researched in various research areas. Recently, many attempts based on prompt-learning have been made to improve the performance of text classification. However, most of these methods are based on heuristic Chain-of-Thought (CoT), and tend to be more complex but less efficient. In this paper, we rethink the LLM-based text classification methodology, propose a simple and effective transfer learning strategy, namely LLMEmbed, to address this classical but challenging task. To illustrate, we first study how to properly extract and fuse the text embeddings via various lightweight LLMs at different network depths to improve their robustness and discrimination, then adapt such embeddings to train the classifier. We perform extensive experiments on publicly available datasets, and the results show that LLMEmbed achieves strong performance while enjoys low training overhead using lightweight LLM backbones compared to recent methods based on larger LLMs, i.e. GPT-3, and sophisticated prompt-based strategies. Our LLMEmbed achieves adequate accuracy on publicly available benchmarks without any fine-tuning while merely use 4% model parameters, 1.8% electricity consumption and 1.5% runtime compared to its counterparts. Code is available at: https://github.com/ChunLiu-cs/LLMEmbed-ACL2024.",
      "paper_authors": [
        "Chun Liu",
        "Hongguang Zhang",
        "Kainan Zhao",
        "Xinghai Ju",
        "Lin Yang"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-06-06",
      "update_time": "2024-06-06",
      "comments": "ACL 2024 main conference",
      "repo_url": "https://github.com/chunliu-cs/llmembed-acl2024"
    },
    "2406.03372": {
      "paper_id": "2406.03372v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.03372v1",
      "paper_key": "2406.03372",
      "paper_title": "Training of Physical Neural Networks",
      "paper_url": "http://arxiv.org/abs/2406.03372v1",
      "paper_abstract": "Physical neural networks (PNNs) are a class of neural-like networks that leverage the properties of physical systems to perform computation. While PNNs are so far a niche research area with small-scale laboratory demonstrations, they are arguably one of the most underappreciated important opportunities in modern AI. Could we train AI models 1000x larger than current ones? Could we do this and also have them perform inference locally and privately on edge devices, such as smartphones or sensors? Research over the past few years has shown that the answer to all these questions is likely \"yes, with enough research\": PNNs could one day radically change what is possible and practical for AI systems. To do this will however require rethinking both how AI models work, and how they are trained - primarily by considering the problems through the constraints of the underlying hardware physics. To train PNNs at large scale, many methods including backpropagation-based and backpropagation-free approaches are now being explored. These methods have various trade-offs, and so far no method has been shown to scale to the same scale and performance as the backpropagation algorithm widely used in deep learning today. However, this is rapidly changing, and a diverse ecosystem of training techniques provides clues for how PNNs may one day be utilized to create both more efficient realizations of current-scale AI models, and to enable unprecedented-scale models.",
      "paper_authors": [
        "Ali Momeni",
        "Babak Rahmani",
        "Benjamin Scellier",
        "Logan G. Wright",
        "Peter L. McMahon",
        "Clara C. Wanjura",
        "Yuhang Li",
        "Anas Skalli",
        "Natalia G. Berloff",
        "Tatsuhiro Onodera",
        "Ilker Oguz",
        "Francesco Morichetti",
        "Philipp del Hougne",
        "Manuel Le Gallo",
        "Abu Sebastian",
        "Azalia Mirhoseini",
        "Cheng Zhang",
        "Danijela Markovi\u0107",
        "Daniel Brunner",
        "Christophe Moser",
        "Sylvain Gigan",
        "Florian Marquardt",
        "Aydogan Ozcan",
        "Julie Grollier",
        "Andrea J. Liu",
        "Demetri Psaltis",
        "Andrea Al\u00f9",
        "Romain Fleury"
      ],
      "primary_category": "physics.app-ph",
      "publish_time": "2024-06-05",
      "update_time": "2024-06-05",
      "comments": "29 pages, 4 figures",
      "repo_url": "#"
    },
    "2406.03330": {
      "paper_id": "2406.03330v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.03330v1",
      "paper_key": "2406.03330",
      "paper_title": "Rethinking Programming Paradigms in the QC-HPC Context",
      "paper_url": "http://arxiv.org/abs/2406.03330v1",
      "paper_abstract": "Programming for today's quantum computers is making significant strides toward modern workflows compatible with high performance computing (HPC), but fundamental challenges still remain in the integration of these vastly different technologies. Quantum computing (QC) programming languages share some common ground, as well as their emerging runtimes and algorithmic modalities. In this short paper, we explore avenues of refinement for the quantum processing unit (QPU) in the context of many-tasks management, asynchronous or otherwise, in order to understand the value it can play in linking QC with HPC. Through examples, we illustrate how its potential for scientific discovery might be realized.",
      "paper_authors": [
        "Silvina Caino-Lores",
        "Daniel Claudino",
        "Eugene Dumitrescu",
        "Travis S. Humble",
        "Sonia Lopez Alarcon",
        "Elaine Wong"
      ],
      "primary_category": "quant-ph",
      "publish_time": "2024-06-05",
      "update_time": "2024-06-05",
      "comments": null,
      "repo_url": "#"
    },
    "2406.03177": {
      "paper_id": "2406.03177v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.03177v1",
      "paper_key": "2406.03177",
      "paper_title": "FAPNet: An Effective Frequency Adaptive Point-based Eye Tracker",
      "paper_url": "http://arxiv.org/abs/2406.03177v1",
      "paper_abstract": "Eye tracking is crucial for human-computer interaction in different domains. Conventional cameras encounter challenges such as power consumption and image quality during different eye movements, prompting the need for advanced solutions with ultra-fast, low-power, and accurate eye trackers. Event cameras, fundamentally designed to capture information about moving objects, exhibit low power consumption and high temporal resolution. This positions them as an alternative to traditional cameras in the realm of eye tracking. Nevertheless, existing event-based eye tracking networks neglect the pivotal sparse and fine-grained temporal information in events, resulting in unsatisfactory performance. Moreover, the energy-efficient features are further compromised by the use of excessively complex models, hindering efficient deployment on edge devices. In this paper, we utilize Point Cloud as the event representation to harness the high temporal resolution and sparse characteristics of events in eye tracking tasks. We rethink the point-based architecture PEPNet with preprocessing the long-term relationships between samples, leading to the innovative design of FAPNet. A frequency adaptive mechanism is designed to realize adaptive tracking according to the speed of the pupil movement and the Inter Sample LSTM module is introduced to utilize the temporal correlation between samples. In the Event-based Eye Tracking Challenge, we utilize vanilla PEPNet, which is the former work to achieve the $p_{10}$ accuracy of 97.95\\%. On the SEET synthetic dataset, FAPNet can achieve state-of-the-art while consuming merely 10\\% of the PEPNet's computational resources. Notably, the computational demand of FAPNet is independent of the sensor's spatial resolution, enhancing its applicability on resource-limited edge devices.",
      "paper_authors": [
        "Xiaopeng Lin",
        "Hongwei Ren",
        "Bojun Cheng"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-05",
      "update_time": "2024-06-05",
      "comments": "Accepted by CVPRW 2024 (AIS)",
      "repo_url": "#"
    },
    "2406.02923": {
      "paper_id": "2406.02923v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.02923v1",
      "paper_key": "2406.02923",
      "paper_title": "Rethinking Spiking Neural Networks as State Space Models",
      "paper_url": "http://arxiv.org/abs/2406.02923v1",
      "paper_abstract": "Spiking neural networks (SNNs) are posited as a biologically plausible alternative to conventional neural architectures, with their core computational framework resting on the extensively studied leaky integrate-and-fire (LIF) neuron design. The stateful nature of LIF neurons has spurred ongoing discussions about the ability of SNNs to process sequential data, akin to recurrent neural networks (RNNs). Despite this, there remains a significant gap in the exploration of current SNNs within the realm of long-range dependency tasks. In this study, to extend the analysis of neuronal dynamics beyond simplistic LIF mechanism, we present a novel class of stochastic spiking neuronal model grounded in state space models. We expand beyond the scalar hidden state representation of LIF neurons, which traditionally comprises only the membrane potential, by proposing an n-dimensional hidden state. Additionally, we enable fine-tuned formulation of neuronal dynamics across each layer by introducing learnable parameters, as opposed to the fixed dynamics in LIF neurons. We also develop a robust framework for scaling these neuronal models to deep SNN-based architectures, ensuring efficient parallel training while also adeptly addressing the challenge of non-differentiability of stochastic spiking operation during the backward phase. Our models attain state-of-the-art performance among SNN models across diverse long-range dependency tasks, encompassing the Long Range Arena benchmark, permuted sequential MNIST, and the Speech Command dataset. Moreover, we provide an analysis of the energy efficiency advantages, emphasizing the sparse activity pattern intrinsic to this spiking model.",
      "paper_authors": [
        "Malyaban Bal",
        "Abhronil Sengupta"
      ],
      "primary_category": "cs.NE",
      "publish_time": "2024-06-05",
      "update_time": "2024-06-05",
      "comments": "Under Review",
      "repo_url": "#"
    },
    "2406.02862": {
      "paper_id": "2406.02862v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.02862v1",
      "paper_key": "2406.02862",
      "paper_title": "Rethinking Guidance Information to Utilize Unlabeled Samples:A Label Encoding Perspective",
      "paper_url": "http://arxiv.org/abs/2406.02862v1",
      "paper_abstract": "Empirical Risk Minimization (ERM) is fragile in scenarios with insufficient labeled samples. A vanilla extension of ERM to unlabeled samples is Entropy Minimization (EntMin), which employs the soft-labels of unlabeled samples to guide their learning. However, EntMin emphasizes prediction discriminability while neglecting prediction diversity. To alleviate this issue, in this paper, we rethink the guidance information to utilize unlabeled samples. By analyzing the learning objective of ERM, we find that the guidance information for labeled samples in a specific category is the corresponding label encoding. Inspired by this finding, we propose a Label-Encoding Risk Minimization (LERM). It first estimates the label encodings through prediction means of unlabeled samples and then aligns them with their corresponding ground-truth label encodings. As a result, the LERM ensures both prediction discriminability and diversity, and it can be integrated into existing methods as a plugin. Theoretically, we analyze the relationships between LERM and ERM as well as EntMin. Empirically, we verify the superiority of the LERM under several label insufficient scenarios. The codes are available at https://github.com/zhangyl660/LERM.",
      "paper_authors": [
        "Yulong Zhang",
        "Yuan Yao",
        "Shuhao Chen",
        "Pengrong Jin",
        "Yu Zhang",
        "Jian Jin",
        "Jiangang Lu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-06-05",
      "update_time": "2024-06-05",
      "comments": "Accepted to ICML 2024",
      "repo_url": "https://github.com/zhangyl660/lerm"
    },
    "2406.03503": {
      "paper_id": "2406.03503v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.03503v1",
      "paper_key": "2406.03503",
      "paper_title": "Position: Rethinking Post-Hoc Search-Based Neural Approaches for Solving Large-Scale Traveling Salesman Problems",
      "paper_url": "http://arxiv.org/abs/2406.03503v1",
      "paper_abstract": "Recent advancements in solving large-scale traveling salesman problems (TSP) utilize the heatmap-guided Monte Carlo tree search (MCTS) paradigm, where machine learning (ML) models generate heatmaps, indicating the probability distribution of each edge being part of the optimal solution, to guide MCTS in solution finding. However, our theoretical and experimental analysis raises doubts about the effectiveness of ML-based heatmap generation. In support of this, we demonstrate that a simple baseline method can outperform complex ML approaches in heatmap generation. Furthermore, we question the practical value of the heatmap-guided MCTS paradigm. To substantiate this, our findings show its inferiority to the LKH-3 heuristic despite the paradigm's reliance on problem-specific, hand-crafted strategies. For the future, we suggest research directions focused on developing more theoretically sound heatmap generation methods and exploring autonomous, generalizable ML approaches for combinatorial problems. The code is available for review: https://github.com/xyfffff/rethink_mcts_for_tsp.",
      "paper_authors": [
        "Yifan Xia",
        "Xianliang Yang",
        "Zichuan Liu",
        "Zhihao Liu",
        "Lei Song",
        "Jiang Bian"
      ],
      "primary_category": "cs.AI",
      "publish_time": "2024-06-02",
      "update_time": "2024-06-02",
      "comments": "Accepted by International Conference on Machine Learning (ICML 2024)",
      "repo_url": "https://github.com/xyfffff/rethink_mcts_for_tsp"
    },
    "2406.00262": {
      "paper_id": "2406.00262v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.00262v1",
      "paper_key": "2406.00262",
      "paper_title": "Contrastive Learning Via Equivariant Representation",
      "paper_url": "http://arxiv.org/abs/2406.00262v1",
      "paper_abstract": "Invariant-based Contrastive Learning (ICL) methods have achieved impressive performance across various domains. However, the absence of latent space representation for distortion (augmentation)-related information in the latent space makes ICL sub-optimal regarding training efficiency and robustness in downstream tasks. Recent studies suggest that introducing equivariance into Contrastive Learning (CL) can improve overall performance. In this paper, we rethink the roles of augmentation strategies and equivariance in improving CL efficacy. We propose a novel Equivariant-based Contrastive Learning (ECL) framework, CLeVER (Contrastive Learning Via Equivariant Representation), compatible with augmentation strategies of arbitrary complexity for various mainstream CL methods and model frameworks. Experimental results demonstrate that CLeVER effectively extracts and incorporates equivariant information from data, thereby improving the training efficiency and robustness of baseline models in downstream tasks.",
      "paper_authors": [
        "Sifan Song",
        "Jinfeng Wang",
        "Qiaochu Zhao",
        "Xiang Li",
        "Dufan Wu",
        "Angelos Stefanidis",
        "Jionglong Su",
        "S. Kevin Zhou",
        "Quanzheng Li"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-06-01",
      "update_time": "2024-06-01",
      "comments": "Preprint. Under review",
      "repo_url": "#"
    },
    "2406.00196": {
      "paper_id": "2406.00196v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.00196v1",
      "paper_key": "2406.00196",
      "paper_title": "A Seamless Phase II/III Design with Dose Optimization for Oncology Drug Development",
      "paper_url": "http://arxiv.org/abs/2406.00196v1",
      "paper_abstract": "The US FDA's Project Optimus initiative that emphasizes dose optimization prior to marketing approval represents a pivotal shift in oncology drug development. It has a ripple effect for rethinking what changes may be made to conventional pivotal trial designs to incorporate a dose optimization component. Aligned with this initiative, we propose a novel Seamless Phase II/III Design with Dose Optimization (SDDO framework). The proposed design starts with dose optimization in a randomized setting, leading to an interim analysis focused on optimal dose selection, trial continuation decisions, and sample size re-estimation (SSR). Based on the decision at interim analysis, patient enrollment continues for both the selected dose arm and control arm, and the significance of treatment effects will be determined at final analysis. The SDDO framework offers increased flexibility and cost-efficiency through sample size adjustment, while stringently controlling the Type I error. This proposed design also facilitates both Accelerated Approval (AA) and regular approval in a \"one-trial\" approach. Extensive simulation studies confirm that our design reliably identifies the optimal dosage and makes preferable decisions with a reduced sample size while retaining statistical power.",
      "paper_authors": [
        "Yuhan Li",
        "Yiding Zhang",
        "Gu Mi",
        "Ji Lin"
      ],
      "primary_category": "stat.ME",
      "publish_time": "2024-05-31",
      "update_time": "2024-05-31",
      "comments": null,
      "repo_url": "#"
    },
    "2405.20829": {
      "paper_id": "2405.20829v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.20829v1",
      "paper_key": "2405.20829",
      "paper_title": "Rethinking Open-World Semi-Supervised Learning: Distribution Mismatch and Inductive Inference",
      "paper_url": "http://arxiv.org/abs/2405.20829v1",
      "paper_abstract": "Open-world semi-supervised learning (OWSSL) extends conventional semi-supervised learning to open-world scenarios by taking account of novel categories in unlabeled datasets. Despite the recent advancements in OWSSL, the success often relies on the assumptions that 1) labeled and unlabeled datasets share the same balanced class prior distribution, which does not generally hold in real-world applications, and 2) unlabeled training datasets are utilized for evaluation, where such transductive inference might not adequately address challenges in the wild. In this paper, we aim to generalize OWSSL by addressing them. Our work suggests that practical OWSSL may require different training settings, evaluation methods, and learning strategies compared to those prevalent in the existing literature.",
      "paper_authors": [
        "Seongheon Park",
        "Hyuk Kwon",
        "Kwanghoon Sohn",
        "Kibok Lee"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-31",
      "update_time": "2024-05-31",
      "comments": "CVPR Workshop on Computer Vision in the Wild (CVinW), 2024",
      "repo_url": "#"
    },
    "2405.20282": {
      "paper_id": "2405.20282v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.20282v1",
      "paper_key": "2405.20282",
      "paper_title": "SemFlow: Binding Semantic Segmentation and Image Synthesis via Rectified Flow",
      "paper_url": "http://arxiv.org/abs/2405.20282v1",
      "paper_abstract": "Semantic segmentation and semantic image synthesis are two representative tasks in visual perception and generation. While existing methods consider them as two distinct tasks, we propose a unified diffusion-based framework (SemFlow) and model them as a pair of reverse problems. Specifically, motivated by rectified flow theory, we train an ordinary differential equation (ODE) model to transport between the distributions of real images and semantic masks. As the training object is symmetric, samples belonging to the two distributions, images and semantic masks, can be effortlessly transferred reversibly. For semantic segmentation, our approach solves the contradiction between the randomness of diffusion outputs and the uniqueness of segmentation results. For image synthesis, we propose a finite perturbation approach to enhance the diversity of generated results without changing the semantic categories. Experiments show that our SemFlow achieves competitive results on semantic segmentation and semantic image synthesis tasks. We hope this simple framework will motivate people to rethink the unification of low-level and high-level vision. Project page: https://github.com/wang-chaoyang/SemFlow.",
      "paper_authors": [
        "Chaoyang Wang",
        "Xiangtai Li",
        "Lu Qi",
        "Henghui Ding",
        "Yunhai Tong",
        "Ming-Hsuan Yang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-30",
      "update_time": "2024-05-30",
      "comments": null,
      "repo_url": "https://github.com/wang-chaoyang/semflow"
    },
    "2405.19694": {
      "paper_id": "2405.19694v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.19694v1",
      "paper_key": "2405.19694",
      "paper_title": "Grade Like a Human: Rethinking Automated Assessment with Large Language Models",
      "paper_url": "http://arxiv.org/abs/2405.19694v1",
      "paper_abstract": "While large language models (LLMs) have been used for automated grading, they have not yet achieved the same level of performance as humans, especially when it comes to grading complex questions. Existing research on this topic focuses on a particular step in the grading procedure: grading using predefined rubrics. However, grading is a multifaceted procedure that encompasses other crucial steps, such as grading rubrics design and post-grading review. There has been a lack of systematic research exploring the potential of LLMs to enhance the entire grading~process.   In this paper, we propose an LLM-based grading system that addresses the entire grading procedure, including the following key components: 1) Developing grading rubrics that not only consider the questions but also the student answers, which can more accurately reflect students' performance. 2) Under the guidance of grading rubrics, providing accurate and consistent scores for each student, along with customized feedback. 3) Conducting post-grading review to better ensure accuracy and fairness. Additionally, we collected a new dataset named OS from a university operating system course and conducted extensive experiments on both our new dataset and the widely used Mohler dataset. Experiments demonstrate the effectiveness of our proposed approach, providing some new insights for developing automated grading systems based on LLMs.",
      "paper_authors": [
        "Wenjing Xie",
        "Juxin Niu",
        "Chun Jason Xue",
        "Nan Guan"
      ],
      "primary_category": "cs.AI",
      "publish_time": "2024-05-30",
      "update_time": "2024-05-30",
      "comments": null,
      "repo_url": "#"
    },
    "2405.19668": {
      "paper_id": "2405.19668v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.19668v1",
      "paper_key": "2405.19668",
      "paper_title": "AutoBreach: Universal and Adaptive Jailbreaking with Efficient Wordplay-Guided Optimization",
      "paper_url": "http://arxiv.org/abs/2405.19668v1",
      "paper_abstract": "Despite the widespread application of large language models (LLMs) across various tasks, recent studies indicate that they are susceptible to jailbreak attacks, which can render their defense mechanisms ineffective. However, previous jailbreak research has frequently been constrained by limited universality, suboptimal efficiency, and a reliance on manual crafting. In response, we rethink the approach to jailbreaking LLMs and formally define three essential properties from the attacker' s perspective, which contributes to guiding the design of jailbreak methods. We further introduce AutoBreach, a novel method for jailbreaking LLMs that requires only black-box access. Inspired by the versatility of wordplay, AutoBreach employs a wordplay-guided mapping rule sampling strategy to generate a variety of universal mapping rules for creating adversarial prompts. This generation process leverages LLMs' automatic summarization and reasoning capabilities, thus alleviating the manual burden. To boost jailbreak success rates, we further suggest sentence compression and chain-of-thought-based mapping rules to correct errors and wordplay misinterpretations in target LLMs. Additionally, we propose a two-stage mapping rule optimization strategy that initially optimizes mapping rules before querying target LLMs to enhance the efficiency of AutoBreach. AutoBreach can efficiently identify security vulnerabilities across various LLMs, including three proprietary models: Claude-3, GPT-3.5, GPT-4 Turbo, and two LLMs' web platforms: Bingchat, GPT-4 Web, achieving an average success rate of over 80% with fewer than 10 queries",
      "paper_authors": [
        "Jiawei Chen",
        "Xiao Yang",
        "Zhengwei Fang",
        "Yu Tian",
        "Yinpeng Dong",
        "Zhaoxia Yin",
        "Hang Su"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-30",
      "update_time": "2024-05-30",
      "comments": "Under review",
      "repo_url": "#"
    },
    "2405.19121": {
      "paper_id": "2405.19121v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.19121v2",
      "paper_key": "2405.19121",
      "paper_title": "Spatio-Spectral Graph Neural Networks",
      "paper_url": "http://arxiv.org/abs/2405.19121v2",
      "paper_abstract": "Spatial Message Passing Graph Neural Networks (MPGNNs) are widely used for learning on graph-structured data. However, key limitations of l-step MPGNNs are that their \"receptive field\" is typically limited to the l-hop neighborhood of a node and that information exchange between distant nodes is limited by over-squashing. Motivated by these limitations, we propose Spatio-Spectral Graph Neural Networks (S$^2$GNNs) -- a new modeling paradigm for Graph Neural Networks (GNNs) that synergistically combines spatially and spectrally parametrized graph filters. Parameterizing filters partially in the frequency domain enables global yet efficient information propagation. We show that S$^2$GNNs vanquish over-squashing and yield strictly tighter approximation-theoretic error bounds than MPGNNs. Further, rethinking graph convolutions at a fundamental level unlocks new design spaces. For example, S$^2$GNNs allow for free positional encodings that make them strictly more expressive than the 1-Weisfeiler-Lehman (WL) test. Moreover, to obtain general-purpose S$^2$GNNs, we propose spectrally parametrized filters for directed graphs. S$^2$GNNs outperform spatial MPGNNs, graph transformers, and graph rewirings, e.g., on the peptide long-range benchmark tasks, and are competitive with state-of-the-art sequence modeling. On a 40 GB GPU, S$^2$GNNs scale to millions of nodes.",
      "paper_authors": [
        "Simon Geisler",
        "Arthur Kosmala",
        "Daniel Herbst",
        "Stephan G\u00fcnnemann"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-05-29",
      "update_time": "2024-06-02",
      "comments": "46 pages, 27 figures, 12 tables",
      "repo_url": "#"
    },
    "2405.18718": {
      "paper_id": "2405.18718v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.18718v1",
      "paper_key": "2405.18718",
      "paper_title": "Efficient Model-agnostic Alignment via Bayesian Persuasion",
      "paper_url": "http://arxiv.org/abs/2405.18718v1",
      "paper_abstract": "With recent advancements in large language models (LLMs), alignment has emerged as an effective technique for keeping LLMs consensus with human intent. Current methods primarily involve direct training through Supervised Fine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF), both of which require substantial computational resources and extensive ground truth data. This paper explores an efficient method for aligning black-box large models using smaller models, introducing a model-agnostic and lightweight Bayesian Persuasion Alignment framework. We formalize this problem as an optimization of the signaling strategy from the small model's perspective. In the persuasion process, the small model (Advisor) observes the information item (i.e., state) and persuades large models (Receiver) to elicit improved responses. The Receiver then generates a response based on the input, the signal from the Advisor, and its updated belief about the information item. Through training using our framework, we demonstrate that the Advisor can significantly enhance the performance of various Receivers across a range of tasks. We theoretically analyze our persuasion framework and provide an upper bound on the Advisor's regret, confirming its effectiveness in learning the optimal signaling strategy. Our Empirical results demonstrates that GPT-2 can significantly improve the performance of various models, achieving an average enhancement of 16.1% in mathematical reasoning ability and 13.7% in code generation. We hope our work can provide an initial step toward rethinking the alignment framework from the Bayesian Persuasion perspective.",
      "paper_authors": [
        "Fengshuo Bai",
        "Mingzhi Wang",
        "Zhaowei Zhang",
        "Boyuan Chen",
        "Yinda Xu",
        "Ying Wen",
        "Yaodong Yang"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-05-29",
      "update_time": "2024-05-29",
      "comments": null,
      "repo_url": "#"
    },
    "2405.18638": {
      "paper_id": "2405.18638v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.18638v2",
      "paper_key": "2405.18638",
      "paper_title": "ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation for Generative Large Language Models",
      "paper_url": "http://arxiv.org/abs/2405.18638v2",
      "paper_abstract": "In this position paper, we argue that human evaluation of generative large language models (LLMs) should be a multidisciplinary undertaking that draws upon insights from disciplines such as user experience research and human behavioral psychology to ensure that the experimental design and results are reliable. The conclusions from these evaluations, thus, must consider factors such as usability, aesthetics, and cognitive biases. We highlight how cognitive biases can conflate fluent information and truthfulness, and how cognitive uncertainty affects the reliability of rating scores such as Likert. Furthermore, the evaluation should differentiate the capabilities and weaknesses of increasingly powerful large language models -- which requires effective test sets. The scalability of human evaluation is also crucial to wider adoption. Hence, to design an effective human evaluation system in the age of generative NLP, we propose the ConSiDERS-The-Human evaluation framework consisting of 6 pillars -- Consistency, Scoring Criteria, Differentiating, User Experience, Responsible, and Scalability.",
      "paper_authors": [
        "Aparna Elangovan",
        "Ling Liu",
        "Lei Xu",
        "Sravan Bodapati",
        "Dan Roth"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-05-28",
      "update_time": "2024-08-31",
      "comments": "Accepted in ACL 2024",
      "repo_url": "#"
    },
    "2405.18179": {
      "paper_id": "2405.18179v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.18179v1",
      "paper_key": "2405.18179",
      "paper_title": "Rethinking the A in STEAM: Insights from and for AI Literacy Education",
      "paper_url": "http://arxiv.org/abs/2405.18179v1",
      "paper_abstract": "This article rethinks the role of arts in STEAM education, emphasizing its importance in AI literacy within K-12 contexts. Arguing against the marginalization of arts, the paper is structured around four key domains: language studies, philosophy, social studies, and visual arts. Each section addresses critical AI-related phenomena and provides pedagogical strate-gies for effective integration into STEAM education. Language studies focus on media representations and the probabilistic nature of AI language models. The philosophy section examines anthropomorphism, ethics, and the misconstrued human-like capabilities of AI. Social studies discuss AI's societal impacts, biases, and ethical considerations in data prac-tices. Visual arts explore the implications of generative AI on artistic processes and intellec-tual property. The article concludes by advocating for a robust inclusion of arts in STEAM to foster a holistic, equitable, and sustainable understanding of AI, ultimately inspiring technologies that promote fairness and creativity.",
      "paper_authors": [
        "Pekka Mertala",
        "JAnne Fagerlund",
        "Tomi Slotte Dufva"
      ],
      "primary_category": "cs.CY",
      "publish_time": "2024-05-28",
      "update_time": "2024-05-28",
      "comments": null,
      "repo_url": "#"
    },
    "2405.18011": {
      "paper_id": "2405.18011v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.18011v1",
      "paper_key": "2405.18011",
      "paper_title": "Rethinking Recommender Systems: Cluster-based Algorithm Selection",
      "paper_url": "http://arxiv.org/abs/2405.18011v1",
      "paper_abstract": "Cluster-based algorithm selection deals with selecting recommendation algorithms on clusters of users to obtain performance gains. No studies have been attempted for many combinations of clustering approaches and recommendation algorithms. We want to show that clustering users prior to algorithm selection increases the performance of recommendation algorithms. Our study covers eight datasets, four clustering approaches, and eight recommendation algorithms. We select the best performing recommendation algorithm for each cluster. Our work shows that cluster-based algorithm selection is an effective technique for optimizing recommendation algorithm performance. For five out of eight datasets, we report an increase in nDCG@10 between 19.28% (0.032) and 360.38% (0.191) compared to algorithm selection without prior clustering.",
      "paper_authors": [
        "Andreas Lizenberger",
        "Ferdinand Pfeifer",
        "Bastian Polewka"
      ],
      "primary_category": "cs.IR",
      "publish_time": "2024-05-28",
      "update_time": "2024-05-28",
      "comments": "16 pages, 8 figures, 2 tables",
      "repo_url": "#"
    },
    "2405.17746": {
      "paper_id": "2405.17746v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.17746v1",
      "paper_key": "2405.17746",
      "paper_title": "Rethinking Pruning for Backdoor Mitigation: An Optimization Perspective",
      "paper_url": "http://arxiv.org/abs/2405.17746v1",
      "paper_abstract": "Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks, posing concerning threats to their reliable deployment. Recent research reveals that backdoors can be erased from infected DNNs by pruning a specific group of neurons, while how to effectively identify and remove these backdoor-associated neurons remains an open challenge. Most of the existing defense methods rely on defined rules and focus on neuron's local properties, ignoring the exploration and optimization of pruning policies. To address this gap, we propose an Optimized Neuron Pruning (ONP) method combined with Graph Neural Network (GNN) and Reinforcement Learning (RL) to repair backdoor models. Specifically, ONP first models the target DNN as graphs based on neuron connectivity, and then uses GNN-based RL agents to learn graph embeddings and find a suitable pruning policy. To the best of our knowledge, this is the first attempt to employ GNN and RL for optimizing pruning policies in the field of backdoor defense. Experiments show, with a small amount of clean data, ONP can effectively prune the backdoor neurons implanted by a set of backdoor attacks at the cost of negligible performance degradation, achieving a new state-of-the-art performance for backdoor mitigation.",
      "paper_authors": [
        "Nan Li",
        "Haiyang Yu",
        "Ping Yi"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-05-28",
      "update_time": "2024-05-28",
      "comments": null,
      "repo_url": "#"
    },
    "2405.17418": {
      "paper_id": "2405.17418v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.17418v1",
      "paper_key": "2405.17418",
      "paper_title": "Self-Corrected Multimodal Large Language Model for End-to-End Robot Manipulation",
      "paper_url": "http://arxiv.org/abs/2405.17418v1",
      "paper_abstract": "Robot manipulation policies have shown unsatisfactory action performance when confronted with novel task or object instances. Hence, the capability to automatically detect and self-correct failure action is essential for a practical robotic system. Recently, Multimodal Large Language Models (MLLMs) have shown promise in visual instruction following and demonstrated strong reasoning abilities in various tasks. To unleash general MLLMs as an end-to-end robotic agent, we introduce a Self-Corrected (SC)-MLLM, equipping our model not only to predict end-effector poses but also to autonomously recognize and correct failure actions. Specifically, we first conduct parameter-efficient fine-tuning to empower MLLM with pose prediction ability, which is reframed as a language modeling problem. When facing execution failures, our model learns to identify low-level action error causes (i.e., position and rotation errors) and adaptively seeks prompt feedback from experts. Based on the feedback, SC-MLLM rethinks the current failure scene and generates the corrected actions. Furthermore, we design a continuous policy learning method for successfully corrected samples, enhancing the model's adaptability to the current scene configuration and reducing the frequency of expert intervention. To evaluate our SC-MLLM, we conduct extensive experiments in both simulation and real-world settings. SC-MLLM agent significantly improve manipulation accuracy compared to previous state-of-the-art robotic MLLM (ManipLLM), increasing from 57\\% to 79\\% on seen object categories and from 47\\% to 69\\% on unseen novel categories.",
      "paper_authors": [
        "Jiaming Liu",
        "Chenxuan Li",
        "Guanqun Wang",
        "Lily Lee",
        "Kaichen Zhou",
        "Sixiang Chen",
        "Chuyan Xiong",
        "Jiaxin Ge",
        "Renrui Zhang",
        "Shanghang Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-27",
      "update_time": "2024-05-27",
      "comments": null,
      "repo_url": "#"
    },
    "2405.17358": {
      "paper_id": "2405.17358v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.17358v3",
      "paper_key": "2405.17358",
      "paper_title": "Rethinking Transformers in Solving POMDPs",
      "paper_url": "http://arxiv.org/abs/2405.17358v3",
      "paper_abstract": "Sequential decision-making algorithms such as reinforcement learning (RL) in real-world scenarios inevitably face environments with partial observability. This paper scrutinizes the effectiveness of a popular architecture, namely Transformers, in Partially Observable Markov Decision Processes (POMDPs) and reveals its theoretical limitations. We establish that regular languages, which Transformers struggle to model, are reducible to POMDPs. This poses a significant challenge for Transformers in learning POMDP-specific inductive biases, due to their lack of inherent recurrence found in other models like RNNs. This paper casts doubt on the prevalent belief in Transformers as sequence models for RL and proposes to introduce a point-wise recurrent structure. The Deep Linear Recurrent Unit (LRU) emerges as a well-suited alternative for Partially Observable RL, with empirical results highlighting the sub-optimal performance of the Transformer and considerable strength of LRU.",
      "paper_authors": [
        "Chenhao Lu",
        "Ruizhe Shi",
        "Yuyao Liu",
        "Kaizhe Hu",
        "Simon S. Du",
        "Huazhe Xu"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-05-27",
      "update_time": "2024-05-30",
      "comments": "Accepted by ICML 2024; references added; typos fixed",
      "repo_url": "https://github.com/ctp314/tfporl"
    },
    "2405.16877": {
      "paper_id": "2405.16877v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.16877v1",
      "paper_key": "2405.16877",
      "paper_title": "Are Self-Attentions Effective for Time Series Forecasting?",
      "paper_url": "http://arxiv.org/abs/2405.16877v1",
      "paper_abstract": "Time series forecasting is crucial for applications across multiple domains and various scenarios. Although Transformer models have dramatically shifted the landscape of forecasting, their effectiveness remains debated. Recent findings have indicated that simpler linear models might outperform complex Transformer-based approaches, highlighting the potential for more streamlined architectures. In this paper, we shift focus from the overall architecture of the Transformer to the effectiveness of self-attentions for time series forecasting. To this end, we introduce a new architecture, Cross-Attention-only Time Series transformer (CATS), that rethinks the traditional Transformer framework by eliminating self-attention and leveraging cross-attention mechanisms instead. By establishing future horizon-dependent parameters as queries and enhanced parameter sharing, our model not only improves long-term forecasting accuracy but also reduces the number of parameters and memory usage. Extensive experiment across various datasets demonstrates that our model achieves superior performance with the lowest mean squared error and uses fewer parameters compared to existing models.",
      "paper_authors": [
        "Dongbin Kim",
        "Jinseong Park",
        "Jaewook Lee",
        "Hoki Kim"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-05-27",
      "update_time": "2024-05-27",
      "comments": "20 pages, 14 figures, 13 tables. Submitted to NeurIPS 2024 (under\n  review)",
      "repo_url": "#"
    },
    "2405.16860": {
      "paper_id": "2405.16860v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.16860v1",
      "paper_key": "2405.16860",
      "paper_title": "Think Before You Act: A Two-Stage Framework for Mitigating Gender Bias Towards Vision-Language Tasks",
      "paper_url": "http://arxiv.org/abs/2405.16860v1",
      "paper_abstract": "Gender bias in vision-language models (VLMs) can reinforce harmful stereotypes and discrimination. In this paper, we focus on mitigating gender bias towards vision-language tasks. We identify object hallucination as the essence of gender bias in VLMs. Existing VLMs tend to focus on salient or familiar attributes in images but ignore contextualized nuances. Moreover, most VLMs rely on the co-occurrence between specific objects and gender attributes to infer the ignored features, ultimately resulting in gender bias. We propose GAMA, a task-agnostic generation framework to mitigate gender bias. GAMA consists of two stages: narrative generation and answer inference. During narrative generation, GAMA yields all-sided but gender-obfuscated narratives, which prevents premature concentration on localized image features, especially gender attributes. During answer inference, GAMA integrates the image, generated narrative, and a task-specific question prompt to infer answers for different vision-language tasks. This approach allows the model to rethink gender attributes and answers. We conduct extensive experiments on GAMA, demonstrating its debiasing and generalization ability.",
      "paper_authors": [
        "Yunqi Zhang",
        "Songda Li",
        "Chunyuan Deng",
        "Luyi Wang",
        "Hui Zhao"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-27",
      "update_time": "2024-05-27",
      "comments": "Accept to NAACL 2024(main)",
      "repo_url": "https://github.com/zyq0000/gama"
    },
    "2405.16374": {
      "paper_id": "2405.16374v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.16374v1",
      "paper_key": "2405.16374",
      "paper_title": "Isolate and then Identify: Rethinking Adaptive Group Testing",
      "paper_url": "http://arxiv.org/abs/2405.16374v1",
      "paper_abstract": "Group testing (GT) is the art of identifying binary signals and the marketplace for exchanging new ideas for related fields such as unique-element counting, compressed sensing, traitor tracing, and geno-typing. A GT scheme can be nonadaptive or adaptive; the latter is preferred when latency is ess of an issue. To construct adaptive GT schemes, a popular strategy is to spend the majority of tests in the first few rounds to gain as much information as possible, and uses later rounds to refine details. In this paper, we propose a transparent strategy called \"isolate and then identify\" (I@I). In the first few rounds, I@I divides the population into teams until every team contains at most one sick person. Then, in the last round, I@I identifies the sick person in each team. Performance-wise, I@I is the first GT scheme that achieves the optimal coefficient $1/$capacity$(Z)$ for the $k \\log_2 (n/k)$ term in the number of tests when $Z$ is a generic channel corrupting the test outcomes. I@I follows a modular methodology whereby the isolating part and the identification part can be optimized separately.",
      "paper_authors": [
        "Hsin-Po Wang",
        "Venkatesan Guruswami"
      ],
      "primary_category": "cs.IT",
      "publish_time": "2024-05-25",
      "update_time": "2024-05-25",
      "comments": "6 pages, 1 figure, ISIT 2024",
      "repo_url": "#"
    },
    "2406.01604": {
      "paper_id": "2406.01604v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2406.01604v2",
      "paper_key": "2406.01604",
      "paper_title": "An Empirical Study of Excitation and Aggregation Design Adaptions in CLIP4Clip for Video-Text Retrieval",
      "paper_url": "http://arxiv.org/abs/2406.01604v2",
      "paper_abstract": "CLIP4Clip model transferred from the CLIP has been the de-factor standard to solve the video clip retrieval task from frame-level input, triggering the surge of CLIP4Clip-based models in the video-text retrieval domain. In this work, we rethink the inherent limitation of widely-used mean pooling operation in the frame features aggregation and investigate the adaptions of excitation and aggregation design for discriminative video representation generation. We present a novel excitationand-aggregation design, including (1) The excitation module is available for capturing non-mutuallyexclusive relationships among frame features and achieving frame-wise features recalibration, and (2) The aggregation module is applied to learn exclusiveness used for frame representations aggregation. Similarly, we employ the cascade of sequential module and aggregation design to generate discriminative video representation in the sequential type. Besides, we adopt the excitation design in the tight type to obtain representative frame features for multi-modal interaction. The proposed modules are evaluated on three benchmark datasets of MSR-VTT, ActivityNet and DiDeMo, achieving MSR-VTT (43.9 R@1), ActivityNet (44.1 R@1) and DiDeMo (31.0 R@1). They outperform the CLIP4Clip results by +1.2% (+0.5%), +4.5% (+1.9%) and +9.5% (+2.7%) relative (absolute) improvements, demonstrating the superiority of our proposed excitation and aggregation designs. We hope our work will serve as an alternative for frame representations aggregation and facilitate future research.",
      "paper_authors": [
        "Xiaolun Jing",
        "Genke Yang",
        "Jian Chu"
      ],
      "primary_category": "cs.IR",
      "publish_time": "2024-05-25",
      "update_time": "2024-06-08",
      "comments": "20 pages",
      "repo_url": "#"
    },
    "2405.16038": {
      "paper_id": "2405.16038v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.16038v2",
      "paper_key": "2405.16038",
      "paper_title": "Rethinking Early-Fusion Strategies for Improved Multispectral Object Detection",
      "paper_url": "http://arxiv.org/abs/2405.16038v2",
      "paper_abstract": "Most recent multispectral object detectors employ a two-branch structure to extract features from RGB and thermal images. While the two-branch structure achieves better performance than a single-branch structure, it overlooks inference efficiency. This conflict is increasingly aggressive, as recent works solely pursue higher performance rather than both performance and efficiency. In this paper, we address this issue by improving the performance of efficient single-branch structures. We revisit the reasons causing the performance gap between these structures. For the first time, we reveal the information interference problem in the naive early-fusion strategy adopted by previous single-branch structures. Besides, we find that the domain gap between multispectral images, and weak feature representation of the single-branch structure are also key obstacles for performance. Focusing on these three problems, we propose corresponding solutions, including a novel shape-priority early-fusion strategy, a weakly supervised learning method, and a core knowledge distillation technique. Experiments demonstrate that single-branch networks equipped with these three contributions achieve significant performance enhancements while retaining high efficiency. Our code will be available at \\url{https://github.com/XueZ-phd/Efficient-RGB-T-Early-Fusion-Detection}.",
      "paper_authors": [
        "Xue Zhang",
        "Si-Yuan Cao",
        "Fang Wang",
        "Runmin Zhang",
        "Zhe Wu",
        "Xiaohan Zhang",
        "Xiaokai Bai",
        "Hui-Liang Shen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-25",
      "update_time": "2024-09-19",
      "comments": "This paper has been accepted by IEEE T-IV journal. Please jump to\n  External DOI to view the official version",
      "repo_url": "https://github.com/xuez-phd/efficient-rgb-t-early-fusion-detection"
    },
    "2405.15903": {
      "paper_id": "2405.15903v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.15903v1",
      "paper_key": "2405.15903",
      "paper_title": "UnitNorm: Rethinking Normalization for Transformers in Time Series",
      "paper_url": "http://arxiv.org/abs/2405.15903v1",
      "paper_abstract": "Normalization techniques are crucial for enhancing Transformer models' performance and stability in time series analysis tasks, yet traditional methods like batch and layer normalization often lead to issues such as token shift, attention shift, and sparse attention. We propose UnitNorm, a novel approach that scales input vectors by their norms and modulates attention patterns, effectively circumventing these challenges. Grounded in existing normalization frameworks, UnitNorm's effectiveness is demonstrated across diverse time series analysis tasks, including forecasting, classification, and anomaly detection, via a rigorous evaluation on 6 state-of-the-art models and 10 datasets. Notably, UnitNorm shows superior performance, especially in scenarios requiring robust attention mechanisms and contextual comprehension, evidenced by significant improvements by up to a 1.46 decrease in MSE for forecasting, and a 4.89% increase in accuracy for classification. This work not only calls for a reevaluation of normalization strategies in time series Transformers but also sets a new direction for enhancing model performance and stability. The source code is available at https://anonymous.4open.science/r/UnitNorm-5B84.",
      "paper_authors": [
        "Nan Huang",
        "Christian K\u00fcmmerle",
        "Xiang Zhang"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-05-24",
      "update_time": "2024-05-24",
      "comments": null,
      "repo_url": "#"
    },
    "2405.15564": {
      "paper_id": "2405.15564v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.15564v2",
      "paper_key": "2405.15564",
      "paper_title": "Rethinking Independent Cross-Entropy Loss For Graph-Structured Data",
      "paper_url": "http://arxiv.org/abs/2405.15564v2",
      "paper_abstract": "Graph neural networks (GNNs) have exhibited prominent performance in learning graph-structured data. Considering node classification task, based on the i.i.d assumption among node labels, the traditional supervised learning simply sums up cross-entropy losses of the independent training nodes and applies the average loss to optimize GNNs' weights. But different from other data formats, the nodes are naturally connected. It is found that the independent distribution modeling of node labels restricts GNNs' capability to generalize over the entire graph and defend adversarial attacks. In this work, we propose a new framework, termed joint-cluster supervised learning, to model the joint distribution of each node with its corresponding cluster. We learn the joint distribution of node and cluster labels conditioned on their representations, and train GNNs with the obtained joint loss. In this way, the data-label reference signals extracted from the local cluster explicitly strengthen the discrimination ability on the target node. The extensive experiments demonstrate that our joint-cluster supervised learning can effectively bolster GNNs' node classification accuracy. Furthermore, being benefited from the reference signals which may be free from spiteful interference, our learning paradigm significantly protects the node classification from being affected by the adversarial attack.",
      "paper_authors": [
        "Rui Miao",
        "Kaixiong Zhou",
        "Yili Wang",
        "Ninghao Liu",
        "Ying Wang",
        "Xin Wang"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-05-24",
      "update_time": "2024-05-27",
      "comments": "20 pages, 4 figures",
      "repo_url": "https://github.com/mr9812/joint-cluster-supervised-learning"
    },
    "2405.15157": {
      "paper_id": "2405.15157v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.15157v1",
      "paper_key": "2405.15157",
      "paper_title": "Rethinking Class-Incremental Learning from a Dynamic Imbalanced Learning Perspective",
      "paper_url": "http://arxiv.org/abs/2405.15157v1",
      "paper_abstract": "Deep neural networks suffer from catastrophic forgetting when continually learning new concepts. In this paper, we analyze this problem from a data imbalance point of view. We argue that the imbalance between old task and new task data contributes to forgetting of the old tasks. Moreover, the increasing imbalance ratio during incremental learning further aggravates the problem. To address the dynamic imbalance issue, we propose Uniform Prototype Contrastive Learning (UPCL), where uniform and compact features are learned. Specifically, we generate a set of non-learnable uniform prototypes before each task starts. Then we assign these uniform prototypes to each class and guide the feature learning through prototype contrastive learning. We also dynamically adjust the relative margin between old and new classes so that the feature distribution will be maintained balanced and compact. Finally, we demonstrate through extensive experiments that the proposed method achieves state-of-the-art performance on several benchmark datasets including CIFAR100, ImageNet100 and TinyImageNet.",
      "paper_authors": [
        "Leyuan Wang",
        "Liuyu Xiang",
        "Yunlong Wang",
        "Huijia Wu",
        "Zhaofeng He"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-24",
      "update_time": "2024-05-24",
      "comments": null,
      "repo_url": "https://github.com/Debatrix/UPCL"
    },
    "2405.14768": {
      "paper_id": "2405.14768v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.14768v1",
      "paper_key": "2405.14768",
      "paper_title": "WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models",
      "paper_url": "http://arxiv.org/abs/2405.14768v1",
      "paper_abstract": "Large language models (LLMs) need knowledge updates to meet the ever-growing world facts and correct the hallucinated responses, facilitating the methods of lifelong model editing. Where the updated knowledge resides in memories is a fundamental question for model editing. In this paper, we find that editing either long-term memory (direct model parameters) or working memory (non-parametric knowledge of neural network activations/representations by retrieval) will result in an impossible triangle -- reliability, generalization, and locality can not be realized together in the lifelong editing settings. For long-term memory, directly editing the parameters will cause conflicts with irrelevant pretrained knowledge or previous edits (poor reliability and locality). For working memory, retrieval-based activations can hardly make the model understand the edits and generalize (poor generalization). Therefore, we propose WISE to bridge the gap between memories. In WISE, we design a dual parametric memory scheme, which consists of the main memory for the pretrained knowledge and a side memory for the edited knowledge. We only edit the knowledge in the side memory and train a router to decide which memory to go through when given a query. For continual editing, we devise a knowledge-sharding mechanism where different sets of edits reside in distinct subspaces of parameters, and are subsequently merged into a shared memory without conflicts. Extensive experiments show that WISE can outperform previous model editing methods and overcome the impossible triangle under lifelong model editing of question answering, hallucination, and out-of-distribution settings across trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code will be released at https://github.com/zjunlp/EasyEdit.",
      "paper_authors": [
        "Peng Wang",
        "Zexi Li",
        "Ningyu Zhang",
        "Ziwen Xu",
        "Yunzhi Yao",
        "Yong Jiang",
        "Pengjun Xie",
        "Fei Huang",
        "Huajun Chen"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-05-23",
      "update_time": "2024-05-23",
      "comments": "Work in progress",
      "repo_url": "https://github.com/zjunlp/easyedit"
    },
    "2405.14264": {
      "paper_id": "2405.14264v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.14264v2",
      "paper_key": "2405.14264",
      "paper_title": "Reassessing Evaluation Functions in Algorithmic Recourse: An Empirical Study from a Human-Centered Perspective",
      "paper_url": "http://arxiv.org/abs/2405.14264v2",
      "paper_abstract": "In this study, we critically examine the foundational premise of algorithmic recourse - a process of generating counterfactual action plans (i.e., recourses) assisting individuals to reverse adverse decisions made by AI systems. The assumption underlying algorithmic recourse is that individuals accept and act on recourses that minimize the gap between their current and desired states. This assumption, however, remains empirically unverified. To address this issue, we conducted a user study with 362 participants and assessed whether minimizing the distance function, a metric of the gap between the current and desired states, indeed prompts them to accept and act upon suggested recourses. Our findings reveal a nuanced landscape: participants' acceptance of recourses did not correlate with the recourse distance. Moreover, participants' willingness to act upon recourses peaked at the minimal recourse distance but was otherwise constant. These findings cast doubt on the prevailing assumption of algorithmic recourse research and signal the need to rethink the evaluation functions to pave the way for human-centered recourse generation.",
      "paper_authors": [
        "Tomu Tominaga",
        "Naomi Yamashita",
        "Takeshi Kurashima"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-05-23",
      "update_time": "2024-08-03",
      "comments": "Accepted at IJCAI 2024 (this is the extended version with\n  supplementary materials)",
      "repo_url": "#"
    },
    "2405.17461": {
      "paper_id": "2405.17461v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.17461v1",
      "paper_key": "2405.17461",
      "paper_title": "EMR-Merging: Tuning-Free High-Performance Model Merging",
      "paper_url": "http://arxiv.org/abs/2405.17461v1",
      "paper_abstract": "The success of pretrain-finetune paradigm brings about the release of numerous model weights. In this case, merging models finetuned on different tasks to enable a single model with multi-task capabilities is gaining increasing attention for its practicability. Existing model merging methods usually suffer from (1) significant performance degradation or (2) requiring tuning by additional data or training. In this paper, we rethink and analyze the existing model merging paradigm. We discover that using a single model's weights can hardly simulate all the models' performance. To tackle this issue, we propose Elect, Mask & Rescale-Merging (EMR-Merging). We first (a) elect a unified model from all the model weights and then (b) generate extremely lightweight task-specific modulators, including masks and rescalers, to align the direction and magnitude between the unified model and each specific model, respectively. EMR-Merging is tuning-free, thus requiring no data availability or any additional training while showing impressive performance. We find that EMR-Merging shows outstanding performance compared to existing merging methods under different classical and newly-established settings, including merging different numbers of vision models (up to 30), NLP models, PEFT models, and multi-modal models.",
      "paper_authors": [
        "Chenyu Huang",
        "Peng Ye",
        "Tao Chen",
        "Tong He",
        "Xiangyu Yue",
        "Wanli Ouyang"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-05-23",
      "update_time": "2024-05-23",
      "comments": null,
      "repo_url": "#"
    },
    "2405.15817": {
      "paper_id": "2405.15817v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.15817v1",
      "paper_key": "2405.15817",
      "paper_title": "Rethinking the Elementary Function Fusion for Single-Image Dehazing",
      "paper_url": "http://arxiv.org/abs/2405.15817v1",
      "paper_abstract": "This paper addresses the limitations of physical models in the current field of image dehazing by proposing an innovative dehazing network (CL2S). Building on the DM2F model, it identifies issues in its ablation experiments and replaces the original logarithmic function model with a trigonometric (sine) model. This substitution aims to better fit the complex and variable distribution of haze. The approach also integrates the atmospheric scattering model and other elementary functions to enhance dehazing performance. Experimental results demonstrate that CL2S achieves outstanding performance on multiple dehazing datasets, particularly in maintaining image details and color authenticity. Additionally, systematic ablation experiments supplementing DM2F validate the concerns raised about DM2F and confirm the necessity and effectiveness of the functional components in the proposed CL2S model. Our code is available at \\url{https://github.com/YesianRohn/CL2S}, where the corresponding pre-trained models can also be accessed.",
      "paper_authors": [
        "Yesian Rohn"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-23",
      "update_time": "2024-05-23",
      "comments": null,
      "repo_url": "https://github.com/YesianRohn/CL2S"
    },
    "2405.13707": {
      "paper_id": "2405.13707v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.13707v1",
      "paper_key": "2405.13707",
      "paper_title": "Rethinking and Accelerating Graph Condensation: A Training-Free Approach with Class Partition",
      "paper_url": "http://arxiv.org/abs/2405.13707v1",
      "paper_abstract": "The increasing prevalence of large-scale graphs poses a significant challenge for graph neural network training, attributed to their substantial computational requirements. In response, graph condensation (GC) emerges as a promising data-centric solution aiming to substitute the large graph with a small yet informative condensed graph to facilitate data-efficient GNN training. However, existing GC methods suffer from intricate optimization processes, necessitating excessive computing resources. In this paper, we revisit existing GC optimization strategies and identify two pervasive issues: 1. various GC optimization strategies converge to class-level node feature matching between the original and condensed graphs, making the optimization target coarse-grained despite the complex computations; 2. to bridge the original and condensed graphs, existing GC methods rely on a Siamese graph network architecture that requires time-consuming bi-level optimization with iterative gradient computations. To overcome these issues, we propose a training-free GC framework termed Class-partitioned Graph Condensation (CGC), which refines the node feature matching from the class-to-class paradigm into a novel class-to-node paradigm. Remarkably, this refinement also simplifies the GC optimization as a class partition problem, which can be efficiently solved by any clustering methods. Moreover, CGC incorporates a pre-defined graph structure to enable a closed-form solution for condensed node features, eliminating the back-and-forth gradient descent in existing GC approaches without sacrificing accuracy. Extensive experiments demonstrate that CGC achieves state-of-the-art performance with a more efficient condensation process. For instance, compared with the seminal GC method (i.e., GCond), CGC condenses the largest Reddit graph within 10 seconds, achieving a 2,680X speedup and a 1.4% accuracy increase.",
      "paper_authors": [
        "Xinyi Gao",
        "Tong Chen",
        "Wentao Zhang",
        "Junliang Yu",
        "Guanhua Ye",
        "Quoc Viet Hung Nguyen",
        "Hongzhi Yin"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-05-22",
      "update_time": "2024-05-22",
      "comments": null,
      "repo_url": "#"
    },
    "2405.13536": {
      "paper_id": "2405.13536v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.13536v1",
      "paper_key": "2405.13536",
      "paper_title": "Attention Mechanisms Don't Learn Additive Models: Rethinking Feature Importance for Transformers",
      "paper_url": "http://arxiv.org/abs/2405.13536v1",
      "paper_abstract": "We address the critical challenge of applying feature attribution methods to the transformer architecture, which dominates current applications in natural language processing and beyond. Traditional attribution methods to explainable AI (XAI) explicitly or implicitly rely on linear or additive surrogate models to quantify the impact of input features on a model's output. In this work, we formally prove an alarming incompatibility: transformers are structurally incapable to align with popular surrogate models for feature attribution, undermining the grounding of these conventional explanation methodologies. To address this discrepancy, we introduce the Softmax-Linked Additive Log-Odds Model (SLALOM), a novel surrogate model specifically designed to align with the transformer framework. Unlike existing methods, SLALOM demonstrates the capacity to deliver a range of faithful and insightful explanations across both synthetic and real-world datasets. Showing that diverse explanations computed from SLALOM outperform common surrogate explanations on different tasks, we highlight the need for task-specific feature attributions rather than a one-size-fits-all approach.",
      "paper_authors": [
        "Tobias Leemann",
        "Alina Fastowski",
        "Felix Pfeiffer",
        "Gjergji Kasneci"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-05-22",
      "update_time": "2024-05-22",
      "comments": null,
      "repo_url": "#"
    },
    "2405.13206": {
      "paper_id": "2405.13206v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.13206v1",
      "paper_key": "2405.13206",
      "paper_title": "Identity-free Artificial Emotional Intelligence via Micro-Gesture Understanding",
      "paper_url": "http://arxiv.org/abs/2405.13206v1",
      "paper_abstract": "In this work, we focus on a special group of human body language -- the micro-gesture (MG), which differs from the range of ordinary illustrative gestures in that they are not intentional behaviors performed to convey information to others, but rather unintentional behaviors driven by inner feelings. This characteristic introduces two novel challenges regarding micro-gestures that are worth rethinking. The first is whether strategies designed for other action recognition are entirely applicable to micro-gestures. The second is whether micro-gestures, as supplementary data, can provide additional insights for emotional understanding. In recognizing micro-gestures, we explored various augmentation strategies that take into account the subtle spatial and brief temporal characteristics of micro-gestures, often accompanied by repetitiveness, to determine more suitable augmentation methods. Considering the significance of temporal domain information for micro-gestures, we introduce a simple and efficient plug-and-play spatiotemporal balancing fusion method. We not only studied our method on the considered micro-gesture dataset but also conducted experiments on mainstream action datasets. The results show that our approach performs well in micro-gesture recognition and on other datasets, achieving state-of-the-art performance compared to previous micro-gesture recognition methods. For emotional understanding based on micro-gestures, we construct complex emotional reasoning scenarios. Our evaluation, conducted with large language models, shows that micro-gestures play a significant and positive role in enhancing comprehensive emotional understanding. The scenarios we developed can be extended to other micro-gesture-based tasks such as deception detection and interviews. We confirm that our new insights contribute to advancing research in micro-gesture and emotional artificial intelligence.",
      "paper_authors": [
        "Rong Gao",
        "Xin Liu",
        "Bohao Xing",
        "Zitong Yu",
        "Bjorn W. Schuller",
        "Heikki K\u00e4lvi\u00e4inen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-21",
      "update_time": "2024-05-21",
      "comments": null,
      "repo_url": "#"
    },
    "2405.13129": {
      "paper_id": "2405.13129v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.13129v1",
      "paper_key": "2405.13129",
      "paper_title": "Rethinking the production and publication of machine-reusable expressions of research findings",
      "paper_url": "http://arxiv.org/abs/2405.13129v1",
      "paper_abstract": "Literature is the primary expression of scientific knowledge and an important source of research data. However, scientific knowledge expressed in narrative text documents is not inherently machine reusable. To facilitate knowledge reuse, e.g. for synthesis research, scientific knowledge must be extracted from articles and organized into databases post-publication. The high time costs and inaccuracies associated with completing these activities manually has driven the development of techniques that automate knowledge extraction. Tackling the problem with a different mindset, we propose a pre-publication approach, known as reborn, that ensures scientific knowledge is born reusable, i.e. produced in a machine-reusable format during knowledge production. We implement the approach using the Open Research Knowledge Graph infrastructure for FAIR scientific knowledge organization. We test the approach with three use cases, and discuss the role of publishers and editors in scaling the approach. Our results suggest that the proposed approach is superior compared to classical manual and semi-automated post-publication extraction techniques in terms of knowledge richness and accuracy as well as technological simplicity.",
      "paper_authors": [
        "Markus Stocker",
        "Lauren Snyder",
        "Matthew Anfuso",
        "Oliver Ludwig",
        "Freya Thie\u00dfen",
        "Kheir Eddine Farfar",
        "Muhammad Haris",
        "Allard Oelen",
        "Mohamad Yaser Jaradeh"
      ],
      "primary_category": "cs.DL",
      "publish_time": "2024-05-21",
      "update_time": "2024-05-21",
      "comments": null,
      "repo_url": "#"
    },
    "2405.12786": {
      "paper_id": "2405.12786v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.12786v3",
      "paper_key": "2405.12786",
      "paper_title": "Rethinking the Vulnerabilities of Face Recognition Systems:From a Practical Perspective",
      "paper_url": "http://arxiv.org/abs/2405.12786v3",
      "paper_abstract": "Face Recognition Systems (FRS) have increasingly integrated into critical applications, including surveillance and user authentication, highlighting their pivotal role in modern security systems. Recent studies have revealed vulnerabilities in FRS to adversarial (e.g., adversarial patch attacks) and backdoor attacks (e.g., training data poisoning), raising significant concerns about their reliability and trustworthiness. Previous studies primarily focus on traditional adversarial or backdoor attacks, overlooking the resource-intensive or privileged-manipulation nature of such threats, thus limiting their practical generalization, stealthiness, universality and robustness. Correspondingly, in this paper, we delve into the inherent vulnerabilities in FRS through user studies and preliminary explorations. By exploiting these vulnerabilities, we identify a novel attack, facial identity backdoor attack dubbed FIBA, which unveils a potentially more devastating threat against FRS:an enrollment-stage backdoor attack. FIBA circumvents the limitations of traditional attacks, enabling broad-scale disruption by allowing any attacker donning a specific trigger to bypass these systems. This implies that after a single, poisoned example is inserted into the database, the corresponding trigger becomes a universal key for any attackers to spoof the FRS. This strategy essentially challenges the conventional attacks by initiating at the enrollment stage, dramatically transforming the threat landscape by poisoning the feature database rather than the training data.",
      "paper_authors": [
        "Jiahao Chen",
        "Zhiqiang Shen",
        "Yuwen Pu",
        "Chunyi Zhou",
        "Changjiang Li",
        "Jiliang Li",
        "Ting Wang",
        "Shouling Ji"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-05-21",
      "update_time": "2024-06-08",
      "comments": "19 pages,version 3",
      "repo_url": "#"
    },
    "2405.12685": {
      "paper_id": "2405.12685v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.12685v2",
      "paper_key": "2405.12685",
      "paper_title": "Rethinking the Effective Field Theory formulation of Gravity",
      "paper_url": "http://arxiv.org/abs/2405.12685v2",
      "paper_abstract": "General relativity is highly successful in explaining a wide range of gravitational phenomena including the gravitational waves emitted by binary systems and the shadows cast by supermassive black holes. From a modern perspective the theory is not fundamental though, but constitutes the lowest order term in an effective field theory description of the gravitational force. As a consequence, the gravitational dynamics should receive corrections by higher-derivative terms. This essay discusses structural aspects associated with these corrections and summarizes their imprint on static, spherically symmetric geometries. Along these lines, we critically reassess the common practice of using local field redefinitions in order to simplify the dynamics at the danger of shifting physics effects into sectors which are beyond the approximation under consideration.",
      "paper_authors": [
        "Jesse Daas",
        "Cristobal Laporte",
        "Frank Saueressig",
        "Tim van Dijk"
      ],
      "primary_category": "gr-qc",
      "publish_time": "2024-05-21",
      "update_time": "2024-07-25",
      "comments": "10 pages. Essay received Honorable Mention at the Gravity Research\n  Foundation 2024 Awards for Essays on Gravitation. v2: fixed typos and updated\n  to match published version",
      "repo_url": "#"
    },
    "2405.12512": {
      "paper_id": "2405.12512v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.12512v1",
      "paper_key": "2405.12512",
      "paper_title": "Rethink Predicting the Optical Flow with the Kinetics Perspective",
      "paper_url": "http://arxiv.org/abs/2405.12512v1",
      "paper_abstract": "Optical flow estimation is one of the fundamental tasks in low-level computer vision, which describes the pixel-wise displacement and can be used in many other tasks. From the apparent aspect, the optical flow can be viewed as the correlation between the pixels in consecutive frames, so continuously refining the correlation volume can achieve an outstanding performance. However, it will make the method have a catastrophic computational complexity. Not only that, the error caused by the occlusion regions of the successive frames will be amplified through the inaccurate warp operation. These challenges can not be solved only from the apparent view, so this paper rethinks the optical flow estimation from the kinetics viewpoint.We propose a method combining the apparent and kinetics information from this motivation. The proposed method directly predicts the optical flow from the feature extracted from images instead of building the correlation volume, which will improve the efficiency of the whole network. Meanwhile, the proposed method involves a new differentiable warp operation that simultaneously considers the warping and occlusion. Moreover, the proposed method blends the kinetics feature with the apparent feature through the novel self-supervised loss function. Furthermore, comprehensive experiments and ablation studies prove that the proposed novel insight into how to predict the optical flow can achieve the better performance of the state-of-the-art methods, and in some metrics, the proposed method outperforms the correlation-based method, especially in situations containing occlusion and fast moving. The code will be public.",
      "paper_authors": [
        "Yuhao Cheng",
        "Siru Zhang",
        "Yiqiang Yan"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-21",
      "update_time": "2024-05-21",
      "comments": null,
      "repo_url": "https://github.com/YuhaoCheng/OpticalFlow_Kinetic"
    },
    "2405.12493": {
      "paper_id": "2405.12493v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.12493v1",
      "paper_key": "2405.12493",
      "paper_title": "Visualizing, Rethinking, and Mining the Loss Landscape of Deep Neural Networks",
      "paper_url": "http://arxiv.org/abs/2405.12493v1",
      "paper_abstract": "The loss landscape of deep neural networks (DNNs) is commonly considered complex and wildly fluctuated. However, an interesting observation is that the loss surfaces plotted along Gaussian noise directions are almost v-basin ones with the perturbed model lying on the basin. This motivates us to rethink whether the 1D or 2D subspace could cover more complex local geometry structures, and how to mine the corresponding perturbation directions. This paper systematically and gradually categorizes the 1D curves from simple to complex, including v-basin, v-side, w-basin, w-peak, and vvv-basin curves. Notably, the latter two types are already hard to obtain via the intuitive construction of specific perturbation directions, and we need to propose proper mining algorithms to plot the corresponding 1D curves. Combining these 1D directions, various types of 2D surfaces are visualized such as the saddle surfaces and the bottom of a bottle of wine that are only shown by demo functions in previous works. Finally, we propose theoretical insights from the lens of the Hessian matrix to explain the observed several interesting phenomena.",
      "paper_authors": [
        "Xin-Chun Li",
        "Lan Li",
        "De-Chuan Zhan"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-05-21",
      "update_time": "2024-05-21",
      "comments": null,
      "repo_url": "#"
    },
    "2405.12424": {
      "paper_id": "2405.12424v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.12424v2",
      "paper_key": "2405.12424",
      "paper_title": "Rethinking Robustness Assessment: Adversarial Attacks on Learning-based Quadrupedal Locomotion Controllers",
      "paper_url": "http://arxiv.org/abs/2405.12424v2",
      "paper_abstract": "Legged locomotion has recently achieved remarkable success with the progress of machine learning techniques, especially deep reinforcement learning (RL). Controllers employing neural networks have demonstrated empirical and qualitative robustness against real-world uncertainties, including sensor noise and external perturbations. However, formally investigating the vulnerabilities of these locomotion controllers remains a challenge. This difficulty arises from the requirement to pinpoint vulnerabilities across a long-tailed distribution within a high-dimensional, temporally sequential space. As a first step towards quantitative verification, we propose a computational method that leverages sequential adversarial attacks to identify weaknesses in learned locomotion controllers. Our research demonstrates that, even state-of-the-art robust controllers can fail significantly under well-designed, low-magnitude adversarial sequence. Through experiments in simulation and on the real robot, we validate our approach's effectiveness, and we illustrate how the results it generates can be used to robustify the original policy and offer valuable insights into the safety of these black-box policies. Project page: https://fanshi14.github.io/me/rss24.html",
      "paper_authors": [
        "Fan Shi",
        "Chong Zhang",
        "Takahiro Miki",
        "Joonho Lee",
        "Marco Hutter",
        "Stelian Coros"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-05-21",
      "update_time": "2024-05-30",
      "comments": "RSS 2024",
      "repo_url": "#"
    },
    "2405.11850": {
      "paper_id": "2405.11850v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.11850v1",
      "paper_key": "2405.11850",
      "paper_title": "Rethinking Overlooked Aspects in Vision-Language Models",
      "paper_url": "http://arxiv.org/abs/2405.11850v1",
      "paper_abstract": "Recent advancements in large vision-language models (LVLMs), such as GPT4-V and LLaVA, have been substantial. LLaVA's modular architecture, in particular, offers a blend of simplicity and efficiency. Recent works mainly focus on introducing more pre-training and instruction tuning data to improve model's performance. This paper delves into the often-neglected aspects of data efficiency during pre-training and the selection process for instruction tuning datasets. Our research indicates that merely increasing the size of pre-training data does not guarantee improved performance and may, in fact, lead to its degradation. Furthermore, we have established a pipeline to pinpoint the most efficient instruction tuning (SFT) dataset, implying that not all SFT data utilized in existing studies are necessary. The primary objective of this paper is not to introduce a state-of-the-art model, but rather to serve as a roadmap for future research, aiming to optimize data usage during pre-training and fine-tuning processes to enhance the performance of vision-language models.",
      "paper_authors": [
        "Yuan Liu",
        "Le Tian",
        "Xiao Zhou",
        "Jie Zhou"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-20",
      "update_time": "2024-05-20",
      "comments": null,
      "repo_url": "#"
    },
    "2405.11497": {
      "paper_id": "2405.11497v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.11497v1",
      "paper_key": "2405.11497",
      "paper_title": "Towards in-situ Psychological Profiling of Cybercriminals Using Dynamically Generated Deception Environments",
      "paper_url": "http://arxiv.org/abs/2405.11497v1",
      "paper_abstract": "Cybercrime is estimated to cost the global economy almost \\$10 trillion annually and with businesses and governments reporting an ever-increasing number of successful cyber-attacks there is a growing demand to rethink the strategy towards cyber security. The traditional, perimeter security approach to cyber defence has so far proved inadequate to combat the growing threat of cybercrime. Cyber deception offers a promising alternative by creating a dynamic defence environment. Deceptive techniques aim to mislead attackers, diverting them from critical assets whilst simultaneously gathering cyber threat intelligence on the threat actor. This article presents a proof-of-concept (POC) cyber deception system that has been developed to capture the profile of an attacker in-situ, during a simulated cyber-attack in real time. By dynamically and autonomously generating deception material based on the observed attacker behaviour and analysing how the attacker interacts with the deception material, the system outputs a prediction on the attacker's motive. The article also explores how this POC can be expanded to infer other features of the attacker's profile such as psychological characteristics. By dynamically and autonomously generating deception material based on observed attacker behaviour and analysing how the attacker interacts with the deception material, the system outputs a prediciton on the attacker's motive. The article also explores how this POC can be expanded to infer other features of the attacker's profile such as psychological characteristics.",
      "paper_authors": [
        "Jacob Quibell"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-05-19",
      "update_time": "2024-05-19",
      "comments": "16 pages, 4 figures, 6 tables",
      "repo_url": "#"
    },
    "2405.11304": {
      "paper_id": "2405.11304v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.11304v2",
      "paper_key": "2405.11304",
      "paper_title": "Quantum-Train: Rethinking Hybrid Quantum-Classical Machine Learning in the Model Compression Perspective",
      "paper_url": "http://arxiv.org/abs/2405.11304v2",
      "paper_abstract": "We introduces the Quantum-Train(QT) framework, a novel approach that integrates quantum computing with classical machine learning algorithms to address significant challenges in data encoding, model compression, and inference hardware requirements. Even with a slight decrease in accuracy, QT achieves remarkable results by employing a quantum neural network alongside a classical mapping model, which significantly reduces the parameter count from $M$ to $O(\\text{polylog} (M))$ during training. Our experiments demonstrate QT's effectiveness in classification tasks, offering insights into its potential to revolutionize machine learning by leveraging quantum computational advantages. This approach not only improves model efficiency but also reduces generalization errors, showcasing QT's potential across various machine learning applications.",
      "paper_authors": [
        "Chen-Yu Liu",
        "En-Jui Kuo",
        "Chu-Hsuan Abraham Lin",
        "Jason Gemsun Young",
        "Yeong-Jar Chang",
        "Min-Hsiu Hsieh",
        "Hsi-Sheng Goan"
      ],
      "primary_category": "quant-ph",
      "publish_time": "2024-05-18",
      "update_time": "2024-06-10",
      "comments": "12 pages, 6 figures",
      "repo_url": "#"
    },
    "2405.11165": {
      "paper_id": "2405.11165v4",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.11165v4",
      "paper_key": "2405.11165",
      "paper_title": "Automated Multi-level Preference for MLLMs",
      "paper_url": "http://arxiv.org/abs/2405.11165v4",
      "paper_abstract": "Current multimodal Large Language Models (MLLMs) suffer from ``hallucination'', occasionally generating responses that are not grounded in the input images. To tackle this challenge, one promising path is to utilize reinforcement learning from human feedback (RLHF), which steers MLLMs towards learning superior responses while avoiding inferior ones. We rethink the common practice of using binary preferences (i.e., superior, inferior), and find that adopting multi-level preferences (e.g., superior, medium, inferior) is better for two benefits: 1) It narrows the gap between adjacent levels, thereby encouraging MLLMs to discern subtle differences. 2) It further integrates cross-level comparisons (beyond adjacent-level comparisons), thus providing a broader range of comparisons with hallucination examples. To verify our viewpoint, we present the Automated Multi-level Preference (AMP) framework for MLLMs. To facilitate this framework, we first develop an automated dataset generation pipeline that provides high-quality multi-level preference datasets without any human annotators. Furthermore, we design the Multi-level Direct Preference Optimization (MDPO) algorithm to robustly conduct complex multi-level preference learning. Additionally, we propose a new hallucination benchmark, MRHal-Bench. Extensive experiments across public hallucination and general benchmarks, as well as our MRHal-Bench, demonstrate the effectiveness of our proposed method. Code is available at https://github.com/takomc/amp.",
      "paper_authors": [
        "Mengxi Zhang",
        "Wenhao Wu",
        "Yu Lu",
        "Yuxin Song",
        "Kang Rong",
        "Huanjin Yao",
        "Jianbo Zhao",
        "Fanglong Liu",
        "Yifan Sun",
        "Haocheng Feng",
        "Jingdong Wang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-18",
      "update_time": "2024-05-29",
      "comments": "Preprint",
      "repo_url": "https://github.com/takomc/amp"
    },
    "2405.10936": {
      "paper_id": "2405.10936v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.10936v1",
      "paper_key": "2405.10936",
      "paper_title": "A Survey on Large Language Models with Multilingualism: Recent Advances and New Frontiers",
      "paper_url": "http://arxiv.org/abs/2405.10936v1",
      "paper_abstract": "The rapid development of Large Language Models (LLMs) demonstrates remarkable multilingual capabilities in natural language processing, attracting global attention in both academia and industry. To mitigate potential discrimination and enhance the overall usability and accessibility for diverse language user groups, it is important for the development of language-fair technology. Despite the breakthroughs of LLMs, the investigation into the multilingual scenario remains insufficient, where a comprehensive survey to summarize recent approaches, developments, limitations, and potential solutions is desirable. To this end, we provide a survey with multiple perspectives on the utilization of LLMs in the multilingual scenario. We first rethink the transitions between previous and current research on pre-trained language models. Then we introduce several perspectives on the multilingualism of LLMs, including training and inference methods, model security, multi-domain with language culture, and usage of datasets. We also discuss the major challenges that arise in these aspects, along with possible solutions. Besides, we highlight future research directions that aim at further enhancing LLMs with multilingualism. The survey aims to help the research community address multilingual problems and provide a comprehensive understanding of the core concepts, key techniques, and latest developments in multilingual natural language processing based on LLMs.",
      "paper_authors": [
        "Kaiyu Huang",
        "Fengran Mo",
        "Hongliang Li",
        "You Li",
        "Yuanchi Zhang",
        "Weijian Yi",
        "Yulong Mao",
        "Jinchen Liu",
        "Yuzhuang Xu",
        "Jinan Xu",
        "Jian-Yun Nie",
        "Yang Liu"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-05-17",
      "update_time": "2024-05-17",
      "comments": "54 pages, Work in Progress",
      "repo_url": "https://github.com/kaiyuhwang/mllm-survey"
    },
    "2405.10879": {
      "paper_id": "2405.10879v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.10879v1",
      "paper_key": "2405.10879",
      "paper_title": "One registration is worth two segmentations",
      "paper_url": "http://arxiv.org/abs/2405.10879v1",
      "paper_abstract": "The goal of image registration is to establish spatial correspondence between two or more images, traditionally through dense displacement fields (DDFs) or parametric transformations (e.g., rigid, affine, and splines). Rethinking the existing paradigms of achieving alignment via spatial transformations, we uncover an alternative but more intuitive correspondence representation: a set of corresponding regions-of-interest (ROI) pairs, which we demonstrate to have sufficient representational capability as other correspondence representation methods.Further, it is neither necessary nor sufficient for these ROIs to hold specific anatomical or semantic significance. In turn, we formulate image registration as searching for the same set of corresponding ROIs from both moving and fixed images - in other words, two multi-class segmentation tasks on a pair of images. For a general-purpose and practical implementation, we integrate the segment anything model (SAM) into our proposed algorithms, resulting in a SAM-enabled registration (SAMReg) that does not require any training data, gradient-based fine-tuning or engineered prompts. We experimentally show that the proposed SAMReg is capable of segmenting and matching multiple ROI pairs, which establish sufficiently accurate correspondences, in three clinical applications of registering prostate MR, cardiac MR and abdominal CT images. Based on metrics including Dice and target registration errors on anatomical structures, the proposed registration outperforms both intensity-based iterative algorithms and DDF-predicting learning-based networks, even yielding competitive performance with weakly-supervised registration which requires fully-segmented training data.",
      "paper_authors": [
        "Shiqi Huang",
        "Tingfa Xu",
        "Ziyi Shen",
        "Shaheer Ullah Saeed",
        "Wen Yan",
        "Dean Barratt",
        "Yipeng Hu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-17",
      "update_time": "2024-05-17",
      "comments": "Early Accepted by MICCAI2024",
      "repo_url": "https://github.com/sqhuang0103/samreg"
    },
    "2405.10757": {
      "paper_id": "2405.10757v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.10757v3",
      "paper_key": "2405.10757",
      "paper_title": "Rethinking Graph Backdoor Attacks: A Distribution-Preserving Perspective",
      "paper_url": "http://arxiv.org/abs/2405.10757v3",
      "paper_abstract": "Graph Neural Networks (GNNs) have shown remarkable performance in various tasks. However, recent works reveal that GNNs are vulnerable to backdoor attacks. Generally, backdoor attack poisons the graph by attaching backdoor triggers and the target class label to a set of nodes in the training graph. A GNN trained on the poisoned graph will then be misled to predict test nodes attached with trigger to the target class. Despite their effectiveness, our empirical analysis shows that triggers generated by existing methods tend to be out-of-distribution (OOD), which significantly differ from the clean data. Hence, these injected triggers can be easily detected and pruned with widely used outlier detection methods in real-world applications. Therefore, in this paper, we study a novel problem of unnoticeable graph backdoor attacks with in-distribution (ID) triggers. To generate ID triggers, we introduce an OOD detector in conjunction with an adversarial learning strategy to generate the attributes of the triggers within distribution. To ensure a high attack success rate with ID triggers, we introduce novel modules designed to enhance trigger memorization by the victim model trained on poisoned graph. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed method in generating in distribution triggers that can by-pass various defense strategies while maintaining a high attack success rate.",
      "paper_authors": [
        "Zhiwei Zhang",
        "Minhua Lin",
        "Enyan Dai",
        "Suhang Wang"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-05-17",
      "update_time": "2024-07-12",
      "comments": "Accepted by KDD 2024",
      "repo_url": "https://github.com/zzwjames/dpgba"
    },
    "2405.10729": {
      "paper_id": "2405.10729v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.10729v2",
      "paper_key": "2405.10729",
      "paper_title": "Contestable AI needs Computational Argumentation",
      "paper_url": "http://arxiv.org/abs/2405.10729v2",
      "paper_abstract": "AI has become pervasive in recent years, but state-of-the-art approaches predominantly neglect the need for AI systems to be contestable. Instead, contestability is advocated by AI guidelines (e.g. by the OECD) and regulation of automated decision-making (e.g. GDPR). In this position paper we explore how contestability can be achieved computationally in and for AI. We argue that contestable AI requires dynamic (human-machine and/or machine-machine) explainability and decision-making processes, whereby machines can (i) interact with humans and/or other machines to progressively explain their outputs and/or their reasoning as well as assess grounds for contestation provided by these humans and/or other machines, and (ii) revise their decision-making processes to redress any issues successfully raised during contestation. Given that much of the current AI landscape is tailored to static AIs, the need to accommodate contestability will require a radical rethinking, that, we argue, computational argumentation is ideally suited to support.",
      "paper_authors": [
        "Francesco Leofante",
        "Hamed Ayoobi",
        "Adam Dejl",
        "Gabriel Freedman",
        "Deniz Gorur",
        "Junqi Jiang",
        "Guilherme Paulino-Passos",
        "Antonio Rago",
        "Anna Rapberger",
        "Fabrizio Russo",
        "Xiang Yin",
        "Dekai Zhang",
        "Francesca Toni"
      ],
      "primary_category": "cs.AI",
      "publish_time": "2024-05-17",
      "update_time": "2024-08-03",
      "comments": "Accepted at KR 2024",
      "repo_url": "#"
    },
    "2405.10474": {
      "paper_id": "2405.10474v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.10474v1",
      "paper_key": "2405.10474",
      "paper_title": "Rethinking ChatGPT's Success: Usability and Cognitive Behaviors Enabled by Auto-regressive LLMs' Prompting",
      "paper_url": "http://arxiv.org/abs/2405.10474v1",
      "paper_abstract": "Over the last decade, a wide range of training and deployment strategies for Large Language Models (LLMs) have emerged. Among these, the prompting paradigms of Auto-regressive LLMs (AR-LLMs) have catalyzed a significant surge in Artificial Intelligence (AI). This paper aims to emphasize the significance of utilizing free-form modalities (forms of input and output) and verbal free-form contexts as user-directed channels (methods for transforming modalities) for downstream deployment. Specifically, we analyze the structure of modalities within both two types of LLMs and six task-specific channels during deployment. From the perspective of users, our analysis introduces and applies the analytical metrics of task customizability, transparency, and complexity to gauge their usability, highlighting the superior nature of AR-LLMs' prompting paradigms. Moreover, we examine the stimulation of diverse cognitive behaviors in LLMs through the adoption of free-form text and verbal contexts, mirroring human linguistic expressions of such behaviors. We then detail four common cognitive behaviors to underscore how AR-LLMs' prompting successfully imitate human-like behaviors using this free-form modality and channel. Lastly, the potential for improving LLM deployment, both as autonomous agents and within multi-agent systems, is identified via cognitive behavior concepts and principles.",
      "paper_authors": [
        "Xinzhe Li",
        "Ming Liu"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-05-17",
      "update_time": "2024-05-17",
      "comments": null,
      "repo_url": "#"
    },
    "2405.10187": {
      "paper_id": "2405.10187v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.10187v1",
      "paper_key": "2405.10187",
      "paper_title": "Influence Maximization in Hypergraphs using Multi-Objective Evolutionary Algorithms",
      "paper_url": "http://arxiv.org/abs/2405.10187v1",
      "paper_abstract": "The Influence Maximization (IM) problem is a well-known NP-hard combinatorial problem over graphs whose goal is to find the set of nodes in a network that spreads influence at most. Among the various methods for solving the IM problem, evolutionary algorithms (EAs) have been shown to be particularly effective. While the literature on the topic is particularly ample, only a few attempts have been made at solving the IM problem over higher-order networks, namely extensions of standard graphs that can capture interactions that involve more than two nodes. Hypergraphs are a valuable tool for modeling complex interaction networks in various domains; however, they require rethinking of several graph-based problems, including IM. In this work, we propose a multi-objective EA for the IM problem over hypergraphs that leverages smart initialization and hypergraph-aware mutation. While the existing methods rely on greedy or heuristic methods, to our best knowledge this is the first attempt at applying EAs to this problem. Our results over nine real-world datasets and three propagation models, compared with five baseline algorithms, reveal that our method achieves in most cases state-of-the-art results in terms of hypervolume and solution diversity.",
      "paper_authors": [
        "Stefano Genetti",
        "Eros Ribaga",
        "Elia Cunegatti",
        "Quintino Francesco Lotito",
        "Giovanni Iacca"
      ],
      "primary_category": "cs.SI",
      "publish_time": "2024-05-16",
      "update_time": "2024-05-16",
      "comments": null,
      "repo_url": "https://github.com/diol-unitn/hn-moea-im"
    },
    "2405.10148": {
      "paper_id": "2405.10148v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.10148v1",
      "paper_key": "2405.10148",
      "paper_title": "SpecDETR: A Transformer-based Hyperspectral Point Object Detection Network",
      "paper_url": "http://arxiv.org/abs/2405.10148v1",
      "paper_abstract": "Hyperspectral target detection (HTD) aims to identify specific materials based on spectral information in hyperspectral imagery and can detect point targets, some of which occupy a smaller than one-pixel area. However, existing HTD methods are developed based on per-pixel binary classification, which limits the feature representation capability for point targets. In this paper, we rethink the hyperspectral point target detection from the object detection perspective, and focus more on the object-level prediction capability rather than the pixel classification capability. Inspired by the token-based processing flow of Detection Transformer (DETR), we propose the first specialized network for hyperspectral multi-class point object detection, SpecDETR. Without the backbone part of the current object detection framework, SpecDETR treats the spectral features of each pixel in hyperspectral images as a token and utilizes a multi-layer Transformer encoder with local and global coordination attention modules to extract deep spatial-spectral joint features. SpecDETR regards point object detection as a one-to-many set prediction problem, thereby achieving a concise and efficient DETR decoder that surpasses the current state-of-the-art DETR decoder in terms of parameters and accuracy in point object detection. We develop a simulated hyperSpectral Point Object Detection benchmark termed SPOD, and for the first time, evaluate and compare the performance of current object detection networks and HTD methods on hyperspectral multi-class point object detection. SpecDETR demonstrates superior performance as compared to current object detection networks and HTD methods on the SPOD dataset. Additionally, we validate on a public HTD dataset that by using data simulation instead of manual annotation, SpecDETR can detect real-world single-spectral point objects directly.",
      "paper_authors": [
        "Zhaoxu Li",
        "Wei An",
        "Gaowei Guo",
        "Longguang Wang",
        "Yingqian Wang",
        "Zaiping Lin"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-16",
      "update_time": "2024-05-16",
      "comments": null,
      "repo_url": "https://github.com/zhaoxuli123/specdetr"
    },
    "2405.09866": {
      "paper_id": "2405.09866v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.09866v1",
      "paper_key": "2405.09866",
      "paper_title": "Rethinking Multi-User Semantic Communications with Deep Generative Models",
      "paper_url": "http://arxiv.org/abs/2405.09866v1",
      "paper_abstract": "In recent years, novel communication strategies have emerged to face the challenges that the increased number of connected devices and the higher quality of transmitted information are posing. Among them, semantic communication obtained promising results especially when combined with state-of-the-art deep generative models, such as large language or diffusion models, able to regenerate content from extremely compressed semantic information. However, most of these approaches focus on single-user scenarios processing the received content at the receiver on top of conventional communication systems. In this paper, we propose to go beyond these methods by developing a novel generative semantic communication framework tailored for multi-user scenarios. This system assigns the channel to users knowing that the lost information can be filled in with a diffusion model at the receivers. Under this innovative perspective, OFDMA systems should not aim to transmit the largest part of information, but solely the bits necessary to the generative model to semantically regenerate the missing ones. The thorough experimental evaluation shows the capabilities of the novel diffusion model and the effectiveness of the proposed framework, leading towards a GenAI-based next generation of communications.",
      "paper_authors": [
        "Eleonora Grassucci",
        "Jinho Choi",
        "Jihong Park",
        "Riccardo F. Gramaccioni",
        "Giordano Cicchetti",
        "Danilo Comminiello"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2024-05-16",
      "update_time": "2024-05-16",
      "comments": "Under review in IEEE Journal on Selected Areas in Communications",
      "repo_url": "#"
    },
    "2405.09782": {
      "paper_id": "2405.09782v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.09782v2",
      "paper_key": "2405.09782",
      "paper_title": "Size-invariance Matters: Rethinking Metrics and Losses for Imbalanced Multi-object Salient Object Detection",
      "paper_url": "http://arxiv.org/abs/2405.09782v2",
      "paper_abstract": "This paper explores the size-invariance of evaluation metrics in Salient Object Detection (SOD), especially when multiple targets of diverse sizes co-exist in the same image. We observe that current metrics are size-sensitive, where larger objects are focused, and smaller ones tend to be ignored. We argue that the evaluation should be size-invariant because bias based on size is unjustified without additional semantic information. In pursuit of this, we propose a generic approach that evaluates each salient object separately and then combines the results, effectively alleviating the imbalance. We further develop an optimization framework tailored to this goal, achieving considerable improvements in detecting objects of different sizes. Theoretically, we provide evidence supporting the validity of our new metrics and present the generalization analysis of SOD. Extensive experiments demonstrate the effectiveness of our method. The code is available at https://github.com/Ferry-Li/SI-SOD.",
      "paper_authors": [
        "Feiran Li",
        "Qianqian Xu",
        "Shilong Bao",
        "Zhiyong Yang",
        "Runmin Cong",
        "Xiaochun Cao",
        "Qingming Huang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-16",
      "update_time": "2024-05-27",
      "comments": "This paper has been accepted by ICML2024",
      "repo_url": "https://github.com/ferry-li/si-sod"
    },
    "2405.09777": {
      "paper_id": "2405.09777v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.09777v3",
      "paper_key": "2405.09777",
      "paper_title": "Rethinking Barely-Supervised Volumetric Medical Image Segmentation from an Unsupervised Domain Adaptation Perspective",
      "paper_url": "http://arxiv.org/abs/2405.09777v3",
      "paper_abstract": "This paper investigates an extremely challenging problem: barely-supervised volumetric medical image segmentation (BSS). A BSS training dataset consists of two parts: 1) a barely-annotated labeled set, where each labeled image contains only a single-slice annotation, and 2) an unlabeled set comprising numerous unlabeled volumetric images. State-of-the-art BSS methods employ a registration-based paradigm, which uses inter-slice image registration to propagate single-slice annotations into volumetric pseudo labels, constructing a completely annotated labeled set, to which a semi-supervised segmentation scheme can be applied. However, the paradigm has a critical limitation: the pseudo-labels generated by image registration are unreliable and noisy. Motivated by this, we propose a new perspective: instead of solving BSS within a semi-supervised learning scheme, this work formulates BSS as an unsupervised domain adaptation problem. To this end, we propose a novel BSS framework, \\textbf{B}arely-supervised learning \\textbf{via} unsupervised domain \\textbf{A}daptation (BvA), as an alternative to the dominant registration paradigm. Specifically, we first design a novel noise-free labeled data construction algorithm (NFC) for slice-to-volume labeled data synthesis. Then, we introduce a frequency and spatial Mix-Up strategy (FSX) to mitigate the domain shifts. Extensive experiments demonstrate that our method provides a promising alternative for BSS. Remarkably, the proposed method, trained on the left atrial segmentation dataset with \\textbf{only one} barely-labeled image, achieves a Dice score of 81.20%, outperforming the state-of-the-art by 61.71%. The code is available at https://github.com/Senyh/BvA.",
      "paper_authors": [
        "Zhiqiang Shen",
        "Peng Cao",
        "Junming Su",
        "Jinzhu Yang",
        "Osmar R. Zaiane"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-16",
      "update_time": "2024-09-04",
      "comments": null,
      "repo_url": "https://github.com/senyh/bva"
    },
    "2405.09465": {
      "paper_id": "2405.09465v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.09465v1",
      "paper_key": "2405.09465",
      "paper_title": "Flashback: Enhancing Proposer-Builder Design with Future-Block Auctions in Proof-of-Stake Ethereum",
      "paper_url": "http://arxiv.org/abs/2405.09465v1",
      "paper_abstract": "Maximal extractable value (MEV) in which block proposers unethically gain profits by manipulating the order in which transactions are included within a block, is a key challenge facing blockchains such as Ethereum today. Left unchecked, MEV can lead to a centralization of stake distribution thereby ultimately compromising the security of blockchain consensus. To preserve proposer decentralization (and hence security) of the blockchain, Ethereum has advocated for a proposer-builder separation (PBS) in which the functionality of transaction ordering is separated from proposers and assigned to separate entities called builders. Builders accept transaction bundles from searchers, who compete to find the most profitable bundles. Builders then bid completed blocks to proposers, who accept the most profitable blocks for publication. The auction mechanisms used between searchers, builders and proposers are crucial to the overall health of the blockchain. In this paper, we consider PBS design in Ethereum as a game between searchers, builders and proposers. A key novelty in our design is the inclusion of future block proposers, as all proposers of an epoch are decided ahead of time in proof-of-stake (PoS) Ethereum within the game model. Our analysis shows the existence of alternative auction mechanisms that result in a better (more profitable) equilibrium to players compared to state-of-the-art. Experimental evaluations based on synthetic and real-world data traces corroborate the analysis. Our results highlight that a rethinking of auction mechanism designs is necessary in PoS Ethereum to prevent disruption.",
      "paper_authors": [
        "Yifan Mao",
        "Mengya Zhang",
        "Shaileshh Bojja Venkatakrishnan",
        "Zhiqiang Lin"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-05-15",
      "update_time": "2024-05-15",
      "comments": null,
      "repo_url": "#"
    },
    "2405.09155": {
      "paper_id": "2405.09155v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.09155v1",
      "paper_key": "2405.09155",
      "paper_title": "TunnelSense: Low-power, Non-Contact Sensing using Tunnel Diodes",
      "paper_url": "http://arxiv.org/abs/2405.09155v1",
      "paper_abstract": "Sensing the motion of physical objects in an environment enables numerous applications, from tracking occupancy in buildings and monitoring vital signs to diagnosing faults in machines. Typically, these application scenarios involve attaching a sensor, such as an accelerometer, to the object of interest, like a wearable device that tracks our steps. However, many of these scenarios require tracking motion in a noncontact manner where the sensor is not in touch with the object. A sensor in such a scenario observes variations in radio, light, acoustic, and infrared fields disturbed by the object's motion. Current noncontact sensing mechanisms often require substantial energy and involve complex processing on sophisticated hardware. We present TunnelSense, a novel mechanism that rethinks noncontact sensing using tunnel diode oscillators. They are highly sensitive to changes in their electromagnetic environments. The motion of an object near a tunnel diode oscillator induces corresponding changes in its resonant frequency and thus in the generated radio waves. Additionally, the low-power characteristics of the tunnel diode allow tags designed using them to operate on less than 100microwatt of power consumption and with a biasing voltage starting at 70 millivolt. This enables prolonged tag operation on a small battery or energy harvested from the environment. Among numerous applications enabled by the TunnelSense system, this work demonstrates its ability to detect breathing at distances up to 30 centimeter between the subject and the TunnelSense tag.",
      "paper_authors": [
        "Lim Chang Quan Thaddeus",
        "C. Rajashekar Reddy",
        "Yuvraj Singh Bhadauria",
        "Dhairya Shah",
        "Manoj Gulati",
        "Ambuj Varshney"
      ],
      "primary_category": "cs.ET",
      "publish_time": "2024-05-15",
      "update_time": "2024-05-15",
      "comments": "This work is accepted at IEEE RFID 2024",
      "repo_url": "#"
    },
    "2405.08570": {
      "paper_id": "2405.08570v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.08570v1",
      "paper_key": "2405.08570",
      "paper_title": "Rethinking the adaptive relationship between Encoder Layers and Decoder Layers",
      "paper_url": "http://arxiv.org/abs/2405.08570v1",
      "paper_abstract": "This article explores the adaptive relationship between Encoder Layers and Decoder Layers using the SOTA model Helsinki-NLP/opus-mt-de-en, which translates German to English. The specific method involves introducing a bias-free fully connected layer between the Encoder and Decoder, with different initializations of the layer's weights, and observing the outcomes of fine-tuning versus retraining. Four experiments were conducted in total. The results suggest that directly modifying the pre-trained model structure for fine-tuning yields suboptimal performance. However, upon observing the outcomes of the experiments with retraining, this structural adjustment shows significant potential.",
      "paper_authors": [
        "Yubo Song"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-05-14",
      "update_time": "2024-05-14",
      "comments": null,
      "repo_url": "https://github.com/parallelsucc/opus"
    },
    "2405.08493": {
      "paper_id": "2405.08493v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.08493v1",
      "paper_key": "2405.08493",
      "paper_title": "Rethinking Scanning Strategies with Vision Mamba in Semantic Segmentation of Remote Sensing Imagery: An Experimental Study",
      "paper_url": "http://arxiv.org/abs/2405.08493v1",
      "paper_abstract": "Deep learning methods, especially Convolutional Neural Networks (CNN) and Vision Transformer (ViT), are frequently employed to perform semantic segmentation of high-resolution remotely sensed images. However, CNNs are constrained by their restricted receptive fields, while ViTs face challenges due to their quadratic complexity. Recently, the Mamba model, featuring linear complexity and a global receptive field, has gained extensive attention for vision tasks. In such tasks, images need to be serialized to form sequences compatible with the Mamba model. Numerous research efforts have explored scanning strategies to serialize images, aiming to enhance the Mamba model's understanding of images. However, the effectiveness of these scanning strategies remains uncertain. In this research, we conduct a comprehensive experimental investigation on the impact of mainstream scanning directions and their combinations on semantic segmentation of remotely sensed images. Through extensive experiments on the LoveDA, ISPRS Potsdam, and ISPRS Vaihingen datasets, we demonstrate that no single scanning strategy outperforms others, regardless of their complexity or the number of scanning directions involved. A simple, single scanning direction is deemed sufficient for semantic segmentation of high-resolution remotely sensed images. Relevant directions for future research are also recommended.",
      "paper_authors": [
        "Qinfeng Zhu",
        "Yuan Fang",
        "Yuanzhi Cai",
        "Cheng Chen",
        "Lei Fan"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-14",
      "update_time": "2024-05-14",
      "comments": null,
      "repo_url": "#"
    },
    "2405.08458": {
      "paper_id": "2405.08458v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.08458v1",
      "paper_key": "2405.08458",
      "paper_title": "Rethinking Prior Information Generation with CLIP for Few-Shot Segmentation",
      "paper_url": "http://arxiv.org/abs/2405.08458v1",
      "paper_abstract": "Few-shot segmentation remains challenging due to the limitations of its labeling information for unseen classes. Most previous approaches rely on extracting high-level feature maps from the frozen visual encoder to compute the pixel-wise similarity as a key prior guidance for the decoder. However, such a prior representation suffers from coarse granularity and poor generalization to new classes since these high-level feature maps have obvious category bias. In this work, we propose to replace the visual prior representation with the visual-text alignment capacity to capture more reliable guidance and enhance the model generalization. Specifically, we design two kinds of training-free prior information generation strategy that attempts to utilize the semantic alignment capability of the Contrastive Language-Image Pre-training model (CLIP) to locate the target class. Besides, to acquire more accurate prior guidance, we build a high-order relationship of attention maps and utilize it to refine the initial prior information. Experiments on both the PASCAL-5{i} and COCO-20{i} datasets show that our method obtains a clearly substantial improvement and reaches the new state-of-the-art performance.",
      "paper_authors": [
        "Jin Wang",
        "Bingfeng Zhang",
        "Jian Pang",
        "Honglong Chen",
        "Weifeng Liu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-14",
      "update_time": "2024-05-14",
      "comments": "Accepted by CVPR 2024; The camera-ready version",
      "repo_url": "https://github.com/vangjin/PI-CLIP"
    },
    "2405.08169": {
      "paper_id": "2405.08169v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.08169v1",
      "paper_key": "2405.08169",
      "paper_title": "Rethinking Histology Slide Digitization Workflows for Low-Resource Settings",
      "paper_url": "http://arxiv.org/abs/2405.08169v1",
      "paper_abstract": "Histology slide digitization is becoming essential for telepathology (remote consultation), knowledge sharing (education), and using the state-of-the-art artificial intelligence algorithms (augmented/automated end-to-end clinical workflows). However, the cumulative costs of digital multi-slide high-speed brightfield scanners, cloud/on-premises storage, and personnel (IT and technicians) make the current slide digitization workflows out-of-reach for limited-resource settings, further widening the health equity gap; even single-slide manual scanning commercial solutions are costly due to hardware requirements (high-resolution cameras, high-spec PC/workstation, and support for only high-end microscopes). In this work, we present a new cloud slide digitization workflow for creating scanner-quality whole-slide images (WSIs) from uploaded low-quality videos, acquired from cheap and inexpensive microscopes with built-in cameras. Specifically, we present a pipeline to create stitched WSIs while automatically deblurring out-of-focus regions, upsampling input 10X images to 40X resolution, and reducing brightness/contrast and light-source illumination variations. We demonstrate the WSI creation efficacy from our workflow on World Health Organization-declared neglected tropical disease, Cutaneous Leishmaniasis (prevalent only in the poorest regions of the world and only diagnosed by sub-specialist dermatopathologists, rare in poor countries), as well as other common pathologies on core biopsies of breast, liver, duodenum, stomach and lymph node. The code and pretrained models will be accessible via our GitHub (https://github.com/nadeemlab/DeepLIIF), and the cloud platform will be available at https://deepliif.org for uploading microscope videos and downloading/viewing WSIs with shareable links (no sign-in required) for telepathology and knowledge sharing.",
      "paper_authors": [
        "Talat Zehra",
        "Joseph Marino",
        "Wendy Wang",
        "Grigoriy Frantsuzov",
        "Saad Nadeem"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2024-05-13",
      "update_time": "2024-05-13",
      "comments": "MICCAI 2024 Early Accept. First four authors contributed equally",
      "repo_url": "https://github.com/nadeemlab/deepliif"
    },
    "2405.07318": {
      "paper_id": "2405.07318v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.07318v1",
      "paper_key": "2405.07318",
      "paper_title": "AdaptNet: Rethinking Sensing and Communication for a Seamless Internet of Drones Experience",
      "paper_url": "http://arxiv.org/abs/2405.07318v1",
      "paper_abstract": "In the evolving era of Unmanned Aerial Vehicles (UAVs), the emphasis has moved from mere data collection to strategically obtaining timely and relevant data within the Internet of Drones (IoDs) ecosystem. However, the unpredictable conditions in dynamic IoDs pose safety challenges for drones. Addressing this, our approach introduces a multi-UAV framework using spatial-temporal clustering and the Frechet distance for enhancing reliability. Seamlessly coupled with Integrated Sensing and Communication (ISAC), it enhances the precision and agility of UAV networks. Our Multi-Agent Reinforcement Learning (MARL) mechanism ensures UAVs adapt strategies through ongoing environmental interactions and enhancing intelligent sensing. This focus ensures operational safety and efficiency, considering data capture and transmission viability. By evaluating the relevance of the sensed information, we can communicate only the most crucial data variations beyond a set threshold and optimize bandwidth usage. Our methodology transforms the UAV domain, transitioning drones from data gatherers to adept information orchestrators, establishing a benchmark for efficiency and adaptability in modern aerial systems.",
      "paper_authors": [
        "Ananya Hazarika",
        "Mehdi Rahmati"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2024-05-12",
      "update_time": "2024-05-12",
      "comments": null,
      "repo_url": "#"
    },
    "2405.06116": {
      "paper_id": "2405.06116v3",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.06116v3",
      "paper_key": "2405.06116",
      "paper_title": "Rethinking Efficient and Effective Point-based Networks for Event Camera Classification and Regression: EventMamba",
      "paper_url": "http://arxiv.org/abs/2405.06116v3",
      "paper_abstract": "Event cameras, drawing inspiration from biological systems, efficiently detect changes in ambient light with low latency and high dynamic range while consuming minimal power. The most current approach to processing event data often involves converting it into frame-based representations, which is well-established in traditional vision. However, this approach neglects the sparsity of event data, loses fine-grained temporal information during the transformation process, and increases the computational burden, making it ineffective for characterizing event camera properties. In contrast, Point Cloud is a popular representation for 3D processing and is better suited to match the sparse and asynchronous nature of the event camera. Nevertheless, despite the theoretical compatibility of point-based methods with event cameras, the results show a performance gap that is not yet satisfactory compared to frame-based methods. In order to bridge the performance gap, we propose EventMamba, an efficient and effective Point Cloud framework that achieves competitive results even compared to the state-of-the-art (SOTA) frame-based method in both classification and regression tasks. This notable accomplishment is facilitated by our rethinking of the distinction between Event Cloud and Point Cloud, emphasizing effective temporal information extraction through optimized network structures. Specifically, EventMamba leverages temporal aggregation and State Space Model (SSM) based Mamba boasting enhanced temporal information extraction capabilities. Through a hierarchical structure, EventMamba is adept at abstracting local and global spatial features and implicit and explicit temporal features. By adhering to the lightweight design principle, EventMamba delivers impressive results with minimal computational resource utilization, demonstrating its efficiency and effectiveness.",
      "paper_authors": [
        "Hongwei Ren",
        "Yue Zhou",
        "Jiadong Zhu",
        "Haotian Fu",
        "Yulong Huang",
        "Xiaopeng Lin",
        "Yuetong Fang",
        "Fei Ma",
        "Hao Yu",
        "Bojun Cheng"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-09",
      "update_time": "2024-07-03",
      "comments": "Extension Journal of TTPOINT and PEPNet, modify the dataset split\n  method",
      "repo_url": "#"
    },
    "2405.06710": {
      "paper_id": "2405.06710v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.06710v1",
      "paper_key": "2405.06710",
      "paper_title": "Mobile Sequencers",
      "paper_url": "http://arxiv.org/abs/2405.06710v1",
      "paper_abstract": "The article is an attempt to contribute to explorations of a common origin for language and planned-collaborative action. It gives `semantics of change' the central stage in the synthesis, from its history and recordkeeping to its development, its syntax, delivery and reception, including substratal aspects.   It is suggested that to arrive at a common core, linguistic semantics must be understood as studying through syntax mobile agent's representing, tracking and coping with change and no change. Semantics of actions can be conceived the same way, but through plans instead of syntax. The key point is the following: Sequencing itself, of words and action sequences, brings in more structural interpretation to the sequence than which is immediately evident from the sequents themselves. Mobile sequencers can be understood as subjects structuring reporting, understanding and keeping track of change and no change. The idea invites rethinking of the notion of category, both in language and in planning.   Understanding understanding change by mobile agents is suggested to be about human extended practice, not extended-human practice. That's why linguistics is as important as computer science in the synthesis. It must rely on representational history of acts, thoughts and expressions, personal and public, crosscutting overtness and covertness of these phenomena. It has implication for anthropology in the extended practice, which is covered briefly.",
      "paper_authors": [
        "Cem Bozsahin"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-05-09",
      "update_time": "2024-05-09",
      "comments": null,
      "repo_url": "#"
    },
    "2405.03875": {
      "paper_id": "2405.03875v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.03875v1",
      "paper_key": "2405.03875",
      "paper_title": "Rethinking Data Shapley for Data Selection Tasks: Misleads and Merits",
      "paper_url": "http://arxiv.org/abs/2405.03875v1",
      "paper_abstract": "Data Shapley provides a principled approach to data valuation and plays a crucial role in data-centric machine learning (ML) research. Data selection is considered a standard application of Data Shapley. However, its data selection performance has shown to be inconsistent across settings in the literature. This study aims to deepen our understanding of this phenomenon. We introduce a hypothesis testing framework and show that Data Shapley's performance can be no better than random selection without specific constraints on utility functions. We identify a class of utility functions, monotonically transformed modular functions, within which Data Shapley optimally selects data. Based on this insight, we propose a heuristic for predicting Data Shapley's effectiveness in data selection tasks. Our experiments corroborate these findings, adding new insights into when Data Shapley may or may not succeed.",
      "paper_authors": [
        "Jiachen T. Wang",
        "Tianji Yang",
        "James Zou",
        "Yongchan Kwon",
        "Ruoxi Jia"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-05-06",
      "update_time": "2024-05-06",
      "comments": "ICML 2024",
      "repo_url": "#"
    },
    "2405.02730": {
      "paper_id": "2405.02730v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.02730v2",
      "paper_key": "2405.02730",
      "paper_title": "U-DiTs: Downsample Tokens in U-Shaped Diffusion Transformers",
      "paper_url": "http://arxiv.org/abs/2405.02730v2",
      "paper_abstract": "Diffusion Transformers (DiTs) introduce the transformer architecture to diffusion tasks for latent-space image generation. With an isotropic architecture that chains a series of transformer blocks, DiTs demonstrate competitive performance and good scalability; but meanwhile, the abandonment of U-Net by DiTs and their following improvements is worth rethinking. To this end, we conduct a simple toy experiment by comparing a U-Net architectured DiT with an isotropic one. It turns out that the U-Net architecture only gain a slight advantage amid the U-Net inductive bias, indicating potential redundancies within the U-Net-style DiT. Inspired by the discovery that U-Net backbone features are low-frequency-dominated, we perform token downsampling on the query-key-value tuple for self-attention that bring further improvements despite a considerable amount of reduction in computation. Based on self-attention with downsampled tokens, we propose a series of U-shaped DiTs (U-DiTs) in the paper and conduct extensive experiments to demonstrate the extraordinary performance of U-DiT models. The proposed U-DiT could outperform DiT-XL/2 with only 1/6 of its computation cost. Codes are available at https://github.com/YuchuanTian/U-DiT.",
      "paper_authors": [
        "Yuchuan Tian",
        "Zhijun Tu",
        "Hanting Chen",
        "Jie Hu",
        "Chao Xu",
        "Yunhe Wang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-05-04",
      "update_time": "2024-06-03",
      "comments": "12 pages, 5 figures",
      "repo_url": "https://github.com/yuchuantian/u-dit"
    },
    "2405.02200": {
      "paper_id": "2405.02200v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.02200v2",
      "paper_key": "2405.02200",
      "paper_title": "Position: Why We Must Rethink Empirical Research in Machine Learning",
      "paper_url": "http://arxiv.org/abs/2405.02200v2",
      "paper_abstract": "We warn against a common but incomplete understanding of empirical research in machine learning that leads to non-replicable results, makes findings unreliable, and threatens to undermine progress in the field. To overcome this alarming situation, we call for more awareness of the plurality of ways of gaining knowledge experimentally but also of some epistemic limitations. In particular, we argue most current empirical machine learning research is fashioned as confirmatory research while it should rather be considered exploratory.",
      "paper_authors": [
        "Moritz Herrmann",
        "F. Julian D. Lange",
        "Katharina Eggensperger",
        "Giuseppe Casalicchio",
        "Marcel Wever",
        "Matthias Feurer",
        "David R\u00fcgamer",
        "Eyke H\u00fcllermeier",
        "Anne-Laure Boulesteix",
        "Bernd Bischl"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-05-03",
      "update_time": "2024-05-25",
      "comments": "20 pages, accepted for publication at ICML 2024, camera-ready version",
      "repo_url": "#"
    },
    "2405.01229": {
      "paper_id": "2405.01229v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.01229v1",
      "paper_key": "2405.01229",
      "paper_title": "Boosting Jailbreak Attack with Momentum",
      "paper_url": "http://arxiv.org/abs/2405.01229v1",
      "paper_abstract": "Large Language Models (LLMs) have achieved remarkable success across diverse tasks, yet they remain vulnerable to adversarial attacks, notably the well-documented \\textit{jailbreak} attack. Recently, the Greedy Coordinate Gradient (GCG) attack has demonstrated efficacy in exploiting this vulnerability by optimizing adversarial prompts through a combination of gradient heuristics and greedy search. However, the efficiency of this attack has become a bottleneck in the attacking process. To mitigate this limitation, in this paper we rethink the generation of adversarial prompts through an optimization lens, aiming to stabilize the optimization process and harness more heuristic insights from previous iterations. Specifically, we introduce the \\textbf{M}omentum \\textbf{A}ccelerated G\\textbf{C}G (\\textbf{MAC}) attack, which incorporates a momentum term into the gradient heuristic. Experimental results showcase the notable enhancement achieved by MAP in gradient-based attacks on aligned language models. Our code is available at https://github.com/weizeming/momentum-attack-llm.",
      "paper_authors": [
        "Yihao Zhang",
        "Zeming Wei"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-05-02",
      "update_time": "2024-05-02",
      "comments": "ICLR 2024 Workshop on Reliable and Responsible Foundation Models",
      "repo_url": "https://github.com/weizeming/momentum-attack-llm"
    },
    "2405.00982": {
      "paper_id": "2405.00982v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2405.00982v2",
      "paper_key": "2405.00982",
      "paper_title": "On the Evaluation of Machine-Generated Reports",
      "paper_url": "http://arxiv.org/abs/2405.00982v2",
      "paper_abstract": "Large Language Models (LLMs) have enabled new ways to satisfy information needs. Although great strides have been made in applying them to settings like document ranking and short-form text generation, they still struggle to compose complete, accurate, and verifiable long-form reports. Reports with these qualities are necessary to satisfy the complex, nuanced, or multi-faceted information needs of users. In this perspective paper, we draw together opinions from industry and academia, and from a variety of related research areas, to present our vision for automatic report generation, and -- critically -- a flexible framework by which such reports can be evaluated. In contrast with other summarization tasks, automatic report generation starts with a detailed description of an information need, stating the necessary background, requirements, and scope of the report. Further, the generated reports should be complete, accurate, and verifiable. These qualities, which are desirable -- if not required -- in many analytic report-writing settings, require rethinking how to build and evaluate systems that exhibit these qualities. To foster new efforts in building these systems, we present an evaluation framework that draws on ideas found in various evaluations. To test completeness and accuracy, the framework uses nuggets of information, expressed as questions and answers, that need to be part of any high-quality generated report. Additionally, evaluation of citations that map claims made in the report to their source documents ensures verifiability.",
      "paper_authors": [
        "James Mayfield",
        "Eugene Yang",
        "Dawn Lawrie",
        "Sean MacAvaney",
        "Paul McNamee",
        "Douglas W. Oard",
        "Luca Soldaini",
        "Ian Soboroff",
        "Orion Weller",
        "Efsun Kayi",
        "Kate Sanders",
        "Marc Mason",
        "Noah Hibbler"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-05-02",
      "update_time": "2024-05-10",
      "comments": "12 pages, 4 figures, accepted at SIGIR 2024 as perspective paper",
      "repo_url": "#"
    },
    "2404.19573": {
      "paper_id": "2404.19573v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2404.19573v1",
      "paper_key": "2404.19573",
      "paper_title": "War Elephants: Rethinking Combat AI and Human Oversight",
      "paper_url": "http://arxiv.org/abs/2404.19573v1",
      "paper_abstract": "This paper explores the changes that pervasive AI is having on the nature of combat. We look beyond the substitution of AI for experts to an approach where complementary human and machine abilities are blended. Using historical and modern examples, we show how autonomous weapons systems can be effectively managed by teams of human \"AI Operators\" combined with AI/ML \"Proxy Operators.\" By basing our approach on the principles of complementation, we provide for a flexible and dynamic approach to managing lethal autonomous systems. We conclude by presenting a path to achieving an integrated vision of machine-speed combat where the battlefield AI is operated by AI Operators that watch for patterns of behavior within battlefield to assess the performance of lethal autonomous systems. This approach enables the development of combat systems that are likely to be more ethical, operate at machine speed, and are capable of responding to a broader range of dynamic battlefield conditions than any purely autonomous AI system could support.",
      "paper_authors": [
        "Philip Feldman",
        "Aaron Dant",
        "Harry Dreany"
      ],
      "primary_category": "cs.CY",
      "publish_time": "2024-04-30",
      "update_time": "2024-04-30",
      "comments": "15 pages, 2 figures",
      "repo_url": "#"
    },
    "2409.08572": {
      "paper_id": "2409.08572v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08572v1",
      "paper_key": "2409.08572",
      "paper_title": "DiffFAS: Face Anti-Spoofing via Generative Diffusion Models",
      "paper_url": "http://arxiv.org/abs/2409.08572v1",
      "paper_abstract": "Face anti-spoofing (FAS) plays a vital role in preventing face recognition (FR) systems from presentation attacks. Nowadays, FAS systems face the challenge of domain shift, impacting the generalization performance of existing FAS methods. In this paper, we rethink about the inherence of domain shift and deconstruct it into two factors: image style and image quality. Quality influences the purity of the presentation of spoof information, while style affects the manner in which spoof information is presented. Based on our analysis, we propose DiffFAS framework, which quantifies quality as prior information input into the network to counter image quality shift, and performs diffusion-based high-fidelity cross-domain and cross-attack types generation to counter image style shift. DiffFAS transforms easily collectible live faces into high-fidelity attack faces with precise labels while maintaining consistency between live and spoof face identities, which can also alleviate the scarcity of labeled data with novel type attacks faced by nowadays FAS system. We demonstrate the effectiveness of our framework on challenging cross-domain and cross-attack FAS datasets, achieving the state-of-the-art performance. Available at https://github.com/murphytju/DiffFAS.",
      "paper_authors": [
        "Xinxu Ge",
        "Xin Liu",
        "Zitong Yu",
        "Jingang Shi",
        "Chun Qi",
        "Jie Li",
        "Heikki K\u00e4lvi\u00e4inen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "ECCV 24",
      "repo_url": "https://github.com/murphytju/difffas"
    },
    "2409.08474": {
      "paper_id": "2409.08474v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08474v1",
      "paper_key": "2409.08474",
      "paper_title": "Rethinking Meta-Learning from a Learning Lens",
      "paper_url": "http://arxiv.org/abs/2409.08474v1",
      "paper_abstract": "Meta-learning has emerged as a powerful approach for leveraging knowledge from previous tasks to solve new tasks. The mainstream methods focus on training a well-generalized model initialization, which is then adapted to different tasks with limited data and updates. However, it pushes the model overfitting on the training tasks. Previous methods mainly attributed this to the lack of data and used augmentations to address this issue, but they were limited by sufficient training and effective augmentation strategies. In this work, we focus on the more fundamental ``learning to learn'' strategy of meta-learning to explore what causes errors and how to eliminate these errors without changing the environment. Specifically, we first rethink the algorithmic procedure of meta-learning from a ``learning'' lens. Through theoretical and empirical analyses, we find that (i) this paradigm faces the risk of both overfitting and underfitting and (ii) the model adapted to different tasks promote each other where the effect is stronger if the tasks are more similar. Based on this insight, we propose using task relations to calibrate the optimization process of meta-learning and propose a plug-and-play method called Task Relation Learner (TRLearner) to achieve this goal. Specifically, it first obtains task relation matrices from the extracted task-specific meta-data. Then, it uses the obtained matrices with relation-aware consistency regularization to guide optimization. Extensive theoretical and empirical analyses demonstrate the effectiveness of TRLearner.",
      "paper_authors": [
        "Jingyao Wang",
        "Wenwen Qiang",
        "Jiangmeng Li",
        "Lingyu Si",
        "Changwen Zheng"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": null,
      "repo_url": "#"
    },
    "2409.08381": {
      "paper_id": "2409.08381v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08381v1",
      "paper_key": "2409.08381",
      "paper_title": "Rethinking Prompting Strategies for Multi-Label Recognition with Partial Annotations",
      "paper_url": "http://arxiv.org/abs/2409.08381v1",
      "paper_abstract": "Vision-language models (VLMs) like CLIP have been adapted for Multi-Label Recognition (MLR) with partial annotations by leveraging prompt-learning, where positive and negative prompts are learned for each class to associate their embeddings with class presence or absence in the shared vision-text feature space. While this approach improves MLR performance by relying on VLM priors, we hypothesize that learning negative prompts may be suboptimal, as the datasets used to train VLMs lack image-caption pairs explicitly focusing on class absence. To analyze the impact of positive and negative prompt learning on MLR, we introduce PositiveCoOp and NegativeCoOp, where only one prompt is learned with VLM guidance while the other is replaced by an embedding vector learned directly in the shared feature space without relying on the text encoder. Through empirical analysis, we observe that negative prompts degrade MLR performance, and learning only positive prompts, combined with learned negative embeddings (PositiveCoOp), outperforms dual prompt learning approaches. Moreover, we quantify the performance benefits that prompt-learning offers over a simple vision-features-only baseline, observing that the baseline displays strong performance comparable to dual prompt learning approach (DualCoOp), when the proportion of missing labels is low, while requiring half the training compute and 16 times fewer parameters",
      "paper_authors": [
        "Samyak Rawlekar",
        "Shubhang Bhatnagar",
        "Narendra Ahuja"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": null,
      "repo_url": "#"
    },
    "2409.09613": {
      "paper_id": "2409.09613v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09613v1",
      "paper_key": "2409.09613",
      "paper_title": "Rethinking KenLM: Good and Bad Model Ensembles for Efficient Text Quality Filtering in Large Web Corpora",
      "paper_url": "http://arxiv.org/abs/2409.09613v1",
      "paper_abstract": "With the increasing demand for substantial amounts of high-quality data to train large language models (LLMs), efficiently filtering large web corpora has become a critical challenge. For this purpose, KenLM, a lightweight n-gram-based language model that operates on CPUs, is widely used. However, the traditional method of training KenLM utilizes only high-quality data and, consequently, does not explicitly learn the linguistic patterns of low-quality data. To address this issue, we propose an ensemble approach that leverages two contrasting KenLMs: (i) Good KenLM, trained on high-quality data; and (ii) Bad KenLM, trained on low-quality data. Experimental results demonstrate that our approach significantly reduces noisy content while preserving high-quality content compared to the traditional KenLM training method. This indicates that our method can be a practical solution with minimal computational overhead for resource-constrained environments.",
      "paper_authors": [
        "Yungi Kim",
        "Hyunsoo Ha",
        "Sukyung Lee",
        "Jihoo Kim",
        "Seonghoon Yang",
        "Chanjun Park"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": null,
      "repo_url": "#"
    },
    "2409.09584": {
      "paper_id": "2409.09584v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09584v1",
      "paper_key": "2409.09584",
      "paper_title": "RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for Code Generation",
      "paper_url": "http://arxiv.org/abs/2409.09584v1",
      "paper_abstract": "LLM agents enhanced by tree search algorithms have yielded notable performances in code generation. However, current search algorithms in this domain suffer from low search quality due to several reasons: 1) Ineffective design of the search space for the high-reasoning demands of code generation tasks, 2) Inadequate integration of code feedback with the search algorithm, and 3) Poor handling of negative feedback during the search, leading to reduced search efficiency and quality. To address these challenges, we propose to search for the reasoning process of the code and use the detailed feedback of code execution to refine erroneous thoughts during the search. In this paper, we introduce RethinkMCTS, which employs the Monte Carlo Tree Search (MCTS) algorithm to conduct thought-level searches before generating code, thereby exploring a wider range of strategies. More importantly, we construct verbal feedback from fine-grained code execution feedback to refine erroneous thoughts during the search. This ensures that the search progresses along the correct reasoning paths, thus improving the overall search quality of the tree by leveraging execution feedback. Through extensive experiments, we demonstrate that RethinkMCTS outperforms previous search-based and feedback-based code generation baselines. On the HumanEval dataset, it improves the pass@1 of GPT-3.5-turbo from 70.12 to 89.02 and GPT-4o-mini from 87.20 to 94.51. It effectively conducts more thorough exploration through thought-level searches and enhances the search quality of the entire tree by incorporating rethink operation.",
      "paper_authors": [
        "Qingyao Li",
        "Wei Xia",
        "Kounianhua Du",
        "Xinyi Dai",
        "Ruiming Tang",
        "Yasheng Wang",
        "Yong Yu",
        "Weinan Zhang"
      ],
      "primary_category": "cs.SE",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": "11 pages, 4 figures",
      "repo_url": "#"
    },
    "2409.09464": {
      "paper_id": "2409.09464v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09464v2",
      "paper_key": "2409.09464",
      "paper_title": "Rethinking the Influence of Source Code on Test Case Generation",
      "paper_url": "http://arxiv.org/abs/2409.09464v2",
      "paper_abstract": "Large language models (LLMs) have been widely applied to assist test generation with the source code under test provided as the context. This paper aims to answer the question: If the source code under test is incorrect, will LLMs be misguided when generating tests? The effectiveness of test cases is measured by their accuracy, coverage, and bug detection effectiveness. Our evaluation results with five open- and six closed-source LLMs on four datasets demonstrate that incorrect code can significantly mislead LLMs in generating correct, high-coverage, and bug-revealing tests. For instance, in the HumanEval dataset, LLMs achieve 80.45% test accuracy when provided with task descriptions and correct code, but only 57.12% when given task descriptions and incorrect code. For the APPS dataset, prompts with correct code yield tests that detect 39.85% of the bugs, while prompts with incorrect code detect only 19.61%. These findings have important implications for the deployment of LLM-based testing: using it on mature code may help protect against future regression, but on early-stage immature code, it may simply bake in errors. Our findings also underscore the need for further research to improve LLMs resilience against incorrect code in generating reliable and bug-revealing tests.",
      "paper_authors": [
        "Dong Huang",
        "Jie M. Zhang",
        "Mingzhe Du",
        "Mark Harman",
        "Heming Cui"
      ],
      "primary_category": "cs.SE",
      "publish_time": "2024-09-14",
      "update_time": "2024-09-19",
      "comments": "23 pages",
      "repo_url": "https://github.com/huangd1999/EmpiricalStudyofTestGeneration"
    }
  },
  "Survey": {
    "2409.08262": {
      "paper_id": "2409.08262v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08262v1",
      "paper_key": "2409.08262",
      "paper_title": "Learning incomplete factorization preconditioners for GMRES",
      "paper_url": "http://arxiv.org/abs/2409.08262v1",
      "paper_abstract": "In this paper, we develop a data-driven approach to generate incomplete LU factorizations of large-scale sparse matrices. The learned approximate factorization is utilized as a preconditioner for the corresponding linear equation system in the GMRES method. Incomplete factorization methods are one of the most commonly applied algebraic preconditioners for sparse linear equation systems and are able to speed up the convergence of Krylov subspace methods. However, they are sensitive to hyper-parameters and might suffer from numerical breakdown or lead to slow convergence when not properly applied. We replace the typically hand-engineered algorithms with a graph neural network based approach that is trained against data to predict an approximate factorization. This allows us to learn preconditioners tailored for a specific problem distribution. We analyze and empirically evaluate different loss functions to train the learned preconditioners and show their effectiveness to decrease the number of GMRES iterations and improve the spectral properties on our synthetic dataset. The code is available at https://github.com/paulhausner/neural-incomplete-factorization.",
      "paper_authors": [
        "Paul H\u00e4usner",
        "Aleix Nieto Juscafresa",
        "Jens Sj\u00f6lund"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "The first two authors contributed equally, under review. 12 pages, 5\n  figures",
      "repo_url": "https://github.com/paulhausner/neural-incomplete-factorization"
    },
    "2409.08247": {
      "paper_id": "2409.08247v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08247v1",
      "paper_key": "2409.08247",
      "paper_title": "A review of compact geodesic orbit manifolds and the g.o. condition for $\\SU(5)/\\s(\\U(2)\\times \\U(2))$",
      "paper_url": "http://arxiv.org/abs/2409.08247v1",
      "paper_abstract": "Geodesic orbit manifolds (or g.o. manifolds) are those Riemannian manifolds $(M,g)$ whose geodesics are integral curves of Killing vector fields. Equivalently, there exists a Lie group $G$ of isometries of $(M,g)$ such that any geodesic $\\gamma$ has the simple form $\\gamma(t)=e^{tX}\\cdot p$, where $e$ denotes the exponential map on $G$. The classification of g.o. manifolds is a longstanding problem in Riemannian geometry. In this brief survey, we present some recent results and open questions on the subject focusing on the compact case. In addition we study the geodesic orbit condition for the space $\\SU(5)/\\s(\\U(2)\\times \\U(2))$.",
      "paper_authors": [
        "Andreas Arvanitoyeorgos",
        "Nikolaos Panagiotis Souris",
        "Marina Statha"
      ],
      "primary_category": "math.DG",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "9 pages. arXiv admin note: text overlap with arXiv:2103.02908",
      "repo_url": "#"
    },
    "2409.08229": {
      "paper_id": "2409.08229v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08229v1",
      "paper_key": "2409.08229",
      "paper_title": "Photonic Quantum Computers",
      "paper_url": "http://arxiv.org/abs/2409.08229v1",
      "paper_abstract": "In the pursuit of scalable and fault-tolerant quantum computing architectures, photonic-based quantum computers have emerged as a leading frontier. This article provides a comprehensive overview of advancements in photonic quantum computing, developed by leading industry players, examining current performance, architectural designs, and strategies for developing large-scale, fault-tolerant photonic quantum computers. It also highlights recent groundbreaking experiments that leverage the unique advantages of photonic technologies, underscoring their transformative potential. This review captures a pivotal moment of photonic quantum computing in the noisy intermediate-scale quantum (NISQ) era, offering insights into how photonic quantum computers might reshape the future of quantum computing.",
      "paper_authors": [
        "M. AbuGhanem"
      ],
      "primary_category": "quant-ph",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "47 pages, 16 figures",
      "repo_url": "#"
    },
    "2409.08193": {
      "paper_id": "2409.08193v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08193v2",
      "paper_key": "2409.08193",
      "paper_title": "Radii of Mirror Nuclei and Isobaric Triplets",
      "paper_url": "http://arxiv.org/abs/2409.08193v2",
      "paper_abstract": "We present a review of absolute root-mean-square charge radii of stable nuclei up to $Z=32$, which includes a previously overlooked uncertainty in the combined analysis of muonic x-ray and electron scattering experiments. From these \\textit{reference radii} and isotope shift measurements, we obtain those of 12 mirror pairs with a traceable and realistic uncertainty budget. The difference in radii between mirror nuclei is found to be proportional to the isospin asymmetry, confirming recent calculations by Novario \\emph{et al.} [PRL~130, 032501]. From the fitted proportionality constant and its uncertainty, the radii of 73 previously unknown mirror partners are predicted. These are useful e.g. for benchmarking atomic and nuclear theory, calibrating entire chains, and as an input to nuclear beta-decay calculations. The radii of $(T=1,T_z=0)$ nuclei are interpolated assuming negligible isospin symmetry breaking. This completes a model-independent, high-precision extraction of the charge and weak radii of all nuclei involved in the testing of the unitarity of the CKM matrix.",
      "paper_authors": [
        "Ben Ohayon"
      ],
      "primary_category": "nucl-ex",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-16",
      "comments": null,
      "repo_url": "#"
    },
    "2409.08112": {
      "paper_id": "2409.08112v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08112v1",
      "paper_key": "2409.08112",
      "paper_title": "Review of Recent Advances in Gaussian Process Regression Methods",
      "paper_url": "http://arxiv.org/abs/2409.08112v1",
      "paper_abstract": "Gaussian process (GP) methods have been widely studied recently, especially for large-scale systems with big data and even more extreme cases when data is sparse. Key advantages of these methods consist in: 1) the ability to provide inherent ways to assess the impact of uncertainties (especially in the data, and environment) on the solutions, 2) have efficient factorisation based implementations and 3) can be implemented easily in distributed manners and hence provide scalable solutions. This paper reviews the recently developed key factorised GP methods such as the hierarchical off-diagonal low-rank approximation methods and GP with Kronecker structures. An example illustrates the performance of these methods with respect to accuracy and computational complexity.",
      "paper_authors": [
        "Chenyi Lyu",
        "Xingchi Liu",
        "Lyudmila Mihaylova"
      ],
      "primary_category": "stat.ME",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": null,
      "repo_url": "#"
    },
    "2409.08089": {
      "paper_id": "2409.08089v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08089v1",
      "paper_key": "2409.08089",
      "paper_title": "Mental Stress Detection and Performance Enhancement Using FNIRS and Wrist Vibrator Biofeedback",
      "paper_url": "http://arxiv.org/abs/2409.08089v1",
      "paper_abstract": "Any person in his/her daily life activities experiences different kinds and various amounts of mental stress which has a destructive effect on their performance. Therefore, it is crucial to come up with a systematic way of stress management and performance enhancement. This paper presents a comprehensive portable and real-time biofeedback system that aims at boosting stress management and consequently performance enhancement. For this purpose, a real-time brain signal acquisition device, a wireless vibration biofeedback device, and a software-defined program for stress level classification have been developed. More importantly, the entire system has been designed to present minimum time delay by propitiously bridging all the essential parts of the system together. We have presented different signal processing and feature extraction techniques for an online stress detection application. Accordingly, by testing the stress classification section of the system, an accuracy of 83% and a recall detecting the true mental stress level of 92% was achieved. Moreover, the biofeedback system as integrity has been tested on 20 participants in the controlled experimental setup. Experiment evaluations show promising results of system performances, and the findings reveal that our system is able to help the participants reduce their stress level by 55% and increase their accuracy by 24.5%. It can be concluded from the observations that all primary premises on stress management and performance enhancement through reward learning are valid as well.",
      "paper_authors": [
        "Anita Beigzadeh",
        "Vahid Yazdnian",
        "Kamaledin Setarehdan"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "Under Review in Biomedical Signal Processing and Control journal",
      "repo_url": "#"
    },
    "2409.08087": {
      "paper_id": "2409.08087v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08087v1",
      "paper_key": "2409.08087",
      "paper_title": "Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks",
      "paper_url": "http://arxiv.org/abs/2409.08087v1",
      "paper_abstract": "Large Language Models (LLMs) demonstrate impressive capabilities across various fields, yet their increasing use raises critical security concerns. This article reviews recent literature addressing key issues in LLM security, with a focus on accuracy, bias, content detection, and vulnerability to attacks. Issues related to inaccurate or misleading outputs from LLMs is discussed, with emphasis on the implementation from fact-checking methodologies to enhance response reliability. Inherent biases within LLMs are critically examined through diverse evaluation techniques, including controlled input studies and red teaming exercises. A comprehensive analysis of bias mitigation strategies is presented, including approaches from pre-processing interventions to in-training adjustments and post-processing refinements. The article also probes the complexity of distinguishing LLM-generated content from human-produced text, introducing detection mechanisms like DetectGPT and watermarking techniques while noting the limitations of machine learning enabled classifiers under intricate circumstances. Moreover, LLM vulnerabilities, including jailbreak attacks and prompt injection exploits, are analyzed by looking into different case studies and large-scale competitions like HackAPrompt. This review is concluded by retrospecting defense mechanisms to safeguard LLMs, accentuating the need for more extensive research into the LLM security field.",
      "paper_authors": [
        "Benji Peng",
        "Keyu Chen",
        "Ming Li",
        "Pohsun Feng",
        "Ziqian Bi",
        "Junyu Liu",
        "Qian Niu"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "17 pages, 1 figure",
      "repo_url": "#"
    },
    "2409.08073": {
      "paper_id": "2409.08073v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08073v1",
      "paper_key": "2409.08073",
      "paper_title": "EDGE-INFERNO: Simulating every observable star in faint dwarf galaxies and their consequences for resolved-star photometric surveys",
      "paper_url": "http://arxiv.org/abs/2409.08073v1",
      "paper_abstract": "Interpretation of data from faint dwarf galaxies is made challenging by observations limited to only the brightest stars. We present a major improvement to tackle this challenge by undertaking zoomed cosmological simulations that resolve the evolution of all individual stars more massive than $0.5\\,{\\rm M}_{\\odot}$, thereby explicitly tracking all observable stars for the Hubble time. For the first time, we predict observable color-magnitude diagrams and the spatial distribution of $\\approx 100,000$ stars within four faint ($M_{\\star} \\approx 10^5 \\, \\,{\\rm M}_{\\odot}$) dwarf galaxies directly from their cosmological initial conditions. In all cases, simulations predict complex light profiles with multiple components, implying that typical observational measures of structural parameters can make total V-band magnitudes appear up to 0.5 mag dimmer compared to estimates from simulations. Furthermore, when only small ($\\lessapprox100$) numbers of stars are observable, shot noise from realizations of the color-magnitude diagram introduces uncertainties comparable to the population scatter in, e.g., total magnitude, half-light radius, and mean iron abundance measurements. Estimating these uncertainties with fully self-consistent mass growth, star formation and chemical enrichment histories paves the way for more robust interpretation of dwarf galaxy data.",
      "paper_authors": [
        "Eric P. Andersson",
        "Martin P. Rey",
        "Andrew Pontzen",
        "Corentin Cadiou",
        "Oscar Agertz",
        "Justin I. Read",
        "Nicolas F. Martin"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "Submitted to ApJL, comments welcome",
      "repo_url": "#"
    },
    "2409.08060": {
      "paper_id": "2409.08060v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08060v1",
      "paper_key": "2409.08060",
      "paper_title": "Super-slowly rotating Ap (ssrAp) stars: Spectroscopic study",
      "paper_url": "http://arxiv.org/abs/2409.08060v1",
      "paper_abstract": "To gain better understanding of the Ap stars with the longest rotation periods, we obtained high resolution spectra of a sample of super-slowly rotating Ap (ssrAp) star candidates identified by a TESS photometric survey, to confirm that they are indeed Ap stars, to check that their v sin i values are compatible with super-slow rotation, and to obtain a first estimate of their magnetic field strengths. We determined whenever possible their mean magnetic field modulus, their mean quadratic magnetic field, and an upper limit of their projected equatorial velocities. Eighteen of the 27 stars studied are typical Ap stars; most of the other nine appear to be misclassified. One of the Ap stars is not a slow rotator; it must be seen nearly pole-on. The properties of the remaining 17 are compatible with moderately to extremely long rotation periods. Eight new stars with resolved magnetically split lines in the visible range were discovered; their mean magnetic field modulus and their mean quadratic magnetic field were measured. The mean quadratic field could also be determined in five more stars. Five new spectroscopic binaries containing an Ap star were identified. Among the misclassified stars, one SB2 system with two similar, sharp-lined Am components was also discovered. The technique that we used to carry out a search for ssrAp star candidates using TESS data is validated, but appears limited by uncertainties in the spectral classification of Ap stars. The new magnetic field measurements obtained as part of this study lend further support to the tentative conclusions of our previous studies: the absence of periods longer than ~150 d in stars with magnetic fields stronger than ~7.5 kG, the lower rate of occurrence of super-slow rotation for field strengths less than ~2 kG than in the range ~3-7.5 kG, and the deficiency of slowly rotating Ap stars with field strengths between ~2 and ~3 kG.",
      "paper_authors": [
        "G. Mathys",
        "D. L. Holdsworth",
        "M. Giarrusso",
        "D. W. Kurtz",
        "G. Catanzaro",
        "F. Leone"
      ],
      "primary_category": "astro-ph.SR",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "34 pages, 30 figures, accepted for publication in A&A",
      "repo_url": "#"
    },
    "2409.08045": {
      "paper_id": "2409.08045v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08045v1",
      "paper_key": "2409.08045",
      "paper_title": "Unleashing Worms and Extracting Data: Escalating the Outcome of Attacks against RAG-based Inference in Scale and Severity Using Jailbreaking",
      "paper_url": "http://arxiv.org/abs/2409.08045v1",
      "paper_abstract": "In this paper, we show that with the ability to jailbreak a GenAI model, attackers can escalate the outcome of attacks against RAG-based GenAI-powered applications in severity and scale. In the first part of the paper, we show that attackers can escalate RAG membership inference attacks and RAG entity extraction attacks to RAG documents extraction attacks, forcing a more severe outcome compared to existing attacks. We evaluate the results obtained from three extraction methods, the influence of the type and the size of five embeddings algorithms employed, the size of the provided context, and the GenAI engine. We show that attackers can extract 80%-99.8% of the data stored in the database used by the RAG of a Q&A chatbot. In the second part of the paper, we show that attackers can escalate the scale of RAG data poisoning attacks from compromising a single GenAI-powered application to compromising the entire GenAI ecosystem, forcing a greater scale of damage. This is done by crafting an adversarial self-replicating prompt that triggers a chain reaction of a computer worm within the ecosystem and forces each affected application to perform a malicious activity and compromise the RAG of additional applications. We evaluate the performance of the worm in creating a chain of confidential data extraction about users within a GenAI ecosystem of GenAI-powered email assistants and analyze how the performance of the worm is affected by the size of the context, the adversarial self-replicating prompt used, the type and size of the embeddings algorithm employed, and the number of hops in the propagation. Finally, we review and analyze guardrails to protect RAG-based inference and discuss the tradeoffs.",
      "paper_authors": [
        "Stav Cohen",
        "Ron Bitton",
        "Ben Nassi"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "for Github, see\n  https://github.com/StavC/UnleashingWorms-ExtractingData",
      "repo_url": "https://github.com/stavc/unleashingworms-extractingdata"
    },
    "2409.08035": {
      "paper_id": "2409.08035v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08035v1",
      "paper_key": "2409.08035",
      "paper_title": "Massive Star Formation Starts in Sub-virial Dense Clumps Unless Resisted by Strong Magnetic Fields",
      "paper_url": "http://arxiv.org/abs/2409.08035v1",
      "paper_abstract": "The initial conditions are critical for understanding high-mass star formation, but are not well observed. Built on our previous characterization of a Galaxy-wide sample of 463 candidate high-mass starless clumps (HMSCs), here we investigate the dynamical state of a representative subsample of 44 HMSCs (radii 0.13-1.12 pc) using GBT NH3 (1,1) and (2,2) data from the Radio Ammonia Mid-Plane Survey (RAMPS) pilot data release. By fitting the two NH3 lines simultaneously, we obtain velocity dispersion, gas kinetic temperature, NH3 column density and abundance, Mach number, and virial parameter. Thermodynamic analysis reveals that most HMSCs have Mach number $<$5, inconsistent to what have been considered in theoretical models. All but one (43/44) of the HMSCs are gravitationally bound with virial parameter $\\alpha_{\\mathrm{vir}} < 2$. Either these massive clumps are in collapsing or magnetic field strengths of 0.10-2.65 mG (average 0.51 mG) would be needed to support them against collapsing. The estimated B-field strength correlates tightly with density, $B_{\\rm est}/{\\rm mG}=0.269\\,(n_{\\rm H_2}/10^4\\,{\\rm cm^{-3}})^{0.61}$, with a similar power-law index as found in observations, but a factor of 4.6 higher in strength. For the first time, the initial dynamical state of high-mass formation regions has been statistically constrained to be sub-virial, in contradictory to theoretical models in virial equilibrium, and in agreement with the lack of observed massive starless cores. The findings urge future observations to quantify the magnetic field support in the prestellar stage of massive clumps, which are rarely explored so far, towards a full understanding of the physical conditions that initiate massive star formation.",
      "paper_authors": [
        "Ke Wang",
        "Yueluo Wang",
        "Fengwei Xu"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "Accepted to ApJL, 19 pages, 4 figures, 1 table",
      "repo_url": "#"
    },
    "2409.08016": {
      "paper_id": "2409.08016v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08016v1",
      "paper_key": "2409.08016",
      "paper_title": "A review of the structure of street networks",
      "paper_url": "http://arxiv.org/abs/2409.08016v1",
      "paper_abstract": "We review measures of street network structure proposed in the recent literature, establish their relevance to practice, and identify open challenges facing researchers. These measures' empirical values vary substantially across world regions and development eras, indicating street networks' geometric and topological heterogeneity.",
      "paper_authors": [
        "Marc Barthelemy",
        "Geoff Boeing"
      ],
      "primary_category": "physics.soc-ph",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "7 pages, 3 figures and SI (3 pages)",
      "repo_url": "#"
    },
    "2409.07997": {
      "paper_id": "2409.07997v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07997v1",
      "paper_key": "2409.07997",
      "paper_title": "Privacy-preserving federated prediction of pain intensity change based on multi-center survey data",
      "paper_url": "http://arxiv.org/abs/2409.07997v1",
      "paper_abstract": "Background: Patient-reported survey data are used to train prognostic models aimed at improving healthcare. However, such data are typically available multi-centric and, for privacy reasons, cannot easily be centralized in one data repository. Models trained locally are less accurate, robust, and generalizable. We present and apply privacy-preserving federated machine learning techniques for prognostic model building, where local survey data never leaves the legally safe harbors of the medical centers. Methods: We used centralized, local, and federated learning techniques on two healthcare datasets (GLA:D data from the five health regions of Denmark and international SHARE data of 27 countries) to predict two different health outcomes. We compared linear regression, random forest regression, and random forest classification models trained on local data with those trained on the entire data in a centralized and in a federated fashion. Results: In GLA:D data, federated linear regression (R2 0.34, RMSE 18.2) and federated random forest regression (R2 0.34, RMSE 18.3) models outperform their local counterparts (i.e., R2 0.32, RMSE 18.6, R2 0.30, RMSE 18.8) with statistical significance. We also found that centralized models (R2 0.34, RMSE 18.2, R2 0.32, RMSE 18.5, respectively) did not perform significantly better than the federated models. In SHARE, the federated model (AC 0.78, AUROC: 0.71) and centralized model (AC 0.84, AUROC: 0.66) perform significantly better than the local models (AC: 0.74, AUROC: 0.69). Conclusion: Federated learning enables the training of prognostic models from multi-center surveys without compromising privacy and with only minimal or no compromise regarding model performance.",
      "paper_authors": [
        "Supratim Das",
        "Mahdie Rafie",
        "Paula Kammer",
        "S\u00f8ren T. Skou",
        "Dorte T. Gr\u00f8nne",
        "Ewa M. Roos",
        "Andr\u00e9 Hajek",
        "Hans-Helmut K\u00f6nig",
        "Md Shihab Ullaha",
        "Niklas Probul",
        "Jan Baumbacha",
        "Linda Baumbach"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": null,
      "repo_url": "#"
    },
    "2409.07975": {
      "paper_id": "2409.07975v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07975v1",
      "paper_key": "2409.07975",
      "paper_title": "Deep Learning for Personalized Electrocardiogram Diagnosis: A Review",
      "paper_url": "http://arxiv.org/abs/2409.07975v1",
      "paper_abstract": "The electrocardiogram (ECG) remains a fundamental tool in cardiac diagnostics, yet its interpretation traditionally reliant on the expertise of cardiologists. The emergence of deep learning has heralded a revolutionary era in medical data analysis, particularly in the domain of ECG diagnostics. However, inter-patient variability prohibit the generalibility of ECG-AI model trained on a population dataset, hence degrade the performance of ECG-AI on specific patient or patient group. Many studies have address this challenge using different deep learning technologies. This comprehensive review systematically synthesizes research from a wide range of studies to provide an in-depth examination of cutting-edge deep-learning techniques in personalized ECG diagnosis. The review outlines a rigorous methodology for the selection of pertinent scholarly articles and offers a comprehensive overview of deep learning approaches applied to personalized ECG diagnostics. Moreover, the challenges these methods encounter are investigated, along with future research directions, culminating in insights into how the integration of deep learning can transform personalized ECG diagnosis and enhance cardiac care. By emphasizing both the strengths and limitations of current methodologies, this review underscores the immense potential of deep learning to refine and redefine ECG analysis in clinical practice, paving the way for more accurate, efficient, and personalized cardiac diagnostics.",
      "paper_authors": [
        "Cheng Ding",
        "Tianliang Yao",
        "Chenwei Wu",
        "Jianyuan Ni"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": null,
      "repo_url": "#"
    },
    "2409.07955": {
      "paper_id": "2409.07955v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07955v1",
      "paper_key": "2409.07955",
      "paper_title": "Charting the Nanohertz Gravitational Wave Sky with Pulsar Timing Arrays",
      "paper_url": "http://arxiv.org/abs/2409.07955v1",
      "paper_abstract": "In the summer of 2023, the pulsar timing arrays (PTAs) announced a compelling evidence for the existence of a nanohertz stochastic gravitational wave background (SGWB). Despite this breakthrough, however, several critical questions remain unanswered: What is the source of the signal? How can cosmic variance be accounted for? To what extent can we constrain nanohertz gravity? When will individual supermassive black hole binaries become observable? And how can we achieve a stronger detection? These open questions have spurred significant interests in PTA science, making this an opportune moment to revisit the astronomical and theoretical foundations of the field, as well as the data analysis techniques employed. In this review, we focus on the theoretical aspects of the SGWB as detected by PTAs. We provide a comprehensive derivation of the expected signal and its correlation, presented in a pedagogical manner, while also addressing current constraints. Looking ahead, we explore future milestones in the field, with detailed discussions on emerging theoretical considerations such as cosmic variance, the cumulants of the one- and two-point functions, subluminal gravitational waves, and the anisotropy and polarization of the SGWB.",
      "paper_authors": [
        "Reginald Christian Bernardo",
        "Kin-Wang Ng"
      ],
      "primary_category": "astro-ph.CO",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "31 pages + refs, 7 figures, invited chapter for `One Hundred and Ten\n  Years of General Relativity -- From Genesis and Empirical Foundations to\n  Gravitational Waves, Cosmology and Quantum Gravity', comments welcome",
      "repo_url": "#"
    },
    "2409.07948": {
      "paper_id": "2409.07948v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07948v1",
      "paper_key": "2409.07948",
      "paper_title": "Quickest Change Detection Using Mismatched CUSUM",
      "paper_url": "http://arxiv.org/abs/2409.07948v1",
      "paper_abstract": "The field of quickest change detection (QCD) concerns design and analysis of algorithms to estimate in real time the time at which an important event takes place and identify properties of the post-change behavior. The goal is to devise a stopping time adapted to the observations that minimizes an $L_1$ loss.   Approximately optimal solutions are well known under a variety of assumptions. In the work surveyed here we consider the CUSUM statistic, which is defined as a one-dimensional reflected random walk driven by a functional of the observations. It is known that the optimal functional is a log likelihood ratio subject to special statical assumptions.   The paper concerns model free approaches to detection design, considering the following questions:   1. What is the performance for a given functional of the observations?   2. How do the conclusions change when there is dependency between pre- and post-change behavior?   3. How can techniques from statistics and machine learning be adapted to approximate the best functional in a given class?",
      "paper_authors": [
        "Austin Cooper",
        "Sean Meyn"
      ],
      "primary_category": "math.ST",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "Extended version of extended abstract for the Allerton Conference on\n  Communication, Control, and Computing, September 2024",
      "repo_url": "#"
    },
    "2409.07936": {
      "paper_id": "2409.07936v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07936v1",
      "paper_key": "2409.07936",
      "paper_title": "Detecting and Defending Against Adversarial Attacks on Automatic Speech Recognition via Diffusion Models",
      "paper_url": "http://arxiv.org/abs/2409.07936v1",
      "paper_abstract": "Automatic speech recognition (ASR) systems are known to be vulnerable to adversarial attacks. This paper addresses detection and defence against targeted white-box attacks on speech signals for ASR systems. While existing work has utilised diffusion models (DMs) to purify adversarial examples, achieving state-of-the-art results in keyword spotting tasks, their effectiveness for more complex tasks such as sentence-level ASR remains unexplored. Additionally, the impact of the number of forward diffusion steps on performance is not well understood. In this paper, we systematically investigate the use of DMs for defending against adversarial attacks on sentences and examine the effect of varying forward diffusion steps. Through comprehensive experiments on the Mozilla Common Voice dataset, we demonstrate that two forward diffusion steps can completely defend against adversarial attacks on sentences. Moreover, we introduce a novel, training-free approach for detecting adversarial attacks by leveraging a pre-trained DM. Our experimental results show that this method can detect adversarial attacks with high accuracy.",
      "paper_authors": [
        "Nikolai L. K\u00fchne",
        "Astrid H. F. Kitchen",
        "Marie S. Jensen",
        "Mikkel S. L. Br\u00f8ndt",
        "Martin Gonzalez",
        "Christophe Biscio",
        "Zheng-Hua Tan"
      ],
      "primary_category": "eess.AS",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "Under review at ICASSP 2025",
      "repo_url": "#"
    },
    "2409.07934": {
      "paper_id": "2409.07934v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07934v1",
      "paper_key": "2409.07934",
      "paper_title": "Modeling Human Responses by Ordinal Archetypal Analysis",
      "paper_url": "http://arxiv.org/abs/2409.07934v1",
      "paper_abstract": "This paper introduces a novel framework for Archetypal Analysis (AA) tailored to ordinal data, particularly from questionnaires. Unlike existing methods, the proposed method, Ordinal Archetypal Analysis (OAA), bypasses the two-step process of transforming ordinal data into continuous scales and operates directly on the ordinal data. We extend traditional AA methods to handle the subjective nature of questionnaire-based data, acknowledging individual differences in scale perception. We introduce the Response Bias Ordinal Archetypal Analysis (RBOAA), which learns individualized scales for each subject during optimization. The effectiveness of these methods is demonstrated on synthetic data and the European Social Survey dataset, highlighting their potential to provide deeper insights into human behavior and perception. The study underscores the importance of considering response bias in cross-national research and offers a principled approach to analyzing ordinal data through Archetypal Analysis.",
      "paper_authors": [
        "Anna Emilie J. Wedenborg",
        "Michael Alexander Harborg",
        "Andreas Bigom",
        "Oliver Elmgreen",
        "Marcus Presutti",
        "Andreas R\u00e5skov",
        "Fumiko Kano Gl\u00fcckstad",
        "Mikkel Schmidt",
        "Morten M\u00f8rup"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "Accepted at Machine Learning and Signal Processing 2024",
      "repo_url": "https://github.com/maplewarrior/ordinalarchetypalanalysis"
    },
    "2409.07890": {
      "paper_id": "2409.07890v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07890v1",
      "paper_key": "2409.07890",
      "paper_title": "DNN-based workflow for attenuating seismic interference noise and its application to marine towed streamer data from the Northern Viking Graben",
      "paper_url": "http://arxiv.org/abs/2409.07890v1",
      "paper_abstract": "To separate seismic interference (SI) noise while ensuring high signal fidelity, we propose a deep neural network (DNN)-based workflow applied to common shot gathers (CSGs). In our design, a small subset of the entire to-be-processed data set is first processed by a conventional algorithm to obtain an estimate of the SI noise (from now on called the SI noise model). By manually blending the SI noise model with SI-free CSGs and a set of simulated random noise, we obtain training inputs for the DNN. The SI-free CSGs can be either real SI-free CSGs from the survey or SI-attenuated CSGs produced in parallel with the SI noise model from the conventional algorithm depending on the specific project. To enhance the DNN's output signal fidelity, adjacent shots on both sides of the to-be-processed shot are used as additional channels of the input. We train the DNN to output the SI noise into one channel and the SI-free shot along with the intact random noise into another. Once trained, the DNN can be applied to the entire data set contaminated by the same types of SI in the training process, producing results efficiently. For demonstration, we applied the proposed DNN-based workflow to 3D seismic field data acquired from the Northern Viking Graben (NVG) of the North Sea, and compared it with a conventional algorithm. The studied area has a challenging SI contamination problem with no sail lines free from SI noise during the acquisition. The comparison shows that the proposed DNN-based workflow outperformed the conventional algorithm in processing quality with less noise residual and better signal preservation. This validates its feasibility and value for real processing projects.",
      "paper_authors": [
        "Jing Sun",
        "Song Hou",
        "Alaa Triki"
      ],
      "primary_category": "physics.geo-ph",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": null,
      "repo_url": "#"
    },
    "2409.07878": {
      "paper_id": "2409.07878v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07878v1",
      "paper_key": "2409.07878",
      "paper_title": "Mapping Technical Safety Research at AI Companies: A literature review and incentives analysis",
      "paper_url": "http://arxiv.org/abs/2409.07878v1",
      "paper_abstract": "As artificial intelligence (AI) systems become more advanced, concerns about large-scale risks from misuse or accidents have grown. This report analyzes the technical research into safe AI development being conducted by three leading AI companies: Anthropic, Google DeepMind, and OpenAI.   We define safe AI development as developing AI systems that are unlikely to pose large-scale misuse or accident risks. This encompasses a range of technical approaches aimed at ensuring AI systems behave as intended and do not cause unintended harm, even as they are made more capable and autonomous.   We analyzed all papers published by the three companies from January 2022 to July 2024 that were relevant to safe AI development, and categorized the 61 included papers into eight safety approaches. Additionally, we noted three categories representing nascent approaches explored by academia and civil society, but not currently represented in any papers by the three companies. Our analysis reveals where corporate attention is concentrated and where potential gaps lie.   Some AI research may stay unpublished for good reasons, such as to not inform adversaries about security techniques they would need to overcome to misuse AI systems. Therefore, we also considered the incentives that AI companies have to research each approach. In particular, we considered reputational effects, regulatory burdens, and whether the approaches could make AI systems more useful.   We identified three categories where there are currently no or few papers and where we do not expect AI companies to become more incentivized to pursue this research in the future. These are multi-agent safety, model organisms of misalignment, and safety by design. Our findings provide an indication that these approaches may be slow to progress without funding or efforts from government, civil society, philanthropists, or academia.",
      "paper_authors": [
        "Oscar Delaney",
        "Oliver Guest",
        "Zoe Williams"
      ],
      "primary_category": "cs.CY",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": null,
      "repo_url": "#"
    },
    "2409.07858": {
      "paper_id": "2409.07858v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07858v1",
      "paper_key": "2409.07858",
      "paper_title": "Audio Decoding by Inverse Problem Solving",
      "paper_url": "http://arxiv.org/abs/2409.07858v1",
      "paper_abstract": "We consider audio decoding as an inverse problem and solve it through diffusion posterior sampling. Explicit conditioning functions are developed for input signal measurements provided by an example of a transform domain perceptual audio codec. Viability is demonstrated by evaluating arbitrary pairings of a set of bitrates and task-agnostic prior models. For instance, we observe significant improvements on piano while maintaining speech performance when a speech model is replaced by a joint model trained on both speech and piano. With a more general music model, improved decoding compared to legacy methods is obtained for a broad range of content types and bitrates. The noisy mean model, underlying the proposed derivation of conditioning, enables a significant reduction of gradient evaluations for diffusion posterior sampling, compared to methods based on Tweedie's mean. Combining Tweedie's mean with our conditioning functions improves the objective performance. An audio demo is available at https://dpscodec-demo.github.io/.",
      "paper_authors": [
        "Pedro J. Villasana T.",
        "Lars Villemoes",
        "Janusz Klejsa",
        "Per Hedelin"
      ],
      "primary_category": "eess.AS",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "5 pages, 4 figures, audio demo available at\n  https://dpscodec-demo.github.io/, pre-review version submitted to ICASSP 2025",
      "repo_url": "#"
    },
    "2409.07831": {
      "paper_id": "2409.07831v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07831v1",
      "paper_key": "2409.07831",
      "paper_title": "Fate of Boltzmann's breathers: kinetic theory perspective",
      "paper_url": "http://arxiv.org/abs/2409.07831v1",
      "paper_abstract": "The dynamics of a system composed of elastic hard particles confined by an isotropic harmonic potential are studied. In the low-density limit, the Boltzmann equation provides an excellent description, and the system does not reach equilibrium except for highly specific initial conditions: it generically evolves towards and stays in a breathing mode. This state is periodic in time, with a Gaussian velocity distribution, an oscillating temperature and a density profile that oscillates as well. We characterize this breather in terms of initial conditions, and constants of the motion. For low but finite densities, the analysis requires to take into account the finite size of the particles. Under well-controlle approximations, a closed description is provided, which shows how equilibrium is reached at long times. The (weak) dissipation at work erodes the breather's amplitude, while concomitantly shifting its oscillation frequency. An excellent agreement is found between Molecular Dynamics simulation results and the theoretical predictions for the frequency shift. For the damping time, the agreement is not as accurate as for the frequency and the origin of the discrepancies is discussed.",
      "paper_authors": [
        "P. Maynar",
        "M. I. Garc\u00eda de Soria",
        "D. Gu\u00e9ry-Odelin",
        "E. Trizac"
      ],
      "primary_category": "cond-mat.stat-mech",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "Accepted in Physical Review E",
      "repo_url": "#"
    },
    "2409.07825": {
      "paper_id": "2409.07825v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07825v2",
      "paper_key": "2409.07825",
      "paper_title": "A Comprehensive Survey on Deep Multimodal Learning with Missing Modality",
      "paper_url": "http://arxiv.org/abs/2409.07825v2",
      "paper_abstract": "During multimodal model training and reasoning, data samples may miss certain modalities and lead to compromised model performance due to sensor limitations, cost constraints, privacy concerns, data loss, and temporal and spatial factors. This survey provides an overview of recent progress in Multimodal Learning with Missing Modality (MLMM), focusing on deep learning techniques. It is the first comprehensive survey that covers the historical background and the distinction between MLMM and standard multimodal learning setups, followed by a detailed analysis of current MLMM methods, applications, and datasets, concluding with a discussion about challenges and potential future directions in the field.",
      "paper_authors": [
        "Renjie Wu",
        "Hu Wang",
        "Hsiang-Ting Chen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-13",
      "comments": "Work in progress; open to discussion; planning to submit to ACM CSUR\n  in September",
      "repo_url": "#"
    },
    "2409.07757": {
      "paper_id": "2409.07757v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07757v1",
      "paper_key": "2409.07757",
      "paper_title": "From Uncertainty to Clarity: Uncertainty-Guided Class-Incremental Learning for Limited Biomedical Samples via Semantic Expansion",
      "paper_url": "http://arxiv.org/abs/2409.07757v1",
      "paper_abstract": "In real-world clinical settings, data distributions evolve over time, with a continuous influx of new, limited disease cases. Therefore, class incremental learning is of great significance, i.e., deep learning models are required to learn new class knowledge while maintaining accurate recognition of previous diseases. However, traditional deep neural networks often suffer from severe forgetting of prior knowledge when adapting to new data unless trained from scratch, which undesirably costs much time and computational burden. Additionally, the sample sizes for different diseases can be highly imbalanced, with newly emerging diseases typically having much fewer instances, consequently causing the classification bias. To tackle these challenges, we are the first to propose a class-incremental learning method under limited samples in the biomedical field. First, we propose a novel cumulative entropy prediction module to measure the uncertainty of the samples, of which the most uncertain samples are stored in a memory bank as exemplars for the model's later review. Furthermore, we theoretically demonstrate its effectiveness in measuring uncertainty. Second, we developed a fine-grained semantic expansion module through various augmentations, leading to more compact distributions within the feature space and creating sufficient room for generalization to new classes. Besides, a cosine classifier is utilized to mitigate classification bias caused by imbalanced datasets. Across four imbalanced data distributions over two datasets, our method achieves optimal performance, surpassing state-of-the-art methods by as much as 53.54% in accuracy.",
      "paper_authors": [
        "Yifei Yao",
        "Hanrong Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": null,
      "repo_url": "#"
    },
    "2409.07749": {
      "paper_id": "2409.07749v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07749v1",
      "paper_key": "2409.07749",
      "paper_title": "Contrasting Statistical Phase Estimation with the Variational Quantum Eigensolver in the era of Early Fault Tolerant Quantum Computation",
      "paper_url": "http://arxiv.org/abs/2409.07749v1",
      "paper_abstract": "In this review, we give an overview of the proposed applications in the early-FTQC (EFTQC) era.   Starting from the error correction architecture for EFTQC device, we first review the recently developed space-time efficient analogue rotation (STAR) architecture \\cite{akahoshiPartiallyFaultTolerantQuantum2024}, which is a partially fault-tolerant error correction architecture.   Then, we review the requirements of an EFTQC algorithm.   In particular, the class of ground state energy estimation (GSEE) algorithm known as the statistical phase estimation algorithm (SPE) is studied.   We especially cast our attention on two SPE-type algorithms, the step-function filter-based variant by Lin and Tong (LT22) \\cite{Lin:2021rwb} and Gaussian Filter \\cite{Wang:2022gxu}.   Based on the latter, we introduce the Gaussian Fitting algorithm, which uses an alternative post-processing procedure compared to \\cite{Wang:2022gxu}.   Finally, we systematically simulate the aforementioned algorithms and Variational Quantum Eigensolver (VQE) using the 1-uCJ ansatz with different shot counts.   Most importantly, we perform noisy simulations based on the STAR architecture.   We find that for estimating the ground state energy of the 4-qubit $H_2$ Hamiltonian in the STO-3G basis, SPE becomes more advantageous over VQE when the physical error rate is sufficiently low.",
      "paper_authors": [
        "Ming-Zhi Chung",
        "Andreas Thomasen",
        "Henry Liao",
        "Ryosuke Imai"
      ],
      "primary_category": "quant-ph",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "30 pages, 10 figures",
      "repo_url": "#"
    },
    "2409.07736": {
      "paper_id": "2409.07736v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07736v1",
      "paper_key": "2409.07736",
      "paper_title": "Transfer Learning Applied to Computer Vision Problems: Survey on Current Progress, Limitations, and Opportunities",
      "paper_url": "http://arxiv.org/abs/2409.07736v1",
      "paper_abstract": "The field of Computer Vision (CV) has faced challenges. Initially, it relied on handcrafted features and rule-based algorithms, resulting in limited accuracy. The introduction of machine learning (ML) has brought progress, particularly Transfer Learning (TL), which addresses various CV problems by reusing pre-trained models. TL requires less data and computing while delivering nearly equal accuracy, making it a prominent technique in the CV landscape. Our research focuses on TL development and how CV applications use it to solve real-world problems. We discuss recent developments, limitations, and opportunities.",
      "paper_authors": [
        "Aaryan Panda",
        "Damodar Panigrahi",
        "Shaswata Mitra",
        "Sudip Mittal",
        "Shahram Rahimi"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "16 pages, 8 figures",
      "repo_url": "#"
    },
    "2409.07730": {
      "paper_id": "2409.07730v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07730v1",
      "paper_key": "2409.07730",
      "paper_title": "Music auto-tagging in the long tail: A few-shot approach",
      "paper_url": "http://arxiv.org/abs/2409.07730v1",
      "paper_abstract": "In the realm of digital music, using tags to efficiently organize and retrieve music from extensive databases is crucial for music catalog owners. Human tagging by experts is labor-intensive but mostly accurate, whereas automatic tagging through supervised learning has approached satisfying accuracy but is restricted to a predefined set of training tags. Few-shot learning offers a viable solution to expand beyond this small set of predefined tags by enabling models to learn from only a few human-provided examples to understand tag meanings and subsequently apply these tags autonomously. We propose to integrate few-shot learning methodology into multi-label music auto-tagging by using features from pre-trained models as inputs to a lightweight linear classifier, also known as a linear probe. We investigate different popular pre-trained features, as well as different few-shot parametrizations with varying numbers of classes and samples per class. Our experiments demonstrate that a simple model with pre-trained features can achieve performance close to state-of-the-art models while using significantly less training data, such as 20 samples per tag. Additionally, our linear probe performs competitively with leading models when trained on the entire training dataset. The results show that this transfer learning-based few-shot approach could effectively address the issue of automatically assigning long-tail tags with only limited labeled data.",
      "paper_authors": [
        "T. Aleksandra Ma",
        "Alexander Lerch"
      ],
      "primary_category": "eess.AS",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "Published in Audio Engineering Society NY Show 2024 as a Peer\n  Reviewed (Category 1) paper",
      "repo_url": "#"
    },
    "2409.07715": {
      "paper_id": "2409.07715v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07715v1",
      "paper_key": "2409.07715",
      "paper_title": "FIReStereo: Forest InfraRed Stereo Dataset for UAS Depth Perception in Visually Degraded Environments",
      "paper_url": "http://arxiv.org/abs/2409.07715v1",
      "paper_abstract": "Robust depth perception in visually-degraded environments is crucial for autonomous aerial systems. Thermal imaging cameras, which capture infrared radiation, are robust to visual degradation. However, due to lack of a large-scale dataset, the use of thermal cameras for unmanned aerial system (UAS) depth perception has remained largely unexplored. This paper presents a stereo thermal depth perception dataset for autonomous aerial perception applications. The dataset consists of stereo thermal images, LiDAR, IMU and ground truth depth maps captured in urban and forest settings under diverse conditions like day, night, rain, and smoke. We benchmark representative stereo depth estimation algorithms, offering insights into their performance in degraded conditions. Models trained on our dataset generalize well to unseen smoky conditions, highlighting the robustness of stereo thermal imaging for depth perception. We aim for this work to enhance robotic perception in disaster scenarios, allowing for exploration and operations in previously unreachable areas. The dataset and source code are available at https://firestereo.github.io.",
      "paper_authors": [
        "Devansh Dhrafani",
        "Yifei Liu",
        "Andrew Jong",
        "Ukcheol Shin",
        "Yao He",
        "Tyler Harp",
        "Yaoyu Hu",
        "Jean Oh",
        "Sebastian Scherer"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "Under review in RA-L. The first 2 authors contributed equally",
      "repo_url": "#"
    },
    "2409.07696": {
      "paper_id": "2409.07696v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07696v1",
      "paper_key": "2409.07696",
      "paper_title": "Visualization in Motion in Video Games for Different Types of Data",
      "paper_url": "http://arxiv.org/abs/2409.07696v1",
      "paper_abstract": "We contribute an analysis of situated visualizations in motion in video games for different types of data, with a focus on quantitative and categorical data representations. Video games convey a lot of data to players, to help them succeed in the game. These visualizations frequently move across the screen due to camera changes or because the game elements themselves move. Our ultimate goal is to understand how motion factors affect visualization readability in video games and subsequently the players' performance in the game. We started our work by surveying the characteristics of how motion currently influences which kind of data representations in video games. We conducted a systematic review of 160 visualizations in motion in video games and extracted patterns and considerations regarding was what, and how visualizations currently exhibit motion factors in video games.",
      "paper_authors": [
        "Federica Bucchieri",
        "Lijie Yao",
        "Petra Isenberg"
      ],
      "primary_category": "cs.HC",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": null,
      "repo_url": "#"
    },
    "2409.07695": {
      "paper_id": "2409.07695v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07695v1",
      "paper_key": "2409.07695",
      "paper_title": "Situated Visualization in Motion for Swimming",
      "paper_url": "http://arxiv.org/abs/2409.07695v1",
      "paper_abstract": "Competitive sports coverage increasingly includes information on athlete or team statistics and records. Sports video coverage has traditionally embedded representations of this data in fixed locations on the screen, but more recently also attached representations to athletes or other targets in motion. These publicly used representations so far have been rather simple and systematic investigations of the research space of embedded visualizations in motion are still missing. Here we report on our preliminary research in the domain of professional and amateur swimming. We analyzed how visualizations are currently added to the coverage of Olympics swimming competitions and then plan to derive a design space for embedded data representations for swimming competitions. We are currently conducting a crowdsourced survey to explore which kind of swimming-related data general audiences are interested in, in order to identify opportunities for additional visualizations to be added to swimming competition coverage.",
      "paper_authors": [
        "Lijie Yao",
        "Anastasia Bezerianos",
        "Romain Vuillemot",
        "Petra Isenberg"
      ],
      "primary_category": "cs.HC",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": null,
      "repo_url": "#"
    },
    "2409.07689": {
      "paper_id": "2409.07689v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07689v1",
      "paper_key": "2409.07689",
      "paper_title": "Entropy Contractions in Markov Chains: Half-Step, Full-Step and Continuous-Time",
      "paper_url": "http://arxiv.org/abs/2409.07689v1",
      "paper_abstract": "This paper considers the speed of convergence (mixing) of a finite Markov kernel $P$ with respect to the Kullback-Leibler divergence (entropy). Given a Markov kernel one defines either a discrete-time Markov chain (with the $n$-step transition kernel given by the matrix power $P^n$) or a continuous-time Markov process (with the time-$t$ transition kernel given by $e^{t(P-\\mathrm{Id})}$). The contraction of entropy for $n=1$ or $t=0+$ are characterized by the famous functional inequalities, the strong data processing inequality (SDPI) and the modified log-Sobolev inequality (MLSI), respectively. When $P=KK^*$ is written as the product of a kernel and its adjoint, one could also consider the ``half-step'' contraction, which is the SDPI for $K$, while the ``full-step'' contraction refers to the SDPI for $P$. The work [DMLM03] claimed that these contraction coefficients (half-step, full-step, and continuous-time) are generally within a constant factor of each other. We disprove this and related conjectures by working out a number of different counterexamples. In particular, we construct (a) a continuous-time Markov process that contracts arbitrarily faster than its discrete-time counterpart; and (b) a kernel $P$ such that $P^{m+1}$ contracts arbitrarily better than $P^m$. Hence, our main conclusion is that the four standard inequalities comparing five common notions of entropy and variance contraction are generally not improvable.   In the process of analyzing the counterexamples, we survey and sharpen the tools for bounding the contraction coefficients and characterize properties of extremizers of the respective functional inequalities. As our examples range from Bernoulli-Laplace model, random walks on graphs, to birth-death chains, the paper is also intended as a tutorial on computing MLSI, SDPI and other constants for these types of commonly occurring Markov chains.",
      "paper_authors": [
        "Pietro Caputo",
        "Zongchen Chen",
        "Yuzhou Gu",
        "Yury Polyanskiy"
      ],
      "primary_category": "math.PR",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": null,
      "repo_url": "#"
    },
    "2409.07670": {
      "paper_id": "2409.07670v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07670v1",
      "paper_key": "2409.07670",
      "paper_title": "A Deep Dive Into How Open-Source Project Maintainers Review and Resolve Bug Bounty Reports",
      "paper_url": "http://arxiv.org/abs/2409.07670v1",
      "paper_abstract": "Researchers have investigated the bug bounty ecosystem from the lens of platforms, programs, and bug hunters. Understanding the perspectives of bug bounty report reviewers, especially those who historically lack a security background and little to no funding for bug hunters, is currently understudied. In this paper, we primarily investigate the perspective of open-source software (OSS) maintainers who have used \\texttt{huntr}, a bug bounty platform that pays bounties to bug hunters who find security bugs in GitHub projects and have had valid vulnerabilities patched as a result. We address this area by conducting three studies: identifying characteristics through a listing survey ($n_1=51$), their ranked importance with Likert-scale survey data ($n_2=90$), and conducting semi-structured interviews to dive deeper into real-world experiences ($n_3=17$). As a result, we categorize 40 identified characteristics into benefits, challenges, helpful features, and wanted features. We find that private disclosure and project visibility are the most important benefits, while hunters focused on money or CVEs and pressure to review are the most challenging to overcome. Surprisingly, lack of communication with bug hunters is the least challenging, and CVE creation support is the second-least helpful feature for OSS maintainers when reviewing bug bounty reports. We present recommendations to make the bug bounty review process more accommodating to open-source maintainers and identify areas for future work.",
      "paper_authors": [
        "Jessy Ayala",
        "Steven Ngo",
        "Joshua Garcia"
      ],
      "primary_category": "cs.SE",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": null,
      "repo_url": "#"
    },
    "2409.07669": {
      "paper_id": "2409.07669v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07669v1",
      "paper_key": "2409.07669",
      "paper_title": "A Mixed-Methods Study of Open-Source Software Maintainers On Vulnerability Management and Platform Security Features",
      "paper_url": "http://arxiv.org/abs/2409.07669v1",
      "paper_abstract": "In open-source software (OSS), software vulnerabilities have significantly increased. Although researchers have investigated the perspectives of vulnerability reporters and OSS contributor security practices, understanding the perspectives of OSS maintainers on vulnerability management and platform security features is currently understudied. In this paper, we investigate the perspectives of OSS maintainers who maintain projects listed in the GitHub Advisory Database. We explore this area by conducting two studies: identifying aspects through a listing survey ($n_1=80$) and gathering insights from semi-structured interviews ($n_2=22$). Of the 37 identified aspects, we find that supply chain mistrust and lack of automation for vulnerability management are the most challenging, and barriers to adopting platform security features include a lack of awareness and the perception that they are not necessary. Surprisingly, we find that despite being previously vulnerable, some maintainers still allow public vulnerability reporting, or ignore reports altogether. Based on our findings, we discuss implications for OSS platforms and how the research community can better support OSS vulnerability management efforts.",
      "paper_authors": [
        "Jessy Ayala",
        "Yu-Jye Tung",
        "Joshua Garcia"
      ],
      "primary_category": "cs.SE",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": null,
      "repo_url": "#"
    },
    "2409.07645": {
      "paper_id": "2409.07645v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07645v1",
      "paper_key": "2409.07645",
      "paper_title": "Feature Importance in Pedestrian Intention Prediction: A Context-Aware Review",
      "paper_url": "http://arxiv.org/abs/2409.07645v1",
      "paper_abstract": "Recent advancements in predicting pedestrian crossing intentions for Autonomous Vehicles using Computer Vision and Deep Neural Networks are promising. However, the black-box nature of DNNs poses challenges in understanding how the model works and how input features contribute to final predictions. This lack of interpretability delimits the trust in model performance and hinders informed decisions on feature selection, representation, and model optimisation; thereby affecting the efficacy of future research in the field. To address this, we introduce Context-aware Permutation Feature Importance (CAPFI), a novel approach tailored for pedestrian intention prediction. CAPFI enables more interpretability and reliable assessments of feature importance by leveraging subdivided scenario contexts, mitigating the randomness of feature values through targeted shuffling. This aims to reduce variance and prevent biased estimations in importance scores during permutations. We divide the Pedestrian Intention Estimation (PIE) dataset into 16 comparable context sets, measure the baseline performance of five distinct neural network architectures for intention prediction in each context, and assess input feature importance using CAPFI. We observed nuanced differences among models across various contextual characteristics. The research reveals the critical role of pedestrian bounding boxes and ego-vehicle speed in predicting pedestrian intentions, and potential prediction biases due to the speed feature through cross-context permutation evaluation. We propose an alternative feature representation by considering proximity change rate for rendering dynamic pedestrian-vehicle locomotion, thereby enhancing the contributions of input features to intention prediction. These findings underscore the importance of contextual features and their diversity to develop accurate and robust intent-predictive models.",
      "paper_authors": [
        "Mohsen Azarmi",
        "Mahdi Rezaei",
        "He Wang",
        "Ali Arabian"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": null,
      "repo_url": "#"
    },
    "2409.07641": {
      "paper_id": "2409.07641v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07641v1",
      "paper_key": "2409.07641",
      "paper_title": "SimulBench: Evaluating Language Models with Creative Simulation Tasks",
      "paper_url": "http://arxiv.org/abs/2409.07641v1",
      "paper_abstract": "We introduce SimulBench, a benchmark designed to evaluate large language models (LLMs) across a diverse collection of creative simulation scenarios, such as acting as a Linux terminal or playing text games with users. While these simulation tasks serve as effective measures of an LLM's general intelligence, they are seldom incorporated into existing benchmarks. A major challenge is to develop an evaluation framework for testing different LLMs fairly while preserving the multi-round interactive nature of simulation tasks between users and AI. To tackle this issue, we suggest using a fixed LLM as a user agent to engage with an LLM to collect dialogues first under different tasks. Then, challenging dialogue scripts are extracted for evaluating different target LLMs. To facilitate automatic assessment on \\DataName{}, GPT-4 is employed as the evaluator, tasked with reviewing the quality of the final response generated by the target LLMs given multi-turn dialogue scripts. Our comprehensive experiments indicate that these simulation tasks continue to pose a significant challenge with their unique natures and show the gap between proprietary models and the most advanced open LLMs. For example, GPT-4-turbo outperforms LLaMA-3-70b-Chat on 18.55\\% more cases.",
      "paper_authors": [
        "Qi Jia",
        "Xiang Yue",
        "Tianyu Zheng",
        "Jie Huang",
        "Bill Yuchen Lin"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "Website: https://simulbench.github.io/",
      "repo_url": "#"
    },
    "2409.07627": {
      "paper_id": "2409.07627v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07627v1",
      "paper_key": "2409.07627",
      "paper_title": "Leveraging User-Generated Reviews for Recommender Systems with Dynamic Headers",
      "paper_url": "http://arxiv.org/abs/2409.07627v1",
      "paper_abstract": "E-commerce platforms have a vast catalog of items to cater to their customers' shopping interests. Most of these platforms assist their customers in the shopping process by offering optimized recommendation carousels, designed to help customers quickly locate their desired items. Many models have been proposed in academic literature to generate and enhance the ranking and recall set of items in these carousels. Conventionally, the accompanying carousel title text (header) of these carousels remains static. In most instances, a generic text such as \"Items similar to your current viewing\" is utilized. Fixed variations such as the inclusion of specific attributes \"Other items from a similar seller\" or \"Items from a similar brand\" in addition to \"frequently bought together\" or \"considered together\" are observed as well. This work proposes a novel approach to customize the header generation process of these carousels. Our work leverages user-generated reviews that lay focus on specific attributes (aspects) of an item that were favorably perceived by users during their interaction with the given item. We extract these aspects from reviews and train a graph neural network-based model under the framework of a conditional ranking task. We refer to our innovative methodology as Dynamic Text Snippets (DTS) which generates multiple header texts for an anchor item and its recall set. Our approach demonstrates the potential of utilizing user-generated reviews and presents a unique paradigm for exploring increasingly context-aware recommendation systems.",
      "paper_authors": [
        "Shanu Vashishtha",
        "Abhay Kumar",
        "Lalitesh Morishetti",
        "Kaushiki Nag",
        "Kannan Achan"
      ],
      "primary_category": "cs.IR",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "7 pages, 3 figures, PAIS 2024 (ECAI)",
      "repo_url": "#"
    },
    "2409.07626": {
      "paper_id": "2409.07626v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07626v1",
      "paper_key": "2409.07626",
      "paper_title": "Generalization Error Bound for Quantum Machine Learning in NISQ Era -- A Survey",
      "paper_url": "http://arxiv.org/abs/2409.07626v1",
      "paper_abstract": "Despite the mounting anticipation for the quantum revolution, the success of Quantum Machine Learning (QML) in the Noisy Intermediate-Scale Quantum (NISQ) era hinges on a largely unexplored factor: the generalization error bound, a cornerstone of robust and reliable machine learning models. Current QML research, while exploring novel algorithms and applications extensively, is predominantly situated in the context of noise-free, ideal quantum computers. However, Quantum Circuit (QC) operations in NISQ-era devices are susceptible to various noise sources and errors. In this article, we conduct a Systematic Mapping Study (SMS) to explore the state-of-the-art generalization bound for supervised QML in NISQ-era and analyze the latest practices in the field. Our study systematically summarizes the existing computational platforms with quantum hardware, datasets, optimization techniques, and the common properties of the bounds found in the literature. We further present the performance accuracy of various approaches in classical benchmark datasets like the MNIST and IRIS datasets. The SMS also highlights the limitations and challenges in QML in the NISQ era and discusses future research directions to advance the field. Using a detailed Boolean operators query in five reliable indexers, we collected 544 papers and filtered them to a small set of 37 relevant articles. This filtration was done following the best practice of SMS with well-defined research questions and inclusion and exclusion criteria.",
      "paper_authors": [
        "Bikram Khanal",
        "Pablo Rivas",
        "Arun Sanjel",
        "Korn Sooksatra",
        "Ernesto Quevedo",
        "Alejandro Rodriguez"
      ],
      "primary_category": "quant-ph",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": null,
      "repo_url": "#"
    },
    "2409.07622": {
      "paper_id": "2409.07622v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07622v1",
      "paper_key": "2409.07622",
      "paper_title": "The Impact of Shear on Disk Galaxy Star Formation Rates",
      "paper_url": "http://arxiv.org/abs/2409.07622v1",
      "paper_abstract": "Determining the physical processes that control galactic-scale star formation rates is essential for an improved understanding of galaxy evolution. The role of orbital shear is currently unclear, with some models expecting reduced star formation rates (SFRs) and efficiencies (SFEs) with increasing shear, e.g., if shear stabilizes gas against gravitational collapse, while others predicting enhanced rates, e.g., if shear-driven collisions between giant molecular clouds (GMCs) trigger star formation. Expanding on the analysis of 16 galaxies by Suwannajak, Tan, & Leroy (2014), we assess the shear dependence of SFE per orbital time ($\\epsilon_\\mathrm{orb}$) in 49 galaxies selected from the PHANGS-ALMA survey. In particular, we test a prediction of the shear-driven GMC collision model that $\\epsilon_\\mathrm{orb}\\propto(1-0.7\\beta)$, where $\\beta\\equiv{d}\\:\\mathrm{ln}\\:v_\\mathrm{circ}/d\\:\\mathrm{ln}\\:r$, i.e., SFE per orbital time declines with decreasing shear. We fit the function $\\epsilon_\\mathrm{orb}=\\epsilon_\\mathrm{orb,\\,0}(1-\\alpha_\\mathrm{CC}\\beta)$ finding $\\alpha_\\mathrm{CC}\\simeq0.76\\pm0.16$; an alternative fit with $\\epsilon_\\mathrm{orb}$ normalized by the median value in each galaxy yields $\\alpha_\\mathrm{CC}^*=0.80\\pm0.15$. These results are in good agreement with the prediction of the shear-driven GMC collision theory. We also examine the impact of a galactic bar on $\\epsilon_\\mathrm{orb}$ finding a modest decrease in SFE in the presence of bar, which can be attributed to lower rates of shear in these regions. We discuss the implications of our results for the GMC life cycle and environmental dependence of star formation activity.",
      "paper_authors": [
        "Xena L. Fortune-Bashee",
        "Jiayi Sun",
        "Jonathan C. Tan"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "Submitted to ApJL, 9 pages, 4 figures, Comments welcome",
      "repo_url": "#"
    },
    "2409.07595": {
      "paper_id": "2409.07595v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07595v1",
      "paper_key": "2409.07595",
      "paper_title": "Low temperature ferroelectric state in strontium titanate microcrystals using in situ multi-reflection Bragg coherent X-ray diffraction imaging",
      "paper_url": "http://arxiv.org/abs/2409.07595v1",
      "paper_abstract": "Strontium titanate is a classic quantum paraelectric oxide material that has been widely studied in bulk and thin films. It exhibits a well-known cubic-to-tetragonal antiferrodistortive phase transition at 105 K, characterized by the rotation of oxygen octahedra. A possible second phase transition at lower temperature is suppressed by quantum fluctuations, preventing the onset of ferroelectric order. However, recent studies have shown that ferroelectric order can be established at low temperatures by inducing strain and other means. Here, we used in situ multi-reflection Bragg coherent X-ray diffraction imaging to measure the strain and rotation tensors for two strontium titanate microcrystals at low temperature. We observe strains induced by dislocations and inclusion-like impurities in the microcrystals. Based on radial magnitude plots, these strains increase in magnitude and spread as the temperature decreases. Pearson's correlation heatmaps show a structural transition at 50 K, which we associate with the formation of a low-temperature ferroelectric phase in the presence of strain. We do not observe any change in local strains associated with the tetragonal phase transition at 105 K.",
      "paper_authors": [
        "David Yang",
        "Sung Soo Ha",
        "Sungwook Choi",
        "Jialun Liu",
        "Daniel Treuherz",
        "Nan Zhang",
        "Zheyi An",
        "Hieu Minh Ngo",
        "Muhammad Mahmood Nawaz",
        "Ana F. Suzana",
        "Longlong Wu",
        "Gareth Nisbet",
        "Daniel G. Porter",
        "Hyunjung Kim",
        "Ian K. Robinson"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "15 pages, 19 figures. Zenodo link will be active once published in a\n  peer-reviewed journal",
      "repo_url": "#"
    },
    "2409.07587": {
      "paper_id": "2409.07587v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07587v1",
      "paper_key": "2409.07587",
      "paper_title": "Exploring LLMs for Malware Detection: Review, Framework Design, and Countermeasure Approaches",
      "paper_url": "http://arxiv.org/abs/2409.07587v1",
      "paper_abstract": "The rising use of Large Language Models (LLMs) to create and disseminate malware poses a significant cybersecurity challenge due to their ability to generate and distribute attacks with ease. A single prompt can initiate a wide array of malicious activities. This paper addresses this critical issue through a multifaceted approach. First, we provide a comprehensive overview of LLMs and their role in malware detection from diverse sources. We examine five specific applications of LLMs: Malware honeypots, identification of text-based threats, code analysis for detecting malicious intent, trend analysis of malware, and detection of non-standard disguised malware. Our review includes a detailed analysis of the existing literature and establishes guiding principles for the secure use of LLMs. We also introduce a classification scheme to categorize the relevant literature. Second, we propose performance metrics to assess the effectiveness of LLMs in these contexts. Third, we present a risk mitigation framework designed to prevent malware by leveraging LLMs. Finally, we evaluate the performance of our proposed risk mitigation strategies against various factors and demonstrate their effectiveness in countering LLM-enabled malware. The paper concludes by suggesting future advancements and areas requiring deeper exploration in this fascinating field of artificial intelligence.",
      "paper_authors": [
        "Jamal Al-Karaki",
        "Muhammad Al-Zafar Khan",
        "Marwan Omar"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "26 pages, 7 figures, 4 tables",
      "repo_url": "#"
    },
    "2409.07569": {
      "paper_id": "2409.07569v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07569v1",
      "paper_key": "2409.07569",
      "paper_title": "A Survey of Inverse Constrained Reinforcement Learning: Definitions, Progress and Challenges",
      "paper_url": "http://arxiv.org/abs/2409.07569v1",
      "paper_abstract": "Inverse Constrained Reinforcement Learning (ICRL) is the task of inferring the implicit constraints followed by expert agents from their demonstration data. As an emerging research topic, ICRL has received considerable attention in recent years. This article presents a categorical survey of the latest advances in ICRL. It serves as a comprehensive reference for machine learning researchers and practitioners, as well as starters seeking to comprehend the definitions, advancements, and important challenges in ICRL. We begin by formally defining the problem and outlining the algorithmic framework that facilitates constraint inference across various scenarios. These include deterministic or stochastic environments, environments with limited demonstrations, and multiple agents. For each context, we illustrate the critical challenges and introduce a series of fundamental methods to tackle these issues. This survey encompasses discrete, virtual, and realistic environments for evaluating ICRL agents. We also delve into the most pertinent applications of ICRL, such as autonomous driving, robot control, and sports analytics. To stimulate continuing research, we conclude the survey with a discussion of key unresolved questions in ICRL that can effectively foster a bridge between theoretical understanding and practical industrial applications.",
      "paper_authors": [
        "Guiliang Liu",
        "Sheng Xu",
        "Shicheng Liu",
        "Ashish Gaurav",
        "Sriram Ganapathi Subramanian",
        "Pascal Poupart"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "28 pages",
      "repo_url": "#"
    },
    "2409.07528": {
      "paper_id": "2409.07528v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07528v1",
      "paper_key": "2409.07528",
      "paper_title": "Euclid preparation. Deep learning true galaxy morphologies for weak lensing shear bias calibration",
      "paper_url": "http://arxiv.org/abs/2409.07528v1",
      "paper_abstract": "To date, galaxy image simulations for weak lensing surveys usually approximate the light profiles of all galaxies as a single or double S\\'ersic profile, neglecting the influence of galaxy substructures and morphologies deviating from such a simplified parametric characterization. While this approximation may be sufficient for previous data sets, the stringent cosmic shear calibration requirements and the high quality of the data in the upcoming Euclid survey demand a consideration of the effects that realistic galaxy substructures have on shear measurement biases. Here we present a novel deep learning-based method to create such simulated galaxies directly from HST data. We first build and validate a convolutional neural network based on the wavelet scattering transform to learn noise-free representations independent of the point-spread function of HST galaxy images that can be injected into simulations of images from Euclid's optical instrument VIS without introducing noise correlations during PSF convolution or shearing. Then, we demonstrate the generation of new galaxy images by sampling from the model randomly and conditionally. Next, we quantify the cosmic shear bias from complex galaxy shapes in Euclid-like simulations by comparing the shear measurement biases between a sample of model objects and their best-fit double-S\\'ersic counterparts. Using the KSB shape measurement algorithm, we find a multiplicative bias difference between these branches with realistic morphologies and parametric profiles on the order of $6.9\\times 10^{-3}$ for a realistic magnitude-S\\'ersic index distribution. Moreover, we find clear detection bias differences between full image scenes simulated with parametric and realistic galaxies, leading to a bias difference of $4.0\\times 10^{-3}$ independent of the shape measurement method. This makes it relevant for stage IV weak lensing surveys such as Euclid.",
      "paper_authors": [
        "Euclid Collaboration",
        "B. Csizi",
        "T. Schrabback",
        "S. Grandis",
        "H. Hoekstra",
        "H. Jansen",
        "L. Linke",
        "G. Congedo",
        "A. N. Taylor",
        "A. Amara",
        "S. Andreon",
        "C. Baccigalupi",
        "M. Baldi",
        "S. Bardelli",
        "P. Battaglia",
        "R. Bender",
        "C. Bodendorf",
        "D. Bonino",
        "E. Branchini",
        "M. Brescia",
        "J. Brinchmann",
        "S. Camera",
        "V. Capobianco",
        "C. Carbone",
        "J. Carretero",
        "S. Casas",
        "F. J. Castander",
        "M. Castellano",
        "G. Castignani",
        "S. Cavuoti",
        "A. Cimatti",
        "C. Colodro-Conde",
        "C. J. Conselice",
        "L. Conversi",
        "Y. Copin",
        "F. Courbin",
        "H. M. Courtois",
        "M. Cropper",
        "A. Da Silva",
        "H. Degaudenzi",
        "G. De Lucia",
        "J. Dinis",
        "M. Douspis",
        "F. Dubath",
        "X. Dupac",
        "S. Dusini",
        "M. Farina",
        "S. Farrens",
        "F. Faustini",
        "S. Ferriol",
        "S. Fotopoulou",
        "M. Frailis",
        "E. Franceschi",
        "S. Galeotta",
        "B. Gillis",
        "C. Giocoli",
        "A. Grazian",
        "F. Grupp",
        "L. Guzzo",
        "S. V. H. Haugan",
        "W. Holmes",
        "I. Hook",
        "F. Hormuth",
        "A. Hornstrup",
        "P. Hudelot",
        "S. Ili\u0107",
        "K. Jahnke",
        "M. Jhabvala",
        "B. Joachimi",
        "E. Keih\u00e4nen",
        "S. Kermiche",
        "A. Kiessling",
        "M. Kilbinger",
        "B. Kubik",
        "K. Kuijken",
        "M. K\u00fcmmel",
        "M. Kunz",
        "H. Kurki-Suonio",
        "S. Ligori",
        "P. B. Lilje",
        "V. Lindholm",
        "I. Lloro",
        "D. Maino",
        "E. Maiorano",
        "O. Mansutti",
        "S. Marcin",
        "O. Marggraf",
        "K. Markovic",
        "M. Martinelli",
        "N. Martinet",
        "F. Marulli",
        "R. Massey",
        "E. Medinaceli",
        "S. Mei",
        "M. Melchior",
        "Y. Mellier",
        "M. Meneghetti",
        "G. Meylan",
        "M. Moresco",
        "L. Moscardini",
        "S. -M. Niemi",
        "C. Padilla",
        "S. Paltani",
        "F. Pasian",
        "K. Pedersen",
        "V. Pettorino",
        "S. Pires",
        "G. Polenta",
        "M. Poncet",
        "L. A. Popa",
        "F. Raison",
        "A. Renzi",
        "J. Rhodes",
        "G. Riccio",
        "E. Romelli",
        "M. Roncarelli",
        "E. Rossetti",
        "R. Saglia",
        "Z. Sakr",
        "A. G. S\u00e1nchez",
        "B. Sartoris",
        "P. Schneider",
        "A. Secroun",
        "G. Seidel",
        "S. Serrano",
        "C. Sirignano",
        "G. Sirri",
        "L. Stanco",
        "J. Steinwagner",
        "P. Tallada-Cresp\u00ed",
        "D. Tavagnacco",
        "H. I. Teplitz",
        "I. Tereno",
        "R. Toledo-Moreo",
        "F. Torradeflot",
        "I. Tutusaus",
        "E. A. Valentijn",
        "L. Valenziano",
        "T. Vassallo",
        "G. Verdoes Kleijn",
        "A. Veropalumbo",
        "Y. Wang",
        "J. Weller",
        "G. Zamorani",
        "E. Zucca",
        "A. Biviano",
        "M. Bolzonella",
        "E. Bozzo",
        "C. Burigana",
        "M. Calabrese",
        "D. Di Ferdinando",
        "J. A. Escartin Vigo",
        "R. Farinelli",
        "J. Gracia-Carpio",
        "S. Matthew",
        "N. Mauri",
        "A. Pezzotta",
        "M. P\u00f6ntinen",
        "V. Scottez",
        "M. Tenti",
        "M. Viel",
        "M. Wiesmann",
        "Y. Akrami",
        "V. Allevato",
        "S. Anselmi",
        "M. Archidiacono",
        "F. Atrio-Barandela",
        "M. Ballardini",
        "A. Blanchard",
        "L. Blot",
        "S. Borgani",
        "S. Bruton",
        "R. Cabanac",
        "A. Calabro",
        "G. Ca\u00f1as-Herrera",
        "A. Cappi",
        "F. Caro",
        "C. S. Carvalho",
        "T. Castro",
        "K. C. Chambers",
        "S. Contarini",
        "A. R. Cooray",
        "G. Desprez",
        "A. D\u00edaz-S\u00e1nchez",
        "J. J. Diaz",
        "S. Di Domizio",
        "H. Dole",
        "S. Escoffier",
        "A. G. Ferrari",
        "P. G. Ferreira",
        "I. Ferrero",
        "A. Finoguenov",
        "A. Fontana",
        "F. Fornari",
        "L. Gabarra",
        "K. Ganga",
        "J. Garc\u00eda-Bellido",
        "T. Gasparetto",
        "E. Gaztanaga",
        "F. Giacomini",
        "F. Gianotti",
        "G. Gozaliasl",
        "C. M. Gutierrez",
        "A. Hall",
        "H. Hildebrandt",
        "J. Hjorth",
        "A. Jimenez Mu\u00f1oz",
        "S. Joudaki",
        "J. J. E. Kajava",
        "V. Kansal",
        "D. Karagiannis",
        "C. C. Kirkpatrick",
        "A. M. C. Le Brun",
        "J. Le Graet",
        "L. Legrand",
        "J. Lesgourgues",
        "T. I. Liaudat",
        "A. Loureiro",
        "J. Macias-Perez",
        "G. Maggio",
        "M. Magliocchetti",
        "C. Mancini",
        "F. Mannucci",
        "R. Maoli",
        "J. Mart\u00edn-Fleitas",
        "C. J. A. P. Martins",
        "L. Maurin",
        "R. B. Metcalf",
        "M. Miluzio",
        "P. Monaco",
        "A. Montoro",
        "A. Mora",
        "C. Moretti",
        "G. Morgante",
        "Nicholas A. Walton",
        "L. Pagano",
        "L. Patrizii",
        "V. Popa",
        "D. Potter",
        "I. Risso",
        "P. -F. Rocci",
        "M. Sahl\u00e9n",
        "E. Sarpa",
        "A. Schneider",
        "M. Sereno",
        "P. Simon",
        "A. Spurio Mancini",
        "J. Stadel",
        "K. Tanidis",
        "C. Tao",
        "N. Tessore",
        "G. Testera",
        "R. Teyssier",
        "S. Toft",
        "S. Tosi",
        "A. Troja",
        "M. Tucci",
        "C. Valieri",
        "J. Valiviita",
        "D. Vergani",
        "G. Verza",
        "P. Vielzeuf"
      ],
      "primary_category": "astro-ph.CO",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "Submitted to A&A. 29 pages, 20 figures, 2 tables",
      "repo_url": "#"
    },
    "2409.07523": {
      "paper_id": "2409.07523v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07523v1",
      "paper_key": "2409.07523",
      "paper_title": "Using Neural Network Models to Estimate Stellar Ages from Lithium Equivalent Widths: An EAGLES Expansion",
      "paper_url": "http://arxiv.org/abs/2409.07523v1",
      "paper_abstract": "We present an Artificial Neural Network (ANN) model of photospheric lithium depletion in cool stars (3000 < Teff / K < 6500), producing estimates and probability distributions of age from Li I 6708A equivalent width (LiEW) and effective temperature data inputs. The model is trained on the same sample of 6200 stars from 52 open clusters, observed in the Gaia-ESO spectroscopic survey, and used to calibrate the previously published analytical EAGLES model, with ages 2 - 6000 Myr and -0.3 < [Fe/H] < 0.2. The additional flexibility of the ANN provides some improvements, including better modelling of the \"lithium dip\" at ages < 50 Myr and Teff ~ 3500K, and of the intrinsic dispersion in LiEW at all ages. Poor age discrimination is still an issue at ages > 1 Gyr, confirming that additional modelling flexibility is not sufficient to fully represent the LiEW - age - Teff relationship, and suggesting the involvement of further astrophysical parameters. Expansion to include such parameters - rotation, accretion, and surface gravity - is discussed, and the use of an ANN means these can be more easily included in future iterations, alongside more flexible functional forms for the LiEW dispersion. Our methods and ANN model are provided in an updated version 2.0 of the EAGLES software.",
      "paper_authors": [
        "George Weaver",
        "Robin D. Jeffries",
        "Richard J. Jackson"
      ],
      "primary_category": "astro-ph.SR",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "Accepted for publication in Monthly Notices of the Royal Astronomical\n  Society. Code available at https://github.com/robdjeff/eagles. Electronic\n  tables are available from the author",
      "repo_url": "https://github.com/robdjeff/eagles"
    },
    "2409.07524": {
      "paper_id": "2409.07524v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07524v1",
      "paper_key": "2409.07524",
      "paper_title": "Grand Unification at the Cosmological Collider with Chemical Potential",
      "paper_url": "http://arxiv.org/abs/2409.07524v1",
      "paper_abstract": "We introduce a tree-level chemical potential mechanism for spin-1 particles within cosmological collider physics, allowing them to be detected in primordial non-Gaussianities for masses above the inflationary Hubble scale. We apply this mechanism to orbifold grand unification and the massive unification partners of the standard model gauge bosons. Our mechanism requires at least a pair of massive vector fields which are singlets of the standard model, a condition which is satisfied in the classic \"trinification\" scenario. Assuming that the gauge hierarchy problem is solved by supersymmetry, gauge coupling running points to unification partners at ~ $10^{15}$ GeV. We show that, within high-scale inflation, chemical potential enhancement can lead to observably strong signals for trinification partners in future cosmological surveys.",
      "paper_authors": [
        "Arushi Bodas",
        "Edward Broadberry",
        "Raman Sundrum"
      ],
      "primary_category": "hep-ph",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "33 pages, 7 figures",
      "repo_url": "#"
    },
    "2409.07525": {
      "paper_id": "2409.07525v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07525v1",
      "paper_key": "2409.07525",
      "paper_title": "A survey of extremely metal-poor gas at cosmic noon: evidence of elevated [O/Fe]",
      "paper_url": "http://arxiv.org/abs/2409.07525v1",
      "paper_abstract": "We aim to study the high-precision chemical abundances of metal-poor gas clouds at cosmic noon (2<z<4) and investigate the associated enrichment histories. We analyse the abundances of four newly discovered metal-poor gas clouds utilising observations conducted with Keck/HIRES and VLT/UVES. These systems are classified as very metal-poor (VMP), with [Fe/H]<-2.57, and one system qualifies as an extremely metal-poor (EMP) Damped Lyman-alpha (DLA) system with [Fe/H]=-3.13+/-0.06. In combination with new high-resolution data of two previously known EMP DLAs and 2 systems reported in the literature, we conduct a comprehensive analysis of eight of the most metal-poor gas clouds currently known. We focus on high-precision abundance measurements using the elements: C, N, O, Al, Si, and Fe. Our findings indicate increasing evidence of elevated [O/Fe] abundances when [Fe/H]<-3. EMP DLAs are well-modelled with a mean value of [O/Fe]=+0.50 +/- 0.04 and an intrinsic scatter of $\\sigma_{int,[O/Fe]}=0.13^{+0.06}_{-0.04}$. While VMP DLAs are well-modelled with [O/Fe]=+0.40 +/- 0.02 and $\\sigma_{int,[O/Fe]}$=0.06 +/- 0.02. We further find tentative evidence of a redshift evolution of [C/O] across these most metal-poor DLAs with lower redshift systems showing elevated [C/O] ratios. Using the measured abundances, combined with a stochastic chemical enrichment model, we investigate the properties of the stellar population responsible for enriching EMP gas at cosmic noon. We find that the chemistry of these systems is best explained via the enrichment of just two massive progenitors, N_*=2+/-1, that ended their lives as core collapse SNe with a typical explosion energy E_exp=(1.6 +/- 0.6)x10$^{51}$ erg. These progenitors formed obeying a Salpeter-like power-law IMF, where all stars of mass greater than M_max=32$^{+10}_{-4}$ M_sun collapse directly to black holes and do not contribute to the metal enrichment.",
      "paper_authors": [
        "Louise Welsh",
        "Ryan Cooke",
        "Michele Fumagalli",
        "Max Pettini",
        "Gwen C. Rudie"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "26 pages, 16 figures, accepted for publication in Astronomy &\n  Astrophysics",
      "repo_url": "#"
    },
    "2409.07518": {
      "paper_id": "2409.07518v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07518v1",
      "paper_key": "2409.07518",
      "paper_title": "The PIPER Survey. II. The Globular Cluster Systems of Low Surface Brightness Galaxies in the Perseus Cluster",
      "paper_url": "http://arxiv.org/abs/2409.07518v1",
      "paper_abstract": "We present Hubble Space Telescope ACS/WFC and WFC3/UVIS imaging for a sample of 50 low surface brightness (LSB) galaxies in the $\\sim$10$^{15}$ M$_{\\odot}$ Perseus cluster, which were originally identified in ground-based imaging. We measure the structural properties of these galaxies and estimate the total number of globular clusters (GCs) they host. Around half of our sample galaxies meet the strict definition of an ultra-diffuse galaxy (UDG), while the others are UDG-like but are either somewhat more compact or slightly brighter. A small number of galaxies reveal systems with many tens of GCs, rivalling some of the richest GC systems known around UDGs in the Coma cluster. We find the sizes of rich GC systems, in terms of their half-number radii, extending to $\\sim$1.2 times the half-light radii of their host galaxy on average. The mean colours of the GC systems are the same, within the uncertainties, as those of their host galaxy stars. This suggests that GCs and galaxy field stars may have formed at the same epoch from the same enriched gas. It may also indicate a significant contribution from disrupted GCs to the stellar component of the host galaxy as might be expected in the 'failed galaxy' formation scenario for UDGs.",
      "paper_authors": [
        "Steven R. Janssens",
        "Duncan A. Forbes",
        "Aaron J. Romanowsky",
        "Jonah Gannon",
        "Joel Pfeffer",
        "Warrick J. Couch",
        "Jean P. Brodie",
        "William E. Harris",
        "Patrick R. Durrell",
        "Kenji Bekki"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "16 pages, 11 figures, 2 tables, accepted for publication in MNRAS",
      "repo_url": "#"
    },
    "2409.07517": {
      "paper_id": "2409.07517v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07517v1",
      "paper_key": "2409.07517",
      "paper_title": "A test of the conjectured critical black-hole formation -- null geodesic correspondence: The case of self-gravitating scalar fields",
      "paper_url": "http://arxiv.org/abs/2409.07517v1",
      "paper_abstract": "It has recently been conjectured [A. Ianniccari {\\it et al.}, Phys. Rev. Lett. {\\bf 133}, 081401 (2024)] that there exists a correspondence between the critical threshold of black-hole formation and the stability properties of null circular geodesics in the curved spacetime of the collapsing matter configuration. In the present compact paper we provide a non-trivial test of this intriguing conjecture. In particular, using analytical techniques we study the physical and mathematical properties of self-gravitating scalar field configurations that possess marginally-stable (degenerate) null circular geodesics. We reveal the interesting fact that the {\\it analytically} calculated critical compactness parameter ${\\cal C}^{\\text{analytical}}\\equiv{\\text{max}_r}\\{m(r)/r\\}=6/25$, which signals the appearance of the first (marginally-stable) null circular geodesic in the curved spacetime of the self-gravitating scalar fields, agrees quite well (to within $\\sim10\\%$) with the exact compactness parameter ${\\cal C}^{\\text{numerical}}\\equiv\\text{max}_t\\{\\text{max}_r\\{m(r)/r\\}\\}\\simeq0.265$ which is computed {\\it numerically} using fully non-linear numerical simulations of the gravitational collapse of scalar fields at the threshold of black-hole formation [here $m(r)$ is the gravitational mass contained within a sphere of radius $r$].",
      "paper_authors": [
        "Shahar Hod"
      ],
      "primary_category": "gr-qc",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "5 pages",
      "repo_url": "#"
    },
    "2409.07446": {
      "paper_id": "2409.07446v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07446v1",
      "paper_key": "2409.07446",
      "paper_title": "Adaptive Adapter Routing for Long-Tailed Class-Incremental Learning",
      "paper_url": "http://arxiv.org/abs/2409.07446v1",
      "paper_abstract": "In our ever-evolving world, new data exhibits a long-tailed distribution, such as e-commerce platform reviews. This necessitates continuous model learning imbalanced data without forgetting, addressing the challenge of long-tailed class-incremental learning (LTCIL). Existing methods often rely on retraining linear classifiers with former data, which is impractical in real-world settings. In this paper, we harness the potent representation capabilities of pre-trained models and introduce AdaPtive Adapter RouTing (APART) as an exemplar-free solution for LTCIL. To counteract forgetting, we train inserted adapters with frozen pre-trained weights for deeper adaptation and maintain a pool of adapters for selection during sequential model updates. Additionally, we present an auxiliary adapter pool designed for effective generalization, especially on minority classes. Adaptive instance routing across these pools captures crucial correlations, facilitating a comprehensive representation of all classes. Consequently, APART tackles the imbalance problem as well as catastrophic forgetting in a unified framework. Extensive benchmark experiments validate the effectiveness of APART. Code is available at: https://github.com/vita-qzh/APART",
      "paper_authors": [
        "Zhi-Hong Qi",
        "Da-Wei Zhou",
        "Yiran Yao",
        "Han-Jia Ye",
        "De-Chuan Zhan"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "Accepted to Machine Learning Journal. Code is available at:\n  https://github.com/vita-qzh/APART",
      "repo_url": "https://github.com/vita-qzh/apart"
    },
    "2409.07406": {
      "paper_id": "2409.07406v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07406v1",
      "paper_key": "2409.07406",
      "paper_title": "Trust Dynamics in Human-Autonomy Interaction: Uncover Associations between Trust Dynamics and Personal Characteristics",
      "paper_url": "http://arxiv.org/abs/2409.07406v1",
      "paper_abstract": "While personal characteristics influence people's snapshot trust towards autonomous systems, their relationships with trust dynamics remain poorly understood. We conducted a human-subject experiment with 130 participants performing a simulated surveillance task aided by an automated threat detector. A comprehensive pre-experimental survey collected data on participants' personal characteristics across 12 constructs and 28 dimensions. Based on data collected in the experiment, we clustered participants' trust dynamics into three types and assessed differences among the three clusters in terms of personal characteristics, behaviors, performance, and post-experiment ratings. Participants were clustered into three groups, namely Bayesian decision makers, disbelievers, and oscillators. Results showed that the clusters differ significantly in seven personal characteristics: masculinity, positive affect, extraversion, neuroticism, intellect, performance expectancy, and high expectations. The disbelievers tend to have high neuroticism and low performance expectancy. The oscillators tend to have higher scores in masculinity, positive affect, extraversion and intellect. We also found significant differences in the behaviors and post-experiment ratings among the three groups. The disbelievers are the least likely to blindly follow the recommendations made by the automated threat detector. Based on the significant personal characteristics, we developed a decision tree model to predict cluster types with an accuracy of 70%.",
      "paper_authors": [
        "Hyesun Chung",
        "X. Jessie Yang"
      ],
      "primary_category": "cs.HC",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible",
      "repo_url": "#"
    },
    "2409.07388": {
      "paper_id": "2409.07388v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07388v1",
      "paper_key": "2409.07388",
      "paper_title": "Recent Trends of Multimodal Affective Computing: A Survey from NLP Perspective",
      "paper_url": "http://arxiv.org/abs/2409.07388v1",
      "paper_abstract": "Multimodal affective computing (MAC) has garnered increasing attention due to its broad applications in analyzing human behaviors and intentions, especially in text-dominated multimodal affective computing field. This survey presents the recent trends of multimodal affective computing from NLP perspective through four hot tasks: multimodal sentiment analysis, multimodal emotion recognition in conversation, multimodal aspect-based sentiment analysis and multimodal multi-label emotion recognition. The goal of this survey is to explore the current landscape of multimodal affective research, identify development trends, and highlight the similarities and differences across various tasks, offering a comprehensive report on the recent progress in multimodal affective computing from an NLP perspective. This survey covers the formalization of tasks, provides an overview of relevant works, describes benchmark datasets, and details the evaluation metrics for each task. Additionally, it briefly discusses research in multimodal affective computing involving facial expressions, acoustic signals, physiological signals, and emotion causes. Additionally, we discuss the technical approaches, challenges, and future directions in multimodal affective computing. To support further research, we released a repository that compiles related works in multimodal affective computing, providing detailed resources and references for the community.",
      "paper_authors": [
        "Guimin Hu",
        "Yi Xin",
        "Weimin Lyu",
        "Haojian Huang",
        "Chang Sun",
        "Zhihong Zhu",
        "Lin Gui",
        "Ruichu Cai"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": null,
      "repo_url": "https://github.com/lemei/affective-computing"
    },
    "2409.07376": {
      "paper_id": "2409.07376v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07376v1",
      "paper_key": "2409.07376",
      "paper_title": "The microbiome science of composting and human excrement composting: a review",
      "paper_url": "http://arxiv.org/abs/2409.07376v1",
      "paper_abstract": "Linear waste management systems are unsustainable and contribute to environmental degradation, economic inequity, and health disparities. Among the array of environmental challenges stemming from anthropogenic impacts, the management of human excrement (human feces and urine) stands as a significant concern. Over two billion people do not have access to adequate sanitation resulting in a global public health crisis.   Composting is the microbial biotechnology aimed at cycling organic waste, including human excrement, for improved public health, agricultural productivity and safety, and environmental sustainability. Applications of modern microbiome-omics and related technologies have vast capacity to support continued advances in composting science and praxis. In this article, we review literature focused on applications of microbiome technologies to study composting systems and reactions. The studies we survey generally fall into the categories of animal manure composting, food and landscaping waste composting, biosolids composting, and human excrement composting. We review experiments utilizing microbiome technologies to investigate strategies for enhancing pathogen suppression and accelerating the biodegradation of organic matter. Additionally, we explore studies focused on the bioengineering potential of microbes as inoculants to facilitate degradation of toxins such as pharmaceuticals or per- and polyfluoroalkyl substances (PFAS). The findings from these studies underscore the importance of advancing our understanding of composting processes through the integration of emerging microbiome-omics technologies.   We conclude that work to-date has demonstrated exciting basic and applied science potential from studying compost microbiomes, with promising implications for enhancing global environmental sustainability and public health.",
      "paper_authors": [
        "Jeff Meilander",
        "J. Gregory Caporaso"
      ],
      "primary_category": "q-bio.QM",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "18 page, 1 figure, 1 table",
      "repo_url": "#"
    },
    "2409.07368": {
      "paper_id": "2409.07368v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07368v2",
      "paper_key": "2409.07368",
      "paper_title": "Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code",
      "paper_url": "http://arxiv.org/abs/2409.07368v2",
      "paper_abstract": "This paper introduces SGCode, a flexible prompt-optimizing system to generate secure code with large language models (LLMs). SGCode integrates recent prompt-optimization approaches with LLMs in a unified system accessible through front-end and back-end APIs, enabling users to 1) generate secure code, which is free of vulnerabilities, 2) review and share security analysis, and 3) easily switch from one prompt optimization approach to another, while providing insights on model and system performance. We populated SGCode on an AWS server with PromSec, an approach that optimizes prompts by combining an LLM and security tools with a lightweight generative adversarial graph neural network to detect and fix security vulnerabilities in the generated code. Extensive experiments show that SGCode is practical as a public tool to gain insights into the trade-offs between model utility, secure code generation, and system cost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is available at: http://3.131.141.63:8501/.",
      "paper_authors": [
        "Khiem Ton",
        "Nhi Nguyen",
        "Mahmoud Nazzal",
        "Abdallah Khreishah",
        "Cristian Borcea",
        "NhatHai Phan",
        "Ruoming Jin",
        "Issa Khalil",
        "Yelong Shen"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-16",
      "comments": null,
      "repo_url": "#"
    },
    "2409.07347": {
      "paper_id": "2409.07347v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07347v2",
      "paper_key": "2409.07347",
      "paper_title": "The Role of Explainable AI in Revolutionizing Human Health Monitoring",
      "paper_url": "http://arxiv.org/abs/2409.07347v2",
      "paper_abstract": "The complex nature of disease mechanisms and the variability of patient symptoms present significant obstacles in developing effective diagnostic tools. Although machine learning has made considerable advances in medical diagnosis, its decision-making processes frequently lack transparency, which can jeopardize patient outcomes. This underscores the critical need for Explainable AI (XAI), which not only offers greater clarity but also has the potential to significantly improve patient care. In this literature review, we conduct a detailed analysis of analyzing XAI methods identified through searches across various databases, focusing on chronic conditions such as Parkinson's, stroke, depression, cancer, heart disease, and Alzheimer's disease. The literature search revealed the application of 9 trending XAI algorithms in the field of healthcare and highlighted the pros and cons of each of them. Thus, the article is concluded with a critical appraisal of the challenges and future research opportunities for XAI in human health monitoring.",
      "paper_authors": [
        "Abdullah Alharthi",
        "Ahmed Alqurashi",
        "Turki Alharbi",
        "Mohammed Alammar",
        "Nasser Aldosari",
        "Houssem Bouchekara",
        "Yusuf Shaaban",
        "Mohammad Shoaib Shahriar",
        "Abdulrahman Al Ayidh"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-13",
      "comments": null,
      "repo_url": "#"
    },
    "2409.07290": {
      "paper_id": "2409.07290v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07290v1",
      "paper_key": "2409.07290",
      "paper_title": "PSZ2 G282.28+49.94, a recently discovered analogue of the famous Bullet Cluster",
      "paper_url": "http://arxiv.org/abs/2409.07290v1",
      "paper_abstract": "We present a detailed study of the gas and galaxy properties of the cluster PSZ2 G282.28+49.94 detected in the Planck all-sky survey. The intracluster medium (ICM) of this object at z=0.56 exhibits a cometary-like shape. Combining Chandra and TNG observations, we characterised the spatially resolved thermodynamical properties of the gas and the spatial and velocity distribution of 73 galaxy members. The cluster structure is quite complex with an elongated core region containing the two brightest cluster galaxies and one dense group to the south-east. Since there is no velocity difference between the core and the south-east group, we suggest the presence of a merger along the plane of the sky. This structure is related to complex X-ray and radio features, and thus the merger has likely been caught during the post-merger phase. Comparing the distribution of the ICM and of member galaxies, we find a large offset of $\\sim 350$ kpc between the position of the X-ray peak and the centre of a concentration of galaxies, preceding it in the likely direction of motion. This configuration is similar to the famous Bullet Cluster, leading us to dub PSZ2 G282.28+49.94 the \"Planck bullet\", and represents an ideal situation to provide astrophysical constraints to the self-interaction cross-section ($\\sigma/m$) of dark matter particles. These results illustrate the power of a multi-wavelength approach to probe the merging scenario of such complex and distant systems.",
      "paper_authors": [
        "I. Bartalucci",
        "M. Rossetti",
        "W. Boschin",
        "M. Girardi",
        "M. Nonino",
        "E. Baraldi",
        "M. Balboni",
        "D. Coe",
        "S. De Grandi",
        "F. Gastaldello",
        "S. Ghizzardi",
        "S. Giacintucci",
        "C. Grillo",
        "D. Harvey",
        "L. Lovisari",
        "S. Molendi",
        "T. Resseguier",
        "G. Riva",
        "T. Venturi",
        "A. Zitrin"
      ],
      "primary_category": "astro-ph.CO",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "Accepted for publication in A&A",
      "repo_url": "#"
    },
    "2409.07277": {
      "paper_id": "2409.07277v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07277v1",
      "paper_key": "2409.07277",
      "paper_title": "Mechanisms for belief elicitation without ground truth",
      "paper_url": "http://arxiv.org/abs/2409.07277v1",
      "paper_abstract": "This review article examines the challenge of eliciting truthful information from multiple individuals when such information cannot be verified against an objective truth, a problem known as information elicitation without verification (IEWV). This article reviews over 25 mechanisms designed to incentivize truth-telling in such scenarios, and their effectiveness in empirical studies. The analysis finds that although many mechanisms theoretically ensure truthfulness as a Bayesian Nash Equilibrium, empirical evidence of such mechanisms working in practice is very limited and generally weak. Consequently, more empirical research is needed to validate mechanisms. Given that many mechanisms are very complex and cannot be easily conveyed to research subjects, this review suggests that simpler, more intuitive mechanisms may be easier to test and apply.",
      "paper_authors": [
        "Niklas Valentin Lehmann"
      ],
      "primary_category": "econ.GN",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": null,
      "repo_url": "#"
    },
    "2409.07253": {
      "paper_id": "2409.07253v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07253v2",
      "paper_key": "2409.07253",
      "paper_title": "Alignment of Diffusion Models: Fundamentals, Challenges, and Future",
      "paper_url": "http://arxiv.org/abs/2409.07253v2",
      "paper_abstract": "Diffusion models have emerged as the leading paradigm in generative modeling, excelling in various applications. Despite their success, these models often misalign with human intentions, generating outputs that may not match text prompts or possess desired properties. Inspired by the success of alignment in tuning large language models, recent studies have investigated aligning diffusion models with human expectations and preferences. This work mainly reviews alignment of diffusion models, covering advancements in fundamentals of alignment, alignment techniques of diffusion models, preference benchmarks, and evaluation for diffusion models. Moreover, we discuss key perspectives on current challenges and promising future directions on solving the remaining challenges in alignment of diffusion models. To the best of our knowledge, our work is the first comprehensive review paper for researchers and engineers to comprehend, practice, and research alignment of diffusion models.",
      "paper_authors": [
        "Buhua Liu",
        "Shitong Shao",
        "Bao Li",
        "Lichen Bai",
        "Zhiqiang Xu",
        "Haoyi Xiong",
        "James Kwok",
        "Sumi Helal",
        "Zeke Xie"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-12",
      "comments": "35 pages, 5 figures, 3 tables",
      "repo_url": "https://github.com/xie-lab-ml/awesome-alignment-of-diffusion-models"
    },
    "2409.07237": {
      "paper_id": "2409.07237v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07237v1",
      "paper_key": "2409.07237",
      "paper_title": "Negative Sampling in Recommendation: A Survey and Future Directions",
      "paper_url": "http://arxiv.org/abs/2409.07237v1",
      "paper_abstract": "Recommender systems aim to capture users' personalized preferences from the cast amount of user behaviors, making them pivotal in the era of information explosion. However, the presence of the dynamic preference, the \"information cocoons\", and the inherent feedback loops in recommendation make users interact with a limited number of items. Conventional recommendation algorithms typically focus on the positive historical behaviors, while neglecting the essential role of negative feedback in user interest understanding. As a promising but easy-to-ignored area, negative sampling is proficients in revealing the genuine negative aspect inherent in user behaviors, emerging as an inescapable procedure in recommendation. In this survey, we first discuss the role of negative sampling in recommendation and thoroughly analyze challenges that consistently impede its progress. Then, we conduct an extensive literature review on the existing negative sampling strategies in recommendation and classify them into five categories with their discrepant techniques. Finally, we detail the insights of the tailored negative sampling strategies in diverse recommendation scenarios and outline an overview of the prospective research directions toward which the community may engage and benefit.",
      "paper_authors": [
        "Haokai Ma",
        "Ruobing Xie",
        "Lei Meng",
        "Fuli Feng",
        "Xiaoyu Du",
        "Xingwu Sun",
        "Zhanhui Kang",
        "Xiangxu Meng"
      ],
      "primary_category": "cs.IR",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "38 pages, 9 figures; Under review",
      "repo_url": "#"
    },
    "2409.07212": {
      "paper_id": "2409.07212v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07212v1",
      "paper_key": "2409.07212",
      "paper_title": "The early Solar System and its meteoritical witnesses",
      "paper_url": "http://arxiv.org/abs/2409.07212v1",
      "paper_abstract": "Meteorites, and in particular primitive meteorites (chondrites), are irreplaceable probes of the solar protoplanetary disk. We review their essential properties and endeavour to place them in astrophysical context. The earliest solar system solids, refractory inclusions, may have formed over the innermost au of the disk and have been transported outward by its expansion or turbulent diffusion. The age spread of chondrite components may be reconciled with the tendency of drag-induced radial drift if they were captured in pressure maxima, which may account for the non-carbonaceous/carbonaceous meteorite isotopic dichotomy. The solid/gas ratio around unity witnessed by chondrules, if interpreted as nebular (non-impact) products, suggests efficient radial concentration and settling at such locations, conducive to planetesimal formation by the streaming instability. The cause of the pressure bumps, e.g. Jupiter or condensation lines, remains to be ascertained.",
      "paper_authors": [
        "Emmanuel Jacquet",
        "Cornelis Dullemond",
        "Joanna Dr\u0105\u017ckowska",
        "Steven Desch"
      ],
      "primary_category": "astro-ph.EP",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "Accepted to Space Science Reviews",
      "repo_url": "#"
    },
    "2409.07505": {
      "paper_id": "2409.07505v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07505v1",
      "paper_key": "2409.07505",
      "paper_title": "A Survey of Anomaly Detection in In-Vehicle Networks",
      "paper_url": "http://arxiv.org/abs/2409.07505v1",
      "paper_abstract": "Modern vehicles are equipped with Electronic Control Units (ECU) that are used for controlling important vehicle functions including safety-critical operations. ECUs exchange information via in-vehicle communication buses, of which the Controller Area Network (CAN bus) is by far the most widespread representative. Problems that may occur in the vehicle's physical parts or malicious attacks may cause anomalies in the CAN traffic, impairing the correct vehicle operation. Therefore, the detection of such anomalies is vital for vehicle safety. This paper reviews the research on anomaly detection for in-vehicle networks, more specifically for the CAN bus. Our main focus is the evaluation of methods used for CAN bus anomaly detection together with the datasets used in such analysis. To provide the reader with a more comprehensive understanding of the subject, we first give a brief review of related studies on time series-based anomaly detection. Then, we conduct an extensive survey of recent deep learning-based techniques as well as conventional techniques for CAN bus anomaly detection. Our comprehensive analysis delves into anomaly detection algorithms employed in in-vehicle networks, specifically focusing on their learning paradigms, inherent strengths, and weaknesses, as well as their efficacy when applied to CAN bus datasets. Lastly, we highlight challenges and open research problems in CAN bus anomaly detection.",
      "paper_authors": [
        "\u00d6vg\u00fc \u00d6zdemir",
        "M. Tu\u011fberk \u0130\u015fyapar",
        "P\u0131nar Karag\u00f6z",
        "Klaus Werner Schmidt",
        "Demet Demir",
        "N. Alpay Karag\u00f6z"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": null,
      "repo_url": "#"
    },
    "2409.07194": {
      "paper_id": "2409.07194v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07194v1",
      "paper_key": "2409.07194",
      "paper_title": "Cyber Deception: State of the art, Trends and Open challenges",
      "paper_url": "http://arxiv.org/abs/2409.07194v1",
      "paper_abstract": "The growing interest in cybersecurity has significantly increased articles designing and implementing various Cyber Deception (CYDEC) mechanisms. This trend reflects the urgent need for new strategies to address cyber threats effectively. Since its emergence, CYDEC has established itself as an innovative defense against attackers, thanks to its proactive and reactive capabilities, finding applications in numerous real-life scenarios. Despite the considerable work devoted to CYDEC, the literature still presents significant gaps. In particular, there has not been (i) a comprehensive analysis of the main components characterizing CYDEC, (ii) a generic classification covering all types of solutions, nor (iii) a survey of the current state of the literature in various contexts. This article aims to fill these gaps through a detailed review of the main features that comprise CYDEC, developing a comprehensive classification taxonomy. In addition, the different frameworks used to generate CYDEC are reviewed, presenting a more comprehensive one. Existing solutions in the literature using CYDEC, both without Artificial Intelligence (AI) and with AI, are studied and compared. Finally, the most salient trends of the current state of the art are discussed, offering a list of pending challenges for future research.",
      "paper_authors": [
        "Pedro Beltr\u00e1n L\u00f3pez",
        "Manuel Gil P\u00e9rez",
        "Pantaleone Nespoli"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "38 pages",
      "repo_url": "#"
    },
    "2409.07189": {
      "paper_id": "2409.07189v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07189v1",
      "paper_key": "2409.07189",
      "paper_title": "A Perspective on AI-Guided Molecular Simulations in VR: Exploring Strategies for Imitation Learning in Hyperdimensional Molecular Systems",
      "paper_url": "http://arxiv.org/abs/2409.07189v1",
      "paper_abstract": "Molecular dynamics simulations are a crucial computational tool for researchers to understand and engineer molecular structure and function in areas such as drug discovery, protein engineering, and material design. Despite their utility, MD simulations are expensive, owing to the high dimensionality of molecular systems. Interactive molecular dynamics in virtual reality (iMD-VR) has recently been developed as a 'human-in-the-loop' strategy, which leverages high-performance computing to accelerate the researcher's ability to solve the hyperdimensional sampling problem. By providing an immersive 3D environment that enables visualization and manipulation of real-time molecular motion, iMD-VR enables researchers and students to efficiently and intuitively explore and navigate these complex, high-dimensional systems. iMD-VR platforms offer a unique opportunity to quickly generate rich datasets that capture human experts' spatial insight regarding molecular structure and function. This paper explores the possibility of employing user-generated iMD-VR datasets to train AI agents via imitation learning (IL). IL is an important technique in robotics that enables agents to mimic complex behaviors from expert demonstrations, thus circumventing the need for explicit programming or intricate reward design. We review the utilization of IL for manipulation tasks in robotics and discuss how iMD-VR recordings could be used to train IL models for solving specific molecular 'tasks'. We then investigate how such approaches could be applied to the data captured from iMD-VR recordings. Finally, we outline the future research directions and potential challenges of using AI agents to augment human expertise to efficiently navigate conformational spaces, highlighting how this approach could provide valuable insight across domains such as materials science, protein engineering, and computer-aided drug design.",
      "paper_authors": [
        "Mohamed Dhouioui",
        "Jonathan Barnoud",
        "Rhoslyn Roebuck Williams",
        "Harry J. Stroud",
        "Phil Bates",
        "David R. Glowacki"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "(Accepted for presentation at the First Workshop on \"eXtended Reality\n  \\& Intelligent Agents\" (XRIA24) @ ECAI24, Santiago De Compostela (Spain), 20\n  October 2024)",
      "repo_url": "#"
    },
    "2409.07185": {
      "paper_id": "2409.07185v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07185v2",
      "paper_key": "2409.07185",
      "paper_title": "Deriving Cosmological Parameters from the Euclid mission",
      "paper_url": "http://arxiv.org/abs/2409.07185v2",
      "paper_abstract": "The Euclid mission is a visionary project undertaken by the European Space Agency (ESA) to probe the universe's evolution and geometry by surveying the position and gravitational shape distortion of billions of galaxies. These observations bear the potential to offer unprecedented measurements of the cosmological parameters, thereby advancing our understanding of the cosmos. This work revolves around the central theme of quantifying the constraining power of the upcoming Euclid 3$\\times$2pt photometric survey, accounting for several factors which have been neglected to this date in the official forecasts, especially more subtle sources of uncertainty which need to be included in the forecast (and data) analysis due to the precision of the observations. First, we include and study the impact of super-sample covariance, a source of sample variance coming from the incomplete sampling of the density and shear field Fourier modes caused by the limited survey volume. Second, we examine the effect of scale cuts, translating them from Fourier to harmonic space through the use of the BNT transform, which offers an efficient way of separating angular scales for the cosmic shear signal. This analysis allows quantifying and mitigating the bias coming from the uncertainty on our modelling of small scales. These updated forecasts, validated against the reference Euclid ones, provide insights into the expected precision achieved on the cosmological and nuisance parameters, for a variety of survey settings and for the inclusion of different realistic systematics, such as multiplicative shear bias, magnification bias, uncertainty in the mean of the redshift distribution and so on.",
      "paper_authors": [
        "Davide Sciotti"
      ],
      "primary_category": "astro-ph.CO",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-16",
      "comments": "158 pages, 41 figures. arXiv admin note: text overlap with\n  arXiv:2310.15731",
      "repo_url": "#"
    },
    "2409.07166": {
      "paper_id": "2409.07166v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07166v1",
      "paper_key": "2409.07166",
      "paper_title": "ALMACAL XII. Data characterisation and products",
      "paper_url": "http://arxiv.org/abs/2409.07166v1",
      "paper_abstract": "The ALMACAL survey is based on a database of reprocessed ALMA calibration scans suitable for scientific analysis, observed as part of regular PI observations. We present all the data accumulated from the start of ALMA operations until May 2022 for 1047 calibrator fields across the southern sky spanning ALMA Bands 3 to 10 (~ 84 - 950 GHz), so-called ALMACAL-22. Encompassing over 1000 square arcmin and accumulating over 2000 hours of integration time, ALMACAL is not only one of the largest ALMA surveys to date, but it continues to grow with each new scientific observation. We outline the methods for processing and imaging a subset of the highest-quality data ('pruned sample'). Using deconvolution techniques within the visibility data (uv plane), we created data cubes as the final product for further scientific analysis. We describe the properties and shortcomings of ALMACAL and compare its area and sensitivity with other sub-millimetre surveys. Notably, ALMACAL overcomes limitations of previous sub-millimetre surveys, such as small sky coverage and the effects of cosmic variance. Moreover, we discuss the improvements introduced by the latest version of this dataset that will enhance our understanding of dusty star-forming galaxies, extragalactic absorption lines, active galactic nucleus physics, and ultimately the evolution of molecular gas.",
      "paper_authors": [
        "Victoria Bollo",
        "Martin Zwaan",
        "Celine Peroux",
        "Aleksandra Hamanowicz",
        "Jianhang Chen",
        "Simon Weng",
        "Rob J. Ivison",
        "Andrew Biggs"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "10 pages, 5 figures. Accepted for publication in A&A",
      "repo_url": "#"
    },
    "2409.07162": {
      "paper_id": "2409.07162v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07162v1",
      "paper_key": "2409.07162",
      "paper_title": "A Fine-grained Sentiment Analysis of App Reviews using Large Language Models: An Evaluation Study",
      "paper_url": "http://arxiv.org/abs/2409.07162v1",
      "paper_abstract": "Analyzing user reviews for sentiment towards app features can provide valuable insights into users' perceptions of app functionality and their evolving needs. Given the volume of user reviews received daily, an automated mechanism to generate feature-level sentiment summaries of user reviews is needed. Recent advances in Large Language Models (LLMs) such as ChatGPT have shown impressive performance on several new tasks without updating the model's parameters i.e. using zero or a few labeled examples. Despite these advancements, LLMs' capabilities to perform feature-specific sentiment analysis of user reviews remain unexplored. This study compares the performance of state-of-the-art LLMs, including GPT-4, ChatGPT, and LLama-2-chat variants, for extracting app features and associated sentiments under 0-shot, 1-shot, and 5-shot scenarios. Results indicate the best-performing GPT-4 model outperforms rule-based approaches by 23.6% in f1-score with zero-shot feature extraction; 5-shot further improving it by 6%. GPT-4 achieves a 74% f1-score for predicting positive sentiment towards correctly predicted app features, with 5-shot enhancing it by 7%. Our study suggests that LLM models are promising for generating feature-specific sentiment summaries of user reviews.",
      "paper_authors": [
        "Faiz Ali Shah",
        "Ahmed Sabir",
        "Rajesh Sharma"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "The summary of the project is available at\n  https://ahmed.jp/project_page/App_LLMs_2024/app_llms.html",
      "repo_url": "https://github.com/faiz-ut/eval-feature-sentiment-extraction-llms"
    },
    "2409.07161": {
      "paper_id": "2409.07161v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07161v1",
      "paper_key": "2409.07161",
      "paper_title": "The IXPE View of Neutron Star Low-Mass X-ray Binaries",
      "paper_url": "http://arxiv.org/abs/2409.07161v1",
      "paper_abstract": "Low-mass X-ray binaries hosting weakly magnetized neutron stars (NS-LMXBs) are among the brightest sources in the X-ray sky. Since 2021, the Imaging X-ray Polarimetry Explorer (IXPE) has provided new measurements of the X-ray polarization of these sources. IXPE observations have revealed that most NS-LMXBs are significantly polarized in the X-rays, providing unprecedented insight into the geometry of their accretion flow. In this review paper, we summarize the first results obtained by IXPE on NS-LMXBs, the emerging trends within each class of sources (atoll/Z), and possible physical interpretations.",
      "paper_authors": [
        "Francesco Ursini",
        "Andrea Gnarini",
        "Fiamma Capitanio",
        "Anna Bobrikova",
        "Massimo Cocchi",
        "Alessandro Di Marco",
        "Sergio Fabiani",
        "Ruben Farinelli",
        "Fabio La Monaca",
        "John Rankin",
        "Mary Lynne Saade",
        "Juri Poutanen"
      ],
      "primary_category": "astro-ph.HE",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "8 pages, 3 figures, invited review for the Special Issue X-ray\n  Polarization: A New Era Begins",
      "repo_url": "#"
    },
    "2409.07128": {
      "paper_id": "2409.07128v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07128v1",
      "paper_key": "2409.07128",
      "paper_title": "Deep Learning Techniques for Hand Vein Biometrics: A Comprehensive Review",
      "paper_url": "http://arxiv.org/abs/2409.07128v1",
      "paper_abstract": "Biometric authentication has garnered significant attention as a secure and efficient method of identity verification. Among the various modalities, hand vein biometrics, including finger vein, palm vein, and dorsal hand vein recognition, offer unique advantages due to their high accuracy, low susceptibility to forgery, and non-intrusiveness. The vein patterns within the hand are highly complex and distinct for each individual, making them an ideal biometric identifier. Additionally, hand vein recognition is contactless, enhancing user convenience and hygiene compared to other modalities such as fingerprint or iris recognition. Furthermore, the veins are internally located, rendering them less susceptible to damage or alteration, thus enhancing the security and reliability of the biometric system. The combination of these factors makes hand vein biometrics a highly effective and secure method for identity verification. This review paper delves into the latest advancements in deep learning techniques applied to finger vein, palm vein, and dorsal hand vein recognition. It encompasses all essential fundamentals of hand vein biometrics, summarizes publicly available datasets, and discusses state-of-the-art metrics used for evaluating the three modes. Moreover, it provides a comprehensive overview of suggested approaches for finger, palm, dorsal, and multimodal vein techniques, offering insights into the best performance achieved, data augmentation techniques, and effective transfer learning methods, along with associated pretrained deep learning models. Additionally, the review addresses research challenges faced and outlines future directions and perspectives, encouraging researchers to enhance existing methods and propose innovative techniques.",
      "paper_authors": [
        "Mustapha Hemis",
        "Hamza Kheddar",
        "Sami Bourouis",
        "Nasir Saleem"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": null,
      "repo_url": "#"
    },
    "2409.07123": {
      "paper_id": "2409.07123v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07123v1",
      "paper_key": "2409.07123",
      "paper_title": "Cross-Refine: Improving Natural Language Explanation Generation by Learning in Tandem",
      "paper_url": "http://arxiv.org/abs/2409.07123v1",
      "paper_abstract": "Natural language explanations (NLEs) are vital for elucidating the reasoning behind large language model (LLM) decisions. Many techniques have been developed to generate NLEs using LLMs. However, like humans, LLMs might not always produce optimal NLEs on first attempt. Inspired by human learning processes, we introduce Cross-Refine, which employs role modeling by deploying two LLMs as generator and critic, respectively. The generator outputs a first NLE and then refines this initial explanation using feedback and suggestions provided by the critic. Cross-Refine does not require any supervised training data or additional training. We validate Cross-Refine across three NLP tasks using three state-of-the-art open-source LLMs through automatic and human evaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which only utilizes self-feedback to refine the explanations. Our findings from automatic evaluation and a user study indicate that Cross-Refine outperforms Self-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful LLMs, whereas Self-Refine only yields strong results with ChatGPT. Additionally, we conduct an ablation study to assess the importance of feedback and suggestions. Both of them play an important role in refining explanations. We further evaluate Cross-Refine on a bilingual dataset in English and German.",
      "paper_authors": [
        "Qianli Wang",
        "Tatiana Anikina",
        "Nils Feldhus",
        "Simon Ostermann",
        "Sebastian M\u00f6ller",
        "Vera Schmitt"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "17 pages; under review",
      "repo_url": "#"
    },
    "2409.07086": {
      "paper_id": "2409.07086v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07086v1",
      "paper_key": "2409.07086",
      "paper_title": "Diophantine stability for curves over finite fields",
      "paper_url": "http://arxiv.org/abs/2409.07086v1",
      "paper_abstract": "We carry out a survey on curves defined over finite fields that are Diophantine stable; that is, with the property that the set of points of the curve is not altered under a proper field extension. First, we derive some general results of such curves and then we analyze several families of curves that happen to be Diophantine stable.",
      "paper_authors": [
        "Francesc Bars",
        "Joan Carles Lario"
      ],
      "primary_category": "math.NT",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": null,
      "repo_url": "#"
    },
    "2409.07072": {
      "paper_id": "2409.07072v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07072v1",
      "paper_key": "2409.07072",
      "paper_title": "Latent Space Interpretation for Stylistic Analysis and Explainable Authorship Attribution",
      "paper_url": "http://arxiv.org/abs/2409.07072v1",
      "paper_abstract": "Recent state-of-the-art authorship attribution methods learn authorship representations of texts in a latent, non-interpretable space, hindering their usability in real-world applications. Our work proposes a novel approach to interpreting these learned embeddings by identifying representative points in the latent space and utilizing LLMs to generate informative natural language descriptions of the writing style of each point. We evaluate the alignment of our interpretable space with the latent one and find that it achieves the best prediction agreement compared to other baselines. Additionally, we conduct a human evaluation to assess the quality of these style descriptions, validating their utility as explanations for the latent space. Finally, we investigate whether human performance on the challenging AA task improves when aided by our system's explanations, finding an average improvement of around +20% in accuracy.",
      "paper_authors": [
        "Milad Alshomary",
        "Narutatsu Ri",
        "Marianna Apidianaki",
        "Ajay Patel",
        "Smaranda Muresan",
        "Kathleen McKeown"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "8 pages, 8 figures, under review",
      "repo_url": "#"
    },
    "2409.07059": {
      "paper_id": "2409.07059v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07059v1",
      "paper_key": "2409.07059",
      "paper_title": "Neutron skin and its effects in heavy-ion collisions",
      "paper_url": "http://arxiv.org/abs/2409.07059v1",
      "paper_abstract": "Neutron skin is an exotic phenomena in unstable nuclei. The various effects in nuclear reactions caused by the neutron skin and also its relation with the properties of nuclear structure are reviewed in this article. Based on numerous studies with theoretical models, strong correlations have been found between the neutron skin thickness and neutron removal cross section, neutron/proton yield ratio, t/\\ch{^3He} yield ratio, neutron-proton momentum difference, isoscaling parameter, photon production, reaction cross sections for neutron induced reactions, charge-changing cross sections difference of mirror nuclei, astrophysical $S$-factor, and other quantities in nuclear reactions induced by neutron-rich nuclei. Moreover, the relationships between neutron skin thickness and some properties of nuclear structure, such as $\\alpha$-cluster formation, $\\alpha$ decay, nuclear surface, nuclear temperature, and proton radii difference of mirror nuclei, have also been investigated. It also has been shown that the neutron skin plays a crucial role in relativistic heavy-ion collisions. Experimentally, an unstable nucleus with neutron skin can be generated by radioactive nuclear beam facilities, and the thickness of neutron skin could be extracted by measuring the sensitive probes, which further helps giving stringent constraints on the equation of state of asymmetric nuclear matter and properties of neutron stars.",
      "paper_authors": [
        "Meng-Qi Ding",
        "De-Qing Fang",
        "Yu-Gang Ma"
      ],
      "primary_category": "nucl-th",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "20 pages, 16 figures; Nucl. Sci. Tech. (accepted)",
      "repo_url": "#"
    },
    "2409.07031": {
      "paper_id": "2409.07031v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07031v1",
      "paper_key": "2409.07031",
      "paper_title": "Situated Visualization in Motion for Video Games",
      "paper_url": "http://arxiv.org/abs/2409.07031v1",
      "paper_abstract": "We contribute a systematic review of situated visualizations in motion in the context of video games. Video games produce rich dynamic datasets during gameplay that are often visualized to help players succeed in a game. Often these visualizations are moving either because they are attached to moving game elements or due to camera changes. We want to understand to what extent this motion and contextual game factors impact how players can read these visualizations. In order to ground our work, we surveyed 160 visualizations in motion and their embeddings in the game world. Here, we report on our analysis and categorization of these visualizations.",
      "paper_authors": [
        "Federica Bucchieri",
        "Lijie Yao",
        "Petra Isenberg"
      ],
      "primary_category": "cs.HC",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": null,
      "repo_url": "#"
    },
    "2409.07015": {
      "paper_id": "2409.07015v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07015v1",
      "paper_key": "2409.07015",
      "paper_title": "A Unified Phenomenology of Ion Heating in Low-$\u03b2$ Plasmas: Test-Particle Simulations",
      "paper_url": "http://arxiv.org/abs/2409.07015v1",
      "paper_abstract": "We argue that two prominent theories of ion heating in low-$\\beta$ collisionless plasmas -- stochastic and quasi-linear heating -- represent similar physical processes in turbulence with different normalized cross helicities. To capture both, we propose a simple phenomenology based on the power in scales at which critically balanced fluctuations reach their smallest parallel scale. Simulations of test ions interacting with turbulence confirm our scalings across a wide range of different ion and turbulence properties, including with a steep ion-kinetic transition range as relevant to the solar wind.",
      "paper_authors": [
        "Zade Johnston",
        "Jonathan Squire",
        "Romain Meyrand"
      ],
      "primary_category": "astro-ph.SR",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "12 pages, 4 figures. Submitted to Physical Review Letters; includes\n  supplemental material (6 figures)",
      "repo_url": "#"
    },
    "2409.06986": {
      "paper_id": "2409.06986v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06986v1",
      "paper_key": "2409.06986",
      "paper_title": "The Blending ToolKit: A simulation framework for evaluation of galaxy detection and deblending",
      "paper_url": "http://arxiv.org/abs/2409.06986v1",
      "paper_abstract": "We present an open source Python library for simulating overlapping (i.e., blended) images of galaxies and performing self-consistent comparisons of detection and deblending algorithms based on a suite of metrics. The package, named Blending Toolkit (BTK), serves as a modular, flexible, easy-to-install, and simple-to-use interface for exploring and analyzing systematic effects related to blended galaxies in cosmological surveys such as the Vera Rubin Observatory Legacy Survey of Space and Time (LSST). BTK has three main components: (1) a set of modules that perform fast image simulations of blended galaxies, using the open source image simulation package GalSim; (2) a module that standardizes the inputs and outputs of existing deblending algorithms; (3) a library of deblending metrics commonly defined in the galaxy deblending literature. In combination, these modules allow researchers to explore the impacts of galaxy blending in cosmological surveys. Additionally, BTK provides researchers who are developing a new deblending algorithm a framework to evaluate algorithm performance and make principled comparisons with existing deblenders. BTK includes a suite of tutorials and comprehensive documentation. The source code is publicly available on GitHub at https://github.com/LSSTDESC/BlendingToolKit.",
      "paper_authors": [
        "Ismael Mendoza",
        "Andrii Torchylo",
        "Thomas Sainrat",
        "Axel Guinot",
        "Alexandre Boucaud",
        "Maxime Paillasa",
        "Camille Avestruz",
        "Prakruth Adari",
        "Eric Aubourg",
        "Biswajit Biswas",
        "James Buchanan",
        "Patricia Burchat",
        "Cyrille Doux",
        "Remy Joseph",
        "Sowmya Kamath",
        "Alex I. Malz",
        "Grant Merz",
        "Hironao Miyatake",
        "C\u00e9cile Roucelle",
        "Tianqing Zhang",
        "the LSST Dark Energy Science Collaboration"
      ],
      "primary_category": "astro-ph.IM",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "15 pages, 9 figures, 2 tables",
      "repo_url": "https://github.com/LSSTDESC/BlendingToolKit"
    },
    "2409.06953": {
      "paper_id": "2409.06953v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06953v1",
      "paper_key": "2409.06953",
      "paper_title": "Neural Algorithmic Reasoning with Multiple Correct Solutions",
      "paper_url": "http://arxiv.org/abs/2409.06953v1",
      "paper_abstract": "Neural Algorithmic Reasoning (NAR) aims to optimize classical algorithms. However, canonical implementations of NAR train neural networks to return only a single solution, even when there are multiple correct solutions to a problem, such as single-source shortest paths. For some applications, it is desirable to recover more than one correct solution. To that end, we give the first method for NAR with multiple solutions. We demonstrate our method on two classical algorithms: Bellman-Ford (BF) and Depth-First Search (DFS), favouring deeper insight into two algorithms over a broader survey of algorithms. This method involves generating appropriate training data as well as sampling and validating solutions from model output. Each step of our method, which can serve as a framework for neural algorithmic reasoning beyond the tasks presented in this paper, might be of independent interest to the field and our results represent the first attempt at this task in the NAR literature.",
      "paper_authors": [
        "Zeno Kujawa",
        "John Poole",
        "Dobrik Georgiev",
        "Danilo Numeroso",
        "Pietro Li\u00f2"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": null,
      "repo_url": "#"
    },
    "2409.06930": {
      "paper_id": "2409.06930v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06930v1",
      "paper_key": "2409.06930",
      "paper_title": "Analysis of Potential Biases and Validity of Studies Using Multiverse Approaches to Assess the Impacts of Government Responses to Epidemics",
      "paper_url": "http://arxiv.org/abs/2409.06930v1",
      "paper_abstract": "We analyze the methodological approach and validity of interpretation of using national-level time-series regression analyses relating epidemic outcomes to policies that estimate many models involving permutations of analytic choices (i.e., a \"multiverse\" approach). Specifically, we evaluate the possible biases and pitfalls of interpretation of a multiverse approach to the context of government responses to epidemics using the example of COVID-19 and a recently published peer-reviewed paper by Bendavid and Patel (2024) along with the subsequent commentary that the two authors published discussing and interpreting the implications of their work. While we identify multiple potential errors and sources of biases in how the specific analyses were undertaken that are also relevant for other studies employing similar approaches, our most important finding involves constructing a counterexample showing that causal model specification-agnostic multiverse analyses can be incorrectly used to suggest that no consistent effect can be discovered in data especially in cases where most specifications estimated with the data are far from causally valid. Finally, we suggest an alternative approach involving hypothesis-drive specifications that explicitly account for infectiousness across jurisdictions in the analysis as well as the interconnected ways that policies and behavioral responses may evolve within and across these jurisdictions.",
      "paper_authors": [
        "Jeremy D. Goldhaber-Fiebert"
      ],
      "primary_category": "q-bio.OT",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": null,
      "repo_url": "#"
    },
    "2409.06924": {
      "paper_id": "2409.06924v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06924v1",
      "paper_key": "2409.06924",
      "paper_title": "The young exoplanetary system TOI-4562: Confirming the presence of a third body in the system",
      "paper_url": "http://arxiv.org/abs/2409.06924v1",
      "paper_abstract": "Young planetary systems represent an opportunity to investigate the early stages of (exo)planetary formation because the gravitational interactions have not yet significantly changed the initial configuration of the system. TOI-4562 b is a highly eccentric temperate Jupiter analogue orbiting a young F7V-type star of $<700$ Myr in age with an orbital period of $P_{orb} \\sim 225$ days and an eccentricity of $e=0.76$, and is one of the largest known exoplanets to have formed in situ. We observed a new transit of TOI-4562 b using the 0.6-m Zeiss telescope at the Pico dos Dias Observatory (OPD/LNA) in Minas Gerais, Brazil, and combine our data with Transiting Exoplanet Survey Satellite (TESS) and archive data, with the aim being to improve the ephemerides of this interesting system. The $O-C$ diagram for the new ephemeris is consistent with the presence of a giant planet in an outer orbit around TOI-4562. TOI-4562 c is a planet with a mass of $M=5.77 M_{Jup}$, an orbital period of $P_{orb}= 3990$ days, and a semi-major axis of $a = 5.219$ AU. We report the discovery of TOI-4562 c, the exoplanet with the longest orbital period discovered to date via the transit timing variation (TTV) method. The TOI-4562 system is in the process of violent evolution with intense dynamical changes - judging by its young age and high eccentricity - and is therefore a prime target for studies of formation and evolution of planetary systems.",
      "paper_authors": [
        "V. Fermiano",
        "R. K. Saito",
        "V. D. Ivanov",
        "C. Caceres",
        "L. A. Almeida",
        "J. Aires",
        "J. C. Beamin",
        "D. Minniti",
        "T. Ferreira",
        "L. Andrade",
        "B. W. Borges",
        "L. de Almeida",
        "F. Jablonski",
        "W. Schlindwein"
      ],
      "primary_category": "astro-ph.EP",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "5 pages and 3 figures (+ appendix). Accepted for publication in\n  Astronomy & Astrophysics, Letters to the Editor",
      "repo_url": "#"
    },
    "2409.06917": {
      "paper_id": "2409.06917v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06917v1",
      "paper_key": "2409.06917",
      "paper_title": "Fermi Blazars in the Zwicky Transient Facility Survey: Properties of Large Optical Variations",
      "paper_url": "http://arxiv.org/abs/2409.06917v1",
      "paper_abstract": "We analyze the optical light-curve data, obtained with the Zwicky Transient Facility (ZTF) survey, for 47 gamma-ray blazars monitored by the Large Area Telescope onboard {\\it the Fermi Gamma-ray Space Telescope (Fermi)}. These 47 sources are selected because they are among the Fermi blazars with the largest optical variations in the ZTF data. Two color-magnitude variation patterns are seen in them, one being redder to stable when brighter (RSWB; in 31 sources) and the other being stable when brighter (in 16 sources). The patterns fit with the results recently reported in several similar studies with different data. Moreover, we find that the colors in the stable state of the sources share similar values, which (after corrected for the Galactic extinction) of most sources are in a range of 0.4--0.55. This feature could be intrinsic and may be applied in, for example, the study of intragalactic medium. We also determine the turning points for the sources showing the RSWB pattern, after which the color changes saturate and become stable. We find a correlation between optical fluxes and gamma-ray fluxes at the turning points. The physical implications of the correlation remain to be investigated, probably better with a sample of high-quality gamma-ray flux measurements.",
      "paper_authors": [
        "Si-Si Sun",
        "Zhongxiang Wang",
        "Shun-hao Ji"
      ],
      "primary_category": "astro-ph.HE",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "14 pages, 12 figures, 2 tables, accepted for publication in Research\n  in Astronomy and Astrophysics",
      "repo_url": "#"
    },
    "2409.06857": {
      "paper_id": "2409.06857v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06857v2",
      "paper_key": "2409.06857",
      "paper_title": "What is the Role of Small Models in the LLM Era: A Survey",
      "paper_url": "http://arxiv.org/abs/2409.06857v2",
      "paper_abstract": "Large Language Models (LLMs) have made significant progress in advancing artificial general intelligence (AGI), leading to the development of increasingly large models such as GPT-4 and LLaMA-405B. However, scaling up model sizes results in exponentially higher computational costs and energy consumption, making these models impractical for academic researchers and businesses with limited resources. At the same time, Small Models (SMs) are frequently used in practical settings, although their significance is currently underestimated. This raises important questions about the role of small models in the era of LLMs, a topic that has received limited attention in prior research. In this work, we systematically examine the relationship between LLMs and SMs from two key perspectives: Collaboration and Competition. We hope this survey provides valuable insights for practitioners, fostering a deeper understanding of the contribution of small models and promoting more efficient use of computational resources. The code is available at https://github.com/tigerchen52/role_of_small_models",
      "paper_authors": [
        "Lihu Chen",
        "Ga\u00ebl Varoquaux"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-12",
      "comments": "a survey paper of small models",
      "repo_url": "https://github.com/tigerchen52/role_of_small_models"
    },
    "2409.06833": {
      "paper_id": "2409.06833v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06833v1",
      "paper_key": "2409.06833",
      "paper_title": "Electron Microscopy-based Automatic Defect Inspection for Semiconductor Manufacturing: A Systematic Review",
      "paper_url": "http://arxiv.org/abs/2409.06833v1",
      "paper_abstract": "In this review, automatic defect inspection algorithms that analyze Electron Microscope (EM) images of Semiconductor Manufacturing (SM) products are identified, categorized, and discussed. This is a topic of critical importance for the SM industry as the continuous shrinking of device patterns has led to increasing defectivity and a greater prevalence of higher-resolution imaging tools such as EM. These aspects among others threaten to increase costs as a result of increased inspection time-to-solution and decreased yield, respectively. Relevant research papers were systematically identified in four popular publication databases in January 2024. A total of 103 papers were selected after screening for novel contributions relating to automatic EM image analysis algorithms for semiconductor defect inspection. These papers were then categorized based on the inspection tasks they addressed, their evaluation metrics, and the type of algorithms used. A notable finding from this categorization is that reference-based defect detection algorithms were the most popular algorithm type until 2020 when deep learning-based inspection algorithms became more popular, especially for defect classification. Furthermore, four broader research questions were discussed to come to the following conclusions: (i) the key components of inspection algorithms are set up, pre-processing, feature extraction, and final prediction; (ii) the maturity of the manufacturing process affects the data availability and required sensitivity of inspection algorithms; (iii) key challenges for these algorithms relate to the desiderata of minimizing time-to-solution which pushes for high imaging throughput, reducing manual input during algorithm setup, and higher processing throughput; and (iv) three promising directions for future work are suggested based on gaps in the reviewed literature that address key remaining limitations.",
      "paper_authors": [
        "Enrique Dehaerne",
        "Bappaditya Dey",
        "Victor Blanco",
        "Jesse Davis"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": null,
      "repo_url": "#"
    },
    "2409.06807": {
      "paper_id": "2409.06807v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06807v1",
      "paper_key": "2409.06807",
      "paper_title": "Kino-PAX: Highly Parallel Kinodynamic Sampling-based Planner",
      "paper_url": "http://arxiv.org/abs/2409.06807v1",
      "paper_abstract": "Sampling-based motion planners (SBMPs) are effective for planning with complex kinodynamic constraints in high-dimensional spaces, but they still struggle to achieve real-time performance, which is mainly due to their serial computation design. We present Kinodynamic Parallel Accelerated eXpansion (Kino-PAX), a novel highly parallel kinodynamic SBMP designed for parallel devices such as GPUs. Kino-PAX grows a tree of trajectory segments directly in parallel. Our key insight is how to decompose the iterative tree growth process into three massively parallel subroutines. Kino-PAX is designed to align with the parallel device execution hierarchies, through ensuring that threads are largely independent, share equal workloads, and take advantage of low-latency resources while minimizing high-latency data transfers and process synchronization. This design results in a very efficient GPU implementation. We prove that Kino-PAX is probabilistically complete and analyze its scalability with compute hardware improvements. Empirical evaluations demonstrate solutions in the order of 10 ms on a desktop GPU and in the order of 100 ms on an embedded GPU, representing up to 1000 times improvement compared to coarse-grained CPU parallelization of state-of-the-art sequential algorithms over a range of complex environments and systems.",
      "paper_authors": [
        "Nicolas Perrault",
        "Qi Heng Ho",
        "Morteza Lahijanian"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": "Preprint Under Review",
      "repo_url": "#"
    },
    "2409.06802": {
      "paper_id": "2409.06802v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06802v1",
      "paper_key": "2409.06802",
      "paper_title": "A photochemical PHO network for hydrogen-dominated exoplanet atmospheres",
      "paper_url": "http://arxiv.org/abs/2409.06802v1",
      "paper_abstract": "Due to the detection of phosphine PH3 in the Solar System gas giants Jupiter and Saturn, PH3 has long been suggested to be detectable in exosolar substellar atmospheres too. However, to date, a direct detection of phosphine has proven to be elusive in exoplanet atmosphere surveys. We construct an updated phosphorus-hydrogen-oxygen (PHO) photochemical network suitable for simulation of gas giant hydrogen-dominated atmospheres. Using this network, we examine PHO photochemistry in hot Jupiter and warm Neptune exoplanet atmospheres at Solar and enriched metallicities. Our results show for HD 189733b-like hot Jupiters that HOPO, PO and P2 are typically the dominant P carriers at pressures important for transit and emission spectra, rather than PH3. For GJ1214b-like warm Neptune atmospheres our results suggest that at Solar metallicity PH3 is dominant in the absence of photochemistry, but is generally not in high abundance for all other chemical environments. At 10 and 100 times Solar, small oxygenated phosphorus molecules such as HOPO and PO dominate for both thermochemical and photochemical simulations. The network is able to reproduce well the observed PH3 abundances on Jupiter and Saturn. Despite progress in improving the accuracy of the PHO network, large portions of the reaction rate data remain with approximate, uncertain or missing values, which could change the conclusions of the current study significantly. Improving understanding of the kinetics of phosphorus-bearing chemical reactions will be a key undertaking for astronomers aiming to detect phosphine and other phosphorus species in both rocky and gaseous exoplanetary atmospheres in the near future.",
      "paper_authors": [
        "Elspeth K. H. Lee",
        "Shang-Min Tsai",
        "Julianne I. Moses",
        "John M. C. Plane",
        "Channon Visscher",
        "Stephen J. Klippenstein"
      ],
      "primary_category": "astro-ph.EP",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": "Submitted to ApJ (12 July 2024)",
      "repo_url": "#"
    },
    "2409.06794": {
      "paper_id": "2409.06794v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06794v1",
      "paper_key": "2409.06794",
      "paper_title": "Asymptotic Hodge Theory in String Compactifications and Integrable Systems",
      "paper_url": "http://arxiv.org/abs/2409.06794v1",
      "paper_abstract": "In this thesis we study the framework of asymptotic Hodge theory and its applications in both the string landscape and the landscape of 2d integrable field theories. We show how this mathematical framework allows for a general characterization of the asymptotic behaviour of physical couplings in low-energy effective theories coming from string theory, and apply this knowledge to investigate the finiteness and geometric structure of the string landscape landscape. At the same time, we find that the defining equations of variations of Hodge structure also arise in the context of certain integrable field theories, which opens the way to finding new classes of very general solutions to said models.   Part I reviews the relevant aspects of type IIB / F-theory flux compactifications and the resulting landscape of 4d low-energy effective $\\mathcal{N}=1$ supergravity theories.   Part II provides an in-depth discussion on asymptotic Hodge theory, including detailed explanations on the nilpotent orbit theorem of Schmid, and the multi-variable Sl(2)-orbit theorem of Cattani, Kaplan, and Schmid. This part of the thesis also contains new results regarding the multi-variable bulk reconstruction procedure, which have not appeared in the author's previous publications.   Part III concerns the application of the aforementioned results to study the finiteness of the F-theory flux landscape. Additionally, motivated by recent advances in the field of o-minimal geometry and the theory of unlikely intersections, we propose three conjectures which aim to address finer features of the flux landscape.   Part IV investigates two corners of the landscape of 2d integrable non-linear sigma-models, namely the $\\lambda$-deformed gauged WZW model and the critical bi-Yang-Baxter model. Notably, it is shown that asymptotic Hodge theory can be used to find broad classes of solutions these models.",
      "paper_authors": [
        "Jeroen Monnee"
      ],
      "primary_category": "hep-th",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": "PhD thesis, 358 pages; based on arXiv:2103.12746, arXiv:2112.00031,\n  arXiv:2212.03893, arXiv:2311.09295",
      "repo_url": "#"
    },
    "2409.06773": {
      "paper_id": "2409.06773v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06773v1",
      "paper_key": "2409.06773",
      "paper_title": "Design, scientific goals, and performance of the SCExAO survey for planets around accelerating stars",
      "paper_url": "http://arxiv.org/abs/2409.06773v1",
      "paper_abstract": "We describe the motivation, design, and early results for our 42-night, 125 star Subaru/SCExAO direct imaging survey for planets around accelerating stars. Unlike prior large surveys, ours focuses only on stars showing evidence for an astrometric acceleration plausibly due to the dynamical pull of an unseen planet or brown dwarf. Our program is motivated by results from a recent pilot program that found the first planet jointly discovered from direct imaging and astrometry and resulted in a planet and brown dwarf discovery rate substantially higher than previous unbiased surveys like GPIES. The first preliminary results from our program reveal multiple new companions; discovered planets and brown dwarfs can be further characterized with follow-up data, including higher-resolution spectra. Finally, we describe the critical role this program plays in supporting the Roman Space Telescope Coronagraphic Instrument, providing a currently-missing list of targets suitable for the CGI technological demonstration without which the CGI tech demo risks failure.",
      "paper_authors": [
        "Mona El Morsy",
        "Thayne Currie",
        "Masayuki Kuzuhara",
        "Jeffrey Chilcote",
        "Olivier Guyon",
        "Taylor L. Tobin",
        "Timothy Brandt",
        "Qier An",
        "Kyohoon Anh",
        "Danielle Bovie",
        "Vincent Deo",
        "Tyler Groff",
        "Ziying Gu",
        "Markus Janson",
        "Nemanja Jovanovic",
        "Yiting Li",
        "Kellen Lawson",
        "Julien Lozi",
        "Miles Lucas",
        "Christian Marois",
        "Naoshi Murakami",
        "Eric Nielsen",
        "Barnaby Norris",
        "Nour Skaf",
        "Motohide Tamura",
        "William Thompson",
        "Taichi Uyama",
        "Sebastien Vievard"
      ],
      "primary_category": "astro-ph.EP",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": "13 pages, 7 figures; Proc. SPIE in press",
      "repo_url": "#"
    },
    "2409.06768": {
      "paper_id": "2409.06768v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06768v2",
      "paper_key": "2409.06768",
      "paper_title": "The Intrinsic Flattening of Extragalactic Stellar Disks",
      "paper_url": "http://arxiv.org/abs/2409.06768v2",
      "paper_abstract": "Highly inclined (edge-on) disk galaxies offer the unique perspective to constrain their intrinsic flattening, $c/a$, where $c$ and $a$ are respectively the vertical and long radial axes of the disk measured at suitable stellar densities. The ratio $c/a$ is a necessary quantity in the assessment of galaxy inclinations, three-dimensional structural reconstructions, total masses, as well as a constraint to galaxy formation models. 3.6 micron maps of 133 edge-on spiral galaxies from the Spitzer Survey of Stellar Structure in Galaxies (S4G) and its early-type galaxy extension are used to revisit the assessment of $c/a$ free from dust extinction and away from the influence of a stellar bulge. We present a simple definition of $c/a$ and explore trends with other galactic physical parameters: total stellar mass, concentration index, total HI mass, mass of the central mass concentration, circular velocity, model-dependent scales, as well as Hubble type. Other than a dependence on early/late Hubble types, and a related trend with light concentration, no other parameters were found to correlate with the intrinsic flattening of spiral galaxies. The latter is mostly constant with $\\langle c/a \\rangle$ = 0.124 $\\pm$ 0.001 (stat) $\\pm$ 0.033 (intrinsic/systematic) and greater for earlier types.",
      "paper_authors": [
        "Jeremy Favaro",
        "St\u00e9phane Courteau",
        "S\u00e9bastien Comer\u00f3n",
        "Connor Stone"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-12",
      "comments": "27 full pages, 14 figures, 6 tables, 1 appendix. Submitted to AAS\n  Journals. Comments welcome",
      "repo_url": "#"
    },
    "2409.06772": {
      "paper_id": "2409.06772v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06772v1",
      "paper_key": "2409.06772",
      "paper_title": "Broad-Line AGN at $3.5<z<6$: The Black Hole Mass Function and a Connection with Little Red Dots",
      "paper_url": "http://arxiv.org/abs/2409.06772v1",
      "paper_abstract": "We present a sample of 50 H-alpha detected broad-line active galactic nuclei (BLAGN) at redshifts 3.5<z<6.8 using data from the CEERS and RUBIES surveys. We select these sources directly from JWST/NIRSpec G395M/F290LP spectra. We use a multi-step pre-selection and a Bayesian fitting procedure to ensure a high-quality sample of sources with broad Balmer lines and narrow forbidden lines. We compute rest-frame ultraviolet and optical spectral slopes for these objects, and determine that 10 BLAGN in our sample are also little red dots (LRDs). These LRD BLAGN, when examined in aggregate, show broader H-alpha line profiles and a higher fraction of broad-to-narrow component H-alpha emission than non-LRD BLAGN. Moreover, we find that ~66% of these objects are intrinsically reddened (beta (optical)>0), independent of the contributions of emission lines to the broadband photometry. We construct the black hole (BH) mass function at 3.5<z<6 after computing robust observational and line detection completeness corrections. This BH mass function shows broad agreement with both recent JWST/NIRSpec and JWST/NIRCam WFSS based BH mass functions, though we extend these earlier results to log(M(BH)/M(sun)) < 7. The derived BH mass function is consistent with a variety of theoretical models, indicating that the observed abundance of black holes in the early universe is not discrepant with physically-motivated predictions. The BH mass function shape resembles a largely featureless power-law, suggesting that any signature from black-hole seeding has been lost by redshift z~5-6. Finally, we compute the BLAGN UV luminosity function and find good agreement with JWST-detected BLAGN samples from recent works, finding that BLAGN hosts constitute <10% of the total observed UV luminosity at all but the brightest luminosities.",
      "paper_authors": [
        "Anthony J. Taylor",
        "Steven L. Finkelstein",
        "Dale D. Kocevski",
        "Junehyoung Jeon",
        "Volker Bromm",
        "Ricardo O. Amorin",
        "Pablo Arrabal Haro",
        "Bren E. Backhaus",
        "Micaela B. Bagley",
        "Eduardo Ba\u00f1ados",
        "Rachana Bhatawdekar",
        "Madisyn Brooks",
        "Antonello Calabro",
        "Oscar A. Chavez Ortiz",
        "Yingjie Cheng",
        "Nikko J. Cleri",
        "Justin W. Cole",
        "Kelcey Davis",
        "Mark Dickinson",
        "Callum Donnan",
        "James S. Dunlop",
        "Richard S. Ellis",
        "Vital Fernandez",
        "Adriano Fontana",
        "Seiji Fujimoto",
        "Mauro Giavalisco",
        "Andrea Grazian",
        "Jingsong Guo",
        "Nimish P. Hathi",
        "Benne W. Holwerda",
        "Michaela Hirschmann",
        "Kohei Inayoshi",
        "Jeyhan S. Kartaltepe",
        "Yana Khusanova",
        "Anton M. Koekemoer",
        "Vasily Kokorev",
        "Rebecca L. Larson",
        "Gene C. K. Leung",
        "Ray A. Lucas",
        "Derek J. McLeod",
        "Lorenzo Napolitano",
        "Masafusa Onoue",
        "Fabio Pacucci",
        "Casey Papovich",
        "Pablo G. P\u00e9rez-Gonz\u00e1lez",
        "Nor Pirzkal",
        "Rachel S. Somerville",
        "Jonathan R. Trump",
        "Stephen M. Wilkins",
        "L. Y. Aaron Yung",
        "Haowen Zhang"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": "28 pages, 14 figures, 4 tables. Submitted to ApJ",
      "repo_url": "#"
    },
    "2409.06700": {
      "paper_id": "2409.06700v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06700v1",
      "paper_key": "2409.06700",
      "paper_title": "MAMBO -- An empirical galaxy and AGN mock catalogue for the exploitation of future surveys",
      "paper_url": "http://arxiv.org/abs/2409.06700v1",
      "paper_abstract": "Aims. We present MAMBO, a flexible and efficient workflow to build empirical galaxy and Active Galactic Nuclei (AGN) mock catalogues that reproduce the physical and observational properties of these sources.   Methods. We start from simulated dark matter (DM) haloes, to preserve the link with the cosmic web, and we populate them with galaxies and AGN using abundance matching techniques. We follow an empirical methodology, using stellar mass functions (SMF), host galaxy AGN mass functions and AGN accretion rate distribution functions studied at different redshifts to assign, among other properties, stellar masses, the fraction of quenched galaxies, or the AGN activity (demography, obscuration, multiwavelength emission, etc.).   Results. As a proof test, we apply the method to a Millennium DM lightcone of 3.14 $\\rm deg^2$ up to redshift $z=10$ and down to stellar masses $\\mathcal{M} \\gtrsim 10^{7.5} \\, M_\\odot$. We show that the AGN population from the mock lightcone here presented reproduces with good accuracy various observables, such as state-of-the-art luminosity functions in the X-ray up to $z \\sim 7$ and in the ultraviolet up to $z \\sim 5$, optical/NIR colour-colour diagrams, and narrow emission line diagnostic diagrams. Finally, we demonstrate how this catalogue can be used to make useful predictions for large surveys. Using \\textit{Euclid} as a case example, we compute, among other forecasts, the expected surface densities of galaxies and AGN detectable in the \\textit{Euclid} $H_{\\rm E}$ band. We find that \\textit{Euclid} might observe (on $H_{\\rm E}$ only) about $10^{7}$ and $8 \\times 10^{7}$ Type 1 and 2 AGN respectively, and $2 \\times 10^{9}$ galaxies at the end of its 14 679 $\\rm deg^2$ Wide survey, in good agreement with other published forecasts.",
      "paper_authors": [
        "X. L\u00f3pez-L\u00f3pez",
        "M. Bolzonella",
        "L. Pozzetti",
        "M. Salvato",
        "L. Bisigello",
        "A. Feltre",
        "I. E. L\u00f3pez",
        "A. Viitanen",
        "V. Allevato",
        "A. Bongiorno",
        "G. Girelli",
        "J. Buchner",
        "S. Charlot",
        "F. Ricci",
        "C. Schreiber",
        "G. Zamorani"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": "22 pages, 18 figures. Accepted for publication in A&A",
      "repo_url": "#"
    },
    "2409.06764": {
      "paper_id": "2409.06764v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06764v1",
      "paper_key": "2409.06764",
      "paper_title": "Modeling Image Tone Dichotomy with the Power Function",
      "paper_url": "http://arxiv.org/abs/2409.06764v1",
      "paper_abstract": "The primary purpose of this paper is to present the concept of dichotomy in image illumination modeling based on the power function. In particular, we review several mathematical properties of the power function to identify the limitations and propose a new mathematical model capable of abstracting illumination dichotomy. The simplicity of the equation opens new avenues for classical and modern image analysis and processing. The article provides practical and illustrative image examples to explain how the new model manages dichotomy in image perception. The article shows dichotomy image space as a viable way to extract rich information from images despite poor contrast linked to tone, lightness, and color perception. Moreover, a comparison with state-of-the-art methods in image enhancement provides evidence of the method's value.",
      "paper_authors": [
        "Axel Martinez",
        "Gustavo Olague",
        "Emilio Hernandez"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": "49 pages, 11 figures and 36 references",
      "repo_url": "#"
    },
    "2409.06672": {
      "paper_id": "2409.06672v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06672v1",
      "paper_key": "2409.06672",
      "paper_title": "Insuring Uninsurable Risks from AI: The State as Insurer of Last Resort",
      "paper_url": "http://arxiv.org/abs/2409.06672v1",
      "paper_abstract": "Many experts believe that AI systems will sooner or later pose uninsurable risks, including existential risks. This creates an extreme judgment-proof problem: few if any parties can be held accountable ex post in the event of such a catastrophe. This paper proposes a novel solution: a government-provided, mandatory indemnification program for AI developers. The program uses risk-priced indemnity fees to induce socially optimal levels of care. Risk-estimates are determined by surveying experts, including indemnified developers. The Bayesian Truth Serum mechanism is employed to incent honest and effortful responses. Compared to alternatives, this approach arguably better leverages all private information, and provides a clearer signal to indemnified developers regarding what risks they must mitigate to lower their fees. It's recommended that collected fees be used to help fund the safety research developers need, employing a fund matching mechanism (Quadratic Financing) to induce an optimal supply of this public good. Under Quadratic Financing, safety research projects would compete for private contributions from developers, signaling how much each is to be supplemented with public funds.",
      "paper_authors": [
        "Cristian Trout"
      ],
      "primary_category": "cs.CY",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": "Accepted to Generative AI and Law Workshop at the International\n  Conference on Machine Learning (ICML 2024)",
      "repo_url": "#"
    },
    "2409.06653": {
      "paper_id": "2409.06653v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06653v1",
      "paper_key": "2409.06653",
      "paper_title": "Human Perception of LLM-generated Text Content in Social Media Environments",
      "paper_url": "http://arxiv.org/abs/2409.06653v1",
      "paper_abstract": "Emerging technologies, particularly artificial intelligence (AI), and more specifically Large Language Models (LLMs) have provided malicious actors with powerful tools for manipulating digital discourse. LLMs have the potential to affect traditional forms of democratic engagements, such as voter choice, government surveys, or even online communication with regulators; since bots are capable of producing large quantities of credible text. To investigate the human perception of LLM-generated content, we recruited over 1,000 participants who then tried to differentiate bot from human posts in social media discussion threads. We found that humans perform poorly at identifying the true nature of user posts on social media. We also found patterns in how humans identify LLM-generated text content in social media discourse. Finally, we observed the Uncanny Valley effect in text dialogue in both user perception and identification. This indicates that despite humans being poor at the identification process, they can still sense discomfort when reading LLM-generated content.",
      "paper_authors": [
        "Kristina Radivojevic",
        "Matthew Chou",
        "Karla Badillo-Urquiola",
        "Paul Brenner"
      ],
      "primary_category": "cs.HC",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": null,
      "repo_url": "#"
    },
    "2409.06755": {
      "paper_id": "2409.06755v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06755v1",
      "paper_key": "2409.06755",
      "paper_title": "A Systematic Approach to Crossing Numbers of Cartesian Products with Paths",
      "paper_url": "http://arxiv.org/abs/2409.06755v1",
      "paper_abstract": "Determining the crossing numbers of Cartesian products of small graphs with arbitrarily large paths has been an ongoing topic of research since the 1970s. Doing so requires the establishment of coincident upper and lower bounds; the former is usually demonstrated by providing a suitable drawing procedure, while the latter often requires substantial theoretical arguments. Many such papers have been published, which typically focus on just one or two small graphs at a time, and use ad hoc arguments specific to those graphs. We propose a general approach which, when successful, establishes the required lower bound. This approach can be applied to the Cartesian product of any graph with arbitrarily large paths, and in each case involves solving a modified version of the crossing number problem on a finite number (typically only two or three) of small graphs. We demonstrate the potency of this approach by applying it to Cartesian products involving all 133 graphs $G$ of orders five or six, and show that it is successful in 128 cases. This includes 60 cases which a recent survey listed as either undetermined, or determined only in journals without adequate peer review.",
      "paper_authors": [
        "Zayed Asiri",
        "Ryan Burdett",
        "Markus Chimani",
        "Michael Haythorpe",
        "Alex Newcombe",
        "Mirko H. Wagner"
      ],
      "primary_category": "math.CO",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": null,
      "repo_url": "#"
    },
    "2409.06616": {
      "paper_id": "2409.06616v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06616v1",
      "paper_key": "2409.06616",
      "paper_title": "Improving redshift-space power spectra of halo intrinsic alignments from perturbation theory",
      "paper_url": "http://arxiv.org/abs/2409.06616v1",
      "paper_abstract": "Intrinsic alignments (IAs) of galaxies/halos observed via galaxy imaging survey, combined with redshift information, offer a novel probe of cosmology as a tracer of tidal force field of large-scale structure. In this paper, we present a perturbation theory based model for the redshift-space power spectra of galaxy/halo IAs that can keep the impact of Finger-of-God damping effect, known as a nonlinear systematics of redshift-space distortions, under control. Focusing particularly on galaxy/halo density and IA cross power spectrum, we derive analytically the explicit expressions for the next-to-leading order corrections. Comparing the model predictions with $N$-body simulations, we show that these corrections indeed play an important role for an unbiased determination of the growth-rate parameter, and hence the model proposed here can be used for a precision test of gravity on cosmological scales.",
      "paper_authors": [
        "Atsushi Taruya",
        "Toshiki Kurita",
        "Teppei Okumura"
      ],
      "primary_category": "astro-ph.CO",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": "21 pages, 8 figures",
      "repo_url": "#"
    },
    "2409.06614": {
      "paper_id": "2409.06614v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06614v1",
      "paper_key": "2409.06614",
      "paper_title": "Fixed-budget and Multiple-issue Quadratic Voting",
      "paper_url": "http://arxiv.org/abs/2409.06614v1",
      "paper_abstract": "Quadratic Voting (QV) is a social choice mechanism that addresses the \"tyranny of the majority\" of one-person-one-vote mechanisms. Agents express not only their preference ordering but also their preference intensity by purchasing $x$ votes at a cost of $x^2$. Although this pricing rule maximizes utilitarian social welfare and is robust against strategic manipulation, it has not yet found many real-life applications. One key reason is that the original QV mechanism does not limit voter budgets. Two variations have since been proposed: a (no-budget) multiple-issue generalization and a fixed-budget version that allocates a constant number of credits to agents for use in multiple binary elections. While some analysis has been undertaken with respect to the multiple-issue variation, the fixed-budget version has not yet been rigorously studied. In this work, we formally propose a novel fixed-budget multiple-issue QV mechanism. This integrates the advantages of both the aforementioned variations, laying the theoretical foundations for practical use cases of QV, such as multi-agent resource allocation. We analyse our fixed-budget multiple-issue QV by comparing it with traditional voting systems, exploring potential collusion strategies, and showing that checking whether strategy profiles form a Nash equilibrium is tractable.",
      "paper_authors": [
        "Laura Georgescu",
        "James Fox",
        "Anna Gautier",
        "Michael Wooldridge"
      ],
      "primary_category": "cs.GT",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": "This preprint is currently under conference peer-review",
      "repo_url": "#"
    },
    "2409.06569": {
      "paper_id": "2409.06569v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06569v1",
      "paper_key": "2409.06569",
      "paper_title": "Cosmological gravity on all scales IV: 3x2pt Fisher forecasts for pixelised phenomenological modified gravity",
      "paper_url": "http://arxiv.org/abs/2409.06569v1",
      "paper_abstract": "Stage IV large scale structure surveys are promising probes of gravity on cosmological scales. Due to the vast model-space in the modified gravity literature, model-independent parameterisations represent useful and scalable ways to test extensions of $\\Lambda$CDM. In this work we use a recently validated approach of computing the non-linear $3\\times 2$pt observables in modified gravity models with a time-varying effective gravitational constant $\\mu$ and a gravitational slip $\\eta$ that is binned in redshift to produce Fisher forecasts for an LSST Y10-like survey. We also include in our modelling an effective nulling scheme for weak-lensing by applying the BNT transformation that localises the weak-lensing kernel enabling well-informed scale cuts. We show that the combination of improved non-linear modelling and better control of the scales that are modelled/cut yields high precision constraints on the cosmological and modified gravity parameters. We find that 4 redshift bins for $\\mu$ of width corresponding to equal incremental $\\Lambda$CDM growth is optimal given the state-of-the-art modelling and show how the BNT transformation can be used to mitigate the impact of small-scale systematic effects, such as baryonic feedback.",
      "paper_authors": [
        "Sankarshana Srinivasan",
        "Daniel B Thomas",
        "Peter L. Taylor"
      ],
      "primary_category": "astro-ph.CO",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": "27 pages, 12 figures Comments welcome!",
      "repo_url": "#"
    },
    "2409.06560": {
      "paper_id": "2409.06560v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06560v1",
      "paper_key": "2409.06560",
      "paper_title": "A Primer on Variational Inference for Physics-Informed Deep Generative Modelling",
      "paper_url": "http://arxiv.org/abs/2409.06560v1",
      "paper_abstract": "Variational inference (VI) is a computationally efficient and scalable methodology for approximate Bayesian inference. It strikes a balance between accuracy of uncertainty quantification and practical tractability. It excels at generative modelling and inversion tasks due to its built-in Bayesian regularisation and flexibility, essential qualities for physics related problems. Deriving the central learning objective for VI must often be tailored to new learning tasks where the nature of the problems dictates the conditional dependence between variables of interest, such as arising in physics problems. In this paper, we provide an accessible and thorough technical introduction to VI for forward and inverse problems, guiding the reader through standard derivations of the VI framework and how it can best be realized through deep learning. We then review and unify recent literature exemplifying the creative flexibility allowed by VI. This paper is designed for a general scientific audience looking to solve physics-based problems with an emphasis on uncertainty quantification.",
      "paper_authors": [
        "Alex Glyn-Davies",
        "Arnaud Vadeboncoeur",
        "O. Deniz Akyildiz",
        "Ieva Kazlauskaite",
        "Mark Girolami"
      ],
      "primary_category": "stat.ML",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": null,
      "repo_url": "#"
    },
    "2409.06557": {
      "paper_id": "2409.06557v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06557v1",
      "paper_key": "2409.06557",
      "paper_title": "Social Mediation through Robots -- A Scoping Review on Improving Group Interactions through Directed Robot Action using an Extended Group Process Model",
      "paper_url": "http://arxiv.org/abs/2409.06557v1",
      "paper_abstract": "Group processes refer to the dynamics that occur within a group and are critical for understanding how groups function. With robots being increasingly placed within small groups, improving these processes has emerged as an important application of social robotics. Social Mediation Robots elicit behavioral change within groups by deliberately influencing the processes of groups. While research in this field has demonstrated that robots can effectively affect interpersonal dynamics, there is a notable gap in integrating these insights to develop coherent understanding and theory. We present a scoping review of literature targeting changes in social interactions between multiple humans through intentional action from robotic agents. To guide our review, we adapt the classical Input-Process-Output (I-P-O) models that we call \"Mediation I-P-O model\". We evaluated 1633 publications, which yielded 89 distinct social mediation concepts. We construct 11 mediation approaches robots can use to shape processes in small groups and teams. This work strives to produce generalizable insights and evaluate the extent to which the potential of social mediation through robots has been realized thus far. We hope that the proposed framework encourages a holistic approach to the study of social mediation and provides a foundation to standardize future reporting in the domain.",
      "paper_authors": [
        "Thomas H. Weisswange",
        "Hifza Javed",
        "Manuel Dietrich",
        "Malte F. Jung",
        "Nawid Jamali"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": null,
      "repo_url": "#"
    },
    "2409.06752": {
      "paper_id": "2409.06752v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06752v1",
      "paper_key": "2409.06752",
      "paper_title": "A tutorial on automatic differentiation with complex numbers",
      "paper_url": "http://arxiv.org/abs/2409.06752v1",
      "paper_abstract": "Automatic differentiation is everywhere, but there exists only minimal documentation of how it works in complex arithmetic beyond stating \"derivatives in $\\mathbb{C}^d$\" $\\cong$ \"derivatives in $\\mathbb{R}^{2d}$\" and, at best, shallow references to Wirtinger calculus. Unfortunately, the equivalence $\\mathbb{C}^d \\cong \\mathbb{R}^{2d}$ becomes insufficient as soon as we need to derive custom gradient rules, e.g., to avoid differentiating \"through\" expensive linear algebra functions or differential equation simulators. To combat such a lack of documentation, this article surveys forward- and reverse-mode automatic differentiation with complex numbers, covering topics such as Wirtinger derivatives, a modified chain rule, and different gradient conventions while explicitly avoiding holomorphicity and the Cauchy--Riemann equations (which would be far too restrictive). To be precise, we will derive, explain, and implement a complex version of Jacobian-vector and vector-Jacobian products almost entirely with linear algebra without relying on complex analysis or differential geometry. This tutorial is a call to action, for users and developers alike, to take complex values seriously when implementing custom gradient propagation rules -- the manuscript explains how.",
      "paper_authors": [
        "Nicholas Kr\u00e4mer"
      ],
      "primary_category": "cs.MS",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": null,
      "repo_url": "#"
    },
    "2409.06751": {
      "paper_id": "2409.06751v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06751v1",
      "paper_key": "2409.06751",
      "paper_title": "The Weak Form Is Stronger Than You Think",
      "paper_url": "http://arxiv.org/abs/2409.06751v1",
      "paper_abstract": "The weak form is a ubiquitous, well-studied, and widely-utilized mathematical tool in modern computational and applied mathematics. In this work we provide a survey of both the history and recent developments for several fields in which the weak form can play a critical role. In particular, we highlight several recent advances in weak form versions of equation learning, parameter estimation, and coarse graining, which offer surprising noise robustness, accuracy, and computational efficiency.   We note that this manuscript is a companion piece to our October 2024 SIAM News article of the same name. Here we provide more detailed explanations of mathematical developments as well as a more complete list of references. Lastly, we note that the software with which to reproduce the results in this manuscript is also available on our group's GitHub website https://github.com/MathBioCU .",
      "paper_authors": [
        "Daniel A. Messenger",
        "April Tran",
        "Vanja Dukic",
        "David M. Bortz"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": null,
      "repo_url": "#"
    },
    "2409.06503": {
      "paper_id": "2409.06503v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06503v1",
      "paper_key": "2409.06503",
      "paper_title": "Advancements in Gesture Recognition Techniques and Machine Learning for Enhanced Human-Robot Interaction: A Comprehensive Review",
      "paper_url": "http://arxiv.org/abs/2409.06503v1",
      "paper_abstract": "In recent years robots have become an important part of our day-to-day lives with various applications. Human-robot interaction creates a positive impact in the field of robotics to interact and communicate with the robots. Gesture recognition techniques combined with machine learning algorithms have shown remarkable progress in recent years, particularly in human-robot interaction (HRI). This paper comprehensively reviews the latest advancements in gesture recognition methods and their integration with machine learning approaches to enhance HRI. Furthermore, this paper represents the vision-based gesture recognition for safe and reliable human-robot-interaction with a depth-sensing system, analyses the role of machine learning algorithms such as deep learning, reinforcement learning, and transfer learning in improving the accuracy and robustness of gesture recognition systems for effective communication between humans and robots.",
      "paper_authors": [
        "Sajjad Hussain",
        "Khizer Saeed",
        "Almas Baimagambetov",
        "Shanay Rab",
        "Md Saad"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": "19 pages,1 Figure",
      "repo_url": "#"
    },
    "2409.06458": {
      "paper_id": "2409.06458v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06458v1",
      "paper_key": "2409.06458",
      "paper_title": "Blind source separation in 3rd generation gravitational-wave detectors",
      "paper_url": "http://arxiv.org/abs/2409.06458v1",
      "paper_abstract": "Third generation and future upgrades of current gravitational-wave detectors will present exquisite sensitivities which will allow to detect a plethora of gravitational wave signals. Hence, a new problem to be solved arises: the detection and parameter estimation of overlapped signals. The problem of separating and identifying two signals that overlap in time, space or frequency is something well known in other fields (e.g. medicine and telecommunication). Blind source separation techniques are all those methods that aim at separating two or more unknown signals. This article provides a methodological review of the most common blind source separation techniques and it analyses whether they can be successfully applied to overlapped gravitational wave signals or not, while comparing the limits and advantages of each method.",
      "paper_authors": [
        "Francesca Badaracco",
        "Biswajit Banerjee",
        "Marica Branchesi",
        "Andrea Chincarini"
      ],
      "primary_category": "astro-ph.IM",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": null,
      "repo_url": "#"
    },
    "2409.06405": {
      "paper_id": "2409.06405v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06405v1",
      "paper_key": "2409.06405",
      "paper_title": "JADES: Measuring reionization properties using Lyman-alpha emission",
      "paper_url": "http://arxiv.org/abs/2409.06405v1",
      "paper_abstract": "Ly$\\alpha$ is the transition to the ground state from the first excited state of hydrogen (the most common element). Resonant scattering of this line by neutral hydrogen greatly impedes its emergence from galaxies, so the fraction of galaxies which show Ly$\\alpha$ is a tracer of the neutral fraction of the intergalactic medium (IGM), and thus the history of reionization. In previous works, we used early JWST/NIRSpec data from the JWST Advanced Deep Extragalactic Survey (JADES) to classify and characterise Ly$\\alpha$ emitting galaxies (LAEs). This survey is now approaching completion, and the current sample is nearly an order of magnitude larger. From a sample of 784 galaxies in JADES at $4.0<z<14.3$, we find evidence for Ly$\\alpha$ emission in 145 sources. We reproduce the previously found correlation between Ly$\\alpha$ escape fraction (\\fesc) - Ly$\\alpha$ rest-frame equivalent width (\\rew) and the negative correlation between Ly$\\alpha$ velocity offset - \\fesc. Both \\fesc and \\rew decrease with redshift ($z\\gtrsim5.5$), indicating the progression of reionization on a population scale. Our data are used to demonstrate an increasing IGM transmission of Ly$\\alpha$ from $z\\sim14-6$. We measure the completeness-corrected fraction of LAEs ($X_{Ly\\alpha}$) from $z=4-9.5$. An application of these $X_{Ly\\alpha}$ values to the results of cosmological models suggests a high neutral fraction at $z=7$ ($\\rm X_{HI}=0.81_{-0.10}^{+0.07}$), likely suggesting the need for models with updated \\rew distributions (based on comparison to other works). This large sample of LAEs and the completeness correction we have detailed will be paramount for unbiased population studies of galaxies in the EoR.",
      "paper_authors": [
        "Gareth C. Jones",
        "Andrew J. Bunker",
        "Aayush Saxena",
        "Santiago Arribas",
        "Rachana Bhatawdekar",
        "Kristan Boyett",
        "Stefano Carniani",
        "Stephane Charlot",
        "Emma Curtis-Lake",
        "Kevin Hainline",
        "Benjamin D. Johnson",
        "Nimisha Kumari",
        "Michael V. Maseda",
        "Hans-Walter Rix",
        "Brant E. Robertson",
        "Sandro Tacchella",
        "Hannah \u00dcbler",
        "Christina C. Williams",
        "Chris Willott",
        "Joris Witstok",
        "Yongda Zhu"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": "26 pages, 20 figures, submitted to MNRAS",
      "repo_url": "#"
    },
    "2409.06403": {
      "paper_id": "2409.06403v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06403v1",
      "paper_key": "2409.06403",
      "paper_title": "Towards few-body QCD on a quantum computer",
      "paper_url": "http://arxiv.org/abs/2409.06403v1",
      "paper_abstract": "Quantum computers are promising tools for the simulation of many-body systems, and among those, QCD stands out by its rich phenomenology. Every simulation starts with a codification, and here we succently review a newly developed compact encoding based on the identification between registers and particles; the quantum memory is divided into registers, and to each we associate a Hilbert space of dimension the number of degrees of freedom of the codified particles. In this way we gain an exponential compression over direct encodings for a low number of particles with many degrees of freedom. As an example we apply this encoding on a two-register memory and implement antisymmetrization and exponentiation algorithms.",
      "paper_authors": [
        "J. J. Galvez-Viruet"
      ],
      "primary_category": "quant-ph",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": "Proceedings of the 27th High-Energy Physics International Conference\n  in Quantum Chromodynamis (QCD24)",
      "repo_url": "#"
    },
    "2409.06383": {
      "paper_id": "2409.06383v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06383v1",
      "paper_key": "2409.06383",
      "paper_title": "Sifting the debris: Patterns in the SNR population with unsupervised ML methods",
      "paper_url": "http://arxiv.org/abs/2409.06383v1",
      "paper_abstract": "Supernova remnants (SNRs) carry vast amounts of mechanical and radiative energy that heavily influence the structural, dynamical, and chemical evolution of galaxies. To this day, more than 300 SNRs have been discovered in the Milky Way, exhibiting a wide variety of observational features. However, existing classification schemes are mainly based on their radio morphology. In this work, we introduce a novel unsupervised deep learning pipeline to analyse a representative subsample of the Galactic SNR population ($\\sim$ 50% of the total) with the aim of finding a connection between their multi-wavelength features and their physical properties. The pipeline involves two stages: (1) a representation learning stage, consisting of a convolutional autoencoder that feeds on imagery from infrared and radio continuum surveys (WISE 22$\\mu$m, Hi-GAL 70 $\\mu$m and SMGPS 30 cm) and produces a compact representation in a lower-dimensionality latent space; and (2) a clustering stage that seeks meaningful clusters in the latent space that can be linked to the physical properties of the SNRs and their surroundings. Our results suggest that this approach, when combined with an intermediate uniform manifold approximation and projection (UMAP) reprojection of the autoencoded embeddings into a more clusterable manifold, enables us to find reliable clusters. Despite a large number of sources being classified as outliers, most clusters relate to the presence of distinctive features, such as the distribution of infrared emission, the presence of radio shells and pulsar wind nebulae, and the existence of dust filaments.",
      "paper_authors": [
        "F. Bufano",
        "C. Bordiu",
        "T. Cecconello",
        "M. Munari",
        "A. Hopkins",
        "A. Ingallinera",
        "P. Leto",
        "S. Loru",
        "S. Riggi",
        "E. Sciacca",
        "G. Vizzari",
        "A. De Marco",
        "C. S. Buemi",
        "F. Cavallaro",
        "C. Trigilio",
        "G. Umana"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": "Accepted in A&A. 17 pages, 11 figures",
      "repo_url": "#"
    },
    "2409.06378": {
      "paper_id": "2409.06378v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06378v2",
      "paper_key": "2409.06378",
      "paper_title": "Note on the existence of classical solutions of derivative semilinear models for one dimensional wave equation",
      "paper_url": "http://arxiv.org/abs/2409.06378v2",
      "paper_abstract": "This note is a supplement with a new result to the review paper by Takamura [13] on nonlinear wave equations in one space dimension. We are focusing here to the long-time existence of classical solutions of semilinear wave equations in one space dimension, especially with derivative nonlinear terms of product-type. Our result is an extension of the single component case, but it is meaningful to provide models as possible as many to cover the optimality of the general theory. The proof is based on the classical iteration argument of the point-wise estimate of the solution.",
      "paper_authors": [
        "Takiko Sasaki",
        "Hiroyuki Takamura"
      ],
      "primary_category": "math.AP",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-13",
      "comments": "8 pages. The first version has a typo in the definition of the norm.\n  There should be no weight of course",
      "repo_url": "#"
    },
    "2409.06332": {
      "paper_id": "2409.06332v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06332v1",
      "paper_key": "2409.06332",
      "paper_title": "Planetary nebulae seen with TESS: New and revisited short-period binary central star candidates from Cycles 1 to 4",
      "paper_url": "http://arxiv.org/abs/2409.06332v1",
      "paper_abstract": "High-precision and high-cadence photometric surveys such as Kepler or TESS are making huge progress not only in the detection of new extrasolar planets but also in the study of a great number of variable stars. This is the case for central stars of planetary nebulae (PNe), which have similarly benefited from the capabilities of these missions, increasing the number of known binary central stars and helping us to constrain the relationship between binarity and the complex morphologies of their host PNe. In this paper, we analyse the TESS light curves of a large sample of central stars of PNe with the aim of detecting signs of variability that may hint at the presence of short-period binary nuclei. We analysed 62 central stars of true, likely, or possible PNe and modelled the detected variability through an MCMC approach accounting for three effects: reflection, ellipsoidal modulations, and Doppler beaming. Among the 62 central stars, only 38 are amenable for this study. The remaining 24 show large contamination from nearby sources preventing an optimal analysis. Also, eight targets are already known binary central stars, which we revisit here with the new high precision of the TESS data. In addition, we find that 18 further central stars show clear signs of periodic variability in the TESS data, probably resulting from different physical effects compatible with the binary scenario. We propose them as new candidate binary central stars. We also discuss the origin of the detected variability in each particular case by using the TESS_localize algorithm. Finally, 12 targets show no or only weak evidence of variability at the sensitivity of TESS. Our study demonstrates the power of space-based photometric surveys in searching for close binary companions of central stars of PNe.",
      "paper_authors": [
        "Alba Aller",
        "Jorge Lillo-Box",
        "David Jones"
      ],
      "primary_category": "astro-ph.SR",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": "32 pages, 17 figures, 8 tables, 2 appendices. Accepted for\n  publication in Astronomy & Astrophysics",
      "repo_url": "#"
    },
    "2409.06305": {
      "paper_id": "2409.06305v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06305v1",
      "paper_key": "2409.06305",
      "paper_title": "High-Performance Few-Shot Segmentation with Foundation Models: An Empirical Study",
      "paper_url": "http://arxiv.org/abs/2409.06305v1",
      "paper_abstract": "Existing few-shot segmentation (FSS) methods mainly focus on designing novel support-query matching and self-matching mechanisms to exploit implicit knowledge in pre-trained backbones. However, the performance of these methods is often constrained by models pre-trained on classification tasks. The exploration of what types of pre-trained models can provide more beneficial implicit knowledge for FSS remains limited. In this paper, inspired by the representation consistency of foundational computer vision models, we develop a FSS framework based on foundation models. To be specific, we propose a simple approach to extract implicit knowledge from foundation models to construct coarse correspondence and introduce a lightweight decoder to refine coarse correspondence for fine-grained segmentation. We systematically summarize the performance of various foundation models on FSS and discover that the implicit knowledge within some of these models is more beneficial for FSS than models pre-trained on classification tasks. Extensive experiments on two widely used datasets demonstrate the effectiveness of our approach in leveraging the implicit knowledge of foundation models. Notably, the combination of DINOv2 and DFN exceeds previous state-of-the-art methods by 17.5% on COCO-20i. Code is available at https://github.com/DUT-CSJ/FoundationFSS.",
      "paper_authors": [
        "Shijie Chang",
        "Lihe Zhang",
        "Huchuan Lu"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": "under review",
      "repo_url": "https://github.com/dut-csj/foundationfss"
    },
    "2409.06303": {
      "paper_id": "2409.06303v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06303v1",
      "paper_key": "2409.06303",
      "paper_title": "S-dual of Hamiltonian $\\mathbf G$ spaces and relative Langlands duality",
      "paper_url": "http://arxiv.org/abs/2409.06303v1",
      "paper_abstract": "The S-dual $(\\mathbf G^\\vee\\curvearrowright\\mathbf M^\\vee)$ of the pair $(\\mathbf G\\curvearrowright\\mathbf M)$ of a smooth affine algebraic symplectic manifold $\\mathbf M$ with hamiltonian action of a complex reductive group $\\mathbf G$ was introduced implicitly in [arXiv:1706.02112] and explicitly in [arXiv:1807.09038] under the cotangent type assumption. The definition was a modification of the definition of Coulomb branches of gauge theories in [arXiv:1601.03586]. It was motivated by the S-duality of boundary conditions of 4-dimensional $\\mathcal N=4$ super Yang-Mills theory, studied by Gaiotto and Witten [arXiv:0807.3720]. It is also relevant to the relative Langlands duality proposed by Ben-Zvi, Sakellaridis and Venkatesh. In this article, we review the definition and properties of S-dual.",
      "paper_authors": [
        "Hiraku Nakajima"
      ],
      "primary_category": "math.AG",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": "11 pages",
      "repo_url": "#"
    },
    "2409.06273": {
      "paper_id": "2409.06273v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06273v1",
      "paper_key": "2409.06273",
      "paper_title": "Mechanistic-statistical model for the expansion of ash dieback",
      "paper_url": "http://arxiv.org/abs/2409.06273v1",
      "paper_abstract": "Hymenoscyphus fraxineus is an invasive forest fungal pathogen that induces severe dieback in European ash populations. The spread of the disease has been closely monitored in France by the forest health survey system. We have developed a mechanisticstatistical model that describes the spread of the disease. It takes into account climate (summer temperature and spring rainfall), pathogen population dynamics (foliar infection, Allee effect induced by limited sexual partner encounters) and host density. We fitted this model using available disease reports. We estimated the parameters of our model, first identifying the appropriate ranges for the parameters, which led to a model reduction, and then using an adaptive multiple importance sampling algorithm for fitting. The model reproduces well the propagation observed in France over the last 20 years. In particular, it predicts the absence of disease impact in the south-east of the country and its weak development in the Garonne valley in south-west France. Summer temperature is the factor with the highest overall effect on disease spread, and explains the limited impact in southern France. Among the different temperature indices tested, the number of summer days with temperatures above 28{\\textdegree}C gave the best qualitative behavior and the best fit. In contrast, the Allee effect and the heterogeneity of spring precipitation did not strongly affect the overall expansion of H. fraxineus in France and could be neglected in the modeling process. The model can be used to infer the average annual dispersal of H. fraxineus in France.",
      "paper_authors": [
        "Coralie Fritsch",
        "Marie Grosdidier",
        "Anne G\u00e9gout-Petit",
        "Benoit Mar\u00e7ais"
      ],
      "primary_category": "q-bio.PE",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": null,
      "repo_url": "#"
    },
    "2409.06255": {
      "paper_id": "2409.06255v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06255v1",
      "paper_key": "2409.06255",
      "paper_title": "Market Reaction to News Flows in Supply Chain Networks",
      "paper_url": "http://arxiv.org/abs/2409.06255v1",
      "paper_abstract": "This study examines whether positive news about firms increases their stock prices and, moreover, whether it increases stock prices of the firms' suppliers and customers, using a large sample of publicly listed firms across the world and another of Japanese listed firms. The level of positiveness of each news article is determined by FinBERT, a natural language processing model fine-tuned specifically for financial information. Supply chains of firms across the world are identified mostly by financial statements, while those of Japanese firms are taken from large-scale firm-level surveys. We find that positive news increases the change rate of stock prices of firms mentioned in the news before its disclosure, most likely because of diffusion of information through informal channels. Positive news also raises stock prices of the firms' suppliers and customers before its disclosure, confirming propagation of market values through supply chains. In addition, we generally find a larger post-news effect on stock prices of the mentioned firms and their suppliers and customers than the pre-news effect. The positive difference between the post- and pre-news effects can be considered as the net effect of the disclosure of positive news, controlling for informal information diffusion. However, the post-news effect on suppliers and customers in Japan is smaller than the pre-news effect, a result opposite to those from firms across the world. This notable result is possibly because supply chain links of Japanese firms are stronger than global supply chains while such knowledge is restricted to selected investors.",
      "paper_authors": [
        "Hiroyasu Inoue",
        "Yasuyuki Todo"
      ],
      "primary_category": "cs.SI",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": null,
      "repo_url": "#"
    },
    "2409.07499": {
      "paper_id": "2409.07499v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07499v1",
      "paper_key": "2409.07499",
      "paper_title": "Multi-Virtual Knot Theory",
      "paper_url": "http://arxiv.org/abs/2409.07499v1",
      "paper_abstract": "This paper discusses a generalization of virtual knot theory that we call multi-virtual knot theory. Multi-virtual knot theory uses a multiplicity of types of virtual crossings. As we will explain, this multiplicity is motivated by the way it arises first in a graph-theoretic setting in relation to generalizing the Penrose evaluation for colorings of planar trivalent graphs to all trivalent graphs, and later by its uses in a virtual knot theory. As a consequence, the paper begins with the graph theory as a basis for our constructions, and then proceeds to the topology of multi-virtual knots and links. The second section of the paper is a review of our previous work (See arXiv:1511.06844). The reader interested in seeing our generalizations of the original Penrose evaluation, can begin this paper at the beginning and see the graph theory. A reader primarily interested in multi-virtual knots and links can begin reading in section 4 with references to the earlier part of the paper.",
      "paper_authors": [
        "Louis H Kauffman"
      ],
      "primary_category": "math.GT",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": "82 pages, LaTeX document, 79 figures",
      "repo_url": "#"
    },
    "2409.07498": {
      "paper_id": "2409.07498v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07498v1",
      "paper_key": "2409.07498",
      "paper_title": "Structural Robustness and Vulnerability of Networks",
      "paper_url": "http://arxiv.org/abs/2409.07498v1",
      "paper_abstract": "Networks are useful descriptions of the structure of many complex systems. Unsurprisingly, it is thus important to analyze the robustness of networks in many scientific disciplines. In applications in communication, logistics, finance, ecology, biomedicine, and many other fields, researchers have studied the robustness of networks to the removal of nodes, edges, or other subnetworks to identify and characterize robust network structures. A major challenge in the study of network robustness is that researchers have reported that different and seemingly contradictory network properties are correlated with a network's robustness. Using a framework by Alderson and Doyle~\\cite{Alderson2010}, we categorize several notions of network robustness and we examine these ostensible contradictions. We survey studies of network robustness with a focus on (1)~identifying robustness specifications in common use, (2)~understanding when these specifications are appropriate, and (3)~understanding the conditions under which one can expect different notions of robustness to yield similar results. With this review, we aim to give researchers an overview of the large, interdisciplinary body of work on network robustness and develop practical guidance for the design of computational experiments to study a network's robustness.",
      "paper_authors": [
        "Alice C. Schwarze",
        "Jessica Jiang",
        "Jonny Wray",
        "Mason A. Porter"
      ],
      "primary_category": "physics.soc-ph",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": "95-page review article",
      "repo_url": "#"
    },
    "2409.06233": {
      "paper_id": "2409.06233v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06233v1",
      "paper_key": "2409.06233",
      "paper_title": "VBIT: Towards Enhancing Privacy Control Over IoT Devices",
      "paper_url": "http://arxiv.org/abs/2409.06233v1",
      "paper_abstract": "Internet-of-Things (IoT) devices are increasingly deployed at home, at work, and in other shared and public spaces. IoT devices collect and share data with service providers and third parties, which poses privacy concerns. Although privacy enhancing tools are quite advanced in other applications domains (\\eg~ advertising and tracker blockers for browsers), users have currently no convenient way to know or manage what and how data is collected and shared by IoT devices. In this paper, we present VBIT, an interactive system combining Mixed Reality (MR) and web-based applications that allows users to: (1) uncover and visualize tracking services by IoT devices in an instrumented space and (2) take action to stop or limit that tracking. We design and implement VBIT to operate at the network traffic level, and we show that it has negligible performance overhead, and offers flexibility and good usability. We perform a mixed-method user study consisting of an online survey and an in-person interview study. We show that VBIT users appreciate VBIT's transparency, control, and customization features, and they become significantly more willing to install an IoT advertising and tracking blocker, after using VBIT. In the process, we obtain design insights that can be used to further iterate and improve the design of VBIT and other systems for IoT transparency and control.",
      "paper_authors": [
        "Jad Al Aaraj",
        "Olivia Figueira",
        "Tu Le",
        "Isabela Figueira",
        "Rahmadi Trimananda",
        "Athina Markopoulou"
      ],
      "primary_category": "cs.HC",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": null,
      "repo_url": "#"
    },
    "2409.06226": {
      "paper_id": "2409.06226v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06226v1",
      "paper_key": "2409.06226",
      "paper_title": "NLP-Powered Repository and Search Engine for Academic Papers: A Case Study on Cyber Risk Literature with CyLit",
      "paper_url": "http://arxiv.org/abs/2409.06226v1",
      "paper_abstract": "As the body of academic literature continues to grow, researchers face increasing difficulties in effectively searching for relevant resources. Existing databases and search engines often fall short of providing a comprehensive and contextually relevant collection of academic literature. To address this issue, we propose a novel framework that leverages Natural Language Processing (NLP) techniques. This framework automates the retrieval, summarization, and clustering of academic literature within a specific research domain. To demonstrate the effectiveness of our approach, we introduce CyLit, an NLP-powered repository specifically designed for the cyber risk literature. CyLit empowers researchers by providing access to context-specific resources and enabling the tracking of trends in the dynamic and rapidly evolving field of cyber risk. Through the automatic processing of large volumes of data, our NLP-powered solution significantly enhances the efficiency and specificity of academic literature searches. We compare the literature categorization results of CyLit to those presented in survey papers or generated by ChatGPT, highlighting the distinctive insights this tool provides into cyber risk research literature. Using NLP techniques, we aim to revolutionize the way researchers discover, analyze, and utilize academic resources, ultimately fostering advancements in various domains of knowledge.",
      "paper_authors": [
        "Linfeng Zhang",
        "Changyue Hu",
        "Zhiyu Quan"
      ],
      "primary_category": "cs.IR",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": null,
      "repo_url": "#"
    },
    "2409.06741": {
      "paper_id": "2409.06741v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06741v1",
      "paper_key": "2409.06741",
      "paper_title": "Generative AI for Requirements Engineering: A Systematic Literature Review",
      "paper_url": "http://arxiv.org/abs/2409.06741v1",
      "paper_abstract": "Context: Generative AI (GenAI) has emerged as a transformative tool in software engineering, with requirements engineering (RE) actively exploring its potential to revolutionize processes and outcomes. The integration of GenAI into RE presents both promising opportunities and significant challenges that necessitate systematic analysis and evaluation. Objective: This paper presents a comprehensive systematic literature review (SLR) analyzing state-of-the-art applications and innovative proposals leveraging GenAI in RE. It surveys studies focusing on the utilization of GenAI to enhance RE processes while identifying key challenges and opportunities in this rapidly evolving field. Method: A rigorous SLR methodology was used to analyze 27 carefully selected primary studies in-depth. The review examined research questions pertaining to the application of GenAI across various RE phases, the models and techniques used, and the challenges encountered in implementation and adoption. Results: The most salient findings include i) a predominant focus on the early stages of RE, particularly the elicitation and analysis of requirements, indicating potential for expansion into later phases; ii) the dominance of large language models, especially the GPT series, highlighting the need for diverse AI approaches; and iii) persistent challenges in domain-specific applications and the interpretability of AI-generated outputs, underscoring areas requiring further research and development. Conclusions: The results highlight the critical need for comprehensive evaluation frameworks, improved human-AI collaboration models, and thorough consideration of ethical implications in GenAI-assisted RE. Future research should prioritize extending GenAI applications across the entire RE lifecycle, enhancing domain-specific capabilities, and developing strategies for responsible AI integration in RE practices.",
      "paper_authors": [
        "Haowei Cheng",
        "Jati H. Husen",
        "Sien Reeve Peralta",
        "Bowen Jiang",
        "Nobukazu Yoshioka",
        "Naoyasu Ubayashi",
        "Hironori Washizaki"
      ],
      "primary_category": "cs.SE",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": null,
      "repo_url": "#"
    },
    "2409.06153": {
      "paper_id": "2409.06153v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06153v1",
      "paper_key": "2409.06153",
      "paper_title": "FAST Ultra-Deep Survey (FUDS): Data Release for FUDS0",
      "paper_url": "http://arxiv.org/abs/2409.06153v1",
      "paper_abstract": "We have used the Five-hundred-meter Aperture Spherical radio Telescope (FAST) to make a blind ultra-deep survey for neutral hydrogen (HI). We present the complete results from the first of six fields (FUDS0). This observation of 95 hours allowed us to achieve a high sensitivity ($\\sim 50~\\mu$Jy beam$^{-1}$) and a high frequency resolution (22.9 kHz) over an area of 0.72 deg$^2$. We detected 128 galaxies in HI distributed over the redshift range of $0<z<0.4$ with HI masses in the range of $6.67 \\leq \\log(M_{\\rm HI}/h_{70}^{-2} \\rm M_\\odot) \\leq 10.92$, and three faint high-velocity clouds (HVCs) with peak column density of $N_{\\rm HI} \\leq 3.1 \\times 10^{17}$ cm$^{-2}$. Of the galaxies, 95 are new detections and six have $z > 0.38$, where no unlensed HI emission has previously been directly detected. Estimates of completeness and reliability are presented for the catalog. Consistency of continuum and HI flux estimates with NVSS and AUDS, respectively, confirms the accuracy of calibration method and data reduction pipeline developed for the full FUDS survey.",
      "paper_authors": [
        "Hongwei Xi",
        "Bo Peng",
        "Lister Staveley-Smith",
        "Bi-Qing For",
        "Bin Liu",
        "Dejian Ding"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": "14 pages, 14 figures",
      "repo_url": "#"
    },
    "2409.06131": {
      "paper_id": "2409.06131v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06131v1",
      "paper_key": "2409.06131",
      "paper_title": "Accelerating Large Language Model Pretraining via LFR Pedagogy: Learn, Focus, and Review",
      "paper_url": "http://arxiv.org/abs/2409.06131v1",
      "paper_abstract": "Large Language Model (LLM) pretraining traditionally relies on autoregressive language modeling on randomly sampled data blocks from web-scale datasets. We take inspiration from human learning techniques like spaced repetition to hypothesize that random data sampling for LLMs leads to high training cost and low quality models which tend to forget data. In order to effectively commit web-scale information to long-term memory, we propose the LFR (Learn, Focus, and Review) pedagogy, a new dynamic training paradigm which focuses and repeatedly reviews complex data blocks at systematic intervals based on the model's learning pace and progress. LFR records the model perplexities for different data blocks and frequently revisits blocks with higher perplexity which are more likely to be forgotten. We pretrain the GPT-2 models (124M - 1.5B) from scratch on the OpenWebText dataset using LFR. We test on downstream tasks from the language modeling, question answering, translation, and problem solving domains to achieve consistently lower perplexity and higher accuracy than the baseline OpenAI models, while obtaining a 20x pretraining speed-up.",
      "paper_authors": [
        "Neha Prakriya",
        "Jui-Nan Yen",
        "Cho-Jui Hsieh",
        "Jason Cong"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-09-10",
      "update_time": "2024-09-10",
      "comments": null,
      "repo_url": "#"
    },
    "2409.06092": {
      "paper_id": "2409.06092v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06092v1",
      "paper_key": "2409.06092",
      "paper_title": "A Census of the beta Pic Moving Group and Other Nearby Associations with Gaia",
      "paper_url": "http://arxiv.org/abs/2409.06092v1",
      "paper_abstract": "I have used the third data release of the Gaia mission to improve the reliability and completeness of membership samples in the beta Pic moving group (BPMG) and other nearby associations with ages of 20-50 Myr (Sco Body, Carina, Columba, chi1 For, Tuc-Hor, IC 2602, IC 2391, NGC 2547). I find that Carina, Columba, and chi1 For are physically related and coeval, and that Carina is the closest fringe of a much larger association. Similarly, Tuc-Hor and IC 2602 form a coeval population that is spatially and kinematically continuous. Both results agree with hypotheses from Gagne et al. (2021). I have used the new catalogs to study the associations in terms of their initial mass functions, X-ray emission, ages, and circumstellar disks. For instance, using the model for Li depletion from Jeffries et al. (2023), I have derived an age of 24.7+0.9/-0.6 Myr for BPMG, which is similar to estimates from previous studies. In addition, I have used infrared photometry from the Wide-field Infrared Survey Explorer to check for excess emission from circumstellar disks among the members of the associations, which has resulted in a dramatic increase in the number of known disks around M stars at ages of 30-50 Myr and a significant improvement in measurements of excess fractions for those spectral types and ages. Most notably, I find that the W3 excess fraction for M0-M6 initially declines with age to a minimum in BPMG <0.015), increases to a maximum in Carina/Columba chi1 For (0.041+0.009/-0.007, 34 Myr), and declines again in the oldest two associations (40-50 Myr). The origin of that peak and the nature of the M dwarf disks at >20 Myr are unclear.",
      "paper_authors": [
        "K. L. Luhman"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "Astronomical Journal, in press",
      "repo_url": "#"
    },
    "2409.06038": {
      "paper_id": "2409.06038v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06038v1",
      "paper_key": "2409.06038",
      "paper_title": "High-Speed Outflows and Dusty Disks during the AGB to PN Transition: The PANORAMA survey",
      "paper_url": "http://arxiv.org/abs/2409.06038v1",
      "paper_abstract": "As mass-losing asymptotic giant branch (AGB) stars evolve to planetary nebulae (PNe), the mass outflow geometries transform from nearly spherical to extreme aspherical. The physical mechanisms governing this transformation are widely believed to be linked to binarity and the associated production of disks and fast jets during transitional (post-AGB) evolutionary stages. We are carrying out a systematic ALMA survey ($P$re-planet$A$ry $N$ebulae high-angular-res$O$lution su$R$vey with $A$L$MA$ or PANORAMA) of a representative sample of bipolar and multipolar post-AGB objects. We have obtained high angular-resolution (0\".1-0\".4) observations of the CO(3--2) and/or 6--5 emission in order to probe the spatio-kinematic structure of the collimated outflows and the central disk/torii. The results are remarkable, generally showing the presence of bipolar or multipolar high-velocity outflows, dense toroidal waists, and in one case, a geometrically-thin circular ring around the central bipolar nebula. A high degree of point-symmetry characterizes the morphology of the mass ejecta. In this contribution, we present these and other highlights from our survey. We aim to use 2D/3D radiative transfer modeling in order to derive accurate outflow momenta, masses and mass-loss rates for our sample, and build hydrodynamical models that can explain the observed spatio-kinematic structures. These results will then be used to distinguish between different classes of PN-shaping binary interaction models.",
      "paper_authors": [
        "Raghvendra Sahai",
        "Javier Alcolea",
        "Bruce Balick",
        "Eric G. Blackman",
        "Valentin Bujarrabal",
        "Arancha Castro-Carrizo",
        "Orsola De Marco",
        "Joel Kastner",
        "Hyosun Kim",
        "Eric Lagadec",
        "Chin-Fei Lee",
        "Laurence Sabin",
        "M. Santander-Garcia",
        "Carmen S\u00e1nchez Contreras",
        "Daniel Tafoya",
        "Toshiya Ueta",
        "Wouter Vlemmings",
        "Albert Zijlstra"
      ],
      "primary_category": "astro-ph.SR",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": null,
      "repo_url": "#"
    },
    "2409.06035": {
      "paper_id": "2409.06035v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06035v1",
      "paper_key": "2409.06035",
      "paper_title": "Analyzing Tumors by Synthesis",
      "paper_url": "http://arxiv.org/abs/2409.06035v1",
      "paper_abstract": "Computer-aided tumor detection has shown great potential in enhancing the interpretation of over 80 million CT scans performed annually in the United States. However, challenges arise due to the rarity of CT scans with tumors, especially early-stage tumors. Developing AI with real tumor data faces issues of scarcity, annotation difficulty, and low prevalence. Tumor synthesis addresses these challenges by generating numerous tumor examples in medical images, aiding AI training for tumor detection and segmentation. Successful synthesis requires realistic and generalizable synthetic tumors across various organs. This chapter reviews AI development on real and synthetic data and summarizes two key trends in synthetic data for cancer imaging research: modeling-based and learning-based approaches. Modeling-based methods, like Pixel2Cancer, simulate tumor development over time using generic rules, while learning-based methods, like DiffTumor, learn from a few annotated examples in one organ to generate synthetic tumors in others. Reader studies with expert radiologists show that synthetic tumors can be convincingly realistic. We also present case studies in the liver, pancreas, and kidneys reveal that AI trained on synthetic tumors can achieve performance comparable to, or better than, AI only trained on real data. Tumor synthesis holds significant promise for expanding datasets, enhancing AI reliability, improving tumor detection performance, and preserving patient privacy.",
      "paper_authors": [
        "Qi Chen",
        "Yuxiang Lai",
        "Xiaoxi Chen",
        "Qixin Hu",
        "Alan Yuille",
        "Zongwei Zhou"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "Accepted as a chapter in the Springer Book: \"Generative Machine\n  Learning Models in Medical Image Computing.\"",
      "repo_url": "#"
    },
    "2409.06011": {
      "paper_id": "2409.06011v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06011v1",
      "paper_key": "2409.06011",
      "paper_title": "Investigating the effects of housing instability on depression, anxiety, and mental health treatment in childhood and adolescence",
      "paper_url": "http://arxiv.org/abs/2409.06011v1",
      "paper_abstract": "Housing instability is a widespread phenomenon in the United States. In combination with other social determinants of health, housing instability affects children's overall health and development. Drawing on data from the 2022 National Survey of Children's Health, we employed multiple logistic regression models to understand how sociodemographic factors, especially housing instability, affect mental health outcomes and treatment access for youth aged 6-17 years. Our results show that youth facing housing instability have a higher likelihood of experiencing anxiety (OR: 1.42, p<0.001) and depression (OR: 1.57, p<0.001). Furthermore, youth experiencing both mental health conditions and housing instability are significantly less likely to receive mental health services in the past year, indicating the substantial barriers they face in accessing mental health care. Based on our findings, we highlight opportunities for digital mental health interventions to provide children experiencing housing instability with more accessible and consistent mental health services.",
      "paper_authors": [
        "Rachael Zehrung",
        "Di Hu",
        "Yawen Guo",
        "Kai Zheng",
        "Yunan Chen"
      ],
      "primary_category": "cs.HC",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "Paper accepted for the proceedings of the 2024 American Medical\n  Informatics Association Annual Symposium (AMIA)",
      "repo_url": "#"
    },
    "2409.06009": {
      "paper_id": "2409.06009v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06009v1",
      "paper_key": "2409.06009",
      "paper_title": "Cosmological constraints from angular homogeneity scale measurements",
      "paper_url": "http://arxiv.org/abs/2409.06009v1",
      "paper_abstract": "In this paper, we obtain new measurements of the angular homogeneity scale ($\\theta_H$) from the BOSS DR12 and eBOSS DR16 catalogs of Luminous Red Galaxies of the Sloan Digital Sky Survey. Considering the flat $\\Lambda$CDM model, we use the $\\theta_H(z)$ data to constrain the matter density parameter ($\\Omega_{m0}$) and the Hubble constant ($H_{0}$). We find $H_0 = 65.7 \\pm 7.0$ km s$^{-1}$ Mpc$^{-1}$ and $\\Omega_{m0}>0.293$. By combining the $\\theta_H$ measurements with current Baryon Acoustic Oscillations (BAO) and Type Ia Supernova (SN) data, we obtain $H_{0}= 69.9^{+4.9}_{-4.5}$ km s$^{-1}$ Mpc$^{-1}$ and $\\Omega_{m0} = 0.294^{+0.013}_{-0.015}$ ($\\theta_H$ + BAO) and $H_{0}=70.7^{+5.2}_{-4.1}$ km s$^{-1}$ Mpc$^{-1}$ and $\\Omega_{m0}=0.299 \\pm 0.022$ ($\\theta_H$ + SN). We show that $\\theta_H$ measurements help break the BAO and SN degeneracies concerning $H_0$, as they do not depend on the sound horizon scale at the drag epoch or the SN absolute magnitude value obtained from the distance ladder method. Hence, despite those constraints being loose compared to other probes, $\\theta_H$ data may provide an independent cosmological probe of $H_0$ in light of the Hubble tension. For completeness, we also forecast the constraining power of future $\\theta_H$ data via Monte Carlo simulations. Considering a relative error of the order of 1$\\%$, we obtain competitive constraints on $\\Omega_{m0}$ and $H_0$ ($\\approx 5\\%$ error) from the joint analysis with current SN and BAO measurements.",
      "paper_authors": [
        "Xiaoyun Shao",
        "Carlos A. P. Bengaly",
        "Rodrigo S. Gon\u00e7alves",
        "Gabriela C. Carvalho",
        "Jailson Alcaniz"
      ],
      "primary_category": "astro-ph.CO",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "10 pages, 10 figures",
      "repo_url": "#"
    },
    "2409.06001": {
      "paper_id": "2409.06001v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06001v1",
      "paper_key": "2409.06001",
      "paper_title": "The cosmological analysis of X-ray cluster surveys: VI. Inference based on analytically simulated observable diagrams",
      "paper_url": "http://arxiv.org/abs/2409.06001v1",
      "paper_abstract": "The number density of galaxy clusters across mass and redshift has been established as a powerful cosmological probe. Cosmological analyses with galaxy clusters traditionally employ scaling relations. However, many challenges arise from this approach as the scaling relations are highly scattered, may be ill-calibrated, depend on the cosmology, and contain many nuisance parameters with low physical significance. In this paper, we use a simulation-based inference method utilizing artificial neural networks to optimally extract cosmological information from a shallow X-ray survey of galaxy clusters, solely using count rates (CR), hardness ratios (HR), and redshifts. This procedure enables us to conduct likelihood-free inference of cosmological parameters $\\Omega_{\\mathrm{m}}$ and $\\sigma_8$. We analytically generate simulations of galaxy cluster distribution in a CR, HR space in multiple redshift bins based on totally random combinations of cosmological and scaling relation parameters. We train Convolutional Neural Networks (CNNs) to retrieve the cosmological parameters from these simulations. We then use neural density estimation (NDE) neural networks to predict the posterior probability distribution of $\\Omega_{\\mathrm{m}}$ and $\\sigma_8$ given an input galaxy cluster sample. The 1 $\\sigma$ errors of our density estimator on one of the target testing simulations are 1000 deg$^2$: 15.2% for $\\Omega_{\\mathrm{m}}$ and 10.0% for $\\sigma_8$; 10000 deg$^2$: 9.6% for $\\Omega_{\\mathrm{m}}$ and 5.6% for $\\sigma_8$. We also compare our results with Fisher analysis. We demonstrate, as a proof of concept, that it is possible to calculate cosmological predictions of $\\Omega_{\\mathrm{m}}$ and $\\sigma_8$ from a galaxy cluster population without explicitly computing cluster masses and even, the scaling relation coefficients, thus avoiding potential biases resulting from such a procedure. [abridged]",
      "paper_authors": [
        "M. Kosiba",
        "N. Cerardi",
        "M. Pierre",
        "F. Lanusse",
        "C. Garrel",
        "N. Werner",
        "M. Shalak"
      ],
      "primary_category": "astro-ph.CO",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "14 pages, 4 tables, 15 figures, accepted for publication in A&A -\n  pre-proofs version",
      "repo_url": "#"
    },
    "2409.05987": {
      "paper_id": "2409.05987v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05987v1",
      "paper_key": "2409.05987",
      "paper_title": "Simulated performance of energy-resolving detectors towards exoplanet imaging with the Habitable Worlds Observatory",
      "paper_url": "http://arxiv.org/abs/2409.05987v1",
      "paper_abstract": "One of the primary science goals of the Habitable Worlds Observatory (HWO) as defined by the Astro2020 decadal survey is the imaging of the first Earth-like planet around a Sun-like star. A key technology gap towards reaching this goal are the development of ultra-low-noise photon counting detectors capable of measuring the incredibly low count rates coming from these planets which are at contrasts of $\\sim 1 \\times 10^{-10}$. Superconducting energy-resolving detectors (ERDs) are a promising technology for this purpose as, despite their technological challenges, needing to be cooled below their superconducting transition temperature ($< 1\\mathrm{K}$), they have essentially zero read noise, dark current, or clock-induced charge, and can get the wavelength of each incident photon without the use of additional throughput-reducing filters or gratings that spread light over many pixels. The use of these detectors on HWO will not only impact the science of the mission by decreasing the required exposure times for exo-Earth detection and characterization, but also in a wavefront sensing and control context when used for starlight suppression to generate a dark zone. We show simulated results using both an EMCCD and an ERD to ``dig a dark zone'' demonstrating that ERDs can achieve the same final contrast as an EMCCD in about half of the total time. We also perform a simple case study using an exposure time calculator tool called the Error Budget Software (EBS) to determine the required integration times to detect water for HWO targets of interest using both EMCCDs and ERDs. This shows that once a dark zone is achieved, using an ERD can decrease these exposure times by factors of 1.5--2 depending on the specific host star properties.",
      "paper_authors": [
        "Sarah Steiger",
        "Laurent Pueyo",
        "Emiel H. Por",
        "Pin Chen",
        "R\u00e9mi Soummer",
        "Rapha\u00ebl Pourcelot",
        "Iva Laginja",
        "Vanessa P. Bailey"
      ],
      "primary_category": "astro-ph.IM",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "13 pages, 7 figures",
      "repo_url": "#"
    },
    "2409.05985": {
      "paper_id": "2409.05985v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05985v1",
      "paper_key": "2409.05985",
      "paper_title": "Advance and Refinement: The Evolution of UAV Detection and Classification Technologies",
      "paper_url": "http://arxiv.org/abs/2409.05985v1",
      "paper_abstract": "This review provides a detailed analysis of the advancements in unmanned aerial vehicle (UAV) detection and classification systems from 2020 to today. It covers various detection methodologies such as radar, radio frequency, optical, and acoustic sensors, and emphasizes their integration via sophisticated sensor fusion techniques. The fundamental technologies driving UAV detection and classification are thoroughly examined, with a focus on their accuracy and range. Additionally, the paper discusses the latest innovations in artificial intelligence and machine learning, illustrating their impact on improving the accuracy and efficiency of these systems. The review concludes by predicting further technological developments in UAV detection, which are expected to enhance both performance and reliability.",
      "paper_authors": [
        "Vladislav Semenyuk",
        "Ildar Kurmashev",
        "Alberto Lupidi",
        "Dmitriy Alyoshin",
        "Liliya Kurmasheva",
        "Alessandro Cantelli-Forti"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "19 pages, 5 figures",
      "repo_url": "#"
    },
    "2409.05979": {
      "paper_id": "2409.05979v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05979v1",
      "paper_key": "2409.05979",
      "paper_title": "CCAT: A status update on the EoR-Spec instrument module for Prime-Cam",
      "paper_url": "http://arxiv.org/abs/2409.05979v1",
      "paper_abstract": "The Epoch of Reionization Spectrometer (EoR-Spec) is an upcoming Line Intensity Mapping (LIM) instrument designed to study the evolution of the early universe (z = 3.5 to 8) by probing the redshifted [CII] 158 $\\mu$m fine-structure line from aggregates of galaxies. The [CII] emission is an excellent tracer of star formation since it is the dominant cooling line from neutral gas heated by OB star light and thus can be used to probe the reionization of the early Universe due to star formation. EoR-Spec will be deployed on Prime-Cam, a modular direct-detection receiver for the 6-meter Fred Young Submillimeter Telescope (FYST), currently under construction by CPI Vertex Antennentechnik GmbH and to be installed near the summit of Cerro Chajnantor in the Atacama Desert. This instrument features an image plane populated with more than 6500 Microwave Kinetic Inductance Detectors (MKIDs) that are illuminated by a 4-lens optical design with a cryogenic, scanning Fabry-Perot Interferometer (FPI) at the pupil of the optical system. The FPI is designed to provide a spectral resolving power of $R\\sim100$ over the full spectral range of 210--420 GHz. EoR-Spec will tomographically survey the E-COSMOS and E-CDFS fields with a depth of about 4000 hours over a 5 year period. Here we give an update on EoR-Spec's final mechanical/optical design and the current status of fabrication, characterization and testing towards first light in 2026.",
      "paper_authors": [
        "Rodrigo Freundt",
        "Yaqiong Li",
        "Doug Henke",
        "Jason Austermann",
        "James R. Burgoyne",
        "Scott Chapman",
        "Steve K. Choi",
        "Cody J. Duell",
        "Zach Huber",
        "Michael Niemack",
        "Thomas Nikola",
        "Lawrence Lin",
        "Dominik A. Riechers",
        "Gordon Stacey",
        "Anna K. Vaskuri",
        "Eve M. Vavagiakis",
        "Jordan Wheeler",
        "Bugao Zou"
      ],
      "primary_category": "astro-ph.IM",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "Presented at SPIE Millimeter, Submillimeter, and Far-Infrared\n  Detectors and Instrumentation for Astronomy XII",
      "repo_url": "#"
    },
    "2409.05969": {
      "paper_id": "2409.05969v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05969v1",
      "paper_key": "2409.05969",
      "paper_title": "Challenges and Opportunities of Teaching Data Visualization Together with Data Science",
      "paper_url": "http://arxiv.org/abs/2409.05969v1",
      "paper_abstract": "With the increasing amount of data globally, analyzing and visualizing data are becoming essential skills across various professions. It is important to equip university students with these essential data skills. To learn, design, and develop data visualization, students need knowledge of programming and data science topics. Many university programs lack dedicated data science courses for undergraduate students, making it important to introduce these concepts through integrated courses. However, combining data science and data visualization into one course can be challenging due to the time constraints and the heavy load of learning. In this paper, we discuss the development of teaching data science and data visualization together in one course and share the results of the post-course evaluation survey. From the survey's results, we identified four challenges, including difficulty in learning multiple tools and diverse data science topics, varying proficiency levels with tools and libraries, and selecting and cleaning datasets. We also distilled five opportunities for developing a successful data science and visualization course. These opportunities include clarifying the course structure, emphasizing visualization literacy early in the course, updating the course content according to student needs, using large real-world datasets, learning from industry professionals, and promoting collaboration among students.",
      "paper_authors": [
        "Shri Harini Ramesh",
        "Fateme Rajabiyazdi"
      ],
      "primary_category": "cs.HC",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "7 pages, to be published in IEEE Explore, accepted to EduVis'24",
      "repo_url": "#"
    },
    "2409.05946": {
      "paper_id": "2409.05946v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05946v1",
      "paper_key": "2409.05946",
      "paper_title": "Dust-UV offsets in high-redshift galaxies in the Cosmic Dawn III simulation",
      "paper_url": "http://arxiv.org/abs/2409.05946v1",
      "paper_abstract": "We investigate the spatial offsets between dust and ultraviolet (UV) emission in high-redshift galaxies using the Cosmic Dawn III (CoDa III) simulation, a state-of-the-art fully coupled radiation-hydrodynamics cosmological simulation. Recent observations have revealed puzzling spatial disparities between ALMA dust continuum and UV emission as seen by HST and JWST in galaxies at z=5-7, compelling us to propose a physical interpretation of such offsets. Our simulation, which incorporates a dynamical dust model, naturally reproduces these offsets in massive, UV-bright galaxies (log$_{10}$(M$_{\\rm{DM}}$/M$_{\\odot}$)>11.5, M$_{\\rm{AB1500}}$<-20). We find that dust-UV offsets increase with halo mass and UV brightness, reaching up to $\\sim 2$ pkpc for the most massive systems, in good agreement with observational data from the ALPINE and REBELS surveys. Our analysis reveals that these offsets primarily result from severe dust extinction in galactic centers rather than a misalignment between dust and stellar mass distributions. The dust remains well-aligned with the bulk stellar component, and we predict the dust continuum should therefore align well with the stellar rest-frame NIR component, less affected by dust attenuation. This study provides crucial insights into the complex interplay between star formation, dust distribution, and observed galaxy morphologies during the epoch of reionization, highlighting the importance of dust in shaping the appearance of early galaxies at UV wavelengths.",
      "paper_authors": [
        "Pierre Ocvirk",
        "Joseph S. W. Lewis",
        "Luke Conaboy",
        "Yohan Dubois",
        "Matthieu Bethermin",
        "Jenny G. Sorce",
        "Dominique Aubert",
        "Paul R. Shapiro",
        "Taha Dawoodbhoy",
        "Joohyun Lee",
        "Romain Teyssier",
        "Gustavo Yepes",
        "Stefan Gottl\u00f6ber",
        "Ilian T. Iliev",
        "Kyungjin Ahn",
        "Hyunbae Park"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "9 pages, 5 figures, submitted to A&A",
      "repo_url": "#"
    },
    "2409.05948": {
      "paper_id": "2409.05948v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05948v1",
      "paper_key": "2409.05948",
      "paper_title": "RUBIES: a complete census of the bright and red distant Universe with JWST/NIRSpec",
      "paper_url": "http://arxiv.org/abs/2409.05948v1",
      "paper_abstract": "We present the Red Unknowns: Bright Infrared Extragalactic Survey (RUBIES), providing JWST/NIRSpec spectroscopy of red sources selected across ~150 arcmin$^2$ from public JWST/NIRCam imaging in the UDS and EGS fields. RUBIES novel observing strategy offers a well-quantified selection function: the survey is optimised to reach high (>70%) completeness for bright and red (F150W-F444W>2) sources that are very rare. To place these rare sources in context, we simultaneously observe a reference sample of the 2<z<7 galaxy population, sampling sources at a rate that is inversely proportional to their number density in the 3D space of F444W magnitude, F150W-F444W colour, and photometric redshift. In total, RUBIES observes ~3000 targets across $1<z_{phot}<10$ with both the PRISM and G395M dispersers, and ~1500 targets at $z_{phot}>3$ using only the G395M disperser. The RUBIES data reveal a highly diverse population of red sources that span a broad redshift range ($z_{spec}\\sim1-9$), with photometric redshift scatter and outlier fraction that are 3 times higher than for similarly bright sources that are less red. This diversity is not apparent from the photometric SEDs. Only spectroscopy reveals that the SEDs encompass a mixture of galaxies with dust-obscured star formation, extreme line emission, a lack of star formation indicating early quenching, and luminous active galactic nuclei. As a first demonstration of our broader selection function we compare the stellar masses and rest-frame U-V colours of the red sources and our reference sample: red sources are typically more massive ($M_*\\sim10^{10-11.5} M_\\odot$) across all redshifts. However, we find that the most massive systems span a wide range in U-V colour. We describe our data reduction procedure and data quality, and publicly release the reduced RUBIES data and vetted spectroscopic redshifts of the first half of the survey through the DJA.",
      "paper_authors": [
        "Anna de Graaff",
        "Gabriel Brammer",
        "Andrea Weibel",
        "Zach Lewis",
        "Michael V. Maseda",
        "Pascal A. Oesch",
        "Rachel Bezanson",
        "Leindert A. Boogaard",
        "Nikko J. Cleri",
        "Olivia R. Cooper",
        "Rashmi Gottumukkala",
        "Jenny E. Greene",
        "Michaela Hirschmann",
        "Raphael E. Hviding",
        "Harley Katz",
        "Ivo Labb\u00e9",
        "Joel Leja",
        "Jorryt Matthee",
        "Ian McConachie",
        "Tim B. Miller",
        "Rohan P. Naidu",
        "Sedona H. Price",
        "Hans-Walter Rix",
        "David J. Setton",
        "Katherine A. Suess",
        "Bingjie Wang",
        "Katherine E. Whitaker",
        "Christina C. Williams"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "21 pages, 16 figures; submitted to A&A",
      "repo_url": "#"
    },
    "2409.05940": {
      "paper_id": "2409.05940v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05940v1",
      "paper_key": "2409.05940",
      "paper_title": "The MAGPI Survey: Orbital distributions, intrinsic shapes, and mass profiles for MAGPI-like Eagle galaxies using Schwarzschild dynamical models",
      "paper_url": "http://arxiv.org/abs/2409.05940v1",
      "paper_abstract": "Schwarzschild dynamical models are now regularly employed in large surveys of galaxies in the local and distant Universe to derive information on galaxies' intrinsic properties such as their orbital structure and their (dark matter and stellar) mass distribution. Comparing the internal orbital structures and mass distributions of galaxies in the distant Universe with simulations is key to understanding what physical processes are responsible for shaping galaxy properties. However it is first crucial to understand whether observationally derived properties are directly comparable with intrinsic ones in simulations. To assess this, we build Schwarzschild dynamical models for MUSE-like IFS cubes (constructed to be like those obtained by the MAGPI survey) of 75 galaxies at z ~ 0.3 from the Eagle simulations. We compare the true particle-derived properties with the galaxies' model-derived properties. In general, we find that the models can recover the true galaxy properties qualitatively well, with the exception of the enclosed dark matter, where we find a median offset of 48%, which is due to the assumed NFW profile not being able to reproduce the dark matter distribution in the inner region of the galaxies. We then compare our model-derived properties with Schwarzschild models-derived properties of observed MAGPI galaxies and find good agreement between MAGPI and Eagle: the majority of our galaxies (57%) have non-oblate shapes within 1 effective radius. More triaxial galaxies show higher fractions of hot orbits in their inner regions and tend to be more radially anisotropic.",
      "paper_authors": [
        "Giulia Santucci",
        "Claudia Del P. Lagos",
        "Katherine E. Harborne",
        "Caro Derkenne",
        "Adriano Poci",
        "Sabine Thater",
        "Richard M. McDermid",
        "J. Trevor Mendel",
        "Emily Wisnioski",
        "Scott M. Croom",
        "Anna Ferr\u00e9-Mateu",
        "Eric G. M. Muller",
        "Jesse van de Sande",
        "Gauri Sharma",
        "Sarah M. Sweet",
        "Takafumi Tsukui",
        "Lucas M. Valenzuela",
        "Glenn van de Ven",
        "Tayyaba Zafar"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "22 pages. Accepted for publication in MNRAS",
      "repo_url": "#"
    },
    "2409.05848": {
      "paper_id": "2409.05848v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05848v1",
      "paper_key": "2409.05848",
      "paper_title": "Enhancement of fusion reactivities using non-Maxwellian energy distributions",
      "paper_url": "http://arxiv.org/abs/2409.05848v1",
      "paper_abstract": "We discuss conditions for the enhancement of fusion reactivities arising from different choices of energy distribution functions for the reactants. The key element for potential gains in fusion reactivity is identified in the functional dependence of the tunnellng coefficient upon the energy, ensuring the existence of a finite range of temperatures for which reactivity of fusion processes is boosted with respect to the Maxwellian case. This is shown, using a convenient parameterization of the tunneling coefficient dependence upon the energy, analytically in the simplified case of a bimodal Maxwell-Boltzmann distribution, and numerically for kappa-distributions. We then consider tunneling potentials progressively better approximating fusion processes, and evaluate in each case the average reactivity in the case of kappa-distributions.",
      "paper_authors": [
        "Ben I. Squarer",
        "Carlo Presilla",
        "Roberto Onofrio"
      ],
      "primary_category": "physics.plasm-ph",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "12 pages, 7 figures",
      "repo_url": "#"
    },
    "2409.05838": {
      "paper_id": "2409.05838v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05838v1",
      "paper_key": "2409.05838",
      "paper_title": "Measuring the speed of light with cosmological observations: current constraints and forecasts",
      "paper_url": "http://arxiv.org/abs/2409.05838v1",
      "paper_abstract": "We measure the speed of light with current observations, such as Type Ia Supernova, galaxy ages, radial BAO mode, as well as simulations of future redshift surveys and gravitational waves as standard sirens. By means of a Gaussian Process reconstruction, we find that the precision of such measurements can be improved from roughly 6\\% to 1.5-2\\%, in light of these forthcoming observations. This result demonstrates that we will be able to perform a cosmological measurement of a fundamental physical constant with unprecedented precision, which will help us underpinning if its value is truly consistent with local measurements, as predicted by the standard model of Cosmology.",
      "paper_authors": [
        "Jaiane Santos",
        "Carlos Bengaly",
        "Jonathan Morais",
        "Rodrigo S. Gon\u00e7alves"
      ],
      "primary_category": "astro-ph.CO",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "12 pages, 1 appendix, 9 figures",
      "repo_url": "#"
    },
    "2409.05826": {
      "paper_id": "2409.05826v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05826v1",
      "paper_key": "2409.05826",
      "paper_title": "Valuation system for telescope observation time proposals for the Gemini Observatory",
      "paper_url": "http://arxiv.org/abs/2409.05826v1",
      "paper_abstract": "The preparation of a telescope observation time proposal is a recurring activity in observational astronomy. It is a necessary investment of time and effort to obtain data to advance a research topic. Therefore, the success of an observation proposal is a condition for progress in observational research. This guide was created to offer a straightforward, practical, and comprehensive resource for applicants who are preparing a Gemini Observatory proposal. It reviews the fundamentals of an observation proposal, including its content and evaluation criteria, to help applicants organize their submissions effectively and improve specific aspects of their presentations. This manuscript is inspired by the recommendations of the User Advisory Council established in the documents ''Criterios de evaluaci\\'on de propuestas por parte del NTAC'' (National Time Allocation Committee), ''Consideraciones b\\'asicas para la presentaci\\'on de propuestas Gemini'', and in experiences of members of the NTAC.",
      "paper_authors": [
        "C. G. D\u00edaz",
        "R. Petrucci",
        "L. V. Ferrero",
        "E. Jofr\u00e9"
      ],
      "primary_category": "astro-ph.IM",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "White paper, 7 pages, 3 tables",
      "repo_url": "#"
    },
    "2409.05811": {
      "paper_id": "2409.05811v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05811v1",
      "paper_key": "2409.05811",
      "paper_title": "Learning control of underactuated double pendulum with Model-Based Reinforcement Learning",
      "paper_url": "http://arxiv.org/abs/2409.05811v1",
      "paper_abstract": "This report describes our proposed solution for the second AI Olympics competition held at IROS 2024. Our solution is based on a recent Model-Based Reinforcement Learning algorithm named MC-PILCO. Besides briefly reviewing the algorithm, we discuss the most critical aspects of the MC-PILCO implementation in the tasks at hand.",
      "paper_authors": [
        "Niccol\u00f2 Turcato",
        "Alberto Dalla Libera",
        "Giulio Giacomuzzo",
        "Ruggero Carli",
        "Diego Romeres"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": null,
      "repo_url": "#"
    },
    "2409.05795": {
      "paper_id": "2409.05795v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05795v1",
      "paper_key": "2409.05795",
      "paper_title": "Environmental dependence on galaxy-halo connections for satellites using HSC weak lensing",
      "paper_url": "http://arxiv.org/abs/2409.05795v1",
      "paper_abstract": "We present the luminosity-halo mass relations of satellite (sLHMRs) galaxies in the SDSS redMaPPer cluster catalogue and the effects of the dense cluster environment on subhalo mass evolution. We use data from the Subaru Hyper Suprime-Cam survey Year-3 catalogue of galaxy shapes to measure the weak lensing signal around these satellites. This signal serves as a probe of the matter distribution around the satellites, thereby providing the masses of their associated subhalos. We bin our satellites based on physical observable quantities such as their luminosity or the host cluster's richness, combined with their cluster-centric radial separations. Our results indicate that although more luminous satellites tend to reside in more massive halos, the sLHMRs depend on the distance of the satellite from the cluster centre. Subhalos near the cluster centre (within $<0.3 h^{-1}Mpc$) are stripped of mass. Consequently, the ratio of subhalo mass to luminosity decreases near the cluster centre. For low luminosity galaxies ($L < 10^{10} h^{-2}L_{\\odot}$), the lack of evidence of increasing subhalo masses with luminosity shows the impact of tidal stripping. We also present stellar-to-subhalo mass relations (sSHMRs) for our satellite sample evolving at different cluster-centric separations. Inferred sSHMRs in the outer radial bin appear to match that observed for the field galaxies. We show that the sSHMRs from the mock-redMaPPer run on galaxy catalogues generated by the empirical UniverseMachine galaxy formation model are in good agreement with our observational results. Satellites, when binned based on the host cluster's richness, show very little dependence of the subhalo mass on the richness.",
      "paper_authors": [
        "Amit Kumar",
        "Surhud More"
      ],
      "primary_category": "astro-ph.CO",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": null,
      "repo_url": "#"
    },
    "2409.05775": {
      "paper_id": "2409.05775v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05775v1",
      "paper_key": "2409.05775",
      "paper_title": "A discussion on estimating small bodies taxonomies using phase curves results",
      "paper_url": "http://arxiv.org/abs/2409.05775v1",
      "paper_abstract": "Upcoming large multiwavelength photometric surveys will provide a leap in our understanding of small body populations, among other fields of modern astrophysics. Serendipitous observations of small bodies in different orbital locations allow us to study diverse phenomena related to how their surfaces scatter solar light.   In particular, multiple observations of the same object in different epochs permit us to construct their phase curves to obtain absolute magnitudes and phase coefficients. In this work, we tackle a series of long-used relationships associating these phase coefficients with the taxa of small bodies and suggest that some may need to be revised in the light of large-number statistics.",
      "paper_authors": [
        "Alvaro Alvarez-Candal"
      ],
      "primary_category": "astro-ph.EP",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "21 pages, 5 figures, accepted in Planetary and Space Science on Sept.\n  9, 2024",
      "repo_url": "#"
    },
    "2409.05716": {
      "paper_id": "2409.05716v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05716v1",
      "paper_key": "2409.05716",
      "paper_title": "Implications of feedback solutions to the $S_8$ tension for the baryon fractions of galaxy groups and clusters",
      "paper_url": "http://arxiv.org/abs/2409.05716v1",
      "paper_abstract": "Recent large-scale structure (LSS) surveys have revealed a persistent tension in the value of $S_8$ compared to predictions from the standard cosmological model. This tension may suggest the need for new physics beyond the standard model, but an accurate characterisation of baryonic effects is essential to avoid biases. Although some studies indicate that baryonic effects are too small to resolve this tension, others propose that more aggressive feedback mechanisms could reconcile differences between cosmic microwave background (CMB) measurements and low-redshift LSS observations. In this paper, we investigate the role of baryonic effects in alleviating the $S_8$ tension. We extend the SP(k) model (Salcido et al. 2023), which was trained on hundreds of cosmological hydrodynamical simulations to map the suppression of the matter power spectrum to the baryon fraction in groups and clusters, to predict the required baryon fraction for a given $P(k)$ suppression. We then compare predictions from recent cosmic shear (weak lensing) analyses with the latest baryon budget measurements from X-ray and weak gravitational lensing studies. Our findings show that studies marginalising over baryonic effects while fixing cosmological parameters to a Planck-like cosmology predict strong $P(k)$ suppression and baryon fractions that are much lower than existing low-redshift baryon budget estimates of galaxy groups and clusters. Conversely, most studies that marginalise over both cosmological parameters and baryonic effects imply baryon fractions that are consistent with observations but lower values of $S_8$ than inferred from the CMB. Unless the observed baryon fractions are biased high by a factor of several, these results suggest that a mechanism beyond baryonic physics alone is required to modify or slow down the growth of structure in the universe in order to resolve the $S_8$ tension.",
      "paper_authors": [
        "Jaime Salcido",
        "Ian G. McCarthy"
      ],
      "primary_category": "astro-ph.CO",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "Submitted to MNRAS. 12 pages, 7 figures",
      "repo_url": "#"
    },
    "2409.05703": {
      "paper_id": "2409.05703v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05703v1",
      "paper_key": "2409.05703",
      "paper_title": "The Influence of Task and Group Disparities over Users' Attitudes Toward Using Large Language Models for Psychotherapy",
      "paper_url": "http://arxiv.org/abs/2409.05703v1",
      "paper_abstract": "The population suffering from mental health disorders has kept increasing in recent years. With the advancements in large language models (LLMs) in diverse fields, LLM-based psychotherapy has also attracted increasingly more attention. However, the factors influencing users' attitudes to LLM-based psychotherapy have rarely been explored. As the first attempt, this paper investigated the influence of task and group disparities on user attitudes toward LLM-based psychotherapy tools. Utilizing the Technology Acceptance Model (TAM) and Automation Acceptance Model (AAM), based on an online survey, we collected and analyzed responses from 222 LLM-based psychotherapy users in mainland China. The results revealed that group disparity (i.e., mental health conditions) can influence users' attitudes toward LLM tools. Further, one of the typical task disparities, i.e., the privacy concern, was not found to have a significant effect on trust and usage intention. These findings can guide the design of future LLM-based psychotherapy services.",
      "paper_authors": [
        "Qihang He",
        "Jiyao Wang",
        "Dengbo He"
      ],
      "primary_category": "cs.HC",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "Accepted by HFES 2025",
      "repo_url": "#"
    },
    "2409.05686": {
      "paper_id": "2409.05686v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05686v1",
      "paper_key": "2409.05686",
      "paper_title": "The Galaxy Activity, Torus, and Outflow Survey (GATOS). V: Unveiling PAH survival and resilience in the circumnuclear regions of AGN with JWST",
      "paper_url": "http://arxiv.org/abs/2409.05686v1",
      "paper_abstract": "We analyze JWST MIRI/MRS observations of the infrared PAH bands in the nuclear and circumnuclear regions of local AGN from the GATOS Survey. In this work, we examine the PAH properties in the circumnuclear regions of AGN and AGN-outflows, and compare them to those in star-forming regions and the innermost regions of AGN. This study employs 4.9-28.1 micron sub-arcsecond angular resolution data to investigate the properties of PAH in three nearby sources (DL~30-40 Mpc). Our findings align with previous JWST studies, showing that the central regions of AGN show a larger fraction of neutral PAH molecules (i.e. elevated 11.3/6.2 and 11.3/7.7 PAH ratios) compared to star-forming galaxies. We find that the AGN might affect not only the PAH population in the innermost region but also in the extended regions up to ~kpc scales. By comparing our observations to PAH diagnostic diagrams, we find that, in general, regions located in the projected direction of the AGN-outflow occupy similar positions on the PAH diagnostic diagrams as those of the innermost regions of AGN. Star-forming regions that are not affected by the AGN in these galaxies share the same part of the diagram as Star-forming galaxies. We examine the potential of the PAH-H2 diagram to disentangle AGN versus star-forming activity. Our results suggest that in Sy-like AGN, illumination and feedback from the AGN might affect the PAH population at nuclear and kpc scales, in particular, the ionization state of the PAH grains. However, PAH sizes are rather similar. The carriers of the ionized PAH bands (6.2 and 7.7 micron) are less resilience than those of neutral PAH bands (11.3 micron), which might be particularly important for strongly AGN-host coupled systems. Therefore, caution must be applied when using PAH bands as star-formation rate indicators in these systems even at kpc scales, with the ionized ones being more affected by the AGN.",
      "paper_authors": [
        "I. Garc\u00eda-Bernete",
        "D. Rigopoulou",
        "F. R. Donnan",
        "A. Alonso-Herrero",
        "M. Pereira-Santella",
        "T. Shimizu",
        "R. Davies",
        "P. F. Roche",
        "S. Garc\u00eda-Burillo",
        "A. Labiano",
        "L. Hermosa Mu\u00f1oz",
        "L. Zhang",
        "A. Audibert",
        "E. Bellocchi",
        "A. Bunker",
        "F. Combes",
        "D. Delaney",
        "D. Esparza-Arredondo",
        "P. Gandhi",
        "O. Gonz\u00e1lez-Mart\u00edn",
        "S. F. H\u00f6nig",
        "M. Imanishi",
        "E. K. S. Hicks",
        "L. Fuller",
        "M. Leist",
        "N. A. Levenson",
        "E. Lopez-Rodriguez",
        "C. Packham",
        "C. Ramos Almeida",
        "C. Ricci",
        "M. Stalevski",
        "M. Villar Mart\u00edn",
        "M. J. Ward"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "Accepted for publication in A&A. 21 pages, 13 Figures",
      "repo_url": "#"
    },
    "2409.05682": {
      "paper_id": "2409.05682v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05682v1",
      "paper_key": "2409.05682",
      "paper_title": "ForestFlow: cosmological emulation of Lyman-$\u03b1$ forest clustering from linear to nonlinear scales",
      "paper_url": "http://arxiv.org/abs/2409.05682v1",
      "paper_abstract": "On large scales, measurements of the Lyman-$\\alpha$ forest offer insights into the expansion history of the Universe, while on small scales, these impose strict constraints on the growth history, the nature of dark matter, and the sum of neutrino masses. This work introduces ForestFlow, a cosmological emulator designed to bridge the gap between large- and small-scale Lyman-$\\alpha$ forest analyses. Using conditional normalizing flows, ForestFlow emulates the 2 Lyman-$\\alpha$ linear biases ($b_\\delta$ and $b_\\eta$) and 6 parameters describing small-scale deviations of the 3D flux power spectrum ($P_\\mathrm{3D}$) from linear theory. These 8 parameters are modeled as a function of cosmology $\\unicode{x2013}$ the small-scale amplitude and slope of the linear power spectrum $\\unicode{x2013}$ and the physics of the intergalactic medium. Thus, in combination with a Boltzmann solver, ForestFlow can predict $P_\\mathrm{3D}$ on arbitrarily large (linear) scales and the 1D flux power spectrum ($P_\\mathrm{1D}$) $\\unicode{x2013}$ the primary observable for small-scale analyses $\\unicode{x2013}$ without the need for interpolation or extrapolation. Consequently, ForestFlow enables for the first time multiscale analyses. Trained on a suite of 30 fixed-and-paired cosmological hydrodynamical simulations spanning redshifts from $z=2$ to $4.5$, ForestFlow achieves $3$ and $1.5\\%$ precision in describing $P_\\mathrm{3D}$ and $P_\\mathrm{1D}$ from linear scales to $k=5\\,\\mathrm{Mpc}^{-1}$ and $k_\\parallel=4\\,\\mathrm{Mpc}^{-1}$, respectively. Thanks to its parameterization, the precision of the emulator is also similar for both ionization histories and two extensions to the $\\Lambda$CDM model $\\unicode{x2013}$ massive neutrinos and curvature $\\unicode{x2013}$ not included in the training set. ForestFlow will be crucial for the cosmological analysis of Lyman-$\\alpha$ forest measurements from the DESI survey.",
      "paper_authors": [
        "J. Chaves-Montero",
        "L. Cabayol-Garcia",
        "M. Lokken",
        "A. Font-Ribera",
        "J. Aguilar",
        "S. Ahlen",
        "D. Bianchi",
        "D. Brooks",
        "T. Claybaugh",
        "S. Cole",
        "A. de la Macorra",
        "S. Ferraro",
        "J. E. Forero-Romero",
        "E. Gazta\u00f1aga",
        "S. Gontcho A Gontcho",
        "G. Gutierrez",
        "K. Honscheid",
        "R. Kehoe",
        "D. Kirkby",
        "A. Kremin",
        "A. Lambert",
        "M. Landriau",
        "M. Manera",
        "P. Martini",
        "R. Miquel",
        "A. Mu\u00f1oz-Guti\u00e9rrez",
        "G. Niz",
        "I. P\u00e9rez-R\u00e0fols",
        "G. Rossi",
        "E. Sanchez",
        "M. Schubnell",
        "D. Sprayberry",
        "G. Tarl\u00e9",
        "B. A. Weaver"
      ],
      "primary_category": "astro-ph.CO",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "17 pages, 11 figures. Submitted to A&A",
      "repo_url": "#"
    },
    "2409.05680": {
      "paper_id": "2409.05680v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05680v1",
      "paper_key": "2409.05680",
      "paper_title": "Cherenkov Imaged Bio-morphological Features Verify Patient Positioning with Deformable Tissue Translocation in Breast Radiotherapy",
      "paper_url": "http://arxiv.org/abs/2409.05680v1",
      "paper_abstract": "Accurate patient positioning is critical for precise radiotherapy dose delivery, as positioning errors can significantly affect treatment outcomes. This study introduces a novel method for tracking loco-regional tissue deformation through Cherenkov image analysis during fractionated breast cancer radiotherapy. The primary goal was to develop and test an algorithm for Cherenkov-based regional position accuracy quantification, specifically for loco-regional deformations, which lack ideal quantification methods in radiotherapy. Blood vessel detection and segmentation were developed in Cherenkov images using a tissue phantom with incremental movements, and later applied to images from fractionated whole breast radiotherapy in human patients (n=10). A combined rigid and non-rigid registration technique was used to detect inter- and intra-fractional positioning variations. This approach quantified positioning variations in two parts: a global shift from rigid registration and a two-dimensional variation map of loco-regional deformation from non-rigid registration. The methodology was validated using an anthropomorphic chest phantom experiment, where known treatment couch translations and respiratory motion were simulated to assess inter- and intra-fractional uncertainties, yielding an average accuracy of 0.83 mm for couch translations up to 20 mm. Analysis of clinical Cherenkov data from ten breast cancer patients showed an inter-fraction setup variation of 3.7 plus minus 2.4 mm relative to the first fraction and loco-regional deformations (95th percentile) of up to 3.3 plus minus 1.9 mm. This study presents a Cherenkov-based approach to quantify global and local positioning variations, demonstrating feasibility in addressing loco-regional deformations that conventional imaging techniques fail to capture.",
      "paper_authors": [
        "Yao Chen",
        "Savannah M. Decker",
        "Petr Bruza",
        "David J. Gladstone",
        "Lesley A. Jarvis",
        "Brian W. Pogue",
        "Kimberley S. Samkoe",
        "Rongxiao Zhang"
      ],
      "primary_category": "physics.med-ph",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "25 pages, 4 figures, 1 table, journal under review",
      "repo_url": "#"
    },
    "2409.05666": {
      "paper_id": "2409.05666v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05666v1",
      "paper_key": "2409.05666",
      "paper_title": "Robust Real-time Segmentation of Bio-Morphological Features in Human Cherenkov Imaging during Radiotherapy via Deep Learning",
      "paper_url": "http://arxiv.org/abs/2409.05666v1",
      "paper_abstract": "Cherenkov imaging enables real-time visualization of megavoltage X-ray or electron beam delivery to the patient during Radiation Therapy (RT). Bio-morphological features, such as vasculature, seen in these images are patient-specific signatures that can be used for verification of positioning and motion management that are essential to precise RT treatment. However until now, no concerted analysis of this biological feature-based tracking was utilized because of the slow speed and accuracy of conventional image processing for feature segmentation. This study demonstrated the first deep learning framework for such an application, achieving video frame rate processing. To address the challenge of limited annotation of these features in Cherenkov images, a transfer learning strategy was applied. A fundus photography dataset including 20,529 patch retina images with ground-truth vessel annotation was used to pre-train a ResNet segmentation framework. Subsequently, a small Cherenkov dataset (1,483 images from 212 treatment fractions of 19 breast cancer patients) with known annotated vasculature masks was used to fine-tune the model for accurate segmentation prediction. This deep learning framework achieved consistent and rapid segmentation of Cherenkov-imaged bio-morphological features on another 19 patients, including subcutaneous veins, scars, and pigmented skin. Average segmentation by the model achieved Dice score of 0.85 and required less than 0.7 milliseconds processing time per instance. The model demonstrated outstanding consistency against input image variances and speed compared to conventional manual segmentation methods, laying the foundation for online segmentation in real-time monitoring in a prospective setting.",
      "paper_authors": [
        "Shiru Wang",
        "Yao Chen",
        "Lesley A. Jarvis",
        "Yucheng Tang",
        "David J. Gladstone",
        "Kimberley S. Samkoe",
        "Brian W. Pogue",
        "Petr Bruza",
        "Rongxiao Zhang"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "9 pages, 7 figures, 1 table, journal under review",
      "repo_url": "#"
    },
    "2409.05558": {
      "paper_id": "2409.05558v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05558v1",
      "paper_key": "2409.05558",
      "paper_title": "Seeing Through the Mask: Rethinking Adversarial Examples for CAPTCHAs",
      "paper_url": "http://arxiv.org/abs/2409.05558v1",
      "paper_abstract": "Modern CAPTCHAs rely heavily on vision tasks that are supposedly hard for computers but easy for humans. However, advances in image recognition models pose a significant threat to such CAPTCHAs. These models can easily be fooled by generating some well-hidden \"random\" noise and adding it to the image, or hiding objects in the image. However, these methods are model-specific and thus can not aid CAPTCHAs in fooling all models. We show in this work that by allowing for more significant changes to the images while preserving the semantic information and keeping it solvable by humans, we can fool many state-of-the-art models. Specifically, we demonstrate that by adding masks of various intensities the Accuracy @ 1 (Acc@1) drops by more than 50%-points for all models, and supposedly robust models such as vision transformers see an Acc@1 drop of 80%-points.   These masks can therefore effectively fool modern image classifiers, thus showing that machines have not caught up with humans -- yet.",
      "paper_authors": [
        "Yahya Jabary",
        "Andreas Plesner",
        "Turlan Kuzhagaliyev",
        "Roger Wattenhofer"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "Under review",
      "repo_url": "https://github.com/ETH-DISCO/advx-bench"
    },
    "2409.05553": {
      "paper_id": "2409.05553v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05553v1",
      "paper_key": "2409.05553",
      "paper_title": "Towards Resilient 6G O-RAN: An Energy-Efficient URLLC Resource Allocation Framework",
      "paper_url": "http://arxiv.org/abs/2409.05553v1",
      "paper_abstract": "The demands of ultra-reliable low-latency communication (URLLC) in ``NextG\" cellular networks necessitate innovative approaches for efficient resource utilisation. The current literature on 6G O-RAN primarily addresses improved mobile broadband (eMBB) performance or URLLC latency optimisation individually, often neglecting the intricate balance required to optimise both simultaneously under practical constraints. This paper addresses this gap by proposing a DRL-based resource allocation framework integrated with meta-learning to manage eMBB and URLLC services adaptively. Our approach efficiently allocates heterogeneous network resources, aiming to maximise energy efficiency (EE) while minimising URLLC latency, even under varying environmental conditions. We highlight the critical importance of accurately estimating the traffic distribution flow in the multi-connectivity (MC) scenario, as its uncertainty can significantly degrade EE. The proposed framework demonstrates superior adaptability across different path loss models, outperforming traditional methods and paving the way for more resilient and efficient 6G networks.",
      "paper_authors": [
        "Rana M. Sohaib",
        "Syed Tariq Shah",
        "Poonam Yadav"
      ],
      "primary_category": "cs.NI",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "This manuscript is being submitted for peer review and potential\n  publication in the IEEE Open Journal of the Communications Society",
      "repo_url": "#"
    },
    "2409.05477": {
      "paper_id": "2409.05477v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05477v2",
      "paper_key": "2409.05477",
      "paper_title": "Retrofitting Temporal Graph Neural Networks with Transformer",
      "paper_url": "http://arxiv.org/abs/2409.05477v2",
      "paper_abstract": "Temporal graph neural networks (TGNNs) outperform regular GNNs by incorporating time information into graph-based operations. However, TGNNs adopt specialized models (e.g., TGN, TGAT, and APAN ) and require tailored training frameworks (e.g., TGL and ETC). In this paper, we propose TF-TGN, which uses Transformer decoder as the backbone model for TGNN to enjoy Transformer's codebase for efficient training. In particular, Transformer achieves tremendous success for language modeling, and thus the community developed high-performance kernels (e.g., flash-attention and memory-efficient attention) and efficient distributed training schemes (e.g., PyTorch FSDP, DeepSpeed, and Megatron-LM). We observe that TGNN resembles language modeling, i.e., the message aggregation operation between chronologically occurring nodes and their temporal neighbors in TGNNs can be structured as sequence modeling. Beside this similarity, we also incorporate a series of algorithm designs including suffix infilling, temporal graph attention with self-loop, and causal masking self-attention to make TF-TGN work. During training, existing systems are slow in transforming the graph topology and conducting graph sampling. As such, we propose methods to parallelize the CSR format conversion and graph sampling. We also adapt Transformer codebase to train TF-TGN efficiently with multiple GPUs. We experiment with 9 graphs and compare with 2 state-of-the-art TGNN training frameworks. The results show that TF-TGN can accelerate training by over 2.20 while providing comparable or even superior accuracy to existing SOTA TGNNs. TF-TGN is available at https://github.com/qianghuangwhu/TF-TGN.",
      "paper_authors": [
        "Qiang Huang",
        "Xiao Yan",
        "Xin Wang",
        "Susie Xi Rao",
        "Zhichao Han",
        "Fangcheng Fu",
        "Wentao Zhang",
        "Jiawei Jiang"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-10",
      "comments": "conference Under review",
      "repo_url": "https://github.com/qianghuangwhu/tf-tgn"
    },
    "2409.05458": {
      "paper_id": "2409.05458v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05458v1",
      "paper_key": "2409.05458",
      "paper_title": "Selecting Differential Splicing Methods: Practical Considerations",
      "paper_url": "http://arxiv.org/abs/2409.05458v1",
      "paper_abstract": "Alternative splicing is crucial in gene regulation, with significant implications in clinical settings and biotechnology. This review article compiles bioinformatics RNA-seq tools for investigating differential splicing; offering a detailed examination of their statistical methods, case applications, and benefits. A total of 22 tools are categorised by their statistical family (parametric, non-parametric, and probabilistic) and level of analysis (transcript, exon, and event). The central challenges in quantifying alternative splicing include correct splice site identification and accurate isoform deconvolution of transcripts. Benchmarking studies show no consensus on tool performance, revealing considerable variability across different scenarios. Tools with high citation frequency and continued developer maintenance, such as DEXSeq and rMATS, are recommended for prospective researchers. To aid in tool selection, a guide schematic is proposed based on variations in data input and the required level of analysis. Additionally, advancements in long-read RNA sequencing are expected to drive the evolution of differential splicing tools, reducing the need for isoform deconvolution and prompting further innovation.",
      "paper_authors": [
        "Ben J Draper",
        "Mark J Dunning",
        "David C James"
      ],
      "primary_category": "q-bio.GN",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "24 pages, 5 figures and 1 supplementary table included",
      "repo_url": "#"
    },
    "2409.05449": {
      "paper_id": "2409.05449v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05449v1",
      "paper_key": "2409.05449",
      "paper_title": "A semi-Lagrangian method for the direct numerical simulation of crystallization and precipitation at the pore scale",
      "paper_url": "http://arxiv.org/abs/2409.05449v1",
      "paper_abstract": "This article introduces a new efficient particle method for the numerical simulation of crystallization and precipitation at the pore scale of real rock geometries extracted by X-Ray tomography. It is based on the coupling between superficial velocity models of porous media, Lagrangian description of chemistry using Transition-State-Theory, involving underlying grids. Its ability to successfully compute dissolution process has been established in the past and is presently generalized to precipitation and crystallization by means of adsorption modeling. Numerical simulations of mineral CO2 trapping are provided, showing evidence of clogging/non-clogging regimes, and one of the main results is the introduction of a new non-dimensional number needed for this characterization.",
      "paper_authors": [
        "Sarah Perez",
        "Jean-Matthieu Etancelin",
        "Philippe Poncet"
      ],
      "primary_category": "cs.CE",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "Under review",
      "repo_url": "#"
    },
    "2409.05448": {
      "paper_id": "2409.05448v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05448v2",
      "paper_key": "2409.05448",
      "paper_title": "Representational Analysis of Binding in Large Language Models",
      "paper_url": "http://arxiv.org/abs/2409.05448v2",
      "paper_abstract": "Entity tracking is essential for complex reasoning. To perform in-context entity tracking, language models (LMs) must bind an entity to its attribute (e.g., bind a container to its content) to recall attribute for a given entity. For example, given a context mentioning ``The coffee is in Box Z, the stone is in Box M, the map is in Box H'', to infer ``Box Z contains the coffee'' later, LMs must bind ``Box Z'' to ``coffee''. To explain the binding behaviour of LMs, Feng and Steinhardt (2023) introduce a Binding ID mechanism and state that LMs use a abstract concept called Binding ID (BI) to internally mark entity-attribute pairs. However, they have not directly captured the BI determinant information from entity activations. In this work, we provide a novel view of the Binding ID mechanism by localizing the prototype of BI information. Specifically, we discover that there exists a low-rank subspace in the hidden state (or activation) of LMs, that primarily encodes the order of entity and attribute and which is used as the prototype of BI to causally determine the binding. To identify this subspace, we choose principle component analysis as our first attempt and it is empirically proven to be effective. Moreover, we also discover that when editing representations along directions in the subspace, LMs tend to bind a given entity to other attributes accordingly. For example, by patching activations along the BI encoding direction we can make the LM to infer ``Box Z contains the stone'' and ``Box Z contains the map''.",
      "paper_authors": [
        "Qin Dai",
        "Benjamin Heinzerling",
        "Kentaro Inui"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-12",
      "comments": "The key phrase \"BI Subspace\" might be misleading, because it sounds\n  like the subspace that directly encodes BI, and which is different with its\n  intended meaning that the subspace that is the base (or prototype) of BI.\n  Therefore, the naming of the subspace and its corresponding wording needs\n  further discussion and review",
      "repo_url": "#"
    },
    "2409.05433": {
      "paper_id": "2409.05433v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05433v1",
      "paper_key": "2409.05433",
      "paper_title": "State-Novelty Guided Action Persistence in Deep Reinforcement Learning",
      "paper_url": "http://arxiv.org/abs/2409.05433v1",
      "paper_abstract": "While a powerful and promising approach, deep reinforcement learning (DRL) still suffers from sample inefficiency, which can be notably improved by resorting to more sophisticated techniques to address the exploration-exploitation dilemma. One such technique relies on action persistence (i.e., repeating an action over multiple steps). However, previous work exploiting action persistence either applies a fixed strategy or learns additional value functions (or policy) for selecting the repetition number. In this paper, we propose a novel method to dynamically adjust the action persistence based on the current exploration status of the state space. In such a way, our method does not require training of additional value functions or policy. Moreover, the use of a smooth scheduling of the repeat probability allows a more effective balance between exploration and exploitation. Furthermore, our method can be seamlessly integrated into various basic exploration strategies to incorporate temporal persistence. Finally, extensive experiments on different DMControl tasks demonstrate that our state-novelty guided action persistence method significantly improves the sample efficiency.",
      "paper_authors": [
        "Jianshu Hu",
        "Paul Weng",
        "Yutong Ban"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "Under review",
      "repo_url": "#"
    },
    "2409.05925": {
      "paper_id": "2409.05925v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05925v1",
      "paper_key": "2409.05925",
      "paper_title": "Assessing SPARQL capabilities of Large Language Models",
      "paper_url": "http://arxiv.org/abs/2409.05925v1",
      "paper_abstract": "The integration of Large Language Models (LLMs) with Knowledge Graphs (KGs) offers significant synergistic potential for knowledge-driven applications. One possible integration is the interpretation and generation of formal languages, such as those used in the Semantic Web, with SPARQL being a core technology for accessing KGs. In this paper, we focus on measuring out-of-the box capabilities of LLMs to work with SPARQL and more specifically with SPARQL SELECT queries applying a quantitative approach.   We implemented various benchmarking tasks in the LLM-KG-Bench framework for automated execution and evaluation with several LLMs. The tasks assess capabilities along the dimensions of syntax, semantic read, semantic create, and the role of knowledge graph prompt inclusion.   With this new benchmarking tasks, we evaluated a selection of GPT, Gemini, and Claude models. Our findings indicate that working with SPARQL SELECT queries is still challenging for LLMs and heavily depends on the specific LLM as well as the complexity of the task. While fixing basic syntax errors seems to pose no problems for the best of the current LLMs evaluated, creating semantically correct SPARQL SELECT queries is difficult in several cases.",
      "paper_authors": [
        "Lars-Peter Meyer",
        "Johannes Frey",
        "Felix Brei",
        "Natanael Arndt"
      ],
      "primary_category": "cs.DB",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "peer reviewed publication at NLP4KGc @ Semantics 2024, see\n  https://sites.google.com/view/3rdnlp4kgc",
      "repo_url": "https://github.com/aksw/llm-kg-bench-results"
    },
    "2409.05405": {
      "paper_id": "2409.05405v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05405v2",
      "paper_key": "2409.05405",
      "paper_title": "A Survey of Multimodal Composite Editing and Retrieval",
      "paper_url": "http://arxiv.org/abs/2409.05405v2",
      "paper_abstract": "In the real world, where information is abundant and diverse across different modalities, understanding and utilizing various data types to improve retrieval systems is a key focus of research. Multimodal composite retrieval integrates diverse modalities such as text, image and audio, etc. to provide more accurate, personalized, and contextually relevant results. To facilitate a deeper understanding of this promising direction, this survey explores multimodal composite editing and retrieval in depth, covering image-text composite editing, image-text composite retrieval, and other multimodal composite retrieval. In this survey, we systematically organize the application scenarios, methods, benchmarks, experiments, and future directions. Multimodal learning is a hot topic in large model era, and have also witnessed some surveys in multimodal learning and vision-language models with transformers published in the PAMI journal. To the best of our knowledge, this survey is the first comprehensive review of the literature on multimodal composite retrieval, which is a timely complement of multimodal fusion to existing reviews. To help readers' quickly track this field, we build the project page for this survey, which can be found at https://github.com/fuxianghuang1/Multimodal-Composite-Editing-and-Retrieval.",
      "paper_authors": [
        "Suyan Li",
        "Fuxiang Huang",
        "Lei Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-11",
      "comments": "20 pages, 3 figures, and 11 tables",
      "repo_url": "https://github.com/fuxianghuang1/multimodal-composite-editing-and-retrieval"
    },
    "2409.05391": {
      "paper_id": "2409.05391v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05391v1",
      "paper_key": "2409.05391",
      "paper_title": "Matching seismic masses for RR Lyrae-type and oscillating red horizontal-branch stars in M4",
      "paper_url": "http://arxiv.org/abs/2409.05391v1",
      "paper_abstract": "Globular clusters offer a powerful way to test the properties of stellar populations and the late stages of low-mass stellar evolution. In this paper we study oscillating giant stars and overtone RR Lyrae-type pulsators in the nearest globular cluster, M4, with the help of high-precision, continuous light curves collected by the Kepler space telescope in the K2 mission. We determine the frequency composition of five RRc stars and model their physical parameters with a grid of linear pulsation models. We are able, for the first time, to compare seismic masses of RR Lyrae stars directly to the masses of the very similar red horizontal branch stars in the same stellar population, independently determined from asteroseismic scaling relations. We find a close match, with an average seismic mass of $0.651\\pm0.028\\,M_\\odot$ for RR Lyrae stars and $0.657\\pm0.034\\,M_\\odot$ for red horizontal-branch stars. While the validity of our RR Lyrae masses still relies on the similarity of neighboring horizontal branch subgroups, this result strongly indicates that RRc stars may indeed exhibit high-degree, $l = 8$ and 9 non-radial modes, and modeling these modes can provide realistic mass estimates. We also determine the He content of the cluster to be $Y = 0.266\\pm 0.008$, and compare the seismic masses for our sample of RR Lyrae to theoretical mass relations and highlight the limitations of these relations.",
      "paper_authors": [
        "L\u00e1szl\u00f3 Moln\u00e1r",
        "Henryka Netzel",
        "Madeline Howell",
        "Csilla Kalup",
        "Meridith Joyce"
      ],
      "primary_category": "astro-ph.SR",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "11 pages, 10 figures, submitted to A&A for review",
      "repo_url": "#"
    },
    "2409.05386": {
      "paper_id": "2409.05386v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05386v1",
      "paper_key": "2409.05386",
      "paper_title": "Predictive Coding with Spiking Neural Networks: a Survey",
      "paper_url": "http://arxiv.org/abs/2409.05386v1",
      "paper_abstract": "In this article, we review a class of neuro-mimetic computational models that we place under the label of spiking predictive coding. Specifically, we review the general framework of predictive processing in the context of neurons that emit discrete action potentials, i.e., spikes. Theoretically, we structure our survey around how prediction errors are represented, which results in an organization of historical neuromorphic generalizations that is centered around three broad classes of approaches: prediction errors in explicit groups of error neurons, in membrane potentials, and implicit prediction error encoding. Furthermore, we examine some applications of spiking predictive coding that utilize more energy-efficient, edge-computing hardware platforms. Finally, we highlight important future directions and challenges in this emerging line of inquiry in brain-inspired computing. Building on the prior results of work in computational cognitive neuroscience, machine intelligence, and neuromorphic engineering, we hope that this review of neuromorphic formulations and implementations of predictive coding will encourage and guide future research and development in this emerging research area.",
      "paper_authors": [
        "Antony W. N'dri",
        "William Gebhardt",
        "C\u00e9line Teuli\u00e8re",
        "Fleur Zeldenrust",
        "Rajesh P. N. Rao",
        "Jochen Triesch",
        "Alexander Ororbia"
      ],
      "primary_category": "q-bio.NC",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": null,
      "repo_url": "#"
    },
    "2409.05383": {
      "paper_id": "2409.05383v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05383v1",
      "paper_key": "2409.05383",
      "paper_title": "Deep Learning for Video Anomaly Detection: A Review",
      "paper_url": "http://arxiv.org/abs/2409.05383v1",
      "paper_abstract": "Video anomaly detection (VAD) aims to discover behaviors or events deviating from the normality in videos. As a long-standing task in the field of computer vision, VAD has witnessed much good progress. In the era of deep learning, with the explosion of architectures of continuously growing capability and capacity, a great variety of deep learning based methods are constantly emerging for the VAD task, greatly improving the generalization ability of detection algorithms and broadening the application scenarios. Therefore, such a multitude of methods and a large body of literature make a comprehensive survey a pressing necessity. In this paper, we present an extensive and comprehensive research review, covering the spectrum of five different categories, namely, semi-supervised, weakly supervised, fully supervised, unsupervised and open-set supervised VAD, and we also delve into the latest VAD works based on pre-trained large models, remedying the limitations of past reviews in terms of only focusing on semi-supervised VAD and small model based methods. For the VAD task with different levels of supervision, we construct a well-organized taxonomy, profoundly discuss the characteristics of different types of methods, and show their performance comparisons. In addition, this review involves the public datasets, open-source codes, and evaluation metrics covering all the aforementioned VAD tasks. Finally, we provide several important research directions for the VAD community.",
      "paper_authors": [
        "Peng Wu",
        "Chengyu Pan",
        "Yuting Yan",
        "Guansong Pang",
        "Peng Wang",
        "Yanning Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible",
      "repo_url": "#"
    },
    "2409.05382": {
      "paper_id": "2409.05382v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05382v1",
      "paper_key": "2409.05382",
      "paper_title": "Detections of He-3 in Ni-based binary metal nanocomposites with Cu in zirconia exposed to hydrogen gas at elevated temperatures",
      "paper_url": "http://arxiv.org/abs/2409.05382v1",
      "paper_abstract": "The present study aims to detect helium-3 in nickel-based metal nano-composites doped with zirconia, which exhibited anomalous heat generation when exposed to hydrogen gas at approximately 450{\\deg}C. Two complementary analytical techniques were employed: Nuclear Reaction Analysis (NRA) utilizing 1.4 MeV deuteron beams from a tandem accelerator, and Thermal Desorption Spectrometry (TDS) using a quadrupole mass spectrometer. Both methods successfully detected helium-3 in the samples. Given the extreme rarity of this isotope, its presence strongly suggests the occurrence of nuclear reactions within the nickel-containing materials. These findings lend support to the 4H/TSC (4 Hydrogen/Tetrahedral Symmetric Condensate) model, which uniquely predicts helium-3 as one of the primary reaction products.",
      "paper_authors": [
        "Tomoya Yamauchi",
        "Yutaka Mori",
        "Shuto Higashi",
        "Hayato Seiichi",
        "Masahiko Hasegawa",
        "Akito Takahashi",
        "Akira Taniike",
        "Masato Kanasaki"
      ],
      "primary_category": "physics.ins-det",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "18 pages, 7 figures. This is the version of the article before peer\n  review, as submitted by an author to Japanese Journal of Applied Physics",
      "repo_url": "#"
    },
    "2409.05374": {
      "paper_id": "2409.05374v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05374v1",
      "paper_key": "2409.05374",
      "paper_title": "Don't Leave Me Out: Designing for Device Inclusivity in Mixed Reality Collaboration",
      "paper_url": "http://arxiv.org/abs/2409.05374v1",
      "paper_abstract": "Modern collaborative Mixed Reality (MR) systems continue to break the boundaries of conventional co-located and remote collaboration and communication. They merge physical and virtual worlds and enable natural interaction, opening up a spectrum of novel opportunities for interpersonal connection. For these connections to be perceived as engaging and positive, collaborators should feel comfortable and experience a sense of belonging. Not having the dedicated devices to smoothly participate in these spaces can hinder this and give users the impression of being left out. To counteract this, we propose to prioritize designing for device inclusivity in MR collaboration, focusing on compensating disadvantages of common non-immersive device classes in cross-device systems.",
      "paper_authors": [
        "Katja Krug",
        "Juli\u00e1n M\u00e9ndez",
        "Weizhou Luo",
        "Raimund Dachselt"
      ],
      "primary_category": "cs.HC",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "Peer reviewed and accepted for the \"ACM CHI 2024 Workshop WS 25:\n  Designing Inclusive Future Augmented Realities\" at ACM CHI 2024",
      "repo_url": "#"
    },
    "2409.05364": {
      "paper_id": "2409.05364v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05364v1",
      "paper_key": "2409.05364",
      "paper_title": "Macroscopic self-trapping in the dynamical tunneling of a Bose-Einstein condensate",
      "paper_url": "http://arxiv.org/abs/2409.05364v1",
      "paper_abstract": "A Bose-Einstein condensate in a modulated, one-dimensional, anharmonic potential can exhibit dynamical tunneling between islands of regular motion in phase space. With increasingly repulsive atomic interactions, dynamical tunneling is predicted to cease due to self-trapping [S. W\\\"uster et al. Phys. Rev. Lett. 109 080401 (2012)]. This suppression of tunneling oscillations is related to the same phenomenon that occurs in the two-mode dynamics of a repulsively interacting Bose-Einstein condensate in a double-well potential. Here we present a two-mode model for dynamical tunnelling based on nonlinear Floquet states and examine the range of validity of the approximation. We characterise nonlinear dynamical tunneling for different trap strengths, modulation amplitudes, and effective Planck constants. Using the linear Floquet states we derive an expression for the critical nonlinearity beyond which tunneling ceases. Finally we demonstrate the dynamical instability of selected nonlinear Floquet states and show how to initialise some Floquet states in experiments. Our detailed survey will enable experiments to target accessible parameter regimes for the study of nonlinear dynamical tunneling.",
      "paper_authors": [
        "Sebastian W\u00fcster",
        "Joy Cree",
        "Matthew J. Davis"
      ],
      "primary_category": "cond-mat.quant-gas",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "18 pages, 17 figures",
      "repo_url": "#"
    },
    "2409.05341": {
      "paper_id": "2409.05341v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05341v1",
      "paper_key": "2409.05341",
      "paper_title": "Quantifying azimuthal variations within the interstellar medium of z ~ 0 spiral galaxies with the TYPHOON survey",
      "paper_url": "http://arxiv.org/abs/2409.05341v1",
      "paper_abstract": "Most star formation in the local Universe occurs in spiral galaxies, but their origin remains an unanswered question. Various theories have been proposed to explain the development of spiral arms, each predicting different spatial distributions of the interstellar medium. This study maps the star formation rate (SFR) and gas-phase metallicity of nine spiral galaxies with the TYPHOON survey to test two dominating theories: density wave theory and dynamic spiral theory. We discuss the environmental effects on our galaxies, considering reported environments and merging events. Taking advantage of the large field of view covering the entire optical disk, we quantify the fluctuation of SFR and metallicity relative to the azimuthal distance from the spiral arms. We find higher SFR and metallicity in the trailing edge of NGC~1365 (by 0.117~dex and 0.068~dex, respectively) and NGC~1566 (by 0.119~dex and 0.037~dex, respectively), which is in line with density wave theory. NGC~2442 shows a different result with higher metallicity (0.093~dex) in the leading edge, possibly attributed to an ongoing merging. The other six spiral galaxies show no statistically significant offset in SFR or metallicity, consistent with dynamic spiral theory. We also compare the behaviour of metallicity inside and outside the co-rotation radius (CR) of NGC~1365 and NGC~1566. We find comparable metallicity fluctuations near and beyond the CR of NGC~1365, indicating gravitational perturbation. NGC~1566 shows the greatest fluctuation near the CR, in line with the analytic spiral arms. Our work highlights that a combination of mechanisms explains the origin of spiral features in the local Universe.",
      "paper_authors": [
        "Qian-Hui Chen",
        "Kathryn Grasha",
        "Andrew J. Battisti",
        "Emily Wisnioski",
        "Zefeng Li",
        "Hye-Jin Park",
        "Brent Groves",
        "Paul Torrey",
        "Trevor Mendel",
        "Barry F. Madore",
        "Mark Seibert",
        "Eva Sextl",
        "Alex M. Garcia",
        "Jeff A. Rich",
        "Rachael L. Beaton",
        "Lisa J. Kewley"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "22 pages, 10 figures, 4 tables, accepted for publication in MNRAS",
      "repo_url": "#"
    },
    "2409.07493": {
      "paper_id": "2409.07493v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07493v1",
      "paper_key": "2409.07493",
      "paper_title": "Complex Emotion Recognition System using basic emotions via Facial Expression, EEG, and ECG Signals: a review",
      "paper_url": "http://arxiv.org/abs/2409.07493v1",
      "paper_abstract": "The Complex Emotion Recognition System (CERS) deciphers complex emotional states by examining combinations of basic emotions expressed, their interconnections, and the dynamic variations. Through the utilization of advanced algorithms, CERS provides profound insights into emotional dynamics, facilitating a nuanced understanding and customized responses. The attainment of such a level of emotional recognition in machines necessitates the knowledge distillation and the comprehension of novel concepts akin to human cognition. The development of AI systems for discerning complex emotions poses a substantial challenge with significant implications for affective computing. Furthermore, obtaining a sizable dataset for CERS proves to be a daunting task due to the intricacies involved in capturing subtle emotions, necessitating specialized methods for data collection and processing. Incorporating physiological signals such as Electrocardiogram (ECG) and Electroencephalogram (EEG) can notably enhance CERS by furnishing valuable insights into the user's emotional state, enhancing the quality of datasets, and fortifying system dependability. A comprehensive literature review was conducted in this study to assess the efficacy of machine learning, deep learning, and meta-learning approaches in both basic and complex emotion recognition utilizing EEG, ECG signals, and facial expression datasets. The chosen research papers offer perspectives on potential applications, clinical implications, and results of CERSs, with the objective of promoting their acceptance and integration into clinical decision-making processes. This study highlights research gaps and challenges in understanding CERSs, encouraging further investigation by relevant studies and organizations. Lastly, the significance of meta-learning approaches in improving CERS performance and guiding future research endeavors is underscored.",
      "paper_authors": [
        "Javad Hassannataj Joloudari",
        "Mohammad Maftoun",
        "Bahareh Nakisa",
        "Roohallah Alizadehsani",
        "Meisam Yadollahzadeh-Tabari"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "29 pages, 11 figures",
      "repo_url": "#"
    },
    "2409.05326": {
      "paper_id": "2409.05326v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05326v1",
      "paper_key": "2409.05326",
      "paper_title": "SCALE at Scale: Cosmological applications of small-scale CMB lensing",
      "paper_url": "http://arxiv.org/abs/2409.05326v1",
      "paper_abstract": "The Small-Correlated-Against-Large Estimator (SCALE) for small-scale lensing of the cosmic microwave background (CMB) provides a novel method for measuring the amplitude of CMB lensing power without the need for reconstruction of the lensing field. In our previous study, we showed that the SCALE method can outperform existing reconstruction methods to detect the presence of lensing at small scales ($\\ell \\gg 3000$). Here we develop a procedure to include information from SCALE in cosmological parameter inference. We construct a precise neural network emulator to quickly map cosmological parameters to desired CMB observables such as temperature and lensing power spectra and SCALE cross spectra. We also outline a method to apply SCALE to full-sky maps of the CMB temperature field, and construct a likelihood for the application of SCALE in parameter estimation. SCALE supplements conventional observables such as the CMB power spectra and baryon acoustic oscillations in constraining parameters that are sensitive to the small-scale lensing amplitude such as the neutrino mass $m_\\nu$. We show that including estimates of the small-scale lensing amplitude from SCALE in such an analysis provides enough constraining information to measure the minimum neutrino mass at $4\\sigma$ significance in the scenario of minimal mass, and higher significance for higher mass. Finally, we show that SCALE will play a powerful role in constraining models of clustering that generate scale-dependent modulation to the distribution of matter and the lensing power spectrum, as predicted by models of warm or fuzzy dark matter.",
      "paper_authors": [
        "Victor C. Chan",
        "Ren\u00e9e Hlo\u017eek",
        "Joel Meyers",
        "Alexander van Engelen"
      ],
      "primary_category": "astro-ph.CO",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "18 pages, 8 figures, 6 tables, submitting to Physical Review D in a\n  few days",
      "repo_url": "#"
    },
    "2409.05320": {
      "paper_id": "2409.05320v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05320v1",
      "paper_key": "2409.05320",
      "paper_title": "Stratified dispersal explains mountain pine beetle's range expansion in Alberta",
      "paper_url": "http://arxiv.org/abs/2409.05320v1",
      "paper_abstract": "The mountain pine beetle (MPB), a destructive pest native to Western North America, has recently extended its range into Alberta, Canada. Predicting the dispersal of MPB is challenging due to their small size and complex dispersal behavior. Because of these challenges, estimates of MPB's typical dispersal distances have varied widely, ranging from 10 meters to 18 kilometers. Here, we use high-quality data from helicopter and field-crew surveys to parameterize a large number of dispersal kernels. We find that fat-tailed kernels -- those which allow for a small number of long-distance dispersal events -- consistently provide the best fit to the data. Specifically, the radially-symmetric Student's t-distribution with parameters {\\nu} = 0.012 and {\\rho} = 1.45 stands out as parsimonious and user-friendly; this model predicts a median dispersal distance of 60 meters, but with the 95th percentile of dispersers travelling nearly 5 kilometers. The best-fitting mathematical models have biological interpretations. The Student's t-distribution, derivable as a mixture of diffusive processes with varying settling times, is consistent with observations that most beetles fly short distances while few travel far; early-emerging beetles fly farther; and larger beetles from larger trees exhibit greater variance in flight distance. Finally, we explain why other studies have found such a wide variation in the length scale in MPB dispersal, and we demonstrate that long-distance dispersal events are critical for modelling MPB range expansion.",
      "paper_authors": [
        "Evan C. Johnson",
        "Micah Brush",
        "Mark A. Lewis"
      ],
      "primary_category": "q-bio.PE",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "32 pages, 11 figures, 8 tables",
      "repo_url": "#"
    },
    "2409.05287": {
      "paper_id": "2409.05287v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05287v2",
      "paper_key": "2409.05287",
      "paper_title": "The Dirac Equation Near Centenary: a Contemporary Introduction to the Dirac Equation Consideration",
      "paper_url": "http://arxiv.org/abs/2409.05287v2",
      "paper_abstract": "More then 35 approaches to the Dirac equation derivation are presented. The various physical principles and mathematical methods are used. A review of well-known and not enough known contributions to the problem is given, the unexpected and unconventional derivations are presented as well. Three original approaches to the problem suggested by the author are considered as well. They are (i) the generalization of H. Sallhofer derivation, (ii) the obtaining of the massless Dirac equation from the Maxwell equations in maximally symmetrical form, (iii) the derivation of the Dirac equation with nonzero mass from the relativistic canonical quantum mechanics of the fermion-antifermion spin s=1/2 doublet. Today we are able to demonstrate new features of our derivations given in original papers. In some sense the important role of the Dirac equation in contemporary theoretical physics is demonstrated. A criterion for the usefulness of one or another derivation of the Dirac equation has been established",
      "paper_authors": [
        "V. M. Simulik"
      ],
      "primary_category": "math-ph",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-11",
      "comments": "31 pages",
      "repo_url": "#"
    },
    "2409.05251": {
      "paper_id": "2409.05251v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05251v1",
      "paper_key": "2409.05251",
      "paper_title": "Online Resynthesis of High-Level Collaborative Tasks for Robots with Changing Capabilities",
      "paper_url": "http://arxiv.org/abs/2409.05251v1",
      "paper_abstract": "Given a collaborative high-level task and a team of heterogeneous robots and behaviors to satisfy it, this work focuses on the challenge of automatically, at runtime, adjusting the individual robot behaviors such that the task is still satisfied, when robots encounter changes to their abilities--either failures or additional actions they can perform. We consider tasks encoded in LTL^\\psi and minimize global teaming reassignments (and as a result, local resynthesis) when robots' capabilities change. We also increase the expressivity of LTL^\\psi by including additional types of constraints on the overall teaming assignment that the user can specify, such as the minimum number of robots required for each assignment. We demonstrate the framework in a simulated warehouse scenario.",
      "paper_authors": [
        "Amy Fang",
        "Tenny Yin",
        "Hadas Kress-Gazit"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-09-09",
      "update_time": "2024-09-09",
      "comments": "Under review in IEEE Robotics and Automation Letters",
      "repo_url": "#"
    },
    "2409.07492": {
      "paper_id": "2409.07492v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.07492v1",
      "paper_key": "2409.07492",
      "paper_title": "Analyzing fisher effort -- Gender differences and the impact of Covid-19",
      "paper_url": "http://arxiv.org/abs/2409.07492v1",
      "paper_abstract": "Fishing is a valuable recreational activity in our society. To assess future fishing activity, identifying variables related to differences in fishing activity, such as gender or Covid-19, is helpful. We conducted a Canada-wide email survey of users of an online fishing platform and analyzed responses with a focus on gender, the impact of Covid-19, and variables directly related to fisher effort. Genders (90.1% male and 9.9% female respondents) significantly differed in demographics, socioeconomic status, and fishing skills but were similar in fishing preferences, fisher effort in terms of trip frequency, and travel distance. For almost half of the fishers, Covid-19 caused a change in trip frequency, determined by the activity level and gender of the fisher. A Bayesian network revealed that travel distance was the main determinant of trip frequency and negatively impacted the fishing activity of 61% of the fishers. Fisher effort was also directly related to fishing expertise. The study shows how online surveys and Bayesian networks can help understand the relationship between fishers' characteristics and activity and predict future fishing trends.",
      "paper_authors": [
        "Julia S. Schmid",
        "Sean Simmons",
        "Mark S. Poesch",
        "Pouria Ramazi",
        "Mark A. Lewis"
      ],
      "primary_category": "physics.soc-ph",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-08",
      "comments": "50 pages",
      "repo_url": "#"
    },
    "2409.05228": {
      "paper_id": "2409.05228v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05228v1",
      "paper_key": "2409.05228",
      "paper_title": "Infinite-Length Limit of Spectral Curves and Inverse Scattering",
      "paper_url": "http://arxiv.org/abs/2409.05228v1",
      "paper_abstract": "Integrability equips models of theoretical physics with efficient methods for the exact construction of useful states and their evolution. Relevant tools for classical integrable field models in one spatial dimensional are spectral curves in the case of periodic fields and inverse scattering for asymptotic boundary conditions. Even though the two methods are quite different in many ways, they ought to be related by taking the periodicity length of closed boundary conditions to infinity.   Using the Korteweg-de Vries equation and the continuous Heisenberg magnet as prototypical classical integrable field models, we discuss and illustrate how data for spectral curves transforms into asymptotic scattering data. In order to gain intuition and also for concreteness, we review how the elliptic states of these models degenerate into solitons at infinite length.",
      "paper_authors": [
        "Niklas Beisert",
        "Kunal Gupta"
      ],
      "primary_category": "math-ph",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-08",
      "comments": "63 pages, figures",
      "repo_url": "#"
    },
    "2409.05222": {
      "paper_id": "2409.05222v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05222v1",
      "paper_key": "2409.05222",
      "paper_title": "Advances in Nanoparticle-Based Targeted Drug Delivery Systems for Colorectal Cancer Therapy: A Review",
      "paper_url": "http://arxiv.org/abs/2409.05222v1",
      "paper_abstract": "Colorectal cancer (CRC) continues to be a significant global health burden, prompting the need for more effective and targeted therapeutic strategies. Nanoparticle-based drug delivery systems have emerged as a promising approach to address the limitations of conventional chemotherapy, offering enhanced specificity, reduced systemic toxicity, and improved therapeutic outcomes. This paper provides an in-depth review of the current advancements in the application of nanoparticles as vehicles for targeted drug delivery in CRC therapy. It covers a variety of nanoparticle types, including liposomes, polymeric nanoparticles, dendrimers, and mesoporous silica nanoparticles (MSNs), with a focus on their design, functionalization, and mechanisms of action. This review also examines the challenges associated with the clinical translation of these technologies and explores future directions, emphasizing the potential of nanoparticle-based systems to revolutionize CRC treatment.",
      "paper_authors": [
        "Mahadi Hasan",
        "Camryn Grace Evett",
        "Jack Burton"
      ],
      "primary_category": "q-bio.OT",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-08",
      "comments": null,
      "repo_url": "#"
    },
    "2409.05220": {
      "paper_id": "2409.05220v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05220v1",
      "paper_key": "2409.05220",
      "paper_title": "Seven decades of exploring planetary interiors with rotating convection experiments",
      "paper_url": "http://arxiv.org/abs/2409.05220v1",
      "paper_abstract": "The interiors of many planets consist mostly of fluid layers. When these layers are subject to superadiabatic temperature or compositional gradients, turbulent convection transports heat and momentum. In addition, planets are fast rotators. Thus, the key process that underpins planetary evolution, the dynamo action, flow patterns and more, is rotating convection. Because planetary interiors are inaccessible to direct observation, experiments offer physically consistent models that are crucial to guide our understanding. If we can fully understand the laboratory model, we may eventually fully understand the original. Experimentally reproducing rotating thermal convection relevant to planetary interiors comes with specific challenges, e.g. modelling the central gravity field of a planet that is parallel to the temperature gradient. Three classes of experiments tackle this challenge. One approach consists of using an alternative central force field, such as the electric force. These are, however, weaker than gravity and require going to space. Another method entails rotating the device fast enough so that the centrifugal force supersedes Earth's gravity. This mimics the equatorial regions of a planet. Lastly, by using the actual lab gravity aligned with the rotation axis, insight into the polar regions is gained. These experiments have been continuously refined during the past seven decades. We review their evolution, from the early days of visualising the onset patterns of convection, over central force field experiments in spacecrafts, liquid metal experiments, to the latest optical velocity mapping of rotating magnetoconvection in sulfuric acid inside high-field magnets. We show how innovative experimental design and emerging experimental techniques advanced our understanding and painted a more realistic picture of planetary interiors, including Earth's liquid metal outer core.",
      "paper_authors": [
        "Alban Poth\u00e9rat",
        "Susanne Horn"
      ],
      "primary_category": "physics.geo-ph",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-08",
      "comments": "49 pages, 16 figures",
      "repo_url": "#"
    },
    "2409.05214": {
      "paper_id": "2409.05214v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05214v2",
      "paper_key": "2409.05214",
      "paper_title": "Advances in colored k-mer sets: essentials for the curious",
      "paper_url": "http://arxiv.org/abs/2409.05214v2",
      "paper_abstract": "This paper provides a comprehensive review of recent advancements in k-mer-based data structures representing collections of several samples (sometimes called colored de Bruijn graphs) and their applications in large-scale sequence indexing and pangenomics. The review explores the evolution of k-mer set representations, highlighting the trade-offs between exact and inexact methods, as well as the integration of compression strategies and modular implementations. I discuss the impact of these structures on practical applications and describe recent utilization of these methods for analysis. By surveying the state-of-the-art techniques and identifying emerging trends, this work aims to guide researchers in selecting and developing methods for large scale and reference-free genomic data. For a broader overview of k-mer set representations and foundational data structures, see the accompanying article on practical k-mer sets.",
      "paper_authors": [
        "Camille Marchet"
      ],
      "primary_category": "q-bio.GN",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-10",
      "comments": null,
      "repo_url": "#"
    },
    "2409.05210": {
      "paper_id": "2409.05210v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05210v2",
      "paper_key": "2409.05210",
      "paper_title": "Advances in practical k-mer sets: essentials for the curious",
      "paper_url": "http://arxiv.org/abs/2409.05210v2",
      "paper_abstract": "This paper provides a comprehensive survey of data structures for representing k-mer sets, which are fundamental in high-throughput sequencing analysis. It categorizes the methods into two main strategies: those using fingerprinting and hashing for compact storage, and those leveraging lexicographic properties for efficient representation. The paper reviews key operations supported by these structures, such as membership queries and dynamic updates, and highlights recent advancements in memory efficiency and query speed. A companion paper explores colored k-mer sets, which extend these concepts to integrate multiple datasets or genomes.",
      "paper_authors": [
        "Camille Marchet"
      ],
      "primary_category": "q-bio.GN",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-12",
      "comments": null,
      "repo_url": "#"
    },
    "2409.05202": {
      "paper_id": "2409.05202v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05202v1",
      "paper_key": "2409.05202",
      "paper_title": "A Survey on Mixup Augmentations and Beyond",
      "paper_url": "http://arxiv.org/abs/2409.05202v1",
      "paper_abstract": "As Deep Neural Networks have achieved thrilling breakthroughs in the past decade, data augmentations have garnered increasing attention as regularization techniques when massive labeled data are unavailable. Among existing augmentations, Mixup and relevant data-mixing methods that convexly combine selected samples and the corresponding labels are widely adopted because they yield high performances by generating data-dependent virtual data while easily migrating to various domains. This survey presents a comprehensive review of foundational mixup methods and their applications. We first elaborate on the training pipeline with mixup augmentations as a unified framework containing modules. A reformulated framework could contain various mixup methods and give intuitive operational procedures. Then, we systematically investigate the applications of mixup augmentations on vision downstream tasks, various data modalities, and some analysis \\& theorems of mixup. Meanwhile, we conclude the current status and limitations of mixup research and point out further work for effective and efficient mixup augmentations. This survey can provide researchers with the current state of the art in mixup methods and provide some insights and guidance roles in the mixup arena. An online project with this survey is available at \\url{https://github.com/Westlake-AI/Awesome-Mixup}.",
      "paper_authors": [
        "Xin Jin",
        "Hongyu Zhu",
        "Siyuan Li",
        "Zedong Wang",
        "Zicheng Liu",
        "Chang Yu",
        "Huafeng Qin",
        "Stan Z. Li"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-08",
      "comments": "Preprint V1 with 27 pages main text. Online project at\n  https://github.com/Westlake-AI/Awesome-Mixup",
      "repo_url": "https://github.com/Westlake-AI/Awesome-Mixup"
    },
    "2409.05193": {
      "paper_id": "2409.05193v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05193v1",
      "paper_key": "2409.05193",
      "paper_title": "Seismic monitoring of CO2 plume dynamics using ensemble Kalman filtering",
      "paper_url": "http://arxiv.org/abs/2409.05193v1",
      "paper_abstract": "Monitoring carbon dioxide (CO2) injected and stored in subsurface reservoirs is critical for avoiding failure scenarios and enables real-time optimization of CO2 injection rates. Sequential Bayesian data assimilation (DA) is a statistical method for combining information over time from multiple sources to estimate a hidden state, such as the spread of the subsurface CO2 plume. An example of scalable and efficient sequential Bayesian DA is the ensemble Kalman filter (EnKF). We improve upon existing DA literature in the seismic-CO2 monitoring domain by applying this scalable DA algorithm to a high-dimensional CO2 reservoir using two-phase flow dynamics and time-lapse full waveform seismic data with a realistic surface-seismic survey design. We show more accurate estimates of the CO2 saturation field using the EnKF compared to using either the seismic data or the fluid physics alone. Furthermore, we test a range of values for the EnKF hyperparameters and give guidance on their selection for seismic CO2 reservoir monitoring.",
      "paper_authors": [
        "Grant Bruer",
        "Abhinav Prakash Gahlot",
        "Edmond Chow",
        "Felix Herrmann"
      ],
      "primary_category": "physics.geo-ph",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-08",
      "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible",
      "repo_url": "#"
    },
    "2409.05180": {
      "paper_id": "2409.05180v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05180v1",
      "paper_key": "2409.05180",
      "paper_title": "The Effect of Radiation and Supernovae Feedback on LyC Escape in Local Star-forming Galaxies",
      "paper_url": "http://arxiv.org/abs/2409.05180v1",
      "paper_abstract": "Feedback is widely recognized as an essential condition for Lyman continuum (LyC) escape in star-forming galaxies. However, the mechanisms by which galactic outflows clear neutral gas and dust remain unclear. In this paper, we model the Mg II 2796\\r{A}, 2804\\r{A} absorption + emission lines in 29 galaxies taken from the Low-z LyC Survey (LzLCS) to investigate the impact of (radiation + mechanical) feedback on LyC escape. Using constraints on Mg$^+$ and photoionization models, we map the outflows' neutral hydrogen content and predict $f_{esc}^{LyC}$ with a multiphase wind model. We measure mass, momentum, and energy loading factors for the neutral winds, which carry up to 10% of the momentum and 1% of the energy in SFR-based deposition rates. We use SED template fitting to determine the relative ages of stellar populations, allowing us to identify radiation feedback dominant systems. We then examine feedback related properties (stellar age, loading factors, etc.) under conditions that optimize feedback efficiency, specifically high star formation rate surface density and compact UV half-light radii. Our findings indicate that the strongest leakers are radiation feedback dominant, lack Mg II outflows, but have extended broad components in higher ionization lines like [O III] 5007\\r{A}, as observed by Amor\\'in et al. (2024). In contrast, galaxies experiencing supernovae feedback typically exhibit weaker $f_{esc}^{LyC}$ and show evidence of outflows in both Mg II and higher ionization lines. We attribute these findings to rapid or \"catastrophic\" cooling in the radiation-dominant systems, which, given the low metallicities in our sample, are likely experiencing delayed supernovae.",
      "paper_authors": [
        "Cody A. Carr",
        "Renyue Cen",
        "Claudia Scarlata",
        "Xinfeng Xu",
        "Alaina Henry",
        "Rui Marques-Chaves",
        "Daniel Schaerer",
        "Ricardo O. Amor\u00edn",
        "M. S. Oey",
        "Lena Komarova",
        "Sophia Flury",
        "Anne Jaskot",
        "Alberto Saldana-Lopez",
        "Zhiyuan Ji",
        "Mason Huberty",
        "Timothy Heckman",
        "G\u00f6ran Ostlin",
        "Omkar Bait",
        "Matthew James Hayes",
        "Trinh Thuan",
        "Danielle A. Berg",
        "Mauro Giavalisco",
        "Sanchayeeta Borthakur",
        "John Chisholm",
        "Harry C. Ferguson",
        "Leo Michel-Dansac",
        "Anne Verhamme",
        "G\u00e1bor Worseck"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-08",
      "comments": "34 pages, 16 figures, 7 tables",
      "repo_url": "#"
    },
    "2409.05172": {
      "paper_id": "2409.05172v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05172v1",
      "paper_key": "2409.05172",
      "paper_title": "Abundant Molecular Gas in the Central Region of Lenticular Galaxy PGC 39535",
      "paper_url": "http://arxiv.org/abs/2409.05172v1",
      "paper_abstract": "Lenticular galaxies (S0s) in the local universe are generally absent of recent star formation and lack molecular gas. In this paper, we investigate one massive ($M_*$$\\sim$5$\\times10^{10}$ M$_\\odot$) star-forming S0, PGC 39535, with the Northern Extended Millimeter Array (NOEMA). Using optical data from SDSS-IV MaNGA survey, we find star formation mainly concentrates in the central region of PGC 39535. The total star formation rate estimated using extinction-corrected H$\\alpha$ flux is 1.57 M$_\\odot$ yr$^{-1}$. Results of NOEMA observation suggest that the molecular gas mainly concentrates in the central regions as a gaseous bar and a ring-like structure, and shows similar kinematics as the stellar and ionized gas components. The total molecular gas mass estimated from CO(1-0) is (5.42$\\pm$1.52)$\\times$10$^{9}$ M$_{\\odot}$. We find PGC 39535 lies on the star-forming main sequence, but falls below Kennicutt-Schmidt relation of spiral galaxies, suggesting that the star formation efficiency may be suppressed by the massive bulge. The existence of a second Gaussian component in the CO spectrum of the central region indicates possible gas flows. Furthermore, our analyses suggest that PGC 39535 resides in the center of a massive group and the derived star formation history indicates it may experience a series of gas-rich mergers over the past 2$\\sim$7 Gyr.",
      "paper_authors": [
        "Jiantong Cui",
        "Qiusheng Gu",
        "Shiying Lu",
        "Zhengyi Chen",
        "Can Xu",
        "Zeyu Gao"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-08",
      "comments": "17 pages, 15 figures, Accepted for publication in ApJ",
      "repo_url": "#"
    },
    "2409.05168": {
      "paper_id": "2409.05168v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05168v1",
      "paper_key": "2409.05168",
      "paper_title": "Magnetospheric control of ionospheric TEC perturbations via whistler-mode and ULF waves",
      "paper_url": "http://arxiv.org/abs/2409.05168v1",
      "paper_abstract": "The weakly ionized plasma in the Earth's ionosphere is controlled by a complex interplay between solar and magnetospheric inputs from above, atmospheric processes from below, and plasma electrodynamics from within. This interaction results in ionosphere structuring and variability that pose major challenges for accurate ionosphere prediction for global navigation satellite system (GNSS) related applications and space weather research. The ionospheric structuring and variability are often probed using the total electron content (TEC) and its relative perturbations (dTEC). Among dTEC variations observed at high latitudes, a unique modulation pattern has been linked to magnetospheric ultra low frequency (ULF) waves, yet its underlying mechanisms remain unclear. Here using magnetically-conjugate observations from the THEMIS spacecraft and a ground-based GPS receiver at Fairbanks, Alaska, we provide direct evidence that these dTEC modulations are driven by magnetospheric electron precipitation induced by ULF-modulated whistler-mode waves. We observed peak-to-peak dTEC amplitudes reaching ~0.5 TECU (1 TECU is equal to 10$^6$ electrons/m$^2$) with modulations spanning scales of ~5--100 km. The cross-correlation between our modeled and observed dTEC reached ~0.8 during the conjugacy period but decreased outside of it. The spectra of whistler-mode waves and dTEC also matched closely at ULF frequencies during the conjugacy period but diverged outside of it. Our findings elucidate the high-latitude dTEC generation from magnetospheric wave-induced precipitation, addressing a significant gap in current physics-based dTEC modeling. Theses results thus improve ionospheric dTEC prediction and enhance our understanding of magnetosphere-ionosphere coupling via ULF waves.",
      "paper_authors": [
        "Yangyang Shen",
        "Olga P. Verkhoglyadova",
        "Anton Artemyev",
        "Michael D. Hartinger",
        "Vassilis Angelopoulos",
        "Xueling Shi",
        "Ying Zou"
      ],
      "primary_category": "physics.space-ph",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-08",
      "comments": "14 pages, 5 figures, manuscript under review in AGU Advances",
      "repo_url": "#"
    },
    "2409.05155": {
      "paper_id": "2409.05155v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05155v1",
      "paper_key": "2409.05155",
      "paper_title": "Difference Between Cyclic and Distributed Approach in Stochastic Optimization for Multi-agent System",
      "paper_url": "http://arxiv.org/abs/2409.05155v1",
      "paper_abstract": "Many stochastic optimization problems in multi-agent systems can be decomposed into smaller subproblems or reduced decision subspaces. The cyclic and distributed approaches are two widely used strategies for solving such problems. In this manuscript, we review four existing methods for addressing these problems and compare them based on their suitable problem frameworks and update rules.",
      "paper_authors": [
        "Jiahao Shi",
        "James C. Spall"
      ],
      "primary_category": "math.OC",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-08",
      "comments": null,
      "repo_url": "#"
    },
    "2409.05140": {
      "paper_id": "2409.05140v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05140v2",
      "paper_key": "2409.05140",
      "paper_title": "Stellar reddening map from DESI imaging and spectroscopy",
      "paper_url": "http://arxiv.org/abs/2409.05140v2",
      "paper_abstract": "We present new Galactic reddening maps of the high Galactic latitude sky using DESI imaging and spectroscopy. We directly measure the reddening of 2.6 million stars by comparing the observed stellar colors in $g-r$ and $r-z$ from DESI imaging with the synthetic colors derived from DESI spectra from the first two years of the survey. The reddening in the two colors is on average consistent with the \\cite{fitzpatrick_correcting_1999} extinction curve with $R_\\mathrm{V}=3.1$. We find that our reddening maps differ significantly from the commonly used \\cite{schlegel_maps_1998} (SFD) reddening map (by up to 80 mmag in $E(B-V)$), and we attribute most of this difference to systematic errors in the SFD map. To validate the reddening map, we select a galaxy sample with extinction correction based on our reddening map, and this yields significantly better uniformity than the SFD extinction correction. Finally, we discuss the potential systematic errors in the DESI reddening measurements, including the photometric calibration errors that are the limiting factor on our accuracy. The $E(g-r)$ and $E(g-r)$ maps presented in this work, and for convenience their corresponding $E(B-V)$ maps with SFD calibration, are publicly available.",
      "paper_authors": [
        "Rongpu Zhou",
        "Julien Guy",
        "Sergey E. Koposov",
        "Edward F. Schlafly",
        "David Schlegel",
        "Jessica Aguilar",
        "Steven Ahlen",
        "Stephen Bailey",
        "David Bianchi",
        "David Brooks",
        "Edmond Chaussidon",
        "Todd Claybaugh",
        "Kyle Dawson",
        "Axel de la Macorra",
        "Biprateep Dey",
        "Daniel J. Eisenstein",
        "Simone Ferraro",
        "Andreu Font-Ribera",
        "Jaime E. Forero-Romero",
        "Enrique Gazta\u00f1aga",
        "Satya Gontcho A Gontcho",
        "Gaston Gutierrez",
        "Klaus Honscheid",
        "Stephanie Juneau",
        "Robert Kehoe",
        "David Kirkby",
        "Theodore Kisner",
        "Anthony Kremin",
        "Andrew Lambert",
        "Martin Landriau",
        "Laurent Le Guillou",
        "Michael E. Levi",
        "Ting S. Li",
        "Marc Manera",
        "Paul Martini",
        "Aaron Meisner",
        "Ramon Miquel",
        "John Moustakas",
        "Adam D. Myers",
        "Jeffrey A. Newman",
        "Gustavo Niz",
        "Nathalie Palanque-Delabrouille",
        "Will J. Percival",
        "Claire Poppett",
        "Francisco Prada",
        "Anand Raichoor",
        "Ashley J. Ross",
        "Graziano Rossi",
        "Eusebio Sanchez",
        "Andrew K. Saydjari",
        "Michael Schubnell",
        "David Sprayberry",
        "Gregory Tarl",
        "Benjamin A. Weaver",
        "Pauline Zarrouk",
        "Hu Zou"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-10",
      "comments": "Submitted to the Open Journal of Astrophysics. Associated data files:\n  https://data.desi.lbl.gov/public/papers/mws/desi_dust/y2/v1/maps/",
      "repo_url": "#"
    },
    "2409.05101": {
      "paper_id": "2409.05101v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05101v1",
      "paper_key": "2409.05101",
      "paper_title": "On the Need to Monitor Continuous Integration Practices -- An Empirical Study",
      "paper_url": "http://arxiv.org/abs/2409.05101v1",
      "paper_abstract": "Continuous Integration (CI) encompasses a set of widely adopted practices that enhance software development. However, there are indications that developers may not adequately monitor CI practices. Hence, this paper explores developers' perceptions regarding the monitoring CI practices. To achieve this, we first perform a Document Analysis to assess developers' expressed need for practice monitoring in pull requests comments generated by developers during the development process. After that, we conduct a survey among developers from 121 open-source projects to understand perception of the significance of monitoring seven CI practices in their projects. Finally, we triangulate the emergent themes from our survey by performing a second Document Analysis to understand the extent of monitoring features supported by existing CI services. Our key findings indicate that: 1) the most frequently mentioned CI practice during the development process is ``Test Coverage'' (> 80\\%), while ``Build Health'' and ``Time to Fix a Broken Build'' present notable opportunities for monitoring CI practices; 2) developers do not adequately monitor all CI practices and express interest in monitoring additional practices; and 3) the most popular CI services currently offer limited native support for monitoring CI practices, requiring the use of third-party tools. Our results lead us to conclude that monitoring CI practices is often overlooked by both CI services and developers. Using third-party tools in conjunction with CI services is challenging, they monitor some redundant practices and still falls short of fully supporting CI practices monitoring. Therefore, CI services should implement CI practices monitoring, which would facilitate and encourage developers to monitor them.",
      "paper_authors": [
        "Jadson Santos",
        "Daniel Alencar da Costa",
        "Shane McIntosh",
        "Uir\u00e1 Kulesza"
      ],
      "primary_category": "cs.SE",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-08",
      "comments": "Submitted to the Empirical Software Engineering Journal",
      "repo_url": "#"
    },
    "2409.05081": {
      "paper_id": "2409.05081v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05081v1",
      "paper_key": "2409.05081",
      "paper_title": "The circular velocity and halo mass functions of galaxies in the nearby Universe",
      "paper_url": "http://arxiv.org/abs/2409.05081v1",
      "paper_abstract": "The circular velocity function (CVF) of galaxies is a fundamental test of the $\\Lambda$ Cold Dark Matter (CDM) paradigm as it traces the variation of galaxy number densities with circular velocity ($v_{\\rm{circ}}$), a proxy for dynamical mass. Previous observational studies of the CVF have either been based on \\ion{H}{I}-rich galaxies, or encompassed low-number statistics and probed narrow ranges in $v_{\\rm{circ}}$. We present a benchmark computation of the CVF between $100-350\\ \\rm{km\\ s^{-1}}$ using a sample of 3527 nearby-Universe galaxies, representative for stellar masses between $10^{9.2}-10^{11.9} \\rm{M_{\\odot}}$. We find significantly larger number densities above 150 $\\rm{km\\ s^{-1}}$ compared to results from \\ion{H}{I} surveys, pertaining to the morphological diversity of our sample. Leveraging the fact that circular velocities are tracing the gravitational potential of halos, we compute the halo mass function (HMF), covering $\\sim$1 dex of previously unprobed halo masses ($10^{11.7}-10^{12.7} \\rm{M_{\\odot}}$). The HMF for our sample, representative of the galaxy population with $M_{200}\\geqslant10^{11.35} \\rm{M_{\\odot}}$, shows that spiral morphologies contribute 67 per cent of the matter density in the nearby Universe, while early types account for the rest. We combine our HMF data with literature measurements based on \\ion{H}{I} kinematics and group/cluster velocity dispersions. We constrain the functional form of the HMF between $10^{10.5}-10^{15.5} \\rm{M_{\\odot}}$, finding a good agreement with $\\Lambda$CDM predictions. The halo mass range probed encompasses 72$\\substack{+5 \\\\ -6}$ per cent ($\\Omega_{\\rm{M,10.5-15.5}} = 0.227 \\pm 0.018$) of the matter density in the nearby Universe; 31$\\substack{+5 \\\\ -6}$ per cent is accounted for by halos below $10^{12.7}\\rm{M_{\\odot}}$ occupied by a single galaxy.",
      "paper_authors": [
        "Andrei Ristea",
        "Luca Cortese",
        "Brent Groves",
        "A. Fraser-McKelvie",
        "Danail Obreschkow",
        "Karl Glazebrook"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-08",
      "comments": "24 pages, 15 figures, accepted for publications in MNRAS",
      "repo_url": "#"
    },
    "2409.05074": {
      "paper_id": "2409.05074v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05074v1",
      "paper_key": "2409.05074",
      "paper_title": "Interesting system of $3$ first-order recursions",
      "paper_url": "http://arxiv.org/abs/2409.05074v1",
      "paper_abstract": "In this paper we firstly review how to \\textit{explicitly} solve a system of $3$ \\textit{first-order linear recursions }and outline the main properties of these solutions. Next, via a change of variables, we identify a class of systems of $3$ \\textit{first-order nonlinear recursions} which also are \\textit{explicitly solvable}. These systems might be of interest for practitioners in \\textit{applied} sciences: they allow a complete display of their solutions, which may feature interesting behaviors, for instance be \\textit{completely periodic} (\"isochronous systems\", if the independent variable $n=0,1,2,3...$is considered a \\textit{ticking time}), or feature this property \\textit{only asymptotically} (as\\textit{\\ }$n\\rightarrow \\infty $).",
      "paper_authors": [
        "Francesco Calogero"
      ],
      "primary_category": "nlin.SI",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-08",
      "comments": null,
      "repo_url": "#"
    },
    "2409.06734": {
      "paper_id": "2409.06734v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.06734v1",
      "paper_key": "2409.06734",
      "paper_title": "ARIM-mdx Data System: Towards a Nationwide Data Platform for Materials Science",
      "paper_url": "http://arxiv.org/abs/2409.06734v1",
      "paper_abstract": "In modern materials science, effective and high-volume data management across leading-edge experimental facilities and world-class supercomputers is indispensable for cutting-edge research. Such facilities and supercomputers are typically utilized by a wide range of researchers across different fields and organizations in academia and industry. However, existing integrated systems that handle data from these resources have primarily focused just on smaller-scale cross-institutional or single-domain operations. As a result, they often lack the scalability, efficiency, agility, and interdisciplinarity, needed for handling substantial volumes of data from various researchers.   In this paper, we introduce ARIM-mdx data system, a nationwide data platform for materials science in Japan. The platform involves 8 universities and institutes all over Japan through the governmental materials science project. Currently in its trial phase, the ARIM-mdx data system is utilized by over 800 researchers from around 140 organizations in academia and industry, being intended to gradually expand its reach. The system employs a hybrid architecture, combining a peta-scale dedicated storage system for security and stability with a high-performance academic cloud for efficiency and scalability. Through direct network connections between them, the system achieves 4.7x latency reduction compared to a conventional approach, resulting in near real-time interactive data analysis. It also utilizes specialized IoT devices for secure data transfer from equipment computers and connects to multiple supercomputers via an academic ultra-fast network, achieving 4x faster data transfer compared to the public Internet. The ARIM-mdx data system, as a pioneering nationwide data platform, has the potential to contribute to the creation of new research communities and accelerates innovations.",
      "paper_authors": [
        "Masatoshi Hanai",
        "Ryo Ishikawa",
        "Mitsuaki Kawamura",
        "Masato Ohnishi",
        "Norio Takenaka",
        "Kou Nakamura",
        "Daiju Matsumura",
        "Seiji Fujikawa",
        "Hiroki Sakamoto",
        "Yukinori Ochiai",
        "Tetsuo Okane",
        "Shin-Ichiro Kuroki",
        "Atsuo Yamada",
        "Toyotaro Suzumura",
        "Junichiro Shiomi",
        "Kenjiro Taura",
        "Yoshio Mita",
        "Naoya Shibata",
        "Yuichi Ikuhara"
      ],
      "primary_category": "cs.DC",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-08",
      "comments": "Under Review",
      "repo_url": "#"
    },
    "2409.05064": {
      "paper_id": "2409.05064v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05064v1",
      "paper_key": "2409.05064",
      "paper_title": "Environmental effects as a key factor in shaping star-forming S0 galaxies",
      "paper_url": "http://arxiv.org/abs/2409.05064v1",
      "paper_abstract": "The origins of lenticular galaxies (S0s) can be classified into two main categories: ``minor mergers\" in low-density environments (LDEs) and ``faded spirals\" in high-density environments (HDEs). The transitional phase in the evolution of S0s, namely, star-forming lenticular galaxies (SFS0s), can serve as an important probe for analyzing the complex processes involved in the transformation between different galaxy types and the quenching of star formation (SF). We attempt to find the impact of different environments on the global properties and spatially resolved quantities of SFS0s. We selected 71 SFS0s from the SDSS-IV MaNGA Survey, comprising 23 SFS0s in HDEs (SFS0s$\\_$HE) and 48 SFS0s in LDEs (SFS0s$\\_$LE). We examined the effects of the environment, by studying the global properties, concentration index, and radial profiles of the derived quantities. The varied environments of SFS0s do not lead to any significant difference in global properties (e.g., S$\\acute{\\rm e}$rsic index). By calculating $CI_{\\rm H_{\\alpha}/cont}$, we observe that different environments may cause varying concentrations of SF. Specifically, SFS0s$\\_$LE, affected by external gas mergers or inflow, exhibit a more centrally concentrated SF (i.e., larger $CI_{\\rm H_{\\alpha}/cont}$). This trend is further supported by $CI_{\\rm SFR, H_{\\alpha}}$, which only considers the gas disk of the galaxy. This observation is aligned with the observed shrinking of gas disks in galaxies affected by ram-pressure stripping in HDEs. Furthermore, their $\\Sigma_{\\rm SFR}$ or resolved sSFR are comparable. On average, SFS0s$\\_$LE display significantly higher values for both quantities. Finally, the observed D$_{\\rm n}4000$ and gas-phase metallicity gradient correspond well to their assumed origins. However, we did not find a significantly lower gas-phase metallicity in SFS0s$\\_$LE. Abridged",
      "paper_authors": [
        "Pei-Bin Chen",
        "Junfeng Wang",
        "Yan-Mei Chen",
        "Xiao-Yu Xu",
        "Tian-Wen Cao"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-08",
      "comments": "15 pages, 7 figures, 4 tables. Accepted for publication in A&A",
      "repo_url": "#"
    },
    "2409.05049": {
      "paper_id": "2409.05049v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05049v1",
      "paper_key": "2409.05049",
      "paper_title": "The Influence of Demographic Variation on the Perception of Industrial Robot Movements",
      "paper_url": "http://arxiv.org/abs/2409.05049v1",
      "paper_abstract": "The influence of individual differences on the perception and evaluation of interactions with robots has been researched for decades. Some human demographic characteristics have been shown to affect how individuals perceive interactions with robots. Still, it is to-date not clear whether, which and to what extent individual differences influence how we perceive robots, and even less is known about human factors and their effect on the perception of robot movements. In addition, most results on the relevance of individual differences investigate human-robot interactions with humanoid or social robots whereas interactions with industrial robots are underrepresented. We present a literature review on the relationship of robot movements and the influence of demographic variation. Our review reveals a limited comparability of existing findings due to a lack of standardized robot manipulations, various dependent variables used and differing experimental setups including different robot types. In addition, most studies have insufficient sample sizes to derive generalizable results. To overcome these shortcomings, we report the results from a Web-based experiment with 930 participants that studies the effect of demographic characteristics on the evaluation of movement behaviors of an articulated robot arm. Our findings demonstrate that most participants prefer an approach from the side, a large movement range, conventional numbers of rotations, smooth movements and neither fast nor slow movement speeds. Regarding individual differences, most of these preferences are robust to demographic variation, and only gender and age was found to cause slight preference differences between slow and fast movements.",
      "paper_authors": [
        "Damian Hostettler"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-08",
      "comments": null,
      "repo_url": "#"
    },
    "2409.05033": {
      "paper_id": "2409.05033v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05033v1",
      "paper_key": "2409.05033",
      "paper_title": "A Survey on Diffusion Models for Recommender Systems",
      "paper_url": "http://arxiv.org/abs/2409.05033v1",
      "paper_abstract": "While traditional recommendation techniques have made significant strides in the past decades, they still suffer from limited generalization performance caused by factors like inadequate collaborative signals, weak latent representations, and noisy data. In response, diffusion models (DMs) have emerged as promising solutions for recommender systems due to their robust generative capabilities, solid theoretical foundations, and improved training stability. To this end, in this paper, we present the first comprehensive survey on diffusion models for recommendation, and draw a bird's-eye view from the perspective of the whole pipeline in real-world recommender systems. We systematically categorize existing research works into three primary domains: (1) diffusion for data engineering & encoding, focusing on data augmentation and representation enhancement; (2) diffusion as recommender models, employing diffusion models to directly estimate user preferences and rank items; and (3) diffusion for content presentation, utilizing diffusion models to generate personalized content such as fashion and advertisement creatives. Our taxonomy highlights the unique strengths of diffusion models in capturing complex data distributions and generating high-quality, diverse samples that closely align with user preferences. We also summarize the core characteristics of the adapting diffusion models for recommendation, and further identify key areas for future exploration, which helps establish a roadmap for researchers and practitioners seeking to advance recommender systems through the innovative application of diffusion models. To further facilitate the research community of recommender systems based on diffusion models, we actively maintain a GitHub repository for papers and other related resources in this rising direction https://github.com/CHIANGEL/Awesome-Diffusion-for-RecSys.",
      "paper_authors": [
        "Jianghao Lin",
        "Jiaqi Liu",
        "Jiachen Zhu",
        "Yunjia Xi",
        "Chengkai Liu",
        "Yangtian Zhang",
        "Yong Yu",
        "Weinan Zhang"
      ],
      "primary_category": "cs.IR",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-08",
      "comments": "Under Review",
      "repo_url": "#"
    },
    "2409.05031": {
      "paper_id": "2409.05031v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05031v1",
      "paper_key": "2409.05031",
      "paper_title": "Room-temperature superconductivity in 1D",
      "paper_url": "http://arxiv.org/abs/2409.05031v1",
      "paper_abstract": "We review the theoretical model underpinning the recently reported room-temperature, ambient-pressure superconductivity along line defects on the surface of highly-oriented pyrolytic graphite. The main ingredients for this 1D room-temperature superconductivity are pairing by effective strain gauge fields, the formation of an effective Josephson junction array in its Bose metal state on the surface and the suppression of phase slips by dimensional embedding in an extremely well-conducting 3D bulk structure.",
      "paper_authors": [
        "Carlo A. Trugenberger"
      ],
      "primary_category": "cond-mat.supr-con",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-08",
      "comments": "Invited lecture at the Superstripes 2025 conference in Ischia, Italy",
      "repo_url": "#"
    },
    "2409.05019": {
      "paper_id": "2409.05019v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05019v1",
      "paper_key": "2409.05019",
      "paper_title": "Unified External Stakeholder Engagement and Requirements Strategy",
      "paper_url": "http://arxiv.org/abs/2409.05019v1",
      "paper_abstract": "Understanding stakeholder needs is essential for project success, as stakeholder importance varies across projects. This study proposes a framework for early stakeholder identification and continuous engagement throughout the project lifecycle. The framework addresses common organizational failures in stakeholder communication that lead to project delays and cancellations. By classifying stakeholders by influence and interest, establishing clear communication channels, and implementing regular feedback loops, the framework ensures effective stakeholder involvement. This approach allows for necessary project adjustments and builds long-term relationships, validated by a survey of IT professionals. Engaging stakeholders strategically at all stages minimizes misunderstandings and project risks, contributing to better project management and lifecycle outcomes.",
      "paper_authors": [
        "Ahmed Abdulaziz Alnhari",
        "Rizwan Qureshi"
      ],
      "primary_category": "cs.SE",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-08",
      "comments": "15 Pages",
      "repo_url": "#"
    },
    "2409.05014": {
      "paper_id": "2409.05014v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05014v1",
      "paper_key": "2409.05014",
      "paper_title": "Unraveling Challenges with Supply-Chain Levels for Software Artifacts (SLSA) for Securing the Software Supply Chain",
      "paper_url": "http://arxiv.org/abs/2409.05014v1",
      "paper_abstract": "In 2023, Sonatype reported a 200\\% increase in software supply chain attacks, including major build infrastructure attacks. To secure the software supply chain, practitioners can follow security framework guidance like the Supply-chain Levels for Software Artifacts (SLSA). However, recent surveys and industry summits have shown that despite growing interest, the adoption of SLSA is not widespread. To understand adoption challenges, \\textit{the goal of this study is to aid framework authors and practitioners in improving the adoption and development of Supply-Chain Levels for Software Artifacts (SLSA) through a qualitative study of SLSA-related issues on GitHub}. We analyzed 1,523 SLSA-related issues extracted from 233 GitHub repositories. We conducted a topic-guided thematic analysis, leveraging the Latent Dirichlet Allocation (LDA) unsupervised machine learning algorithm, to explore the challenges of adopting SLSA and the strategies for overcoming these challenges. We identified four significant challenges and five suggested adoption strategies. The two main challenges reported are complex implementation and unclear communication, highlighting the difficulties in implementing and understanding the SLSA process across diverse ecosystems. The suggested strategies include streamlining provenance generation processes, improving the SLSA verification process, and providing specific and detailed documentation. Our findings indicate that some strategies can help mitigate multiple challenges, and some challenges need future research and tool enhancement.",
      "paper_authors": [
        "Mahzabin Tamanna",
        "Sivana Hamer",
        "Mindy Tran",
        "Sascha Fahl",
        "Yasemin Acar",
        "Laurie Williams"
      ],
      "primary_category": "cs.CE",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-08",
      "comments": null,
      "repo_url": "#"
    },
    "2409.05005": {
      "paper_id": "2409.05005v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.05005v2",
      "paper_key": "2409.05005",
      "paper_title": "Towards Patronizing and Condescending Language in Chinese Videos: A Multimodal Dataset and Detector",
      "paper_url": "http://arxiv.org/abs/2409.05005v2",
      "paper_abstract": "Patronizing and Condescending Language (PCL) is a form of discriminatory toxic speech targeting vulnerable groups, threatening both online and offline safety. While toxic speech research has mainly focused on overt toxicity, such as hate speech, microaggressions in the form of PCL remain underexplored. Additionally, dominant groups' discriminatory facial expressions and attitudes toward vulnerable communities can be more impactful than verbal cues, yet these frame features are often overlooked. In this paper, we introduce the PCLMM dataset, the first Chinese multimodal dataset for PCL, consisting of 715 annotated videos from Bilibili, with high-quality PCL facial frame spans. We also propose the MultiPCL detector, featuring a facial expression detection module for PCL recognition, demonstrating the effectiveness of modality complementarity in this challenging task. Our work makes an important contribution to advancing microaggression detection within the domain of toxic speech.",
      "paper_authors": [
        "Hongbo Wang",
        "Junyu Lu",
        "Yan Han",
        "Kai Ma",
        "Liang Yang",
        "Hongfei Lin"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-10",
      "comments": "Under review in ICASSP 2025",
      "repo_url": "https://github.com/dut-laowang/pclmm"
    },
    "2409.04995": {
      "paper_id": "2409.04995v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.04995v2",
      "paper_key": "2409.04995",
      "paper_title": "Consumer Research with Projective Techniques: A Mixed Methods-Focused Review and Empirical Reanalysis",
      "paper_url": "http://arxiv.org/abs/2409.04995v2",
      "paper_abstract": "This article gives an integrative review of research using projective methods in the consumer research domain. We give a general historical overview of the use of projective methods, both in psychology and in consumer research applications, and discuss the reliability and validity aspects and measurement for projective techniques. We review the literature on projective techniques in the areas of marketing, hospitality & tourism, and consumer & food science, with a mixed methods research focus on the interplay of qualitative and quantitative techniques. We review the use of several quantitative techniques used for structuring and analyzing projective data and run an empirical reanalysis of previously gathered data. We give recommendations for improved rigor and for potential future work involving mixed methods in projective techniques.",
      "paper_authors": [
        "Stephen L. France"
      ],
      "primary_category": "stat.ME",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-11",
      "comments": null,
      "repo_url": "#"
    },
    "2409.04993": {
      "paper_id": "2409.04993v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.04993v1",
      "paper_key": "2409.04993",
      "paper_title": "Pulsating stars in Local Group dwarf galaxies",
      "paper_url": "http://arxiv.org/abs/2409.04993v1",
      "paper_abstract": "The popularity of pulsating stars resides in their capacity of determining several crucial and relevant parameters such as heliocentric distances, ages, metallicity gradients and reddening. RR Lyrae stars are old stellar tracers and have been detected in nearly all nearby galaxies that have been searched for these stars, with just a few exceptions of very low mass dwarfs. Less common but also of great importance are Anomalous Cepheids, indicators of either old or intermediate-age population, depending on their stellar origin. Classical Cepheids are only found within young stellar populations, and because of their brighter absolute magnitudes, they can be detected in galaxies farther than the Local Group. This paper presents a concise review built upon the aforementioned pulsating stars in Local Group dwarf galaxies and some of their applications to infer important properties of their host galaxies.",
      "paper_authors": [
        "Clara E. Mart\u00ednez-V\u00e1zquez"
      ],
      "primary_category": "astro-ph.SR",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-08",
      "comments": "Invited review for Memorie della Societ\\`{a} Astronomica Italiana",
      "repo_url": "#"
    },
    "2409.04962": {
      "paper_id": "2409.04962v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.04962v1",
      "paper_key": "2409.04962",
      "paper_title": "A foundation model enpowered by a multi-modal prompt engine for universal seismic geobody interpretation across surveys",
      "paper_url": "http://arxiv.org/abs/2409.04962v1",
      "paper_abstract": "Seismic geobody interpretation is crucial for structural geology studies and various engineering applications. Existing deep learning methods show promise but lack support for multi-modal inputs and struggle to generalize to different geobody types or surveys. We introduce a promptable foundation model for interpreting any geobodies across seismic surveys. This model integrates a pre-trained vision foundation model (VFM) with a sophisticated multi-modal prompt engine. The VFM, pre-trained on massive natural images and fine-tuned on seismic data, provides robust feature extraction for cross-survey generalization. The prompt engine incorporates multi-modal prior information to iteratively refine geobody delineation. Extensive experiments demonstrate the model's superior accuracy, scalability from 2D to 3D, and generalizability to various geobody types, including those unseen during training. To our knowledge, this is the first highly scalable and versatile multi-modal foundation model capable of interpreting any geobodies across surveys while supporting real-time interactions. Our approach establishes a new paradigm for geoscientific data interpretation, with broad potential for transfer to other tasks.",
      "paper_authors": [
        "Hang Gao",
        "Xinming Wu",
        "Luming Liang",
        "Hanlin Sheng",
        "Xu Si",
        "Gao Hui",
        "Yaxing Li"
      ],
      "primary_category": "physics.geo-ph",
      "publish_time": "2024-09-08",
      "update_time": "2024-09-08",
      "comments": null,
      "repo_url": "#"
    },
    "2409.04876": {
      "paper_id": "2409.04876v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.04876v1",
      "paper_key": "2409.04876",
      "paper_title": "DEPLOYERS: An agent based modeling tool for multi country real world data",
      "paper_url": "http://arxiv.org/abs/2409.04876v1",
      "paper_abstract": "We present recent progress in the design and development of DEPLOYERS, an agent-based macroeconomics modeling (ABM) framework, capable to deploy and simulate a full economic system (individual workers, goods and services firms, government, central and private banks, financial market, external sectors) whose structure and activity analysis reproduce the desired calibration data, that can be, for example a Social Accounting Matrix (SAM) or a Supply-Use Table (SUT) or an Input-Output Table (IOT).Here we extend our previous work to a multi-country version and show an example using data from a 46-countries 64-sectors FIGARO Inter-Country IOT. The simulation of each country runs on a separate thread or CPU core to simulate the activity of one step (month, week, or day) and then interacts (updates imports, exports, transfer) with that country's foreign partners, and proceeds to the next step. This interaction can be chosen to be aggregated (a single row and column IO account) or disaggregated (64 rows and columns) with each partner. A typical run simulates thousands of individuals and firms engaged in their monthly activity and then records the results, much like a survey of the country's economic system. This data can then be subjected to, for example, an Input-Output analysis to find out the sources of observed stylized effects as a function of time in the detailed and realistic modeling environment that can be easily implemented in an ABM framework.",
      "paper_authors": [
        "Martin Jaraiz",
        "Ruth Pinacho"
      ],
      "primary_category": "econ.EM",
      "publish_time": "2024-09-07",
      "update_time": "2024-09-07",
      "comments": null,
      "repo_url": "#"
    },
    "2409.04872": {
      "paper_id": "2409.04872v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.04872v1",
      "paper_key": "2409.04872",
      "paper_title": "Vegetation-climate feedbacks across scales",
      "paper_url": "http://arxiv.org/abs/2409.04872v1",
      "paper_abstract": "Vegetation often understood merely as the result of long-term climate conditions. However, vegetation itself plays a fundamental role in shaping Earth's climate by regulating the energy, water, and biogeochemical cycles across terrestrial landscapes. It exerts influence by altering surface roughness, consuming significant water resources through transpiration and interception, lowering atmospheric CO2 concentration, and controlling net radiation and its partitioning into sensible and latent heat fluxes. This influence propagates through the atmosphere, from microclimate scales to the entire atmospheric boundary layer, subsequently impacting large-scale circulation and the global transport of heat and moisture. Understanding the feedbacks between vegetation and atmosphere across multiple scales is crucial for predicting the influence of land use and cover changes and for accurately representing these processes in climate models. This short review aims to discuss the mechanisms through which vegetation modulates climate across spatial and temporal scales. Particularly, we evaluate the influence of vegetation on circulation patterns, precipitation and temperature, both in terms of trends and extreme events, such as droughts and heatwaves. The main goal is to highlight the state of science and review recent studies that may help advance our collective understanding of vegetation feedbacks and the role they play in climate.",
      "paper_authors": [
        "Diego G. Miralles",
        "Jordi Vila-Guerau de Arellano",
        "Tim R. McVicar",
        "Miguel D. Mahecha"
      ],
      "primary_category": "physics.ao-ph",
      "publish_time": "2024-09-07",
      "update_time": "2024-09-07",
      "comments": "In review",
      "repo_url": "#"
    },
    "2409.04846": {
      "paper_id": "2409.04846v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.04846v1",
      "paper_key": "2409.04846",
      "paper_title": "The Dark Energy Camera Magellanic Clouds Emission-Line Survey",
      "paper_url": "http://arxiv.org/abs/2409.04846v1",
      "paper_abstract": "We have used the Dark Energy Camera (DECam) on the CTIO Blanco 4-m telescope to perform a new emission-line survey of the Large Magellanic Cloud (LMC) using narrow-band H-alpha and [SII] filters in addition to a continuum band for use in creating pure emission-line images. We refer to this new survey as DeMCELS, to distinguish it from the earlier Magellanic Cloud Emission Line Survey (MCELS) that has been in service for nearly 25 years. DeMCELS covers $\\sim 54$ degrees$^{2}$, encompassing most of the bright optical disk of the LMC. With DECam's pixel size of only 0.27\", our DeMCELS survey provides a seeing-limited improvement of 3-5 times over MCELS and is comparable in depth, with surface brightness limits of 3.3E-17 erg cm$^{-2}$ s$^{-1}$ arcsec$^{-2}$ in H-alpha and 2.9E-17 erg cm$^{-2}$ s$^{-1}$ arcsec$^{-2}$ in H-alpha and [SII], respectively. DeMCELS provides detailed morphological information on nebulae of all scales, from the largest supershells to individual [HII] regions and supernova remnants, to bubbles of emission surrounding individual stars, and even to faint structures in the diffuse ionized gas of the LMC. Many complex regions of emission show significant variations in the ratio of [SII] to H-alpha, a sign of the mixture of shocks from stellar winds and/or supernovae with photoionization by embedded hot, young stars. We present the details of the observing strategy and data processing for this survey, and show selected results in comparison with previous data. A companion project for the Small Magellanic Cloud is in progress and will be reported separately. We are making these new data available to the community at large via the NOIRLab's Data Lab site.",
      "paper_authors": [
        "Sean D. Points",
        "Knox S. Long",
        "William P. Blair",
        "Rosa Williams",
        "You-Hua Chu",
        "P. Frank Winkler",
        "Richard L. White",
        "Armin Rest",
        "Chuan-Jui Li",
        "Francisco Valdes"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-07",
      "update_time": "2024-09-07",
      "comments": "30 pages, 16 figures Accepted for publication (ApJ)",
      "repo_url": "#"
    },
    "2409.04833": {
      "paper_id": "2409.04833v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.04833v1",
      "paper_key": "2409.04833",
      "paper_title": "Achieving Peak Performance for Large Language Models: A Systematic Review",
      "paper_url": "http://arxiv.org/abs/2409.04833v1",
      "paper_abstract": "In recent years, large language models (LLMs) have achieved remarkable success in natural language processing (NLP). LLMs require an extreme amount of parameters to attain high performance. As models grow into the trillion-parameter range, computational and memory costs increase significantly. This makes it difficult for many researchers to access the resources needed to train or apply these models. Optimizing LLM performance involves two main approaches: fine-tuning pre-trained models for specific tasks to achieve state-of-the-art performance, and reducing costs or improving training time while maintaining similar performance. This paper presents a systematic literature review (SLR) following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement. We reviewed 65 publications out of 983 from 2017 to December 2023, retrieved from 5 databases. The study presents methods to optimize and accelerate LLMs while achieving cutting-edge results without sacrificing accuracy. We begin with an overview of the development of language modeling, followed by a detailed explanation of commonly used frameworks and libraries, and a taxonomy for improving and speeding up LLMs based on three classes: LLM training, LLM inference, and system serving. We then delve into recent optimization and acceleration strategies such as training optimization, hardware optimization, scalability and reliability, accompanied by the taxonomy and categorization of these strategies. Finally, we provide an in-depth comparison of each class and strategy, with two case studies on optimizing model training and enhancing inference efficiency. These case studies showcase practical approaches to address LLM resource limitations while maintaining performance.",
      "paper_authors": [
        "Zhyar Rzgar K Rostam",
        "S\u00e1ndor Sz\u00e9n\u00e1si",
        "G\u00e1bor Kert\u00e9sz"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-09-07",
      "update_time": "2024-09-07",
      "comments": "34 pages, 7 figures, 8 tables. Journal Article: IEEE Access",
      "repo_url": "#"
    },
    "2409.04830": {
      "paper_id": "2409.04830v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.04830v1",
      "paper_key": "2409.04830",
      "paper_title": "Beyond Dependencies: The Role of Copy-Based Reuse in Open Source Software Development",
      "paper_url": "http://arxiv.org/abs/2409.04830v1",
      "paper_abstract": "In Open Source Software, resources of any project are open for reuse by introducing dependencies or copying the resource itself. In contrast to dependency-based reuse, the infrastructure to systematically support copy-based reuse appears to be entirely missing. Our aim is to enable future research and tool development to increase efficiency and reduce the risks of copy-based reuse. We seek a better understanding of such reuse by measuring its prevalence and identifying factors affecting the propensity to reuse. To identify reused artifacts and trace their origins, our method exploits World of Code infrastructure. We begin with a set of theory-derived factors related to the propensity to reuse, sample instances of different reuse types, and survey developers to better understand their intentions. Our results indicate that copy-based reuse is common, with many developers being aware of it when writing code. The propensity for a file to be reused varies greatly among languages and between source code and binary files, consistently decreasing over time. Files introduced by popular projects are more likely to be reused, but at least half of reused resources originate from ``small'' and ``medium'' projects. Developers had various reasons for reuse but were generally positive about using a package manager.",
      "paper_authors": [
        "Mahmoud Jahanshahi",
        "David Reid",
        "Audris Mockus"
      ],
      "primary_category": "cs.SE",
      "publish_time": "2024-09-07",
      "update_time": "2024-09-07",
      "comments": null,
      "repo_url": "#"
    },
    "2409.04824": {
      "paper_id": "2409.04824v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.04824v1",
      "paper_key": "2409.04824",
      "paper_title": "OSS License Identification at Scale: A Comprehensive Dataset Using World of Code",
      "paper_url": "http://arxiv.org/abs/2409.04824v1",
      "paper_abstract": "The proliferation of open source software (OSS) has led to a complex landscape of licensing practices, making accurate license identification crucial for legal and compliance purposes. This study presents a comprehensive analysis of OSS licenses using the World of Code (WoC) infrastructure. We employ an exhaustive approach, scanning all files containing ``license'' in their filepath, and apply the winnowing algorithm for robust text matching. Our method identifies and matches over 5.5 million distinct license blobs across millions of OSS projects, creating a detailed project-to-license (P2L) map. We verify the accuracy of our approach through stratified sampling and manual review, achieving a final accuracy of 92.08%, with precision of 87.14%, recall of 95.45%, and an F1 score of 91.11%. This work enhances the understanding of OSS licensing practices and provides a valuable resource for developers, researchers, and legal professionals. Future work will expand the scope of license detection to include code files and references to licenses in project documentation.",
      "paper_authors": [
        "Mahmoud Jahanshahi",
        "David Reid",
        "Adam McDaniel",
        "Audris Mockus"
      ],
      "primary_category": "cs.SE",
      "publish_time": "2024-09-07",
      "update_time": "2024-09-07",
      "comments": null,
      "repo_url": "#"
    },
    "2409.04776": {
      "paper_id": "2409.04776v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.04776v2",
      "paper_key": "2409.04776",
      "paper_title": "Algebraic Gromov ellipticity: a brief survey",
      "paper_url": "http://arxiv.org/abs/2409.04776v2",
      "paper_abstract": "We survey on algebraically elliptic varieties in the sense of Gromov.",
      "paper_authors": [
        "Mikhail Zaidenberg"
      ],
      "primary_category": "math.AG",
      "publish_time": "2024-09-07",
      "update_time": "2024-09-10",
      "comments": "20 pages; the formulation of Theorem 2.7 has been corrected, thanks\n  to a remark by J\\'anos Koll\\'ar",
      "repo_url": "#"
    },
    "2409.04773": {
      "paper_id": "2409.04773v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.04773v2",
      "paper_key": "2409.04773",
      "paper_title": "Optimizing reaction and transport fluxes in temperature gradient-driven chemical reaction-diffusion systems",
      "paper_url": "http://arxiv.org/abs/2409.04773v2",
      "paper_abstract": "Temperature gradients represent energy sources that can be harvested to generate steady reaction or transport fluxes. Technological developments could lead to the transfer of free energy from heat sources and sinks to chemical systems for the purpose of extraction, thermal batteries, or nonequilibrium synthesis.   We present a theoretical study of 1D chemical systems subjected to temperature gradients. A complete theoretical framework describes the behavior of the system induced by various temperature profiles. An exact mathematical derivation was established for a simple two-compartment model and was generalized to arbitrary reaction-diffusion systems based on numerical models. An experimental system was eventually scaled and tuned to optimize either nonequilibrium chemical transport or reaction.   The relevant parameters for this description were identified; they focused on the system symmetry for chemical reaction and transport. Nonequilibrium thermodynamic approaches lead to a description analogous to electric circuits. Temperature gradients lead to the onset of a steady chemical force, which maintains steady reaction-diffusion fluxes moderated by chemical resistance. The system activity was then assessed using the entropy production rate as a measure of its dissipated power.   The chemical characteristics of the system can be tuned for general optimization of the nonequilibrium state or for the specific optimization of either transport or reaction processes. The shape of the temperature gradient can be tailored to precisely control the spatial localization of active processes, targeting either precise spatial localization or propagation over large areas. The resulting temperature-driven chemical system can in turn be used to drive secondary processes into either nonequilibrium reaction fluxes or concentration gradients.",
      "paper_authors": [
        "Mohammed Loukili",
        "Ludovic Jullien",
        "Guillaume Baffou",
        "Rapha\u00ebl Plasson"
      ],
      "primary_category": "physics.chem-ph",
      "publish_time": "2024-09-07",
      "update_time": "2024-09-11",
      "comments": "Maint text: 17 pages, 5 figures, 2 tables. Supplementary Information:\n  28 page, 5 figures. Submitted to the Physical Review E",
      "repo_url": "#"
    },
    "2409.04758": {
      "paper_id": "2409.04758v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.04758v1",
      "paper_key": "2409.04758",
      "paper_title": "SGSeg: Enabling Text-free Inference in Language-guided Segmentation of Chest X-rays via Self-guidance",
      "paper_url": "http://arxiv.org/abs/2409.04758v1",
      "paper_abstract": "Segmentation of infected areas in chest X-rays is pivotal for facilitating the accurate delineation of pulmonary structures and pathological anomalies. Recently, multi-modal language-guided image segmentation methods have emerged as a promising solution for chest X-rays where the clinical text reports, depicting the assessment of the images, are used as guidance. Nevertheless, existing language-guided methods require clinical reports alongside the images, and hence, they are not applicable for use in image segmentation in a decision support context, but rather limited to retrospective image analysis after clinical reporting has been completed. In this study, we propose a self-guided segmentation framework (SGSeg) that leverages language guidance for training (multi-modal) while enabling text-free inference (uni-modal), which is the first that enables text-free inference in language-guided segmentation. We exploit the critical location information of both pulmonary and pathological structures depicted in the text reports and introduce a novel localization-enhanced report generation (LERG) module to generate clinical reports for self-guidance. Our LERG integrates an object detector and a location-based attention aggregator, weakly-supervised by a location-aware pseudo-label extraction module. Extensive experiments on a well-benchmarked QaTa-COV19 dataset demonstrate that our SGSeg achieved superior performance than existing uni-modal segmentation methods and closely matched the state-of-the-art performance of multi-modal language-guided segmentation methods.",
      "paper_authors": [
        "Shuchang Ye",
        "Mingyuan Meng",
        "Mingjian Li",
        "Dagan Feng",
        "Jinman Kim"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-07",
      "update_time": "2024-09-07",
      "comments": "This preprint has not undergone peer review or any post-submission\n  improvments or corrections",
      "repo_url": "https://github.com/shuchangye-bib/sgseg"
    },
    "2409.04720": {
      "paper_id": "2409.04720v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.04720v1",
      "paper_key": "2409.04720",
      "paper_title": "A Comprehensive Survey on Evidential Deep Learning and Its Applications",
      "paper_url": "http://arxiv.org/abs/2409.04720v1",
      "paper_abstract": "Reliable uncertainty estimation has become a crucial requirement for the industrial deployment of deep learning algorithms, particularly in high-risk applications such as autonomous driving and medical diagnosis. However, mainstream uncertainty estimation methods, based on deep ensembling or Bayesian neural networks, generally impose substantial computational overhead. To address this challenge, a novel paradigm called Evidential Deep Learning (EDL) has emerged, providing reliable uncertainty estimation with minimal additional computation in a single forward pass. This survey provides a comprehensive overview of the current research on EDL, designed to offer readers a broad introduction to the field without assuming prior knowledge. Specifically, we first delve into the theoretical foundation of EDL, the subjective logic theory, and discuss its distinctions from other uncertainty estimation frameworks. We further present existing theoretical advancements in EDL from four perspectives: reformulating the evidence collection process, improving uncertainty estimation via OOD samples, delving into various training strategies, and evidential regression networks. Thereafter, we elaborate on its extensive applications across various machine learning paradigms and downstream tasks. In the end, an outlook on future directions for better performances and broader adoption of EDL is provided, highlighting potential research avenues.",
      "paper_authors": [
        "Junyu Gao",
        "Mengyuan Chen",
        "Liangyu Xiang",
        "Changsheng Xu"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-07",
      "update_time": "2024-09-07",
      "comments": null,
      "repo_url": "https://github.com/mengyuanchen21/awesome-evidential-deep-learning"
    },
    "2409.04711": {
      "paper_id": "2409.04711v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.04711v1",
      "paper_key": "2409.04711",
      "paper_title": "Algorithmic Scenario Generation as Quality Diversity Optimization",
      "paper_url": "http://arxiv.org/abs/2409.04711v1",
      "paper_abstract": "The increasing complexity of robots and autonomous agents that interact with people highlights the critical need for approaches that systematically test them before deployment. This review paper presents a general framework for solving this problem, describes the insights that we have gained from working on each component of the framework, and shows how integrating these components leads to the discovery of a diverse range of realistic and challenging scenarios that reveal previously unknown failures in deployed robotic systems interacting with people.",
      "paper_authors": [
        "Stefanos Nikolaidis"
      ],
      "primary_category": "cs.AI",
      "publish_time": "2024-09-07",
      "update_time": "2024-09-07",
      "comments": null,
      "repo_url": "#"
    },
    "2409.04676": {
      "paper_id": "2409.04676v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.04676v1",
      "paper_key": "2409.04676",
      "paper_title": "Exploring Crowdworkers' Perceptions, Current Practices, and Desired Practices Regarding Using Non-Workstation Devices for Crowdwork",
      "paper_url": "http://arxiv.org/abs/2409.04676v1",
      "paper_abstract": "Despite a plethora of research dedicated to designing HITs for non-workstations, there is a lack of research looking specifically into workers' perceptions of the suitability of these devices for managing and completing work. In this work, we fill this research gap by conducting an online survey of 148 workers on Amazon Mechanical Turk to explore 1. how crowdworkers currently use their non-workstation devices to complete and manage crowdwork, 2. what challenges they face using those devices, and 3. to what extent they wish they could use those devices if their concerns were addressed. Our results show that workers unanimously favor using a desktop to complete and manage crowdwork. While workers occasionally use smartphones or tablets, they find their suitability marginal at best and have little interest in smart speakers and smartwatches, viewing them as unsuitable for crowdwork. When investigating the reason for these views, we find that the key issue is that non workstation devices lack the tooling necessary to automatically find and accept HITs, tooling that workers view as essential in their efforts to compete with bots in accepting high paying work. To address this problem, we propose a new paradigm for finding, accepting, and completing crowdwork that puts crowdworkers on equal footing with bots in these tasks. We also describe future research directions for tailoring HITs to non workstation devices and definitely answering whether smart speakers and smartwatches have a place in crowdwork.",
      "paper_authors": [
        "Senjuti Dutta",
        "Scott Ruoti",
        "Rhema Linder",
        "Alex C. Williams",
        "Anastasia Kuzminykh"
      ],
      "primary_category": "cs.HC",
      "publish_time": "2024-09-07",
      "update_time": "2024-09-07",
      "comments": null,
      "repo_url": "#"
    },
    "2409.09030": {
      "paper_id": "2409.09030v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09030v1",
      "paper_key": "2409.09030",
      "paper_title": "Agents in Software Engineering: Survey, Landscape, and Vision",
      "paper_url": "http://arxiv.org/abs/2409.09030v1",
      "paper_abstract": "In recent years, Large Language Models (LLMs) have achieved remarkable success and have been widely used in various downstream tasks, especially in the tasks of the software engineering (SE) field. We find that many studies combining LLMs with SE have employed the concept of agents either explicitly or implicitly. However, there is a lack of an in-depth survey to sort out the development context of existing works, analyze how existing works combine the LLM-based agent technologies to optimize various tasks, and clarify the framework of LLM-based agents in SE. In this paper, we conduct the first survey of the studies on combining LLM-based agents with SE and present a framework of LLM-based agents in SE which includes three key modules: perception, memory, and action. We also summarize the current challenges in combining the two fields and propose future opportunities in response to existing challenges. We maintain a GitHub repository of the related papers at: https://github.com/DeepSoftwareAnalytics/Awesome-Agent4SE.",
      "paper_authors": [
        "Yanxian Huang",
        "Wanjun Zhong",
        "Ensheng Shi",
        "Min Yang",
        "Jiachi Chen",
        "Hui Li",
        "Yuchi Ma",
        "Qianxiang Wang",
        "Zibin Zheng",
        "Yanlin Wang"
      ],
      "primary_category": "cs.SE",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "12 pages, 4 figures",
      "repo_url": "https://github.com/deepsoftwareanalytics/awesome-agent4se"
    },
    "2409.09028": {
      "paper_id": "2409.09028v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09028v1",
      "paper_key": "2409.09028",
      "paper_title": "The effect of cosmic web filaments on galaxy evolution",
      "paper_url": "http://arxiv.org/abs/2409.09028v1",
      "paper_abstract": "Galaxy properties are known to be affected by their environment. This is well established for the extremes of the density scales, between the high-density cluster environment and the low-density field. It is however not fully understood how the intermediate-density regime of cosmic web filaments affects galaxy evolution. We investigate this environmental effect using a mass complete sample of 23,441 galaxies in the Sloan Digital Sky Survey DR8 Main Galaxy Sample (${M}_{\\text{Stellar}} > 10^{9.91} \\text{M}_{\\odot}$). We define 6 environments, probing different density regimes and representing unique stages in the structure formation process, comparing the differences in star formation activity and morphology between them. We find that galaxies in filaments tend to be less star forming and favour more early-type morphologies than those in the field. These differences persist when considering stellar mass-matched samples, suggesting that this is a consequence of the environment. We further investigate whether these trends are a result of the large scale or local environment through constructing samples matched both in stellar mass and local galaxy density. We find that when also matching in local galaxy density, the differences observed between the filament and field population vanishes, concluding that the environmental effect of filaments can be entirely parameterised by a local galaxy density index. We find that differences can still be seen in comparisons with the interiors of clusters, suggesting these are unique environments which can impart additional physical processes not characterised by local galaxy density.",
      "paper_authors": [
        "Callum J. O'Kane",
        "Ulrike Kuchner",
        "Meghan E. Gray",
        "Alfonso Arag\u00f3n-Salamanca"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "19 pages, 10 figures, Accepted for publication in MNRAS",
      "repo_url": "#"
    },
    "2409.09005": {
      "paper_id": "2409.09005v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09005v1",
      "paper_key": "2409.09005",
      "paper_title": "Dunkl and Cherednik operators",
      "paper_url": "http://arxiv.org/abs/2409.09005v1",
      "paper_abstract": "This survey article, written for the Encyclopedia of Mathematical Physics, 2nd edition, is devoted to the remarkable family of operators introduced by Charles Dunkl and to their $q$-analogues discovered by Ivan Cherednik. The main focus is on the r\\^ole of these operators in studying integrable many-body systems such as the Calogero-Moser and the Ruijsenaars systems. To put these constructions into a wider context, we indicate their relationship with the theory of the rational Cherednik algebras and double affine Hecke algebras. While we do not include proofs, references to the original research articles are provided, accompanied by brief historical comments.",
      "paper_authors": [
        "Oleg Chalykh"
      ],
      "primary_category": "math-ph",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "25 pages",
      "repo_url": "#"
    },
    "2409.08999": {
      "paper_id": "2409.08999v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08999v1",
      "paper_key": "2409.08999",
      "paper_title": "Predicting High Magnification Events in Microlensed Quasars in the Era of LSST using Recurrent Neural Networks",
      "paper_url": "http://arxiv.org/abs/2409.08999v1",
      "paper_abstract": "Upcoming wide field surveys such as the Rubin Observatory's Legacy Survey of Space and Time (LSST) will monitor thousands of strongly lensed quasars over a 10-year period. Many of these monitored quasars will undergo high magnification events (HMEs) through microlensing as the accretion disk crosses a caustic, places of infinite magnification. Microlensing allows us to map the inner regions of the accretion disk as it crosses a caustic, even at large cosmological distances. The observational cadences of LSST are not ideal for probing the inner regions of the accretion disk, so there is a need to predict HMEs as early as possible to trigger high-cadence multi-band or spectroscopic follow-up observations. Here we simulate a diverse and realistic sample of 10-year quasar microlensing light curves to train a recurrent neural network (RNN) to predict HMEs before they occur by classifying the location of the peaks at each time step. This is the first deep learning approach to predict HMEs. We give estimates at how well we expect to predict HME peaks during LSST and benchmark how our metrics change with different cadence strategies. With LSST-like observations, we can predict approximately 55% of HME peaks corresponding to tens to hundreds per year and a false positive rate of around 20% compared to the number of HMEs. Our network can be continuously applied throughout the LSST survey, providing crucial alerts to optimize follow-up resources.",
      "paper_authors": [
        "Joshua Fagin",
        "Eric Paic",
        "Favio Neira",
        "Henry Best",
        "Timo Anguita",
        "Martin Millon",
        "Matthew O'Dowd",
        "Dominique Sluse",
        "Georgios Vernardos"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "16 pages, 9 figures, submitted to ApJ",
      "repo_url": "#"
    },
    "2409.08988": {
      "paper_id": "2409.08988v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08988v1",
      "paper_key": "2409.08988",
      "paper_title": "Assembly of Complex Colloidal Systems Using DNA",
      "paper_url": "http://arxiv.org/abs/2409.08988v1",
      "paper_abstract": "Nearly thirty years after its inception, the field of DNA-programmed colloidal self-assembly has begun to realize its initial promise. In this review, we summarize recent developments in designing effective interactions and understanding the dynamic self-assembly pathways of DNA-coated nanoparticles and microparticles, as well as how these advances have propelled tremendous progress in crystal engineering. We also highlight exciting new directions showing that new classes of subunits combining nanoparticles with DNA origami can be used to engineer novel multicomponent assemblies, including structures with self-limiting, finite sizes. We conclude by providing an outlook on how recent theoretical advances focusing on the kinetics of self-assembly could usher in new materials-design opportunities, like the possibility of retrieving multiple distinct target structures from a single suspension or accessing new classes of materials that are stabilized by energy dissipation, mimicking self-assembly in living systems.",
      "paper_authors": [
        "William M. Jacobs",
        "W. Benjamin Rogers"
      ],
      "primary_category": "cond-mat.soft",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": null,
      "repo_url": "#"
    },
    "2409.08986": {
      "paper_id": "2409.08986v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08986v1",
      "paper_key": "2409.08986",
      "paper_title": "The connection of polaritonic chemistry with the physics of a spin glass",
      "paper_url": "http://arxiv.org/abs/2409.08986v1",
      "paper_abstract": "Polaritonic chemistry has garnered increasing attention in recent years due to pioneering experimental results, which show that site- and bond-selective chemistry at room temperature is achievable through strong collective coupling to field fluctuations in optical cavities. Despite these notable experimental strides, the underlying theoretical mechanisms remain unclear. In this focus review, we highlight a fundamental theoretical link between the seemingly unrelated fields of polaritonic chemistry and spin glasses, exploring its profound implications for the theoretical framework of polaritonic chemistry. Specifically, we present a mapping of the dressed electronic structure problem under collective vibrational strong coupling to the iconic Sherrington-Kirkpatrick model of spin glasses. This mapping uncovers a collectively induced instability in the dressed electronic structure (spontaneous replica symmetry breaking), which could provide the long-sought seed for significant local chemical modifications in polaritonic chemistry. This mapping paves the way to incorporate, adjust and probe numerous spin glass concepts in polaritonic chemistry, such as frustration, aging dynamics, excess of thermal fluctuations, time-reversal symmetry breaking or stochastic resonances. Ultimately, the mapping also offers fresh insights into the applicability of spin glass theory beyond condensed matter systems and it suggests novel theoretical directions such as polarization glasses with explicitly time-dependent order parameter functions.",
      "paper_authors": [
        "Dominik Sidler",
        "Michael Ruggenthaler",
        "Angel Rubio"
      ],
      "primary_category": "physics.chem-ph",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": null,
      "repo_url": "#"
    },
    "2409.08980": {
      "paper_id": "2409.08980v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08980v1",
      "paper_key": "2409.08980",
      "paper_title": "Predicting Trust In Autonomous Vehicles: Modeling Young Adult Psychosocial Traits, Risk-Benefit Attitudes, And Driving Factors With Machine Learning",
      "paper_url": "http://arxiv.org/abs/2409.08980v1",
      "paper_abstract": "Low trust remains a significant barrier to Autonomous Vehicle (AV) adoption. To design trustworthy AVs, we need to better understand the individual traits, attitudes, and experiences that impact people's trust judgements. We use machine learning to understand the most important factors that contribute to young adult trust based on a comprehensive set of personal factors gathered via survey (n = 1457). Factors ranged from psychosocial and cognitive attributes to driving style, experiences, and perceived AV risks and benefits. Using the explainable AI technique SHAP, we found that perceptions of AV risks and benefits, attitudes toward feasibility and usability, institutional trust, prior experience, and a person's mental model are the most important predictors. Surprisingly, psychosocial and many technology- and driving-specific factors were not strong predictors. Results highlight the importance of individual differences for designing trustworthy AVs for diverse groups and lead to key implications for future design and research.",
      "paper_authors": [
        "Robert Kaufman",
        "Emi Lee",
        "Manas Satish Bedmutha",
        "David Kirsh",
        "Nadir Weibel"
      ],
      "primary_category": "cs.HC",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "31 pages (including references and appendix), 7 figures, 7 tables",
      "repo_url": "#"
    },
    "2409.08966": {
      "paper_id": "2409.08966v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08966v1",
      "paper_key": "2409.08966",
      "paper_title": "User Identity Linkage on Social Networks: A Review of Modern Techniques and Applications",
      "paper_url": "http://arxiv.org/abs/2409.08966v1",
      "paper_abstract": "In an Online Social Network (OSN), users can create a unique public persona by crafting a user identity that may encompass profile details, content, and network-related information. As a result, a relevant task of interest is related to the ability to link identities across different OSNs. Linking users across social networks can have multiple implications in several contexts both at the individual level and at the group level. At the individual level, the main interest in linking the same identity across social networks is to enable a better knowledge of each user. At the group level, linking user identities through different OSNs helps in predicting user behaviors, network dynamics, information diffusion, and migration phenomena across social media. The process of tying together user accounts on different OSNs is challenging and has attracted more and more research attention in the last fifteen years. The purpose of this work is to provide a comprehensive review of recent studies (from 2016 to the present) on User Identity Linkage (UIL) methods across online social networks. This review aims to offer guidance for other researchers in the field by outlining the main problem formulations, the different feature extraction strategies, algorithms, machine learning models, datasets, and evaluation metrics proposed by researchers working in this area. The proposed overview takes a pragmatic perspective to highlight the concrete possibilities for accomplishing this task depending on the type of available data.",
      "paper_authors": [
        "Caterina Senette",
        "Marco Siino",
        "Maurizio Tesconi"
      ],
      "primary_category": "cs.SI",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "25 pages, 4 figures",
      "repo_url": "#"
    },
    "2409.08964": {
      "paper_id": "2409.08964v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08964v1",
      "paper_key": "2409.08964",
      "paper_title": "IMMERTWIN: A Mixed Reality Framework for Enhanced Robotic Arm Teleoperation",
      "paper_url": "http://arxiv.org/abs/2409.08964v1",
      "paper_abstract": "We present IMMERTWIN, a mixed reality framework for enhance robotic arm teleoperation using a closed-loop digital twin as a bridge for interaction between the user and the robotic system. We evaluated IMMERTWIN by performing a medium-scale user survey with 26 participants on two robots. Users were asked to teleoperate with both robots inside the virtual environment to pick and place 3 cubes in a tower and to repeat this task as many times as possible in 10 minutes, with only 5 minutes of training beforehand. Our experimental results show that most users were able to succeed by building at least a tower of 3 cubes regardless of the robot used and a maximum of 10 towers (1 tower per minute). In addition, users preferred to use IMMERTWIN over our previous work, TELESIM, as it caused them less mental workload. The project website and source code can be found at: https://cvas-ug.github.io/immertwin",
      "paper_authors": [
        "Florent P. Audonnet",
        "Ixchel G. Ramirez-Alpizar",
        "Gerardo Aragon-Camarasa"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": null,
      "repo_url": "#"
    },
    "2409.08922": {
      "paper_id": "2409.08922v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08922v1",
      "paper_key": "2409.08922",
      "paper_title": "Some recent developments on isometric immersions via compensated compactness and gauge transforms",
      "paper_url": "http://arxiv.org/abs/2409.08922v1",
      "paper_abstract": "We survey recent developments on the analysis of Gauss--Codazzi--Ricci equations, the first-order PDE system arising from the classical problem of isometric immersions in differential geometry, especially in the regime of low Sobolev regularity. Such equations are not purely elliptic, parabolic, or hyperbolic in general, hence calling for analytical tools for PDEs of mixed types. We discuss various recent contributions -- in line with the pioneering works by G.-Q. Chen, M. Slemrod, and D. Wang [Proc. Amer. Math. Soc. (2010); Comm. Math. Phys. (2010)] -- on the weak continuity of Gauss--Codazzi--Ricci equations, the weak stability of isometric immersions, and the fundamental theorem of submanifold theory with low regularity. Two mixed-type PDE techniques are emphasised throughout these developments: the method of compensated compactness and the theory of Coulomb--Uhlenbeck gauges.",
      "paper_authors": [
        "Siran Li"
      ],
      "primary_category": "math.AP",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "18 pages. Survey article dedicated to Prof. Gui-Qiang Chen's 60th\n  birthday",
      "repo_url": "#"
    },
    "2409.08893": {
      "paper_id": "2409.08893v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08893v1",
      "paper_key": "2409.08893",
      "paper_title": "The $M_\\bullet$-$\u03c3_e$ relation for local type 1 AGNs and quasars",
      "paper_url": "http://arxiv.org/abs/2409.08893v1",
      "paper_abstract": "We analyzed MUSE observations of 42 local $z<0.1$ type 1 active galactic nucleus (AGN) host galaxies taken from the Palomar-Green quasar sample and the close AGN reference survey. Our goal was to study the relation between the black hole mass ($M_\\bullet$) and bulge stellar velocity dispersion ($\\sigma_e$) for type 1 active galaxies. The sample spans black hole masses of $10^{6.0}-10^{9.2}\\,M_\\odot$, bolometric luminosities of $10^{42.9}-10^{46.0}\\,$erg$\\,$s$^{-1}$, and Eddington ratios of 0.006-1.2. We avoided AGN emission by extracting the spectra over annular apertures. We modeled the calcium triplet stellar features and measured stellar velocity dispersions of $\\sigma_* = 60-230\\,$km$\\,$s$^{-1}$ for the host galaxies. We find $\\sigma_*$ values in agreement with previous measurements for local AGN host galaxies, but slightly lower compared with those reported for nearby X-ray-selected type 2 quasars. Using a novel annular aperture correction recipe to estimate $\\sigma_e$ from $\\sigma_*$ that considers the bulge morphology and observation beam-smearing, we estimate flux-weighted $\\sigma_e = 60-250\\,$km$\\,$s$^{-1}$. If we consider the bulge type when estimating $M_\\bullet$, we find no statistical difference between the distributions of AGN hosts and the inactive galaxies on the $M_\\bullet - \\sigma_e$ plane for $M_\\bullet \\lesssim 10^8\\,M_\\odot$. Conversely, if we do not consider the bulge type when computing $M_\\bullet$, we find that both distributions disagree. We find no correlation between the degree of offset from the $M_\\bullet - \\sigma_e$ relation and Eddington ratio for $M_\\bullet \\lesssim 10^8\\,M_\\odot$. The current statistics preclude firm conclusions from being drawn for the high-mass range. We argue these observations support notions that a significant fraction of the local type 1 AGNs and quasars have undermassive black holes compared with their host galaxy bulge properties.",
      "paper_authors": [
        "J. Molina",
        "L. C. Ho",
        "K. K. Knudsen"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "18 pages, plus Appendix. 15 Figures, 4 Tables. Accepted for\n  publication on A&A",
      "repo_url": "#"
    },
    "2409.08888": {
      "paper_id": "2409.08888v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08888v1",
      "paper_key": "2409.08888",
      "paper_title": "Characterization of M51 supernova remnants with the imaging spectrometer SITELLE",
      "paper_url": "http://arxiv.org/abs/2409.08888v1",
      "paper_abstract": "We present preliminary results of a detailed 3D study of supernova remnants in the nearby spiral M51 using data from the SIGNALS survey obtained with the imaging Fourier transform spectrometer SITELLE at the Canada-France-Hawaii telescope (CFHT). Data cubes covering the entire galaxy were gathered in three spectral ranges: SN3 (647-685 nm, R = 5000), SN2 (482-513 nm, R = 600) and SN1 (363-386 nm, R = 1000). The spectral resolution of the SN3 cube allows a precise, spatially resolved measurement of the velocity dispersion of each object. While most of the SNRs were known from previous surveys based on imagery and long-slit spectroscopy, we now provide 2D line flux and kinematic maps for all of them and found 20 new candidates. Most of the SNRs show velocity dispersions ($\\sigma$) in the range 30-80 km/s, which is typical for middle-aged SNRs. Finally, we compare the properties of SNRs with those of thousands of HII regions included in the same dataset.",
      "paper_authors": [
        "Billy Gamache",
        "Laurent Drissen",
        "Carmelle Robert",
        "Mykola Posternak"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "Poster presentation at Supernova Remnants III : An Odyssey in Space\n  after Stellar death conference in Crete 2024",
      "repo_url": "#"
    },
    "2409.08887": {
      "paper_id": "2409.08887v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08887v1",
      "paper_key": "2409.08887",
      "paper_title": "Visual Language Tracking with Multi-modal Interaction: A Robust Benchmark",
      "paper_url": "http://arxiv.org/abs/2409.08887v1",
      "paper_abstract": "Visual Language Tracking (VLT) enhances tracking by mitigating the limitations of relying solely on the visual modality, utilizing high-level semantic information through language. This integration of the language enables more advanced human-machine interaction. The essence of interaction is cognitive alignment, which typically requires multiple information exchanges, especially in the sequential decision-making process of VLT. However, current VLT benchmarks do not account for multi-round interactions during tracking. They provide only an initial text and bounding box (bbox) in the first frame, with no further interaction as tracking progresses, deviating from the original motivation of the VLT task. To address these limitations, we propose a novel and robust benchmark, VLT-MI (Visual Language Tracking with Multi-modal Interaction), which introduces multi-round interaction into the VLT task for the first time. (1) We generate diverse, multi-granularity texts for multi-round, multi-modal interaction based on existing mainstream VLT benchmarks using DTLLM-VLT, leveraging the world knowledge of LLMs. (2) We propose a new VLT interaction paradigm that achieves multi-round interaction through text updates and object recovery. When multiple tracking failures occur, we provide the tracker with more aligned texts and corrected bboxes through interaction, thereby expanding the scope of VLT downstream tasks. (3) We conduct comparative experiments on both traditional VLT benchmarks and VLT-MI, evaluating and analyzing the accuracy and robustness of trackers under the interactive paradigm. This work offers new insights and paradigms for the VLT task, enabling a fine-grained evaluation of multi-modal trackers. We believe this approach can be extended to additional datasets in the future, supporting broader evaluations and comparisons of video-language model capabilities.",
      "paper_authors": [
        "Xuchen Li",
        "Shiyu Hu",
        "Xiaokun Feng",
        "Dailing Zhang",
        "Meiqi Wu",
        "Jing Zhang",
        "Kaiqi Huang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "Under Review",
      "repo_url": "#"
    },
    "2409.08881": {
      "paper_id": "2409.08881v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08881v1",
      "paper_key": "2409.08881",
      "paper_title": "Data Efficient Child-Adult Speaker Diarization with Simulated Conversations",
      "paper_url": "http://arxiv.org/abs/2409.08881v1",
      "paper_abstract": "Automating child speech analysis is crucial for applications such as neurocognitive assessments. Speaker diarization, which identifies ``who spoke when'', is an essential component of the automated analysis. However, publicly available child-adult speaker diarization solutions are scarce due to privacy concerns and a lack of annotated datasets, while manually annotating data for each scenario is both time-consuming and costly. To overcome these challenges, we propose a data-efficient solution by creating simulated child-adult conversations using AudioSet. We then train a Whisper Encoder-based model, achieving strong zero-shot performance on child-adult speaker diarization using real datasets. The model performance improves substantially when fine-tuned with only 30 minutes of real train data, with LoRA further improving the transfer learning performance. The source code and the child-adult speaker diarization model trained on simulated conversations are publicly available.",
      "paper_authors": [
        "Anfeng Xu",
        "Tiantian Feng",
        "Helen Tager-Flusberg",
        "Catherine Lord",
        "Shrikanth Narayanan"
      ],
      "primary_category": "eess.AS",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "Under review",
      "repo_url": "https://github.com/usc-sail/child-adult-diarization"
    },
    "2409.08866": {
      "paper_id": "2409.08866v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08866v1",
      "paper_key": "2409.08866",
      "paper_title": "Topological gauge theory of vortices in type-III superconductors",
      "paper_url": "http://arxiv.org/abs/2409.08866v1",
      "paper_abstract": "Traditional superconductors fall into two categories, type-I, expelling magnetic fields, and type-II, into which magnetic fields exceeding a lower critical field $H_{\\rm c1}$ penetrate in form of Abrikosov vortices. Abrikosov vortices are characterized by two spatial scales, the size of the normal core, $\\xi$, where the superconducting order parameter is suppressed and the London penetration depth $\\lambda$, describing the scale at which circulating superconducting currents forming vortices start to noticeably drop. Here we demonstrate that a novel type-III superconductivity, realized in granular media in any dimension hosts a novel vortex physics. Type-III vortices have no cores, are logarithmically confined and carry only a gauge scale $\\lambda$. Accordingly, in type-III superconductors $H_{\\rm c1}=0$ at zero temperature and the Ginzburg-Landau theory must be replaced by a topological gauge theory. Type-III superconductivity is destroyed not by Cooper pair breaking but by vortex proliferation generalizing the Berezinskii-Kosterlitz-Thouless mechanism to any dimension.",
      "paper_authors": [
        "M. C. Diamantini",
        "C. A. Trugenberger",
        "V. M. Vinokur"
      ],
      "primary_category": "cond-mat.supr-con",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": null,
      "repo_url": "#"
    },
    "2409.08823": {
      "paper_id": "2409.08823v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08823v1",
      "paper_key": "2409.08823",
      "paper_title": "AutoIRT: Calibrating Item Response Theory Models with Automated Machine Learning",
      "paper_url": "http://arxiv.org/abs/2409.08823v1",
      "paper_abstract": "Item response theory (IRT) is a class of interpretable factor models that are widely used in computerized adaptive tests (CATs), such as language proficiency tests. Traditionally, these are fit using parametric mixed effects models on the probability of a test taker getting the correct answer to a test item (i.e., question). Neural net extensions of these models, such as BertIRT, require specialized architectures and parameter tuning. We propose a multistage fitting procedure that is compatible with out-of-the-box Automated Machine Learning (AutoML) tools. It is based on a Monte Carlo EM (MCEM) outer loop with a two stage inner loop, which trains a non-parametric AutoML grade model using item features followed by an item specific parametric model. This greatly accelerates the modeling workflow for scoring tests. We demonstrate its effectiveness by applying it to the Duolingo English Test, a high stakes, online English proficiency test. We show that the resulting model is typically more well calibrated, gets better predictive performance, and more accurate scores than existing methods (non-explanatory IRT models and explanatory IRT models like BERT-IRT). Along the way, we provide a brief survey of machine learning methods for calibration of item parameters for CATs.",
      "paper_authors": [
        "James Sharpnack",
        "Phoebe Mulcaire",
        "Klinton Bicknell",
        "Geoff LaFlair",
        "Kevin Yancey"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": null,
      "repo_url": "#"
    },
    "2409.08811": {
      "paper_id": "2409.08811v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08811v1",
      "paper_key": "2409.08811",
      "paper_title": "Mutual Theory of Mind in Human-AI Collaboration: An Empirical Study with LLM-driven AI Agents in a Real-time Shared Workspace Task",
      "paper_url": "http://arxiv.org/abs/2409.08811v1",
      "paper_abstract": "Theory of Mind (ToM) significantly impacts human collaboration and communication as a crucial capability to understand others. When AI agents with ToM capability collaborate with humans, Mutual Theory of Mind (MToM) arises in such human-AI teams (HATs). The MToM process, which involves interactive communication and ToM-based strategy adjustment, affects the team's performance and collaboration process. To explore the MToM process, we conducted a mixed-design experiment using a large language model-driven AI agent with ToM and communication modules in a real-time shared-workspace task. We find that the agent's ToM capability does not significantly impact team performance but enhances human understanding of the agent and the feeling of being understood. Most participants in our study believe verbal communication increases human burden, and the results show that bidirectional communication leads to lower HAT performance. We discuss the results' implications for designing AI agents that collaborate with humans in real-time shared workspace tasks.",
      "paper_authors": [
        "Shao Zhang",
        "Xihuai Wang",
        "Wenhao Zhang",
        "Yongshan Chen",
        "Landi Gao",
        "Dakuo Wang",
        "Weinan Zhang",
        "Xinbing Wang",
        "Ying Wen"
      ],
      "primary_category": "cs.HC",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "34 pages, Preprint Under Review",
      "repo_url": "#"
    },
    "2409.08809": {
      "paper_id": "2409.08809v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08809v1",
      "paper_key": "2409.08809",
      "paper_title": "Statistical Analysis of Quantitative Cancer Imaging Data",
      "paper_url": "http://arxiv.org/abs/2409.08809v1",
      "paper_abstract": "Recent advances in types and extent of medical imaging technologies has led to proliferation of multimodal quantitative imaging data in cancer. Quantitative medical imaging data refer to numerical representations derived from medical imaging technologies, such as radiology and pathology imaging, that can be used to assess and quantify characteristics of diseases, especially cancer. The use of such data in both clinical and research setting enables precise quantifications and analyses of tumor characteristics that can facilitate objective evaluation of disease progression, response to therapy, and prognosis. The scale and size of these imaging biomarkers is vast and presents several analytical and computational challenges that range from high-dimensionality to complex structural correlation patterns. In this review article, we summarize some state-of-the-art statistical methods developed for quantitative medical imaging data ranging from topological, functional and shape data analyses to spatial process models. We delve into common imaging biomarkers with a focus on radiology and pathology imaging in cancer, address the analytical questions and challenges they present, and highlight the innovative statistical and machine learning models that have been developed to answer relevant scientific and clinical questions. We also outline some emerging and open problems in this area for future explorations.",
      "paper_authors": [
        "Shariq Mohammed",
        "Maria Masotti",
        "Nathaniel Osher",
        "Satwik Acharyya",
        "Veerabhadran Baladandayuthapani"
      ],
      "primary_category": "stat.AP",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": null,
      "repo_url": "#"
    },
    "2409.08803": {
      "paper_id": "2409.08803v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08803v1",
      "paper_key": "2409.08803",
      "paper_title": "A Review on Flood Risk Conceptual Frameworks and Development of Hierarchical Structures for Assessment Criteria",
      "paper_url": "http://arxiv.org/abs/2409.08803v1",
      "paper_abstract": "Climate change and rapid urbanization have led to more frequent and severe flooding, causing significant damage. The existing literature on flood risk encompasses a variety of dimensions, such as physical, economic, social, political, environmental, infrastructural, and managerial aspects. This paper aims to provide an extensive review of proposed conceptual frameworks and their components used in flood risk assessment. For this purpose, Initially, conceptual frameworks were extracted to configure the components of flood risk including hazard, vulnerability, exposure, resilience, and susceptibility. Subsequently, a comprehensive set of criteria from the literature were identified, addressing risk components. In this paper, the risk conceptual framework is defined by the intersection of vulnerability and hazard. Vulnerability, shaped by exposure and susceptibility, can be reduced by enhancing resiliency, which includes coping and adaptive capacities. In total, 102 criteria/subcriteria were identified and classified into three hierarchical structures of hazard, susceptibility, and resilience. Finally, flood risk assessment methods were reviewed, with an emphasis on their applicability and characteristics. The review highlighted the strengths and limitations of various methods, providing a comprehensive overview of their suitability for different scenarios. The outcomes of this review could serve as a valuable reference for professionals involved in flood risk assessment, aiding in the identification of the most appropriate risk concepts, assessment criteria, and suitable methods for quantification based on the specific study area and data availability.",
      "paper_authors": [
        "Nazgol Tabasi",
        "Mohammad Fereshtehpour",
        "Bardia Roghani"
      ],
      "primary_category": "physics.soc-ph",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": null,
      "repo_url": "#"
    },
    "2409.08759": {
      "paper_id": "2409.08759v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08759v1",
      "paper_key": "2409.08759",
      "paper_title": "Dark Energy Survey: 2.1% measurement of the Baryon Acoustic Oscillation scale from the final dataset",
      "paper_url": "http://arxiv.org/abs/2409.08759v1",
      "paper_abstract": "Here, we present the angular diameter distance measurement obtained from the measurement of the Baryonic Acoustic Oscillation (BAO) feature using the completed Dark Energy Survey (DES) data, summarizing the main results of [Phys. Rev. D 110, 063514] and [Phys. Rev. D 110, 063515]. We use a galaxy sample optimized for BAO science in the redshift range 0.6 < z < 1.2, with an effective redshift of $z_{\\rm eff}$ = 0.85. Our consensus measurement constrains the ratio of the angular distance to the sound horizon scale to $D_M(z_{\\rm eff})/r_d$ = 19.51 $\\pm$ 0.41. This measurement is found to be 2.13$\\sigma$ below the angular BAO scale predicted by Planck. To date, it represents the most precise measurement from purely photometric data, and the most precise from any Stage-III experiment at such high redshift. The analysis was performed blinded to the BAO position and is shown to be robust against analysis choices, data removal, redshift calibrations and observational systematics.",
      "paper_authors": [
        "Juan Mena-Fern\u00e1ndez",
        "Dark Energy Survey Collaboration"
      ],
      "primary_category": "astro-ph.CO",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "4 pages, 3 figures. Contribution to the 2024 Cosmology session of the\n  58th Rencontres de Moriond",
      "repo_url": "#"
    },
    "2409.08751": {
      "paper_id": "2409.08751v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08751v1",
      "paper_key": "2409.08751",
      "paper_title": "A Grading Rubric for AI Safety Frameworks",
      "paper_url": "http://arxiv.org/abs/2409.08751v1",
      "paper_abstract": "Over the past year, artificial intelligence (AI) companies have been increasingly adopting AI safety frameworks. These frameworks outline how companies intend to keep the potential risks associated with developing and deploying frontier AI systems to an acceptable level. Major players like Anthropic, OpenAI, and Google DeepMind have already published their frameworks, while another 13 companies have signaled their intent to release similar frameworks by February 2025. Given their central role in AI companies' efforts to identify and address unacceptable risks from their systems, AI safety frameworks warrant significant scrutiny. To enable governments, academia, and civil society to pass judgment on these frameworks, this paper proposes a grading rubric. The rubric consists of seven evaluation criteria and 21 indicators that concretize the criteria. Each criterion can be graded on a scale from A (gold standard) to F (substandard). The paper also suggests three methods for applying the rubric: surveys, Delphi studies, and audits. The purpose of the grading rubric is to enable nuanced comparisons between frameworks, identify potential areas of improvement, and promote a race to the top in responsible AI development.",
      "paper_authors": [
        "Jide Alaga",
        "Jonas Schuett",
        "Markus Anderljung"
      ],
      "primary_category": "cs.CY",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "16 pages, 4 tables",
      "repo_url": "#"
    },
    "2409.08740": {
      "paper_id": "2409.08740v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08740v1",
      "paper_key": "2409.08740",
      "paper_title": "Another look at qualitative properties of eigenvalues using effective Hamiltonians",
      "paper_url": "http://arxiv.org/abs/2409.08740v1",
      "paper_abstract": "The goal of this paper is to review several qualitative properties of well-known eigenvalue problems using a different perspective based on the theory of effective Hamiltonians, working exclusively on the Hopf-Cole transform of the equation. We revisit some monotonicity results as well as the derivation of several scaling limits by means of the Donsker-Varadhan formula, and we point out several differences between the case of quadratic Hamiltonians and non-quadratic ones.",
      "paper_authors": [
        "Idriss Mazari-Fouquer"
      ],
      "primary_category": "math.AP",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "Comments welcome!",
      "repo_url": "#"
    },
    "2409.08673": {
      "paper_id": "2409.08673v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08673v1",
      "paper_key": "2409.08673",
      "paper_title": "Acoustic identification of individual animals with hierarchical contrastive learning",
      "paper_url": "http://arxiv.org/abs/2409.08673v1",
      "paper_abstract": "Acoustic identification of individual animals (AIID) is closely related to audio-based species classification but requires a finer level of detail to distinguish between individual animals within the same species. In this work, we frame AIID as a hierarchical multi-label classification task and propose the use of hierarchy-aware loss functions to learn robust representations of individual identities that maintain the hierarchical relationships among species and taxa. Our results demonstrate that hierarchical embeddings not only enhance identification accuracy at the individual level but also at higher taxonomic levels, effectively preserving the hierarchical structure in the learned representations. By comparing our approach with non-hierarchical models, we highlight the advantage of enforcing this structure in the embedding space. Additionally, we extend the evaluation to the classification of novel individual classes, demonstrating the potential of our method in open-set classification scenarios.",
      "paper_authors": [
        "Ines Nolasco",
        "Ilyass Moummad",
        "Dan Stowell",
        "Emmanouil Benetos"
      ],
      "primary_category": "cs.SD",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "Under review; Submitted to ICASSP 2025",
      "repo_url": "#"
    },
    "2409.08672": {
      "paper_id": "2409.08672v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08672v1",
      "paper_key": "2409.08672",
      "paper_title": "The infrared luminosity of retired and post-starburst galaxies: A cautionary tale for star formation rate measurements",
      "paper_url": "http://arxiv.org/abs/2409.08672v1",
      "paper_abstract": "In galaxies with significant ongoing star formation there is an impressively tight correlation between total infrared luminosity (L$_{TIR}$) and H$\\alpha$ luminosity (L$_{H\\alpha}$), when H$\\alpha$ is properly corrected for stellar absorption and dust attenuation. This long-standing result gives confidence that both measurements provide accurate estimates of a galaxy's star formation rate (SFR), despite their differing origins. To test the extent to which this holds in galaxies with lower specific SFR (sSFR=SFR/Mgal, where Mgal is the stellar mass), we combine optical spectroscopy from the Sloan Digital Sky Survey (SDSS) with multi-wavelength (FUV to FIR) photometric observations from the Galaxy And Mass Assembly survey (GAMA). We find that L$_{TIR}$/L$_{H\\alpha}$increases steadily with decreasing H$\\alpha$ equivalent width (W$_{H\\alpha}$, a proxy for sSFR), indicating that both luminosities cannot provide a valid measurement of SFR in galaxies below the canonical star-forming sequence. For both `retired galaxies' and `post-starburst galaxies', L$_{TIR}$/L$_{H\\alpha}$ can be up to a factor of 30 larger than for star-forming galaxies. The smooth change in L$_{TIR}$/L$_{H\\alpha}$, irrespective of star formation history, ionisation or heating source, dust temperature or other properties, suggests that the value of L$_{TIR}$/L$_{H\\alpha}$ is given by the balance between star-forming regions and ambient interstellar medium contributing to both L$_{TIR}$ and L$_{H\\alpha}$. While L$_{H\\alpha}$ can only be used to estimate the SFR for galaxies with W$_{H\\alpha}$ > 3A (sSFR $\\gtrsim 10^{-11.5}$/yr), we argue that the mid- and far-infrared can only be used to estimate the SFR of galaxies on the star-forming sequence, and in particular only for galaxies with W$_{H\\alpha}$ >10 A (sSFR $\\gtrsim 10^{-10.5}$/yr). We find no evidence for dust obscured star-formation in post-starburst galaxies.",
      "paper_authors": [
        "Vivienne Wild",
        "Natalia Vale Asari",
        "Kate Rowlands",
        "Sara L. Ellison",
        "Ho-Hin Leung",
        "Christy Tremonti"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "Submitted to the Open Journal of Astrophysics. Comments welcome. 14\n  pages, 4 figures",
      "repo_url": "#"
    },
    "2409.08662": {
      "paper_id": "2409.08662v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08662v1",
      "paper_key": "2409.08662",
      "paper_title": "Is there a genetic relationship between chondrules and matrix?",
      "paper_url": "http://arxiv.org/abs/2409.08662v1",
      "paper_abstract": "Chondritic components such as chondrules and matrix are the key time capsules that can help us understand the evolution and dynamics of the protoplanetary disk from which the Solar System originated. Knowledge of where and how these components formed and to what extent they were transported in the gaseous disk provides major constraints to astrophysical models that investigate planet formation. Here, we explore whether chondrules and matrix are genetically related to each other and formed from single reservoirs per chondrite group or if every chondrite represents a unique proportion of components transported from a small number of formation reservoirs in the disk. These static versus dynamic disk interpretations of cosmochemical data have profound implications for the accretion history of the planets in the Solar System. To fully understand the relationship between chondrules and matrix and their potential complementarity, we dive into the petrological nature and origin of matrix, the chemical and isotopic compositions of chondrules and matrix and evaluate these data considering the effect of secondary alteration observed in chondrites and the potential complexity of chondrule formation. Even though we, the authors, have used different datasets and arrived at differing interpretations of chondrule-matrix relationships in the past, this review provides clarity on the existing data and has given us new directions towards future research that can resolve the complementarity debate.",
      "paper_authors": [
        "Elishevah van Kooten",
        "Adrian Brearley",
        "Denton Ebel",
        "Conel Alexander",
        "Marina Gemma",
        "Dominik Hezel"
      ],
      "primary_category": "astro-ph.EP",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "accepted in Space Science Reviews",
      "repo_url": "#"
    },
    "2409.08605": {
      "paper_id": "2409.08605v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08605v1",
      "paper_key": "2409.08605",
      "paper_title": "Effective Integration of KAN for Keyword Spotting",
      "paper_url": "http://arxiv.org/abs/2409.08605v1",
      "paper_abstract": "Keyword spotting (KWS) is an important speech processing component for smart devices with voice assistance capability. In this paper, we investigate if Kolmogorov-Arnold Networks (KAN) can be used to enhance the performance of KWS. We explore various approaches to integrate KAN for a model architecture based on 1D Convolutional Neural Networks (CNN). We find that KAN is effective at modeling high-level features in lower-dimensional spaces, resulting in improved KWS performance when integrated appropriately. The findings shed light on understanding KAN for speech processing tasks and on other modalities for future researchers.",
      "paper_authors": [
        "Anfeng Xu",
        "Biqiao Zhang",
        "Shuyu Kong",
        "Yiteng Huang",
        "Zhaojun Yang",
        "Sangeeta Srivastava",
        "Ming Sun"
      ],
      "primary_category": "eess.AS",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "Under review",
      "repo_url": "#"
    },
    "2409.08602": {
      "paper_id": "2409.08602v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08602v1",
      "paper_key": "2409.08602",
      "paper_title": "Deep learning-based shot-domain seismic deblending",
      "paper_url": "http://arxiv.org/abs/2409.08602v1",
      "paper_abstract": "To streamline fast-track processing of large data volumes, we have developed a deep learning approach to deblend seismic data in the shot domain based on a practical strategy for generating high-quality training data along with a list of data conditioning techniques to improve performance of the data-driven model. We make use of unblended shot gathers acquired at the end of each sail line, to which the access requires no additional time or labor costs beyond the blended acquisition. By manually blending these data we obtain training data with good control of the ground truth and fully adapted to the given survey. Furthermore, we train a deep neural network using multi-channel inputs that include adjacent blended shot gathers as additional channels. The prediction of the blending noise is added in as a related and auxiliary task with the main task of the network being the prediction of the primary-source events. Blending noise in the ground truth is scaled down during the training and validation process due to its excessively strong amplitudes. As part of the process, the to-be-deblended shot gathers are aligned by the blending noise. Implementation on field blended-by-acquisition data demonstrates that introducing the suggested data conditioning steps can considerably reduce the leakage of primary-source events in the deep part of the blended section. The complete proposed approach performs almost as well as a conventional algorithm in the shallow section and shows great advantage in efficiency. It performs slightly worse for larger traveltimes, but still removes the blending noise efficiently.",
      "paper_authors": [
        "Jing Sun",
        "Song Hou",
        "Vetle Vinje",
        "Gordon Poole",
        "Leiv-J Gelius"
      ],
      "primary_category": "physics.geo-ph",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": null,
      "repo_url": "#"
    },
    "2409.08579": {
      "paper_id": "2409.08579v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08579v1",
      "paper_key": "2409.08579",
      "paper_title": "Secure Offloading in NOMA-Aided Aerial MEC Systems Based on Deep Reinforcement Learning",
      "paper_url": "http://arxiv.org/abs/2409.08579v1",
      "paper_abstract": "Mobile edge computing (MEC) technology can reduce user latency and energy consumption by offloading computationally intensive tasks to the edge servers. Unmanned aerial vehicles (UAVs) and non-orthogonal multiple access (NOMA) technology enable the MEC networks to provide offloaded computing services for massively accessed terrestrial users conveniently. However, the broadcast nature of signal propagation in NOMA-based UAV-MEC networks makes it vulnerable to eavesdropping by malicious eavesdroppers. In this work, a secure offload scheme is proposed for NOMA-based UAV-MEC systems with the existence of an aerial eavesdropper. The long-term average network computational cost is minimized by jointly designing the UAV's trajectory, the terrestrial users' transmit power, and computational frequency while ensuring the security of users' offloaded data. Due to the eavesdropper's location uncertainty, the worst-case security scenario is considered through the estimated eavesdropping range. Due to the high-dimensional continuous action space, the deep deterministic policy gradient algorithm is utilized to solve the non-convex optimization problem. Simulation results validate the effectiveness of the proposed scheme.",
      "paper_authors": [
        "Hongjiang Lei",
        "Mingxu Yang",
        "Ki-Hong Park",
        "Gaofeng Pan"
      ],
      "primary_category": "cs.IT",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "12 pages, 7 figures, submitted to IEEE Journal for review",
      "repo_url": "#"
    },
    "2409.08528": {
      "paper_id": "2409.08528v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08528v1",
      "paper_key": "2409.08528",
      "paper_title": "Hydrodynamics of an oscillating cylinder inline with steady current",
      "paper_url": "http://arxiv.org/abs/2409.08528v1",
      "paper_abstract": "Wake and force characteristics of an oscillating cylinder in inline steady currents are investigated numerically over a wide parameter space of dimensionless oscillation amplitude ($A^* = 0.01 - 0.50$) and wavelength ($\\lambda^* = 0.4 - 25$) at a fixed Reynolds number $Re = 500$. Fundamental issues addressed in this study are the interactions of wakes induced by steady approaching flow and cylinder oscillations and the influences of the governing parameters of $A^$ and $\\lambda^$ on such interactions. Whilst the collinear flow is dominated by wakes induced by cylinder oscillation at $\\lambda^* \\leq 1.5$ and steady current at $\\lambda^* \\geq 10$, it exhibits characteristics of nonlinear interactions of wakes induced by the cylinder oscillation and steady current at $\\lambda^* = 1.5 - 10$, such as the formation of multiple synchronized modes interleaved with desynchronized modes. The synchronized mode varies with both $\\lambda^$ and $A^$, forming an inclined Arnold's tongue across $\\lambda^-A^$ space. There is a wide variability of the vortex shedding pattern in each synchronized mode. Variations of different hydrodynamic force coefficients with $\\lambda^$ and $A^$ are investigated with physical interpretations based on the wake characteristics. The applicability of the Morison equation in predicting inline force fluctuations is examined. We found that the Morison equation shows reasonable accuracy only for a small range of $\\lambda^* \\leq 1.5$. Beyond this range, its performance deteriorates due to the influence of steady current on wake characteristics.",
      "paper_authors": [
        "Chengjiao Ren",
        "Feifei Tong",
        "Fei He",
        "Liang Cheng"
      ],
      "primary_category": "physics.flu-dyn",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "Under review in JFM",
      "repo_url": "#"
    },
    "2409.08498": {
      "paper_id": "2409.08498v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08498v1",
      "paper_key": "2409.08498",
      "paper_title": "Incorporating Procedural Fairness in Flag Submissions on Social Media Platforms",
      "paper_url": "http://arxiv.org/abs/2409.08498v1",
      "paper_abstract": "Flagging mechanisms on social media platforms allow users to report inappropriate posts/accounts for review by content moderators. These reports are pivotal to platforms' efforts toward regulating norm violations. This paper examines how platforms' design choices in implementing flagging mechanisms influence flaggers' perceptions of content moderation. We conducted a survey experiment asking US respondents (N=2,936) to flag inappropriate posts using one of 54 randomly assigned flagging implementations. After flagging, participants rated their fairness perceptions of the flag submission process along the dimensions of consistency, transparency, and voice (agency). We found that participants perceived greater transparency when flagging interfaces included community guidelines and greater voice when they incorporated a text box for open-ended feedback. Our qualitative analysis highlights user needs for improved accessibility, educational support for reporting, and protections against false flags. We offer design recommendations for building fairer flagging systems without exacerbating the cognitive burden of submitting flags.",
      "paper_authors": [
        "Yunhee Shim",
        "Shagun Jhaver"
      ],
      "primary_category": "cs.HC",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "41 pages, 4 figures, 14 tables, and appendix A and B",
      "repo_url": "#"
    },
    "2409.08459": {
      "paper_id": "2409.08459v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08459v1",
      "paper_key": "2409.08459",
      "paper_title": "Toward satisfactory public accessibility: A crowdsourcing approach through online reviews to inclusive urban design",
      "paper_url": "http://arxiv.org/abs/2409.08459v1",
      "paper_abstract": "As urban populations grow, the need for accessible urban design has become urgent. Traditional survey methods for assessing public perceptions of accessibility are often limited in scope. Crowdsourcing via online reviews offers a valuable alternative to understanding public perceptions, and advancements in large language models can facilitate their use. This study uses Google Maps reviews across the United States and fine-tunes Llama 3 model with the Low-Rank Adaptation technique to analyze public sentiment on accessibility. At the POI level, most categories -- restaurants, retail, hotels, and healthcare -- show negative sentiments. Socio-spatial analysis reveals that areas with higher proportions of white residents and greater socioeconomic status report more positive sentiment, while areas with more elderly, highly-educated residents exhibit more negative sentiment. Interestingly, no clear link is found between the presence of disabilities and public sentiments. Overall, this study highlights the potential of crowdsourcing for identifying accessibility challenges and providing insights for urban planners.",
      "paper_authors": [
        "Lingyao Li",
        "Songhua Hu",
        "Yinpei Dai",
        "Min Deng",
        "Parisa Momeni",
        "Gabriel Laverghetta",
        "Lizhou Fan",
        "Zihui Ma",
        "Xi Wang",
        "Siyuan Ma",
        "Jay Ligatti",
        "Libby Hemphill"
      ],
      "primary_category": "cs.SI",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": null,
      "repo_url": "#"
    },
    "2409.08439": {
      "paper_id": "2409.08439v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08439v1",
      "paper_key": "2409.08439",
      "paper_title": "Input-to-State Stable Coupled Oscillator Networks for Closed-form Model-based Control in Latent Space",
      "paper_url": "http://arxiv.org/abs/2409.08439v1",
      "paper_abstract": "Even though a variety of methods (e.g., RL, MPC, LQR) have been proposed in the literature, efficient and effective latent-space control of physical systems remains an open challenge. A promising avenue would be to leverage powerful and well-understood closed-form strategies from control theory literature in combination with learned dynamics, such as potential-energy shaping. We identify three fundamental shortcomings in existing latent-space models that have so far prevented this powerful combination: (i) they lack the mathematical structure of a physical system, (ii) they do not inherently conserve the stability properties of the real systems. Furthermore, (iii) these methods do not have an invertible mapping between input and latent-space forcing. This work proposes a novel Coupled Oscillator Network (CON) model that simultaneously tackles all these issues. More specifically, (i) we show analytically that CON is a Lagrangian system - i.e., it presses well-defined potential and kinetic energy terms. Then, (ii) we provide formal proof of global Input-to-State stability using Lyapunov arguments. Moving to the experimental side, (iii) we demonstrate that CON reaches SoA performance when learning complex nonlinear dynamics of mechanical systems directly from images. An additional methodological innovation contributing to achieving this third goal is an approximated closed-form solution for efficient integration of network dynamics, which eases efficient training. We tackle (iv) by approximating the forcing-to-input mapping with a decoder that is trained to reconstruct the input based on the encoded latent space force. Finally, we leverage these four properties and show that they enable latent-space control. We use an integral-saturated PID with potential force compensation and demonstrate high-quality performance on a soft robot using raw pixels as the only feedback information.",
      "paper_authors": [
        "Maximilian St\u00f6lzle",
        "Cosimo Della Santina"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "41 pages, currently under review",
      "repo_url": "#"
    },
    "2409.08437": {
      "paper_id": "2409.08437v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08437v1",
      "paper_key": "2409.08437",
      "paper_title": "The overestimation of equipartition magnetic field strengths from synchrotron emission using synthetically observed galaxies",
      "paper_url": "http://arxiv.org/abs/2409.08437v1",
      "paper_abstract": "Understanding the role that magnetic fields play on the stage of galaxy formation requires accurate methods for inferring the properties of extragalactic magnetic fields. Radio synchrotron emission has been the most promising avenue to infer magnetic field strengths across galaxies, with the application of a central assumption: that galactic cosmic rays are in energy equipartition with the magnetic field. In this work, we leverage flexible synthetic observations of a high-resolution magnetohydrodynamic simulation of a Milky Way-like galaxy to review whether true equipartition is capable of reproducing radio observations of galaxies, and investigate its impact on the inference of magnetic field strengths when varying the properties and density distribution of the cosmic rays. We find that imposing equipartition (regardless of scale length) results in cosmic ray electron densities that are unable to generate either the amplitude or the shape of the radio intensity profiles typically observed in spiral galaxies. Instead, observationally motivated smooth distributions of cosmic ray electrons across the galaxy provide a remarkable match to observations. We further demonstrate that assuming equipartition with those mock observations can lead to significant overestimation of the magnetic field strength ($\\sim10-50\\times$). This overestimation varies with cosmic ray electron densities, cosmic ray spectrum power-law index, and galactic environment, aggravated in inter-arm regions and attenuated in star-forming regions. Our results promote caution when assuming equipartition in observations, and suggest that additional theoretical and numerical work is required to leverage the upcoming generation of radio observations poised to revolutionize our understanding of astrophysical magnetic fields.",
      "paper_authors": [
        "Tara Dacunha",
        "Sergio Martin-Alvarez",
        "Susan E. Clark",
        "Enrique Lopez-Rodriguez"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "Submitted to ApJ, 26 pages, 12 figures",
      "repo_url": "#"
    },
    "2409.08421": {
      "paper_id": "2409.08421v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08421v1",
      "paper_key": "2409.08421",
      "paper_title": "A tour of noncommutative spectral theories",
      "paper_url": "http://arxiv.org/abs/2409.08421v1",
      "paper_abstract": "This is a survey of noncommutative generalizations of the spectrum of a ring, written for the Notices of the American Mathematical Society.",
      "paper_authors": [
        "Manuel Reyes"
      ],
      "primary_category": "math.RA",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "12 pages",
      "repo_url": "#"
    },
    "2409.08373": {
      "paper_id": "2409.08373v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08373v1",
      "paper_key": "2409.08373",
      "paper_title": "3D Radiation-Hydrodynamical Simulations of Shadows on Transition Disks",
      "paper_url": "http://arxiv.org/abs/2409.08373v1",
      "paper_abstract": "Shadows are often observed in transition disks, which can result from obscuring by materials closer to the star, such as a misaligned inner disk. While shadows leave apparent darkened emission as observational signatures, they have significant dynamical impact on the disk. We carry out 3D radiation hydrodynamical simulations to study shadows in transition disks and find that the temperature drop due to the shadow acts as an asymmetric driving force, leading to spirals in the cavity. These spirals have zero pattern speed following the fixed shadow. The pitch angle is given by tan$^{-1}$($c_s$/$v_\\phi$) (6$^{\\circ}$ if $h/r$=0.1). These spirals transport mass through the cavity efficiently, with $\\alpha \\sim 10^{-2}$ in our simulation. Besides spirals, the cavity edge can also form vortices and flocculent streamers. When present, these features could disturb the shadow-induced spirals. By carrying out Monte Carlo Radiative Transfer simulations, we show that these features resemble those observed in near-infrared scattered light images. In the vertical direction, the vertical gravity is no longer balanced by the pressure gradient alone. Instead, an azimuthal convective acceleration term balances the gravity-pressure difference, leading to azimuthally periodic upward and downward gas motion reaching 10% of the sound speed, which can be probed by ALMA line observations.",
      "paper_authors": [
        "Shangjia Zhang",
        "Zhaohuan Zhu"
      ],
      "primary_category": "astro-ph.EP",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "11 pages, 6 figures, revised in response to the first-round\n  reviewer's report, resubmitted to ApJ Letters",
      "repo_url": "#"
    },
    "2409.08342": {
      "paper_id": "2409.08342v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08342v1",
      "paper_key": "2409.08342",
      "paper_title": "Undecidability and incompleteness in quantum information theory and operator algebras",
      "paper_url": "http://arxiv.org/abs/2409.08342v1",
      "paper_abstract": "We survey a number of incompleteness results in operator algebras stemming from the recent undecidability result in quantum complexity theory known as $\\operatorname{MIP}^*=\\operatorname{RE}$, the most prominent of which is the G\\\"odelian refutation of the Connes Embedding Problem. We also discuss the very recent use of $\\operatorname{MIP}^*=\\operatorname{RE}$ in refuting the Aldous-Lyons conjecture in probability theory.",
      "paper_authors": [
        "Isaac Goldbring"
      ],
      "primary_category": "math.LO",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "38 pages. To appear in a special issue of Monatshefte f\\\"ur\n  Mathematik celebrating the 100th anniversary of G\\\"odel's matriculation at\n  the University of Vienna",
      "repo_url": "#"
    },
    "2409.08339": {
      "paper_id": "2409.08339v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08339v1",
      "paper_key": "2409.08339",
      "paper_title": "A Virgo Environmental Survey Tracing Ionised Gas Emission (VESTIGE): XVI. The ubiquity of truncated star-forming disks across the Virgo cluster environment",
      "paper_url": "http://arxiv.org/abs/2409.08339v1",
      "paper_abstract": "We examine the prevalence of truncated star-forming disks in the Virgo cluster down to $M_* \\simeq 10^7 ~\\text{M}_{\\odot}$. This work makes use of deep, high-resolution imaging in the H$\\alpha$+[NII] narrow-band from the Virgo Environmental Survey Tracing Ionised Gas Emission (VESTIGE) and optical imaging from the Next Generation Virgo Survey (NGVS). To aid in understanding the effects of the cluster environment on star formation in Virgo galaxies, we take a physically-motivated approach to define the edge of the star-forming disk via a drop-off in the radial specific star formation rate profile. Comparing with the expected sizes of normal galactic disks provides a measure of how truncated star-forming disks are in the cluster. We find that truncated star-forming disks are nearly ubiquitous across all regions of the Virgo cluster, including beyond the virial radius (0.974 Mpc). The majority of truncated disks at large clustercentric radii are of galaxies likely on first infall. As the intra-cluster medium density is low in this region, it is difficult to explain this population with solely ram-pressure stripping. A plausible explanation is that these galaxies are undergoing starvation of their gas supply before ram-pressure stripping becomes the dominant quenching mechanism. A simple model of starvation shows that this mechanism can produce moderate disk truncations within 1-2 Gyr. This model is consistent with `slow-then-rapid' or `delayed-then-rapid' quenching, where the early starvation mode drives disk truncations without significant change to the integrated star formation rate, and the later ram-pressure stripping mode rapidly quenches the galaxy. The origin of starvation may be in the group structures that exist around the main Virgo cluster, which indicates the importance of understanding pre-processing of galaxies beyond the cluster virial radius.",
      "paper_authors": [
        "C. R. Morgan",
        "M. L. Balogh",
        "A. Boselli",
        "M. Fossati",
        "C. Lawlor-Forsyth",
        "E. Sazonova",
        "P. Amram",
        "M. Boquien",
        "J. Braine",
        "L. Cortese",
        "P. C\u00f4t\u00e9",
        "J. C. Cuillandre",
        "L. Ferrarese",
        "S. Gwyn",
        "G. Hensler",
        "Junais",
        "J. Roediger"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "22 pages, 12 figures, accepted in Astronomy&Astrophysics",
      "repo_url": "#"
    },
    "2409.08328": {
      "paper_id": "2409.08328v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08328v1",
      "paper_key": "2409.08328",
      "paper_title": "JWST Reveals Bulge-Dominated Star-forming Galaxies at Cosmic Noon",
      "paper_url": "http://arxiv.org/abs/2409.08328v1",
      "paper_abstract": "Hubble Space Telescope imaging shows that most star-forming galaxies at cosmic noon -- the peak of cosmic star formation history -- appear disk-dominated, leaving the origin of the dense cores in their quiescent descendants unclear. With the James Webb Space Telescope's (JWST) high-resolution imaging to 5 {\\mu}m, we can now map the rest-frame near-infrared emission, a much closer proxy for stellar mass distribution, in these massive galaxies. We selected 70 star-forming galaxies with 10$<$log(M)$<$12 and 1.5$<$z$<$3 in the CEERS survey and compare their morphologies in the rest-frame optical to those in the rest-frame near-IR. While the bulk of these galaxies are disk-dominated in 1.5 {\\mu}m (rest-frame optical) imaging, they appear more bulge-dominated at 4.4 {\\mu}m (rest-frame near-infrared). Our analysis reveals that in massive star-forming galaxies at z$\\sim$2, the radial surface brightness profiles steepen significantly, from a slope of $\\sim$0.3/dex at 1.5 {\\mu}m to $\\sim$1.4/dex at 4.4 {\\mu}m within radii $<$ 1 kpc. Additionally, we find their total flux contained within the central 1 kpc is approximately 7 times higher in F444W than in F150W. In rest-optical emission, a galaxy's central surface density appears to be the strongest indicator of whether it is quenched or star-forming. Our most significant finding is that at redder wavelengths, the central surface density ratio between quiescent and star-forming galaxies dramatically decreases from $\\sim$10 to $\\sim$1. This suggests the high central densities associated with galaxy quenching are already in place during the star-forming phase, imposing new constraints on the transition from star formation to quiescence.",
      "paper_authors": [
        "Chlo\u00eb E. Benton",
        "Erica J. Nelson",
        "Tim B. Miller",
        "Rachel Bezanson",
        "Justus Gibson",
        "Abigail I Hartley",
        "Marco Martorano",
        "Sedona H. Price",
        "Katherine A. Suess",
        "Arjen van der Wel",
        "Pieter van Dokkum",
        "John R. Weaver",
        "Katherine E. Whitaker"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "9 pages, 5 figures, resubmitted to ApJ for publication",
      "repo_url": "#"
    },
    "2409.08322": {
      "paper_id": "2409.08322v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08322v1",
      "paper_key": "2409.08322",
      "paper_title": "Illuminating dark objects with dark matter lampshades",
      "paper_url": "http://arxiv.org/abs/2409.08322v1",
      "paper_abstract": "We demonstrate a new technique to search for dark compact objects. When dark matter comprising a dark compact object interacts with photons, the compact object can disperse light traveling though it. As these objects pass between the Earth and a distant star, they act as \"lampshades\" that dim the star. We examine how dimming effects from clumps of dark matter in the galaxy could be searched for in microlensing surveys, which measure the brightness of stars as a function of time. Using the EROS-2 and OGLE surveys, we show that a dimming analysis of existing data can be used to constrain dark sectors, and could be used to discover dark matter in compact objects.",
      "paper_authors": [
        "Joseph Bramante",
        "Melissa D. Diamond",
        "J. Leo Kim"
      ],
      "primary_category": "hep-ph",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "12 pages, 6 figures",
      "repo_url": "#"
    },
    "2409.08314": {
      "paper_id": "2409.08314v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08314v1",
      "paper_key": "2409.08314",
      "paper_title": "Cross-correlation of Luminous Red Galaxies with ML-selected AGN in HSC-SSP: Unobscured AGN residing in more massive halos",
      "paper_url": "http://arxiv.org/abs/2409.08314v1",
      "paper_abstract": "Active galactic nuclei (AGN) are the signposts of black hole growth, and likely play an important role in galaxy evolution. An outstanding question is whether AGN of different spectral types indicate different evolutionary stages in the coevolution of black holes and galaxies. We present the angular correlation function between an AGN sample selected from the Hyper Suprime Camera Subaru Strategic Program (HSC-SSP) optical + Wide-field Infrared Survey Explorer (WISE) mid-IR photometry, and a luminous red galaxy (LRG) sample from HSC-SSP. We investigate AGN clustering strength as a function of their luminosity and spectral features across three independent HSC fields totaling $\\sim600\\,{\\rm deg^{2}}$, for $z\\in0.6-1.2$ and AGN with $L_{6\\mu m}>3\\times10^{44}{\\rm\\,erg\\,s^{-1}}$. There are $\\sim28,500$ AGN and $\\sim1.5$ million LRGs in our primary analysis. We determine the inferred average halo mass for the full AGN sample ($M_h \\approx 10^{12.9}h^{-1}M_\\odot$), and note that it does not evolve significantly as a function of redshift (over this narrow range) or luminosity. We find that, on average, unobscured AGN ($M_h \\approx10^{13.3}h^{-1}M_\\odot$) occupy $\\sim4.5\\times$ more massive halos than obscured AGN ($M_h \\approx10^{12.6}h^{-1}M_\\odot$), at $5\\sigma$ statistical significance using 1-D uncertainties, and at $3\\sigma$ using the full covariance matrix, suggesting a physical difference between unobscured and obscured AGN, beyond the line-of-sight viewing angle. Furthermore, we find evidence for a halo mass dependence on reddening level within the Type I AGN population, which could support the existence of a previously claimed dust-obscured phase in AGN-host galaxy coevolution. However, we also find that even quite small systematic shifts in the redshift distributions of the AGN sample could plausibly explain current and previously observed differences in $M_{h}$.",
      "paper_authors": [
        "Rodrigo C\u00f3rdova Rosado",
        "Andy D. Goulding",
        "Jenny E. Greene",
        "Grayson C. Petter",
        "Ryan C. Hickox",
        "Nickolas Kokron",
        "Michael A. Strauss",
        "Jahmour J. Givans",
        "Yoshiki Toba",
        "Cassandra Starr Henderson"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-12",
      "update_time": "2024-09-12",
      "comments": "31 pages, 14 figures, submitted to ApJ",
      "repo_url": "#"
    },
    "2409.08306": {
      "paper_id": "2409.08306v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.08306v1",
      "paper_key": "2409.08306",
      "paper_title": "Dataset of Tensile Properties for Sub-sized Specimens of Nuclear Structural Materials",
      "paper_url": "http://arxiv.org/abs/2409.08306v1",
      "paper_abstract": "Mechanical testing with sub-sized specimens plays an important role in the nuclear industry, facilitating tests in confined experimental spaces with lower irradiation levels and accelerating the qualification of new materials. The reduced size of specimens results in different material behavior at the microscale, mesoscale, and macroscale, in comparison to standard-sized specimens, which is referred to as the specimen size effect. Although analytical models have been proposed to correlate the properties of sub-sized specimens to standard-sized specimens, these models lack broad applicability across different materials and testing conditions. The objective of this study is to create the first large public dataset of tensile properties for sub-sized specimens used in nuclear structural materials. We performed an extensive literature review of relevant publications and extracted over 1,000 tensile testing records comprising 54 parameters including material type and composition, manufacturing information, irradiation conditions, specimen dimensions, and tensile properties. The dataset can serve as a valuable resource to investigate the specimen size effect and develop computational methods to correlate the tensile properties of sub-sized specimens.",
      "paper_authors": [
        "Longze Li",
        "John W. Merickel",
        "Yalei Tang",
        "Rongjie Song",
        "Joshua E. Rittenhouse",
        "Aleksandar Vakanski",
        "Fei Xu"
      ],
      "primary_category": "physics.ins-det",
      "publish_time": "2024-09-11",
      "update_time": "2024-09-11",
      "comments": "18 pages",
      "repo_url": "#"
    },
    "2409.10518": {
      "paper_id": "2409.10518v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10518v1",
      "paper_key": "2409.10518",
      "paper_title": "The VIRUS-dE Survey I: Stars in dwarf elliptical galaxies - 3D dynamics and radially resolved stellar initial mass functions",
      "paper_url": "http://arxiv.org/abs/2409.10518v1",
      "paper_abstract": "We analyse the stellar structure of a sample of dwarf ellipticals (dE) inhabiting various environments within the Virgo cluster. Integral-field observations with a high spectral resolution allow us to robustly determine their low velocity dispersions ($\\sim25$ km s$^{-1}$) and higher-order kinematic moments out to the half-light radius. We find the dEs exhibit a diversity in ages with the younger dEs being less enhanced than the older, suggesting a complex star formation history for those dEs that recently entered Virgo while others have been quenched shortly after reionization. Orbit-superposition modeling allowed us to recover viewing angles, stellar mass-to-light ratios (with gradients), as well as the intrinsic orbit structure. We find that the angular momentum of the dEs is strongly suppressed compared to ordinary early-type galaxies and correlates with the environment. Flattened dEs are so because of a suppressed kinetic energy perpendicular to their equatorial plane. Combining population and dynamical modeling results, we find an age-dependent stellar initial mass function (IMF) or, alternatively, evidence for a more extended star formation history for those galaxies that have had higher initial mass and/or inhabited lower density environments. dEs appear to have a spatially homogeneous stellar structure but the state they were `frozen' in as they stopped forming stars varies dramatically according to their initial conditions.",
      "paper_authors": [
        "Mathias Lipka",
        "Jens Thomas",
        "Roberto Saglia",
        "Ralf Bender",
        "Maximilian Fabricius",
        "Gary J. Hill",
        "Matthias Kluge",
        "Martin Landriau",
        "Ximena Mazzalay",
        "Eva Noyola",
        "Taniya Parikh",
        "Jan Snigula"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "Accepted for publication in ApJ, 56 pages, 27 figures, 3 tables",
      "repo_url": "#"
    },
    "2409.10514": {
      "paper_id": "2409.10514v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10514v1",
      "paper_key": "2409.10514",
      "paper_title": "Constraints on axions from patchy screening of the cosmic microwave background",
      "paper_url": "http://arxiv.org/abs/2409.10514v1",
      "paper_abstract": "The resonant conversion of cosmic microwave background (CMB) photons into axions within large-scale structure induces an anisotropic spectral distortion in CMB temperature maps. Applying state-of-the-art foreground cleaning techniques to $\\textit{Planck}$ CMB observations, we construct maps of axion-induced \"patchy screening\" of the CMB. We cross-correlate these maps with data from the $\\textit{unWISE}$ galaxy survey and find no evidence of axions. We constrain the axion-photon coupling, $g_{a\\gamma\\gamma} \\lesssim 2 \\times 10^{-12}~{\\rm GeV}^{-1}$, at the 95% confidence level for axion masses in the range $10^{-13}~{\\rm eV} \\lesssim m_a \\lesssim 10^{-12}~{\\rm eV}$. These constraints are competitive with the tightest astrophysical axion limits in this mass range and are inferred from robust population-level statistics, which makes them complementary to existing searches that rely on modeling of individual systems.",
      "paper_authors": [
        "Samuel Goldstein",
        "Fiona McCarthy",
        "Cristina Mondino",
        "J. Colin Hill",
        "Junwu Huang",
        "Matthew C. Johnson"
      ],
      "primary_category": "astro-ph.CO",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "5+15 pages; 3+15 figures",
      "repo_url": "#"
    },
    "2409.10509": {
      "paper_id": "2409.10509v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10509v1",
      "paper_key": "2409.10509",
      "paper_title": "Pennsieve - A Collaborative Platform for Translational Neuroscience and Beyond",
      "paper_url": "http://arxiv.org/abs/2409.10509v1",
      "paper_abstract": "The exponential growth of neuroscientific data necessitates platforms that facilitate data management and multidisciplinary collaboration. In this paper, we introduce Pennsieve - an open-source, cloud-based scientific data management platform built to meet these needs. Pennsieve supports complex multimodal datasets and provides tools for data visualization and analyses. It takes a comprehensive approach to data integration, enabling researchers to define custom metadata schemas and utilize advanced tools to filter and query their data. Pennsieve's modular architecture allows external applications to extend its capabilities, and collaborative workspaces with peer-reviewed data publishing mechanisms promote high-quality datasets optimized for downstream analysis, both in the cloud and on-premises.   Pennsieve forms the core for major neuroscience research programs including the NIH SPARC Initiative, NIH HEAL Initiative's PRECISION Human Pain Network, and NIH HEAL RE-JOIN Initiative. It serves more than 80 research groups worldwide, along with several large-scale, inter-institutional projects at clinical sites through the University of Pennsylvania. Underpinning the SPARC.Science, Epilepsy.Science, and Pennsieve Discover portals, Pennsieve stores over 125 TB of scientific data, with 35 TB of data publicly available across more than 350 high-impact datasets. It adheres to the findable, accessible, interoperable, and reusable (FAIR) principles of data sharing and is recognized as one of the NIH-approved Data Repositories. By facilitating scientific data management, discovery, and analysis, Pennsieve fosters a robust and collaborative research ecosystem for neuroscience and beyond.",
      "paper_authors": [
        "Zack Goldblum",
        "Zhongchuan Xu",
        "Haoer Shi",
        "Patryk Orzechowski",
        "Jamaal Spence",
        "Kathryn A Davis",
        "Brian Litt",
        "Nishant Sinha",
        "Joost Wagenaar"
      ],
      "primary_category": "cs.CY",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "71 pages, 12 figures",
      "repo_url": "#"
    },
    "2409.10486": {
      "paper_id": "2409.10486v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10486v1",
      "paper_key": "2409.10486",
      "paper_title": "A Host Galaxy Morphology Link Between Quasi-Periodic Eruptions and Tidal Disruption Events",
      "paper_url": "http://arxiv.org/abs/2409.10486v1",
      "paper_abstract": "The physical processes that produce X-ray Quasi-Periodic Eruptions (QPEs) recently discovered from the nuclei of several low-redshift galaxies are mysterious. Several pieces of observational evidence strongly suggest a link between QPEs and Tidal Disruption Events (TDE). Previous studies also reveal that the morphologies of TDE host galaxies are highly concentrated, with high Sersic indicies, bulge-to-total light (B/T) ratios, and stellar surface mass densities relative to the broader galaxy population. We use these distinctive properties to test the link between QPEs and TDEs, by comparing these parameters of QPE host galaxies to TDE host galaxies. We employ archival Legacy Survey images of a sample of 9 QPE host galaxies and a sample of 13 TDE host galaxies, and model their surface brightness profiles. We show that QPE host galaxies have high Sersic indices of ~3, high B/T ratios of ~0.5, and high surface mass densities of ~10^10 Msun kpc^-2. These properties are similar to TDE host galaxies, but are in strong contrast to a mass- and redshift-matched control sample of galaxies. We also find tentative evidence that the central black holes in both QPE and TDE host galaxies are undermassive relative to their stellar mass. The morphological similarities between QPE and TDE host galaxies at the population level add to the mounting evidence of a physical link between these phenomena, and favor QPE models that also invoke TDEs.",
      "paper_authors": [
        "Olivier Gilbert",
        "John J. Ruan",
        "Michael Eracleous",
        "Daryl Haggard",
        "Jessie C. Runnoe"
      ],
      "primary_category": "astro-ph.HE",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "12 pages, 4 figures, submitted to ApJ",
      "repo_url": "#"
    },
    "2409.10484": {
      "paper_id": "2409.10484v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10484v1",
      "paper_key": "2409.10484",
      "paper_title": "XLM for Autonomous Driving Systems: A Comprehensive Review",
      "paper_url": "http://arxiv.org/abs/2409.10484v1",
      "paper_abstract": "Large Language Models (LLMs) have showcased remarkable proficiency in various information-processing tasks. These tasks span from extracting data and summarizing literature to generating content, predictive modeling, decision-making, and system controls. Moreover, Vision Large Models (VLMs) and Multimodal LLMs (MLLMs), which represent the next generation of language models, a.k.a., XLMs, can combine and integrate many data modalities with the strength of language understanding, thus advancing several information-based systems, such as Autonomous Driving Systems (ADS). Indeed, by combining language communication with multimodal sensory inputs, e.g., panoramic images and LiDAR or radar data, accurate driving actions can be taken. In this context, we provide in this survey paper a comprehensive overview of the potential of XLMs towards achieving autonomous driving. Specifically, we review the relevant literature on ADS and XLMs, including their architectures, tools, and frameworks. Then, we detail the proposed approaches to deploy XLMs for autonomous driving solutions. Finally, we provide the related challenges to XLM deployment for ADS and point to future research directions aiming to enable XLM adoption in future ADS frameworks.",
      "paper_authors": [
        "Sonda Fourati",
        "Wael Jaafar",
        "Noura Baccar",
        "Safwan Alfattani"
      ],
      "primary_category": "eess.SY",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "30 pages, 18 figures, submitted to IEEE Open Journal of Intelligent\n  Transportation Systems",
      "repo_url": "#"
    },
    "2409.10469": {
      "paper_id": "2409.10469v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10469v1",
      "paper_key": "2409.10469",
      "paper_title": "Real-Time Whole-Body Control of Legged Robots with Model-Predictive Path Integral Control",
      "paper_url": "http://arxiv.org/abs/2409.10469v1",
      "paper_abstract": "This paper presents a system for enabling real-time synthesis of whole-body locomotion and manipulation policies for real-world legged robots. Motivated by recent advancements in robot simulation, we leverage the efficient parallelization capabilities of the MuJoCo simulator to achieve fast sampling over the robot state and action trajectories. Our results show surprisingly effective real-world locomotion and manipulation capabilities with a very simple control strategy. We demonstrate our approach on several hardware and simulation experiments: robust locomotion over flat and uneven terrains, climbing over a box whose height is comparable to the robot, and pushing a box to a goal position. To our knowledge, this is the first successful deployment of whole-body sampling-based MPC on real-world legged robot hardware. Experiment videos and code can be found at: https://whole-body-mppi.github.io/",
      "paper_authors": [
        "Juan Alvarez-Padilla",
        "John Z. Zhang",
        "Sofia Kwok",
        "John M. Dolan",
        "Zachary Manchester"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "Under review. Code and videos are available on our website:\n  https://whole-body-mppi.github.io/",
      "repo_url": "#"
    },
    "2409.10453": {
      "paper_id": "2409.10453v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10453v1",
      "paper_key": "2409.10453",
      "paper_title": "The Debiased Near-Earth Object Population from ATLAS Telescopes",
      "paper_url": "http://arxiv.org/abs/2409.10453v1",
      "paper_abstract": "This work is dedicated to debias the Near-Earth Objects (NEO) population based on observations from the Asteroid Terrestrial-impact Last Alert System (ATLAS) telescopes. We have applied similar methods used to develop the recently released NEO model generator (NEOMOD), once debiasing the NEO population using data from Catalina Sky Survey (CSS) G96 telescope. ATLAS is composed of four different telescopes. We first analyzed observational data from each of all four telescopes separately and later combined them. Our results highlight main differences between CSS and ATLAS, e.g., sky coverage and survey power at debiasing the NEO population. ATLAS has a much larger sky coverage than CSS, allowing it to find bright NEOs that would be constantly \"hiding\" from CSS. Consequently, ATLAS is more powerful than CSS at debiasing the NEO population for H $\\lesssim$ 19. With its intrinsically greater sensitivity and emphasis on observing near opposition, CSS excels in the debiasing of smaller objects. ATLAS, as an all sky survey designed to find imminent hazardous objects, necessarily spends a significant fraction of time looking at places on the sky where objects do not appear, reducing its power for debiasing the population of small objects. We estimate a NEO population completeness of $\\approx$ 88%$^{+3\\%}_{-2\\%}$ for H $<$ 17.75 and $\\approx$ 36%$^{+1\\%}_{-1\\%}$ for H $<$ 22.25. Those numbers are similar to previous estimates (within error bars for H $<$ 17.75) from CSS, yet, around 3% and 8% smaller at their face values, respectively. We also confirm previous finding that the $\\nu_6$ secular resonance is the main source of small and faint NEOs at H = 28, whereas the 3:1 mean motion resonance with Jupiter dominates for larger and brighter NEOs at H = 15.",
      "paper_authors": [
        "Rogerio Deienno",
        "Larry Denneau",
        "David Nesvorn\u00fd",
        "David Vokrouhlick\u00fd",
        "William F. Bottke",
        "Robert Jedicke",
        "Shantanu Naidu",
        "Steven R. Chesley",
        "Davide Farnocchia",
        "Paul W. Chodas"
      ],
      "primary_category": "astro-ph.EP",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "Accepted for publication in Icarus, 26 pages, 13 figures, 2 tables",
      "repo_url": "#"
    },
    "2409.10447": {
      "paper_id": "2409.10447v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10447v1",
      "paper_key": "2409.10447",
      "paper_title": "Hydrodynamic hovering of swimming bacteria above surfaces",
      "paper_url": "http://arxiv.org/abs/2409.10447v1",
      "paper_abstract": "Flagellated bacteria are hydrodynamically attracted to rigid walls, yet past work shows a 'hovering' state where they swim stably at a finite height above surfaces. We use numerics and theory to reveal the physical origin of hovering. Simulations first show that hovering requires an elongated cell body and results from a tilt away from the wall. Theoretical models then identify two essential asymmetries: the response of width-asymmetric cells to active flows created by length-asymmetric cells. A minimal model reconciles near and far-field hydrodynamics, capturing all key features of hovering.",
      "paper_authors": [
        "Pyae Hein Htet",
        "Debasish Das",
        "Eric Lauga"
      ],
      "primary_category": "physics.bio-ph",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "6 pages, 3 figures. To appear in Physical Review Research (2024)",
      "repo_url": "#"
    },
    "2409.10353": {
      "paper_id": "2409.10353v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10353v1",
      "paper_key": "2409.10353",
      "paper_title": "Taming Diffusion Models for Image Restoration: A Review",
      "paper_url": "http://arxiv.org/abs/2409.10353v1",
      "paper_abstract": "Diffusion models have achieved remarkable progress in generative modelling, particularly in enhancing image quality to conform to human preferences. Recently, these models have also been applied to low-level computer vision for photo-realistic image restoration (IR) in tasks such as image denoising, deblurring, dehazing, etc. In this review paper, we introduce key constructions in diffusion models and survey contemporary techniques that make use of diffusion models in solving general IR tasks. Furthermore, we point out the main challenges and limitations of existing diffusion-based IR frameworks and provide potential directions for future work.",
      "paper_authors": [
        "Ziwei Luo",
        "Fredrik K. Gustafsson",
        "Zheng Zhao",
        "Jens Sj\u00f6lund",
        "Thomas B. Sch\u00f6n"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "Review paper; any comments and suggestions are most welcome!",
      "repo_url": "#"
    },
    "2409.10335": {
      "paper_id": "2409.10335v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10335v1",
      "paper_key": "2409.10335",
      "paper_title": "Phys3DGS: Physically-based 3D Gaussian Splatting for Inverse Rendering",
      "paper_url": "http://arxiv.org/abs/2409.10335v1",
      "paper_abstract": "We propose two novel ideas (adoption of deferred rendering and mesh-based representation) to improve the quality of 3D Gaussian splatting (3DGS) based inverse rendering. We first report a problem incurred by hidden Gaussians, where Gaussians beneath the surface adversely affect the pixel color in the volume rendering adopted by the existing methods. In order to resolve the problem, we propose applying deferred rendering and report new problems incurred in a naive application of deferred rendering to the existing 3DGS-based inverse rendering. In an effort to improve the quality of 3DGS-based inverse rendering under deferred rendering, we propose a novel two-step training approach which (1) exploits mesh extraction and utilizes a hybrid mesh-3DGS representation and (2) applies novel regularization methods to better exploit the mesh. Our experiments show that, under relighting, the proposed method offers significantly better rendering quality than the existing 3DGS-based inverse rendering methods. Compared with the SOTA voxel grid-based inverse rendering method, it gives better rendering quality while offering real-time rendering.",
      "paper_authors": [
        "Euntae Choi",
        "Sungjoo Yoo"
      ],
      "primary_category": "cs.GR",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "Under review",
      "repo_url": "#"
    },
    "2409.10327": {
      "paper_id": "2409.10327v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10327v1",
      "paper_key": "2409.10327",
      "paper_title": "Baking Relightable NeRF for Real-time Direct/Indirect Illumination Rendering",
      "paper_url": "http://arxiv.org/abs/2409.10327v1",
      "paper_abstract": "Relighting, which synthesizes a novel view under a given lighting condition (unseen in training time), is a must feature for immersive photo-realistic experience. However, real-time relighting is challenging due to high computation cost of the rendering equation which requires shape and material decomposition and visibility test to model shadow. Additionally, for indirect illumination, additional computation of rendering equation on each secondary surface point (where reflection occurs) is required rendering real-time relighting challenging. We propose a novel method that executes a CNN renderer to compute primary surface points and rendering parameters, required for direct illumination. We also present a lightweight hash grid-based renderer, for indirect illumination, which is recursively executed to perform the secondary ray tracing process. Both renderers are trained in a distillation from a pre-trained teacher model and provide real-time physically-based rendering under unseen lighting condition at a negligible loss of rendering quality.",
      "paper_authors": [
        "Euntae Choi",
        "Vincent Carpentier",
        "Seunghun Shin",
        "Sungjoo Yoo"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "Under review",
      "repo_url": "#"
    },
    "2409.10316": {
      "paper_id": "2409.10316v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10316v1",
      "paper_key": "2409.10316",
      "paper_title": "The CRAFT Coherent (CRACO) upgrade I: System Description and Results of the 110-ms Radio Transient Pilot Survey",
      "paper_url": "http://arxiv.org/abs/2409.10316v1",
      "paper_abstract": "We present the first results from a new backend on the Australian Square Kilometre Array Pathfinder, the Commensal Realtime ASKAP Fast Transient COherent (CRACO) upgrade. CRACO records millisecond time resolution visibility data, and searches for dispersed fast transient signals including fast radio bursts (FRB), pulsars, and ultra-long period objects (ULPO). With the visibility data, CRACO can localise the transient events to arcsecond-level precision after the detection. Here, we describe the CRACO system and report the result from a sky survey carried out by CRACO at 110ms resolution during its commissioning phase. During the survey, CRACO detected two FRBs (including one discovered solely with CRACO, FRB 20231027A), reported more precise localisations for four pulsars, discovered two new RRATs, and detected one known ULPO, GPM J1839-10, through its sub-pulse structure. We present a sensitivity calibration of CRACO, finding that it achieves the expected sensitivity of 11.6 Jy ms to bursts of 110 ms duration or less. CRACO is currently running at a 13.8 ms time resolution and aims at a 1.7 ms time resolution before the end of 2024. The planned CRACO has an expected sensitivity of 1.5 Jy ms to bursts of 1.7 ms duration or less, and can detect 10x more FRBs than the current CRAFT incoherent sum system (i.e., 0.5-2 localised FRBs per day), enabling us to better constrain the FRB emission mechanism model and use them as cosmological probes.",
      "paper_authors": [
        "Z. Wang",
        "K. W. Bannister",
        "V. Gupta",
        "X. Deng",
        "M. Pilawa",
        "J. Tuthill",
        "J. D. Bunton",
        "C. Flynn",
        "M. Glowacki",
        "A. Jaini",
        "Y. W. J. Lee",
        "E. Lenc",
        "J. Lucero",
        "A. Paek",
        "R. Radhakrishnan",
        "N. Thyagarajan",
        "P. Uttarkar",
        "Y. Wang",
        "N. D. R. Bhat",
        "C. W. James",
        "V. A. Moss",
        "Tara Murphy",
        "J. E. Reynolds",
        "R. M. Shannon",
        "L. G. Spitler",
        "A. Tzioumis",
        "M. Caleb",
        "A. T. Deller",
        "A. C. Gordon",
        "L. Marnoch",
        "S. D. Ryder",
        "S. Simha",
        "C. S. Anderson",
        "L. Ball",
        "D. Brodrick",
        "F. R. Cooray",
        "N. Gupta",
        "D. B. Hayman",
        "A. Ng",
        "S. E. Pearce",
        "C. Phillips",
        "M. A. Voronkov",
        "T. Westmeier"
      ],
      "primary_category": "astro-ph.HE",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "26 pages, 19 figures, 9 tables, Submitted for publication in PASA",
      "repo_url": "#"
    },
    "2409.10302": {
      "paper_id": "2409.10302v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10302v1",
      "paper_key": "2409.10302",
      "paper_title": "Controllability and Inverse Problems for Parabolic Systems with Dynamic Boundary Conditions",
      "paper_url": "http://arxiv.org/abs/2409.10302v1",
      "paper_abstract": "This review surveys previous and recent results on null controllability and inverse problems for parabolic systems with dynamic boundary conditions. We aim to demonstrate how classical methods such as Carleman estimates can be extended to prove null controllability for parabolic systems and Lipschitz stability estimates for inverse problems with dynamic boundary conditions of surface diffusion type. We mainly focus on the substantial difficulties compared to static boundary conditions. Finally, some conclusions and open problems will be mentioned.",
      "paper_authors": [
        "S. E. Chorfi",
        "L. Maniar"
      ],
      "primary_category": "math.OC",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "Dedicated to the memory of Professor Hammadi Bouslous",
      "repo_url": "#"
    },
    "2409.10300": {
      "paper_id": "2409.10300v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10300v1",
      "paper_key": "2409.10300",
      "paper_title": "Many-Body Open Quantum Systems",
      "paper_url": "http://arxiv.org/abs/2409.10300v1",
      "paper_abstract": "These Lecture Notes discuss the recent theoretical advances in the understanding of open quantum many-body physics in platforms where both dissipative and coherent processes can be tuned and controlled to a high degree. We start by reviewing the theoretical frameworks and methods used to describe and tackle open quantum many-body systems. We then discuss the use of dissipative processes to engineer many-body stationary states with desired properties and the emergence of dissipative phase transitions arising out of the competition between coherent evolution and dissipation. We review the dynamics of open quantum many body systems in the presence of correlated many-body dissipative processes, such as heating and many-body losses. Finally we provide a different perspective on open quantum many-body systems by looking at stochastic quantum trajectories, relevant for the case in which the environment represents a monitoring device, and the associated measurement-induced phase transitions.",
      "paper_authors": [
        "Rosario Fazio",
        "Jonathan Keeling",
        "Leonardo Mazza",
        "Marco Schir\u00f2"
      ],
      "primary_category": "quant-ph",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "Lecture Notes, 185 pages, 40 figure",
      "repo_url": "#"
    },
    "2409.10298": {
      "paper_id": "2409.10298v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10298v1",
      "paper_key": "2409.10298",
      "paper_title": "FreeDSM and the Gaia4Sustaniability project: a light pollution meter based on IoT technologies",
      "paper_url": "http://arxiv.org/abs/2409.10298v1",
      "paper_abstract": "Light pollution is a growing environmental issue that affects astronomy, ecosystems, human health. To address this, we introduce the Free Dark Sky Meter (FreeDSM), an affordable IoT-based photometer designed for continuous light pollution monitoring. FreeDSM uses an ESP32 microcontroller with integrated sensors for light, temperature, and humidity, and operates on an open-source platform. Data from multiple devices are centralized and processed using the Gambons model, which leverages Gaia satellite data for accurate real-time assessments of natural light levels. This project is part of the Gaia4Sustainability initiative.",
      "paper_authors": [
        "Mario Casado Diez"
      ],
      "primary_category": "astro-ph.IM",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": null,
      "repo_url": "#"
    },
    "2409.10285": {
      "paper_id": "2409.10285v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10285v1",
      "paper_key": "2409.10285",
      "paper_title": "Efficient transport controlled by biharmonic frictional driving",
      "paper_url": "http://arxiv.org/abs/2409.10285v1",
      "paper_abstract": "Dry friction has been proposed as a rectifying mechanism allowing mass transport over a vibrating surface, even when vibrations are horizontal and unbiased. It has been suggested that the drift velocity will always saturate when the energy of the input oscillation increases, leading to a vanishing efficiency that would hinder the applicability of this phenomenon. Contrary to this conjecture, in this work we experimentally demonstrate that, by carefully controlling the forcing oscillations, this system can maintain a finite transport efficiency for any input energy. A minimal friction model explains the observed dependencies of the drift velocity on the signal parameters in the case of biharmonic base oscillations, which can be extended to obtain efficiency estimates for any periodic excitation.",
      "paper_authors": [
        "Martin Maza-Cuello",
        "Diego Maza"
      ],
      "primary_category": "cond-mat.soft",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "Accepted in Physical Review Letters. 5 pages, 4 figures plus\n  Supplemental Material (5 figures)",
      "repo_url": "#"
    },
    "2409.10256": {
      "paper_id": "2409.10256v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10256v1",
      "paper_key": "2409.10256",
      "paper_title": "Real-bogus scores for active anomaly detection",
      "paper_url": "http://arxiv.org/abs/2409.10256v1",
      "paper_abstract": "In the task of anomaly detection in modern time-domain photometric surveys, the primary goal is to identify astrophysically interesting, rare, and unusual objects among a large volume of data. Unfortunately, artifacts -- such as plane or satellite tracks, bad columns on CCDs, and ghosts -- often constitute significant contaminants in results from anomaly detection analysis. In such contexts, the Active Anomaly Discovery (AAD) algorithm allows tailoring the output of anomaly detection pipelines according to what the expert judges to be scientifically interesting. We demonstrate how the introduction real-bogus scores, obtained from a machine learning classifier, improves the results from AAD. Using labeled data from the SNAD ZTF knowledge database, we train four real-bogus classifiers: XGBoost, CatBoost, Random Forest, and Extremely Randomized Trees. All the models perform real-bogus classification with similar effectiveness, achieving ROC-AUC scores ranging from 0.93 to 0.95. Consequently, we select the Random Forest model as the main model due to its simplicity and interpretability. The Random Forest classifier is applied to 67 million light curves from ZTF DR17. The output real-bogus score is used as an additional feature for two anomaly detection algorithms: static Isolation Forest and AAD. While results from Isolation Forest remained unchanged, the number of artifacts detected by the active approach decreases significantly with the inclusion of the real-bogus score, from 27 to 3 out of 100. We conclude that incorporating the real-bogus classifier result as an additional feature in the active anomaly detection pipeline significantly reduces the number of artifacts in the outputs, thereby increasing the incidence of astrophysically interesting objects presented to human experts.",
      "paper_authors": [
        "T. A. Semenikhin",
        "M. V. Kornilov",
        "M. V. Pruzhinskaya",
        "A. D. Lavrukhina",
        "E. Russeil",
        "E. Gangler",
        "E. E. O. Ishida",
        "V. S. Korolev",
        "K. L. Malanchev",
        "A. A. Volnova",
        "S. Sreejith"
      ],
      "primary_category": "astro-ph.IM",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "8 pages, 6 figures",
      "repo_url": "#"
    },
    "2409.10239": {
      "paper_id": "2409.10239v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10239v1",
      "paper_key": "2409.10239",
      "paper_title": "Precise Tool to Target Positioning Widgets (TOTTA) in Spatial Environments: A Systematic Review",
      "paper_url": "http://arxiv.org/abs/2409.10239v1",
      "paper_abstract": "TOTTA outlines the spatial position and rotation guidance of a real/virtual tool (TO) towards a real/virtual target (TA), which is a key task in Mixed Reality applications. The task error can have critical consequences regarding safety, performance, and quality, such as in surgical implantology or industrial maintenance scenarios. The TOTTA problem lacks a dedicated study and is scattered across different domains with isolated designs. This work contributes to a systematic review of the TOTTA visual widgets, studying 70 unique designs from 24 papers. TOTTA is commonly guided by visual overlap an intuitive, pre-attentive 'collimation' feedback of simple-shaped widgets: Box, 3D Axes, 3D Model, 2D Crosshair, Globe, Tetrahedron, Line, and Plane. Our research discovers that TO and TA are often represented with the same shape. They are distinguished by topological elements (e.g., edges, vertices, faces), colors, transparency levels, and added shapes, widget quantity, and size. Meanwhile, some designs provide continuous 'during manipulation feedback' relative to the distance between TO and TA by text, dynamic color, sonification, and amplified graphical visualization. Some approaches trigger discrete 'TA reached feedback,' such as color alteration, added sound, TA shape change, and added text. We found a lack of golden standards, including in testing procedures, as current ones are limited to partial sets with different and incomparable setups (different target configurations, avatar, background, etc.). We also found a bias in participants: right-handed, young male, non-color impaired.",
      "paper_authors": [
        "Mine Dastan",
        "Michele Fiorentino",
        "Antonio E. Uva"
      ],
      "primary_category": "cs.HC",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": null,
      "repo_url": "#"
    },
    "2409.10224": {
      "paper_id": "2409.10224v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10224v1",
      "paper_key": "2409.10224",
      "paper_title": "Shift-cyclicity in analytic function spaces",
      "paper_url": "http://arxiv.org/abs/2409.10224v1",
      "paper_abstract": "In this survey, we consider Banach spaces of analytic functions in one and several complex variables for which: (i) polynomials are dense, (ii) point-evaluations on the domain are bounded linear functionals, and (iii) the shift operators are bounded for each variable. We discuss the problem of determining the shift-cyclic functions in such a space, i.e., functions whose polynomial multiples form a dense subspace. The problem of determining shift-cyclic functions in certain analytic function spaces is known to be intimately connected to some deep problems in other areas of mathematics, such as the dilation completeness problem and even the Riemann hypothesis.   What makes determining shift-cyclic functions so difficult is that often we need to employ techniques that are specific to the space in consideration. We therefore cover several different function spaces that have frequently appeared in the past such as the Hardy spaces, Dirichlet-type spaces, complete Pick spaces and Bergman spaces. We highlight the similarities and the differences between shift-cyclic functions among these spaces and list some important general properties that shift-cyclic functions in any given analytic function space must share. Throughout this discussion, we also motivate and provide a large list of open problems related to shift-cyclicity.",
      "paper_authors": [
        "Jeet Sampat"
      ],
      "primary_category": "math.FA",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "45 pages. Survey article. Comments welcome",
      "repo_url": "#"
    },
    "2409.10212": {
      "paper_id": "2409.10212v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10212v1",
      "paper_key": "2409.10212",
      "paper_title": "Kernel-Based Learning of Stable Nonlinear Systems",
      "paper_url": "http://arxiv.org/abs/2409.10212v1",
      "paper_abstract": "Learning models of dynamical systems characterized by specific stability properties is of crucial importance in applications. Existing results mainly focus on linear systems or some limited classes of nonlinear systems and stability notions, and the general problem is still open. This article proposes a kernel-based nonlinear identification procedure to directly and systematically learn stable nonlinear discrete-time systems. In particular, the proposed method can be used to enforce, on the learned model, bounded-input-bounded-state stability, asymptotic gain, and input-to-state stability properties, as well as their incremental counterparts. To this aim, we build on the reproducing kernel theory and the Representer Theorem, which are suitably enhanced to handle stability constraints in the kernel properties and in the hyperparameters' selection algorithm. Once the methodology is detailed, and sufficient conditions for stability are singled out, the article reviews some widely used kernels and their applicability within the proposed framework. Finally, numerical results validate the theoretical findings showing, in particular, that stability may have a beneficial impact in long-term simulation with minimal impact on prediction.",
      "paper_authors": [
        "Matteo Scandella",
        "Michelangelo Bin",
        "Thomas Parisini"
      ],
      "primary_category": "eess.SY",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "16 pages, 5 figures, submitted to \"IEEE Transactions on Automatic\n  Control\"",
      "repo_url": "#"
    },
    "2409.10210": {
      "paper_id": "2409.10210v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10210v1",
      "paper_key": "2409.10210",
      "paper_title": "RF-GML: Reference-Free Generative Machine Listener",
      "paper_url": "http://arxiv.org/abs/2409.10210v1",
      "paper_abstract": "This paper introduces a novel reference-free (RF) audio quality metric called the RF-Generative Machine Listener (RF-GML), designed to evaluate coded mono, stereo, and binaural audio at a 48 kHz sample rate. RF-GML leverages transfer learning from a state-of-the-art full-reference (FR) Generative Machine Listener (GML) with minimal architectural modifications. The term \"generative\" refers to the model's ability to generate an arbitrary number of simulated listening scores. Unlike existing RF models, RF-GML accurately predicts subjective quality scores across diverse content types and codecs. Extensive evaluations demonstrate its superiority in rating unencoded audio and distinguishing different levels of coding artifacts. RF-GML's performance and versatility make it a valuable tool for coded audio quality assessment and monitoring in various applications, all without the need for a reference signal.",
      "paper_authors": [
        "Arijit Biswas",
        "Guanxin Jiang"
      ],
      "primary_category": "eess.AS",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "Pre-review version submitted to ICASSP 2025",
      "repo_url": "#"
    },
    "2409.10199": {
      "paper_id": "2409.10199v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10199v1",
      "paper_key": "2409.10199",
      "paper_title": "A Survey Of High Mass Star Forming Regions In The Line Of Singly Deuterated Ammonia NH2D",
      "paper_url": "http://arxiv.org/abs/2409.10199v1",
      "paper_abstract": "The present survey represents a continuation of our study of high mass star forming regions in the lines of deuterated molecules, the first results of which were published in Trofimova et al. (2020). This paper presents the results of observations of 50 objects in the line of ortho modification of singly deuterated ammonia NH$_2$D $1_{11}^s - 1_{01}^a$ at frequency 85.9 GHz, carried out with the 20-m radio telescope of the Onsala Space Observatory (Sweden). This line is detected in 29 sources. The analysis of obtained data, as well as the fact that gas density in the investigated sources, according to independent estimates, is significantly lower than the critical density for this NH$_2$D transition, indicate non-LTE excitation of NH$_2$D. Based on non-LTE modeling, estimates of the relative content of the NH$_2$D molecule and the degree of deuterium enrichment were obtained, and the dependencies of these parameters on temperature and velocity dispersion were analyzed with and without taking into account detection limits assuming the same gas density in all sources. An anti-correlation between the NH$_2$D relative abundances and the kinetic temperature is revealed in the temperature range 15-50K. At the same time, significant decrease in the ratio of the NH$_2$D/NH$_3$ abundances with increasing temperature, predicted by the available chemical models, is not observed under the adopted assumptions. An anti-correlation was also revealed between the relative content of the main isotopologue of ammonia NH$_3$ and the velocity dispersion, while no statistically significant correlation with the kinetic temperature of sources in the same temperature range was found.",
      "paper_authors": [
        "E. Trofimova",
        "I. Zinchenko",
        "P. Zemlyanukha",
        "M. Thomasson"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "36 pages, 9 figures, accepted for publication in Astronomy Reports",
      "repo_url": "#"
    },
    "2409.10192": {
      "paper_id": "2409.10192v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10192v1",
      "paper_key": "2409.10192",
      "paper_title": "PrePaMS: Privacy-Preserving Participant Management System for Studies with Rewards and Prerequisites",
      "paper_url": "http://arxiv.org/abs/2409.10192v1",
      "paper_abstract": "Taking part in surveys, experiments, and studies is often compensated by rewards to increase the number of participants and encourage attendance. While privacy requirements are usually considered for participation, privacy aspects of the reward procedure are mostly ignored. To this end, we introduce PrePaMS, an efficient participation management system that supports prerequisite checks and participation rewards in a privacy-preserving way. Our system organizes participations with potential (dis-)qualifying dependencies and enables secure reward payoffs. By leveraging a set of proven cryptographic primitives and mechanisms such as anonymous credentials and zero-knowledge proofs, participations are protected so that service providers and organizers cannot derive the identity of participants even within the reward process. In this paper, we have designed and implemented a prototype of PrePaMS to show its effectiveness and evaluated its performance under realistic workloads. PrePaMS covers the information whether subjects have participated in surveys, experiments, or studies. When combined with other secure solutions for the actual data collection within these events, PrePaMS can represent a cornerstone for more privacy-preserving empirical research.",
      "paper_authors": [
        "Echo Mei\u00dfner",
        "Frank Kargl",
        "Benjamin Erb",
        "Felix Engelmann"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "Prototype source code: https://github.com/vs-uulm/prepams/ Public\n  test deployment: https://vs-uulm.github.io/prepams/",
      "repo_url": "#"
    },
    "2409.10176": {
      "paper_id": "2409.10176v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10176v1",
      "paper_key": "2409.10176",
      "paper_title": "TCDformer-based Momentum Transfer Model for Long-term Sports Prediction",
      "paper_url": "http://arxiv.org/abs/2409.10176v1",
      "paper_abstract": "Accurate sports prediction is a crucial skill for professional coaches, which can assist in developing effective training strategies and scientific competition tactics. Traditional methods often use complex mathematical statistical techniques to boost predictability, but this often is limited by dataset scale and has difficulty handling long-term predictions with variable distributions, notably underperforming when predicting point-set-game multi-level matches. To deal with this challenge, this paper proposes TM2, a TCDformer-based Momentum Transfer Model for long-term sports prediction, which encompasses a momentum encoding module and a prediction module based on momentum transfer. TM2 initially encodes momentum in large-scale unstructured time series using the local linear scaling approximation (LLSA) module. Then it decomposes the reconstructed time series with momentum transfer into trend and seasonal components. The final prediction results are derived from the additive combination of a multilayer perceptron (MLP) for predicting trend components and wavelet attention mechanisms for seasonal components. Comprehensive experimental results show that on the 2023 Wimbledon men's tournament datasets, TM2 significantly surpasses existing sports prediction models in terms of performance, reducing MSE by 61.64% and MAE by 63.64%.",
      "paper_authors": [
        "Hui Liu",
        "Jiacheng Gu",
        "Xiyuan Huang",
        "Junjie Shi",
        "Tongtong Feng",
        "Ning He"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "Under reviewing",
      "repo_url": "#"
    },
    "2409.10130": {
      "paper_id": "2409.10130v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10130v1",
      "paper_key": "2409.10130",
      "paper_title": "Quantum walks of correlated photons in non-Hermitian photonic lattices",
      "paper_url": "http://arxiv.org/abs/2409.10130v1",
      "paper_abstract": "Entanglement entropy characterizes the correlation of multi-particles and unveils the crucial features of open quantum systems. However, the experimental realization of exploring entanglement in non-Hermitian systems remains a challenge. In parallel, quantum walks have offered the possibility of studying the underlying mechanisms of non-Hermitian physics, which includes exceptional points, the non-Hermitian skin effect, and non-Bloch phase transitions. Unfortunately, these studies have only involved and prevailingly focused on the behavior of a single particle. Here, we propose and experimentally realize quantum walks of two indistinguishable photons in engineered non-Hermitian photonic lattices. We have successfully observed the unidirectional behavior of quantum walks in the bulk far from the edges induced by the skin effect. Moreover, we experimentally reveal the suppression of entanglement that is caused by the skin effect in non-Hermitian systems. Our study may facilitate a deep understanding of entanglement in open quantum many-body systems that are far from thermal equilibrium.",
      "paper_authors": [
        "Mingyuan Gao",
        "Chong Sheng",
        "Yule Zhao",
        "Runqiu He",
        "Liangliang Lu",
        "Wei Chen",
        "Kun Ding",
        "Shining Zhu",
        "Hui Liu"
      ],
      "primary_category": "quant-ph",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "26 pages, 5 figures,",
      "repo_url": "#"
    },
    "2409.10102": {
      "paper_id": "2409.10102v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10102v1",
      "paper_key": "2409.10102",
      "paper_title": "Trustworthiness in Retrieval-Augmented Generation Systems: A Survey",
      "paper_url": "http://arxiv.org/abs/2409.10102v1",
      "paper_abstract": "Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal paradigm in the development of Large Language Models (LLMs). While much of the current research in this field focuses on performance optimization, particularly in terms of accuracy and efficiency, the trustworthiness of RAG systems remains an area still under exploration. From a positive perspective, RAG systems are promising to enhance LLMs by providing them with useful and up-to-date knowledge from vast external databases, thereby mitigating the long-standing problem of hallucination. While from a negative perspective, RAG systems are at the risk of generating undesirable contents if the retrieved information is either inappropriate or poorly utilized. To address these concerns, we propose a unified framework that assesses the trustworthiness of RAG systems across six key dimensions: factuality, robustness, fairness, transparency, accountability, and privacy. Within this framework, we thoroughly review the existing literature on each dimension. Additionally, we create the evaluation benchmark regarding the six dimensions and conduct comprehensive evaluations for a variety of proprietary and open-source models. Finally, we identify the potential challenges for future research based on our investigation results. Through this work, we aim to lay a structured foundation for future investigations and provide practical insights for enhancing the trustworthiness of RAG systems in real-world applications.",
      "paper_authors": [
        "Yujia Zhou",
        "Yan Liu",
        "Xiaoxi Li",
        "Jiajie Jin",
        "Hongjin Qian",
        "Zheng Liu",
        "Chaozhuo Li",
        "Zhicheng Dou",
        "Tsung-Yi Ho",
        "Philip S. Yu"
      ],
      "primary_category": "cs.IR",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": null,
      "repo_url": "https://github.com/smallporridge/trustworthyrag"
    },
    "2409.10067": {
      "paper_id": "2409.10067v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10067v1",
      "paper_key": "2409.10067",
      "paper_title": "Advancing flight physics through natural adaptation and animal learning",
      "paper_url": "http://arxiv.org/abs/2409.10067v1",
      "paper_abstract": "Fluid dynamics, and flight in particular, is a domain where organisms challenge our understanding of its physics. Integrating the current knowledge of animal flight, we propose to revisit the use of live animals to study physical phenomena. After a short description of the physics of flight, we examine the broad literature on animal flight focusing on studies of living animals. We start out reviewing the diverse animal species studied so far and then focus on the experimental techniques used to study them quantitatively. Our network analysis reveals how the three clades of animals performing powered flight - insects, birds and bats - are studied using substantially different combinations of measurement techniques. We then combine these insights with a new paradigm for increasing our physical understanding of flight. This paradigm relies on the concept of Animal Learning, where animals are used as probes to study fluid phenomena and variables involved in flight, harnessing their natural adaptability.",
      "paper_authors": [
        "Ariane Gayout",
        "David Lentink"
      ],
      "primary_category": "physics.flu-dyn",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "Review article (14 pages plus 10 pages references, 7 figures) Author\n  contributions: A.G. designed the main concept, drafted and revised the\n  manuscript. D.L. contributed to the design of the figures and revised drafts\n  of the manuscript",
      "repo_url": "#"
    },
    "2409.10061": {
      "paper_id": "2409.10061v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10061v1",
      "paper_key": "2409.10061",
      "paper_title": "Magnetization dependent anisotropic topological properties in EuCuP",
      "paper_url": "http://arxiv.org/abs/2409.10061v1",
      "paper_abstract": "The correlation between magnetism and nontrivial topological band structure serves as a unique venue for discovering exotic topological properties. Combining magnetotransport measurements and first-principles calculations, we unveil herein that the hexagonal EuCuP holds topologically trivial state in the paramagnetic structure, while strong magnetization dependent anisotropic topological states in the spin-polarization structures. Specifically, it hosts a trivial topological state in the in-plane spin-polarization structure, while a Weyl semimetal state in the out-of-plane spin-polarization structure. Our scaling analysis suggests that the intrinsic Berry curvature in the spin-polarization structures can account for the observed large anisotropic anomalous Hall effect. First-principles calculations show that the magnetization and the spin-orbit coupling simultaneously play essential roles for the appearance of the four pairs of Weyl points in the out-of-plane spin-polarization structure. Our work therefore establishes in EuCuP the intimate relation between magnetism and the nontrivial topological states, which would be instructive for future study on this key issue of topological physics.",
      "paper_authors": [
        "Jian Yuan",
        "Xianbiao Shi",
        "Hong Du",
        "Xia Wang",
        "Jinguang Cheng",
        "Baotian Wang",
        "Ruidan Zhong",
        "Shihao Zhang",
        "Yanfeng Guo"
      ],
      "primary_category": "cond-mat.str-el",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "15 papes, 5 figures and 2 tables",
      "repo_url": "#"
    },
    "2409.10043": {
      "paper_id": "2409.10043v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10043v1",
      "paper_key": "2409.10043",
      "paper_title": "Lunar References Systems, Frames and Time-scales in the context of the ESA Programme Moonlight",
      "paper_url": "http://arxiv.org/abs/2409.10043v1",
      "paper_abstract": "Lunar reference systems represent a fundamental aspect of lunar exploration. This paper presents a review of the topic in the context of the ESA lunar programme, MoonLight. This paper describes the current state of the art in the definition of the lunar reference frame and introduces TCL, a lunar time scale based on IAU resolutions. It also proposes several possible implementations of this time scale for orbiting and ground-based clocks. Finally, it provides an assessment of the improvement of the lunar reference frame that would result from the addition of lunar retro-reflectors on the Moon surface and the use of orbiter altimetry. This document is an appendix dedicated to lunar reference system definition of a more global document dedicated to the presentation of new concepts in orbit determination and time synchronization of a lunar radio navigation system.",
      "paper_authors": [
        "Agnes Fienga",
        "Nicolas Rambaux",
        "Krzysztof Sosnica"
      ],
      "primary_category": "astro-ph.EP",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "Appendix of the ATLAS Concept document ESA contract\n  AO/1-10712/21/NL/CRS",
      "repo_url": "#"
    },
    "2409.10036": {
      "paper_id": "2409.10036v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10036v1",
      "paper_key": "2409.10036",
      "paper_title": "Spin-controlled Electron transport in Chiral Molecular Assemblies for Various Applications",
      "paper_url": "http://arxiv.org/abs/2409.10036v1",
      "paper_abstract": "The chirality-induced spin selectivity (CISS) effect has garnered significant interest in the field of molecular spintronics due to its potential for creating spin-polarized electrons without the need for a magnet. Recent studies devoted to CISS effects in various chiral materials demonstrate exciting prospects for spintronics, chiral recognition, and quantum information applications. Several experimental studies have confirmed the applicability of chiral molecules towards spin-filtering properties, influencing spin-polarized electron transport, and photoemission. Researchers aim to predict CISS phenomena and apply this concept to practical applications by compiling experimental results and enhancing understanding of the CISS effect. To expand the possibilities of spin manipulation and create new opportunities for spin-based technologies, researchers are diligently exploring different chiral organic and inorganic materials for probing the CISS effect. This ongoing research holds promise for developing novel spin-based technologies and advancing the understanding of the intricate relationship between chirality and electron spin. This review showcases the remarkable CISS effect and its impact on spintronics, as well as its relevance in various other scientific areas.",
      "paper_authors": [
        "Ritu Gupta",
        "Anujit Balo",
        "Rabia Garg",
        "Amit Kumar Mondal",
        "Koyel Banerjee Ghosh",
        "Prakash Chandra Mondal"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "29 pages, 20 figures",
      "repo_url": "#"
    },
    "2409.10034": {
      "paper_id": "2409.10034v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10034v1",
      "paper_key": "2409.10034",
      "paper_title": "Altermagnets and beyond: Nodal magnetically-ordered phases",
      "paper_url": "http://arxiv.org/abs/2409.10034v1",
      "paper_abstract": "The recent discovery of altermagnets has opened new perspectives in the field of ordered phases in condensed matter. In strongly-correlated superfluids, the nodal p-wave and d-wave ordered phases of $^{3}$He and cuprates play a prominent role in physics for their rich phenomenology of the symmetry-breaking order parameters. While the p-wave and d-wave superfluids have been extensively studied over the past half a century, material realizations of their magnetic counterparts have remained elusive for many decades. This is resolved in altermagnets, whose recent discovery was driven by research in the field of spintronics towards highly scalable information technologies. Altermagnets feature d, g or i-wave magnetic ordering, with a characteristic alternation of spin polarization and spin-degenerate nodes. Here we review how altermagnetism can be identified from symmetries of collinear spin densities in crystal lattices, and can be realized at normal conditions in a broad family of insulating and conducting materials. We highlight salient electronic-structure signatures of the altermagnetic ordering, discuss extraordinary relativistic and topological phenomena that emerge in their band structures, and comment on strong-correlation effects. We then extend the discussion to non-collinear spin densities in crystals, including the prediction of p-wave magnets, and conclude with a brief summary of the reviewed physical properties of the nodal magnetically-ordered phases.",
      "paper_authors": [
        "Tomas Jungwirth",
        "Rafael M. Fernandes",
        "Jairo Sinova",
        "Libor Smejkal"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "17 pages, 6 figures",
      "repo_url": "#"
    },
    "2409.10012": {
      "paper_id": "2409.10012v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10012v1",
      "paper_key": "2409.10012",
      "paper_title": "Hydrostatic and chemical pressure driven crossover from commensurate to the incommensurate state of the Weyl semimetal Mn$_{3+x}$Sn$_{1-x}$",
      "paper_url": "http://arxiv.org/abs/2409.10012v1",
      "paper_abstract": "The observation of large intrinsic anomalous Hall conductivity (AHC) in the non-collinear antiferromagnetic (AFM) phase of the Weyl semimetal Mn$_3$Sn generates enormous interest in uncovering the entanglement between the real space magnetic ordering and the momentum space band structure. Previous studies show that changes in the magnetic structure induced by the application of hydrostatic and chemical pressure can significantly affect the AHC of Mn$_{3+x}$Sn$_{1-x}$ system. Here, we employ the muon spin relaxation/rotation ($\\mu^+$SR) technique to systematically investigate the evolution of different magnetic states in the Mn$_{3+x}$Sn$_{1-x}$ as a function of hydrostatic and chemical pressure. We find two muon sites experimentally, which is also supported by our \\textit{ab initio} calculations. Our $\\mu^+$SR experiments affirm that the $x = 0.05$ compound exhibits a commensurate magnetic state throughout the magnetically ordered phase below the Neel temperature $T_N \\approx 420$~K in ambient pressure. In contrast, we observe an incommensurate magnetic state below $T_{IC} \\sim 175$~K when a hydrostatic pressure of 1.5~GPa is applied. A similar transition from the commensurate to incommensurate state is also found with chemical pressure for $x = 0.04$ and $x = 0.03$, using $\\mu^+$SR and elastic neutron scattering experiments. Using band structure calculations, we have shown the emergence of Fermi nesting in Mn$_3$Sn and the subsequent development of incommensurate magnetic ordering under hydrostatic/chemical pressure.",
      "paper_authors": [
        "K. Bhattacharya",
        "A. K. Bharatwaj",
        "C. Singh",
        "R. Gupta",
        "R. Khasanov",
        "S. Kanungo",
        "A. K. Nayak",
        "M. Majumder"
      ],
      "primary_category": "cond-mat.str-el",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "13 pages, 8 figures. Accepted at Physical Review B (2024)",
      "repo_url": "#"
    },
    "2409.10008": {
      "paper_id": "2409.10008v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10008v1",
      "paper_key": "2409.10008",
      "paper_title": "Integrating Experiment with Theory to Determine the Structure of Electrode-Electrolyte Interfaces",
      "paper_url": "http://arxiv.org/abs/2409.10008v1",
      "paper_abstract": "Electrode-electrolyte interfaces are crucial for electrochemical energy conversion and storage. At these interfaces, the liquid electrolytes form electrical double layers (EDLs). However, despite more than a century of active research, the fundamental structure of EDLs remains elusive to date. Experimental characterization and theoretical calculations have both provided insights, yet each method by itself only offers incomplete or inexact information of the multifaceted EDL structure. Here we provide a survey of the mainstream approaches for EDL quantification, with a particular focus on the emerging 3D atomic force microscopy (3D-AFM) imaging which provides real-space atomic-scale EDL structures. To overcome the existing limits of EDL characterization methods, we propose a new approach to integrate 3D-AFM with classical molecular dynamics (MD) simulation, to enable realistic, precise, and high-throughput determination and prediction of EDL structures. As examples of real-world application, we will discuss the feasibility of using this joint experiment-theory method to unravel the EDL structure at various carbon-based electrodes for supercapacitors, batteries, and electrocatalysis. Looking forward, we believe 3D-AFM, future versions of scanning probe microscopy, and their integration with theory offer promising platforms to profile liquid structures in many electrochemical systems.",
      "paper_authors": [
        "Lalith Krishna Samanth Bonagiri",
        "Amir Farokh Payam",
        "Narayana R. Aluru",
        "Yingjie Zhang"
      ],
      "primary_category": "physics.chem-ph",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": null,
      "repo_url": "#"
    },
    "2409.10003": {
      "paper_id": "2409.10003v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10003v1",
      "paper_key": "2409.10003",
      "paper_title": "The Quantum Gravity Scale and the Swampland",
      "paper_url": "http://arxiv.org/abs/2409.10003v1",
      "paper_abstract": "This thesis investigates the role of the quantum gravity cut-off for effective field theories (EFTs) coupled to Einstein gravity, with an emphasis on its implications at low energies within the context of the Swampland program. Part I reviews the relevant aspects of string theory compactifications in different number of spacetime dimensions and with different amounts of supersymmetry preserved. In Part II a model-independent approach is employed so as to determine the maximum regime of validity of any such EFT, identifying the species scale as the natural candidate for the quantum gravity cut-off. We review various arguments proposed in the literature as well as include several new considerations on the matter. Part III provides a systematic study of this framework in string theory compactifications, yielding significant agreement with the previous perturbative and non-perturbative analysis. We also analyze various applications of this concept within the Swampland program, including the purported phenomenon of Emergence. Finally, in Part IV we explore the most immediate implications that this picture would have in the infrared regime, thus uncovering intriguing universal properties associated to the aforementioned energy scale, such as precise lower bounds on its exponential decay rates as well as certain patterns holding within the infinite distance corners of moduli space. The thesis includes new results scattered over the different chapters therein, which have not appeared in the author's original publications.",
      "paper_authors": [
        "Alberto Castellano"
      ],
      "primary_category": "hep-th",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "PhD thesis, 255 pages plus appendices. Based on arXiv:2212.03908,\n  arXiv:2306.16450, arXiv:2310.07708, arXiv:2311.01501 and arXiv:2311.01536",
      "repo_url": "#"
    },
    "2409.09992": {
      "paper_id": "2409.09992v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09992v1",
      "paper_key": "2409.09992",
      "paper_title": "Neutrino theory: open questions and future opportunities",
      "paper_url": "http://arxiv.org/abs/2409.09992v1",
      "paper_abstract": "The subtitle of my talk is ``The quest for understanding the origin of neutrino masses''. After reviewing why the discovery of neutrino masses is also the discovery of New Physics, the substance of the talk details mechanisms for generating Majorana neutrino masses and implications for experimental searches and/or cosmology. I review high-scale seesaw, low-scale seesaw and radiative mechanisms, asking at every turn how testable the scenario is. While it is clear that determining the origin of neutrino masses -- knowing what Lagrangian to put into textbooks -- is a distant and ambitious goal, I end with experimental advances that we can reasonably hope for that would constitute progress.",
      "paper_authors": [
        "Raymond R. Volkas"
      ],
      "primary_category": "hep-ph",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "10+'a bit' pages, 1 figure. Invited plenary talk at ICHEP 2024, 17-24\n  July, Prague, Czech Republic. To be submitted to the Proceedings",
      "repo_url": "#"
    },
    "2409.09989": {
      "paper_id": "2409.09989v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09989v1",
      "paper_key": "2409.09989",
      "paper_title": "Comprehensive Study on Sentiment Analysis: From Rule-based to modern LLM based system",
      "paper_url": "http://arxiv.org/abs/2409.09989v1",
      "paper_abstract": "This paper provides a comprehensive survey of sentiment analysis within the context of artificial intelligence (AI) and large language models (LLMs). Sentiment analysis, a critical aspect of natural language processing (NLP), has evolved significantly from traditional rule-based methods to advanced deep learning techniques. This study examines the historical development of sentiment analysis, highlighting the transition from lexicon-based and pattern-based approaches to more sophisticated machine learning and deep learning models. Key challenges are discussed, including handling bilingual texts, detecting sarcasm, and addressing biases. The paper reviews state-of-the-art approaches, identifies emerging trends, and outlines future research directions to advance the field. By synthesizing current methodologies and exploring future opportunities, this survey aims to understand sentiment analysis in the AI and LLM context thoroughly.",
      "paper_authors": [
        "Shailja Gupta",
        "Rajesh Ranjan",
        "Surya Narayan Singh"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "2 Images",
      "repo_url": "#"
    },
    "2409.09981": {
      "paper_id": "2409.09981v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09981v1",
      "paper_key": "2409.09981",
      "paper_title": "ANNZ+: an enhanced photometric redshift estimation algorithm with applications on the PAU Survey",
      "paper_url": "http://arxiv.org/abs/2409.09981v1",
      "paper_abstract": "ANNZ is a fast and simple algorithm which utilises artificial neural networks (ANNs), it was known as one of the pioneers of machine learning approaches to photometric redshift estimation decades ago. We enhanced the algorithm by introducing new activation functions like tanh, softplus, SiLU, Mish and ReLU variants; its new performance is then vigorously tested on legacy samples like the Luminous Red Galaxy (LRG) and Stripe-82 samples from SDSS, as well as modern galaxy samples like the Physics of the Accelerating Universe Survey (PAUS). This work focuses on testing the robustness of activation functions with respect to the choice of ANN architectures, particularly on its depth and width, in the context of galaxy photometric redshift estimation. Our upgraded algorithm, which we named ANNZ+, shows that the tanh and Leaky ReLU activation functions provide more consistent and stable results across deeper and wider architectures with > 1 per cent improvement in root-mean-square error ($\\sigma_{\\textrm{RMS}}$) and 68th percentile error ($\\sigma_{68}$) when tested on SDSS data sets. While assessing its capabilities in handling high dimensional inputs, we achieved an improvement of 11 per cent in $\\sigma_{\\textrm{RMS}}$ and 6 per cent in $\\sigma_{68}$ with the tanh activation function when tested on the 40-narrowband PAUS dataset; it even outperformed ANNZ2, its supposed successor, by 44 per cent in $\\sigma_{\\textrm{RMS}}$. This justifies the effort to upgrade the 20-year-old ANNZ, allowing it to remain viable and competitive within the photo-z community today. The updated algorithm ANNZ+ is publicly available at https://github.com/imdadmpt/ANNzPlus.",
      "paper_authors": [
        "Imdad Mahmud Pathi",
        "John Y. H. Soo",
        "Mao Jie Wee",
        "Sazatul Nadhilah Zakaria",
        "Nur Azwin Ismail",
        "Carlton M. Baugh",
        "Giorgio Manzoni",
        "Enrique Gaztanaga",
        "Francisco J. Castander",
        "Martin Eriksen",
        "Jorge Carretero",
        "Enrique Fernandez",
        "Juan Garcia-Bellido",
        "Ramon Miquel",
        "Cristobal Padilla",
        "Pablo Renard",
        "Eusebio Sanchez",
        "Ignacio Sevilla-Noarbe",
        "Pau Tallada-Cresp\u00ed"
      ],
      "primary_category": "astro-ph.IM",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "37 pages, 9 figures, submitted to JCAP",
      "repo_url": "#"
    },
    "2409.09977": {
      "paper_id": "2409.09977v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09977v1",
      "paper_key": "2409.09977",
      "paper_title": "Redshift Drift fluctuations from N-body simulations",
      "paper_url": "http://arxiv.org/abs/2409.09977v1",
      "paper_abstract": "Measurements of the redshift drift -- the real time variation of the redshift of distance sources -- are expected in the next couple of decades using next generation facilities such as the ANDES spectrograph at the ELT and the SKAO survey. The unprecedented precision of such observations will demand precise theoretical and numerical modeling of the effect in the standard $\\Lambda$CDM cosmology. In this work, we use the Gadget4 $N$-body code to simulate the redshift drift and its fluctuations in $\\Lambda$CDM cosmologies, deriving the corresponding power spectra from a simulation with $1024^3$ particles in a $1\\textrm{Gpc}\\,h^{-1}$ box. Our results provide an estimate for the distribution and amplitude of the fluctuations and the spectra, which match previous work in the literature using Einstein-Boltzmann solvers to within an order of magnitude. Our work provides a methodology for performing statistical analysis of the redshift drift effect and deriving its fluctuation power spectra from future large scale surveys.",
      "paper_authors": [
        "Pedro Bessa",
        "Valerio Marra",
        "Tiago Castro"
      ],
      "primary_category": "astro-ph.CO",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "19 pages, 5 figures, 3 tables",
      "repo_url": "#"
    },
    "2409.09968": {
      "paper_id": "2409.09968v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09968v1",
      "paper_key": "2409.09968",
      "paper_title": "Artificial Intelligence-Based Opportunistic Coronary Calcium Screening in the Veterans Affairs National Healthcare System",
      "paper_url": "http://arxiv.org/abs/2409.09968v1",
      "paper_abstract": "Coronary artery calcium (CAC) is highly predictive of cardiovascular events. While millions of chest CT scans are performed annually in the United States, CAC is not routinely quantified from scans done for non-cardiac purposes. A deep learning algorithm was developed using 446 expert segmentations to automatically quantify CAC on non-contrast, non-gated CT scans (AI-CAC). Our study differs from prior works as we leverage imaging data across the Veterans Affairs national healthcare system, from 98 medical centers, capturing extensive heterogeneity in imaging protocols, scanners, and patients. AI-CAC performance on non-gated scans was compared against clinical standard ECG-gated CAC scoring. Non-gated AI-CAC differentiated zero vs. non-zero and less than 100 vs. 100 or greater Agatston scores with accuracies of 89.4% (F1 0.93) and 87.3% (F1 0.89), respectively, in 795 patients with paired gated scans within a year of a non-gated CT scan. Non-gated AI-CAC was predictive of 10-year all-cause mortality (CAC 0 vs. >400 group: 25.4% vs. 60.2%, Cox HR 3.49, p < 0.005), and composite first-time stroke, MI, or death (CAC 0 vs. >400 group: 33.5% vs. 63.8%, Cox HR 3.00, p < 0.005). In a screening dataset of 8,052 patients with low-dose lung cancer-screening CTs (LDCT), 3,091/8,052 (38.4%) individuals had AI-CAC >400. Four cardiologists qualitatively reviewed LDCT images from a random sample of >400 AI-CAC patients and verified that 527/531 (99.2%) would benefit from lipid-lowering therapy. To the best of our knowledge, this is the first non-gated CT CAC algorithm developed across a national healthcare system, on multiple imaging protocols, without filtering intra-cardiac hardware, and compared against a strong gated CT reference. We report superior performance relative to previous CAC algorithms evaluated against paired gated scans that included patients with intra-cardiac hardware.",
      "paper_authors": [
        "Raffi Hagopian",
        "Timothy Strebel",
        "Simon Bernatz",
        "Gregory A Myers",
        "Erik Offerman",
        "Eric Zuniga",
        "Cy Y Kim",
        "Angie T Ng",
        "James A Iwaz",
        "Sunny P Singh",
        "Evan P Carey",
        "Michael J Kim",
        "R Spencer Schaefer",
        "Jeannie Yu",
        "Amilcare Gentili",
        "Hugo JWL Aerts"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": null,
      "repo_url": "#"
    },
    "2409.09957": {
      "paper_id": "2409.09957v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09957v1",
      "paper_key": "2409.09957",
      "paper_title": "Deep Graph Anomaly Detection: A Survey and New Perspectives",
      "paper_url": "http://arxiv.org/abs/2409.09957v1",
      "paper_abstract": "Graph anomaly detection (GAD), which aims to identify unusual graph instances (nodes, edges, subgraphs, or graphs), has attracted increasing attention in recent years due to its significance in a wide range of applications. Deep learning approaches, graph neural networks (GNNs) in particular, have been emerging as a promising paradigm for GAD, owing to its strong capability in capturing complex structure and/or node attributes in graph data. Considering the large number of methods proposed for GNN-based GAD, it is of paramount importance to summarize the methodologies and findings in the existing GAD studies, so that we can pinpoint effective model designs for tackling open GAD problems. To this end, in this work we aim to present a comprehensive review of deep learning approaches for GAD. Existing GAD surveys are focused on task-specific discussions, making it difficult to understand the technical insights of existing methods and their limitations in addressing some unique challenges in GAD. To fill this gap, we first discuss the problem complexities and their resulting challenges in GAD, and then provide a systematic review of current deep GAD methods from three novel perspectives of methodology, including GNN backbone design, proxy task design for GAD, and graph anomaly measures. To deepen the discussions, we further propose a taxonomy of 13 fine-grained method categories under these three perspectives to provide more in-depth insights into the model designs and their capabilities. To facilitate the experiments and validation, we also summarize a collection of widely-used GAD datasets and empirical comparison. We further discuss multiple open problems to inspire more future high-quality research. A continuously updated repository for datasets, links to the codes of algorithms, and empirical comparison is available at https://github.com/mala-lab/Awesome-Deep-Graph-Anomaly-Detection.",
      "paper_authors": [
        "Hezhe Qiao",
        "Hanghang Tong",
        "Bo An",
        "Irwin King",
        "Charu Aggarwal",
        "Guansong Pang"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "24 pages, 6 figures, and 7 tables",
      "repo_url": "https://github.com/mala-lab/awesome-deep-graph-anomaly-detection"
    },
    "2409.09946": {
      "paper_id": "2409.09946v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09946v1",
      "paper_key": "2409.09946",
      "paper_title": "An Independent Measure of the Kinematic Dipole from SDSS",
      "paper_url": "http://arxiv.org/abs/2409.09946v1",
      "paper_abstract": "We utilize the Sloan Digital Sky Survey (SDSS) extended Baryon Oscillation Spectroscopic Survey (eBOSS) and Baryon Oscillation Spectroscopic Survey (BOSS) catalogs with precise spectroscopic redshifts to estimate the kinematic redshift dipole caused by the proper motion of the Solar system. We find that the velocity extracted from the kinematic dipole is consistent with Cosmic Microwave Background inferred values. Although the small sky coverage and limited number density of the SDSS sources constrain us from obtaining precise and robust measurements, we leverage the redshift dipole method to estimate the kinematic dipole. The velocity measurements in this study are insensitive to intrinsic clustering, associated with the source count dipole. The kinematic dipole measured in this work and its consistency with CMB values do not guarantee isotropy at large scales. The anisotropy (excess dipole) measured with the NRAO VLA Sky Survey (NVSS) and the WISE Catalog (CatWISE) could be due to the intrinsic distribution of galaxies. The results in this work focus solely on the kinematic dipole term.",
      "paper_authors": [
        "Prabhakar Tiwari",
        "Dominik J. Schwarz",
        "Gong-Bo Zhao",
        "Ruth Durrer",
        "Martin Kunz",
        "Hamsa Padmanabhan"
      ],
      "primary_category": "astro-ph.CO",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "13 pages, 4 figures, 4 tables",
      "repo_url": "#"
    },
    "2409.09941": {
      "paper_id": "2409.09941v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09941v1",
      "paper_key": "2409.09941",
      "paper_title": "ROS2WASM: Bringing the Robot Operating System to the Web",
      "paper_url": "http://arxiv.org/abs/2409.09941v1",
      "paper_abstract": "The Robot Operating System (ROS) has become the de facto standard middleware in robotics, widely adopted across domains ranging from education to industrial applications. The RoboStack distribution has extended ROS's accessibility by facilitating installation across all major operating systems and architectures, integrating seamlessly with scientific tools such as PyTorch and Open3D. This paper presents ROS2WASM, a novel integration of RoboStack with WebAssembly, enabling the execution of ROS 2 and its associated software directly within web browsers, without requiring local installations. This approach significantly enhances reproducibility and shareability of research, lowers barriers to robotics education, and leverages WebAssembly's robust security framework to protect against malicious code. We detail our methodology for cross-compiling ROS 2 packages into WebAssembly, the development of a specialized middleware for ROS 2 communication within browsers, and the implementation of a web platform available at www.ros2wasm.dev that allows users to interact with ROS 2 environments. Additionally, we extend support to the Robotics Toolbox for Python and adapt its Swift simulator for browser compatibility. Our work paves the way for unprecedented accessibility in robotics, offering scalable, secure, and reproducible environments that have the potential to transform educational and research paradigms.",
      "paper_authors": [
        "Tobias Fischer",
        "Isabel Paredes",
        "Michael Batchelor",
        "Thorsten Beier",
        "Jesse Haviland",
        "Silvio Traversaro",
        "Wolf Vollprecht",
        "Markus Schmitz",
        "Michael Milford"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "7 pages, 8 figures, under review",
      "repo_url": "#"
    },
    "2409.09919": {
      "paper_id": "2409.09919v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09919v1",
      "paper_key": "2409.09919",
      "paper_title": "Engineering topological states and quantum-inspired information processing using classical circuits",
      "paper_url": "http://arxiv.org/abs/2409.09919v1",
      "paper_abstract": "Based on the correspondence between circuit Laplacian and Schrodinger equation, recent investigations have shown that classical electric circuits can be used to simulate various topological physics and the Schrodinger's equation. Furthermore, a series of quantum-inspired information processing have been implemented by using classical electric circuit networks. In this review, we begin by analyzing the similarity between circuit Laplacian and lattice Hamiltonian, introducing topological physics based on classical circuits. Subsequently, we provide reviews of the research progress in quantum-inspired information processing based on the electric circuit, including discussions of topological quantum computing with classical circuits, quantum walk based on classical circuits, quantum combinational logics based on classical circuits, electric-circuit realization of fast quantum search, implementing unitary transforms and so on.",
      "paper_authors": [
        "Tian Chen",
        "Weixuan Zhang",
        "Deyuan Zou",
        "Yifan Sun",
        "Xiangdong Zhang"
      ],
      "primary_category": "cond-mat.mes-hall",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "21 Figs, 80 pages; Invited review article from Advanced Quantum\n  Technologies a few months ago",
      "repo_url": "#"
    },
    "2409.09902": {
      "paper_id": "2409.09902v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09902v1",
      "paper_key": "2409.09902",
      "paper_title": "TV Mon - post mass transfer Algol type binary with $\u03b4$ Scuti pulsations in primary component",
      "paper_url": "http://arxiv.org/abs/2409.09902v1",
      "paper_abstract": "We present a study of the detached eclipsing binary TV~Mon using spectra from the LAMOST medium-resolution survey and ASAS-SN, CoRoT photometry. We applied multiple-epochs spectral fitting to derive RV and spectral parameters. The analysis of eclipses in CoRoT data told us relative sizes of the stellar components and almost edge-on circular orbit. Combining spectral and photometrical solution we estimated masses and radii of the components: $M_{A,B}=2.063\\pm0.033,~0.218\\pm0.004~M_\\odot$, $R_{A,B}=2.427\\pm0.014,~2.901\\pm0.016~R_\\odot$. SED analysis and Gaia parallax allowed us to get estimation of temperatures $T_{A,B}=7624^{+194}_{-174},~5184^{+130}_{-123}$ K and distance $d=907\\pm11$ pc. We identified three $\\delta$ Scuti type pulsation frequencies in primary component, while we also suspect TV~Mon having a long period variability with period $P_{\\rm long}\\sim128$ days and spot activity in secondary component. This system experienced intensive mass transfer and mass ratio reversal in the past, currently showing no signs of mass transfer in the spectra. The low mass component will lose its outer envelope and shrink to the helium white dwarf, which mass and orbital period are in good agreement with evolutionary models predictions.",
      "paper_authors": [
        "Mikhail Kovalev",
        "Zhenwei Li",
        "Jianping Xiong",
        "Azizbek Matekov",
        "Zhang Bo",
        "Xuefei Chen",
        "Zhanwen Han"
      ],
      "primary_category": "astro-ph.SR",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "submitted in MNRAS",
      "repo_url": "#"
    },
    "2409.09874": {
      "paper_id": "2409.09874v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09874v1",
      "paper_key": "2409.09874",
      "paper_title": "The Landscape of GPU-Centric Communication",
      "paper_url": "http://arxiv.org/abs/2409.09874v1",
      "paper_abstract": "n recent years, GPUs have become the preferred accelerators for HPC and ML applications due to their parallelism and fast memory bandwidth. While GPUs boost computation, inter-GPU communication can create scalability bottlenecks, especially as the number of GPUs per node and cluster grows. Traditionally, the CPU managed multi-GPU communication, but advancements in GPU-centric communication now challenge this CPU dominance by reducing its involvement, granting GPUs more autonomy in communication tasks, and addressing mismatches in multi-GPU communication and computation.   This paper provides a landscape of GPU-centric communication, focusing on vendor mechanisms and user-level library supports. It aims to clarify the complexities and diverse options in this field, define the terminology, and categorize existing approaches within and across nodes. The paper discusses vendor-provided mechanisms for communication and memory management in multi-GPU execution and reviews major communication libraries, their benefits, challenges, and performance insights. Then, it explores key research paradigms, future outlooks, and open research questions. By extensively describing GPU-centric communication techniques across the software and hardware stacks, we provide researchers, programmers, engineers, and library designers insights on how to exploit multi-GPU systems at their best.",
      "paper_authors": [
        "Didem Unat",
        "Ilyas Turimbetov",
        "Mohammed Kefah Taha Issa",
        "Do\u011fan Sa\u011fbili",
        "Flavio Vella",
        "Daniele De Sensi",
        "Ismayil Ismayilov"
      ],
      "primary_category": "cs.DC",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": null,
      "repo_url": "#"
    },
    "2409.09858": {
      "paper_id": "2409.09858v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09858v1",
      "paper_key": "2409.09858",
      "paper_title": "A Survey of Out-of-distribution Generalization for Graph Machine Learning from a Causal View",
      "paper_url": "http://arxiv.org/abs/2409.09858v1",
      "paper_abstract": "Graph machine learning (GML) has been successfully applied across a wide range of tasks. Nonetheless, GML faces significant challenges in generalizing over out-of-distribution (OOD) data, which raises concerns about its wider applicability. Recent advancements have underscored the crucial role of causality-driven approaches in overcoming these generalization challenges. Distinct from traditional GML methods that primarily rely on statistical dependencies, causality-focused strategies delve into the underlying causal mechanisms of data generation and model prediction, thus significantly improving the generalization of GML across different environments. This paper offers a thorough review of recent progress in causality-involved GML generalization. We elucidate the fundamental concepts of employing causality to enhance graph model generalization and categorize the various approaches, providing detailed descriptions of their methodologies and the connections among them. Furthermore, we explore the incorporation of causality in other related important areas of trustworthy GML, such as explanation, fairness, and robustness. Concluding with a discussion on potential future research directions, this review seeks to articulate the continuing development and future potential of causality in enhancing the trustworthiness of graph machine learning.",
      "paper_authors": [
        "Jing Ma"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": "15 pages, 2 figures, 1 table",
      "repo_url": "#"
    },
    "2409.09848": {
      "paper_id": "2409.09848v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09848v1",
      "paper_key": "2409.09848",
      "paper_title": "A Comprehensive Survey of PID and Pure Pursuit Control Algorithms for Autonomous Vehicle Navigation",
      "paper_url": "http://arxiv.org/abs/2409.09848v1",
      "paper_abstract": "The autonomous driving industry is experiencing unprecedented growth, driven by rapid advancements in technology and increasing demand for safer, more efficient transportation. At the heart of this revolution are two critical factors: lateral and longitudinal controls, which together enable vehicles to track complex environments with high accuracy and minimal errors. This paper provides a detailed overview of two of the field's most commonly used and stable control algorithms: proportional-integral-derivative (PID) and pure pursuit. These algorithms have proved useful in solving the issues of lateral (steering) and longitudinal (speed and distance) control in autonomous vehicles. This survey aims to provide researchers, engineers, and industry professionals with an in depth understanding of these fundamental control algorithms, their current applications, and their potential to shape the future of autonomous driving technology.",
      "paper_authors": [
        "Harshit Jain",
        "Priyal Babel"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": "7 pages, 5 figures, 1 table, Autonomous vehicles, Control Algorithms,\n  PID, Pure Pursuit",
      "repo_url": "#"
    },
    "2409.09822": {
      "paper_id": "2409.09822v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09822v1",
      "paper_key": "2409.09822",
      "paper_title": "Causal Inference with Large Language Model: A Survey",
      "paper_url": "http://arxiv.org/abs/2409.09822v1",
      "paper_abstract": "Causal inference has been a pivotal challenge across diverse domains such as medicine and economics, demanding a complicated integration of human knowledge, mathematical reasoning, and data mining capabilities. Recent advancements in natural language processing (NLP), particularly with the advent of large language models (LLMs), have introduced promising opportunities for traditional causal inference tasks. This paper reviews recent progress in applying LLMs to causal inference, encompassing various tasks spanning different levels of causation. We summarize the main causal problems and approaches, and present a comparison of their evaluation results in different causal scenarios. Furthermore, we discuss key findings and outline directions for future research, underscoring the potential implications of integrating LLMs in advancing causal inference methodologies.",
      "paper_authors": [
        "Jing Ma"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": "15 pages, 2 figures, 3 tables",
      "repo_url": "#"
    },
    "2409.09779": {
      "paper_id": "2409.09779v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09779v1",
      "paper_key": "2409.09779",
      "paper_title": "Underwater Image Enhancement via Dehazing and Color Restoration",
      "paper_url": "http://arxiv.org/abs/2409.09779v1",
      "paper_abstract": "With the rapid development of marine engineering projects such as marine resource extraction and oceanic surveys, underwater visual imaging and analysis has become a critical technology. Unfortunately, due to the inevitable non-linear attenuation of light in underwater environments, underwater images and videos often suffer from low contrast, blurriness, and color degradation, which significantly complicate the subsequent research. Existing underwater image enhancement methods often treat the haze and color cast as a unified degradation process and disregard their independence and interdependence, which limits the performance improvement. Here, we propose a Vision Transformer (ViT)-based network (referred to as WaterFormer) to improve the underwater image quality. WaterFormer contains three major components: a dehazing block (DehazeFormer Block) to capture the self-correlated haze features and extract deep-level features, a Color Restoration Block (CRB) to capture self-correlated color cast features, and a Channel Fusion Block (CFB) to capture fusion features within the network. To ensure authenticity, a soft reconstruction layer based on the underwater imaging physics model is included. To improve the quality of the enhanced images, we introduce the Chromatic Consistency Loss and Sobel Color Loss to train the network. Comprehensive experimental results demonstrate that WaterFormer outperforms other state-of-the-art methods in enhancing underwater images.",
      "paper_authors": [
        "Chengqin Wu",
        "Shuai Yu",
        "Qingson Hu",
        "Jingxiang Xu",
        "Lijun Zhang"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": null,
      "repo_url": "#"
    },
    "2409.09772": {
      "paper_id": "2409.09772v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09772v1",
      "paper_key": "2409.09772",
      "paper_title": "The Galaxy Activity, Torus, and Outflow Survey (GATOS). (VI): Polycyclic Aromatic Hydrocarbon Emission in the Central Regions of Three Seyferts",
      "paper_url": "http://arxiv.org/abs/2409.09772v1",
      "paper_abstract": "We analyze JWST MIRI/MRS IFU observations of three Seyferts and showcase the intriguing polycyclic aromatic hydrocarbon (PAH) emission characteristics in regions of $\\sim 500\\,\\rm pc$ scales over or around their active galactic nuclei (AGN). Combining the model predictions and the measurements of PAH features and other infrared emission lines, we find that the central regions containing a high fraction of neutral PAHs with small sizes, e.g., those in ESO137-G034, are in highly heated environments, due to collisional shock heating, with hard and moderately intense radiation fields. Such environments are proposed to be associated with inhibited growth or preferential erosion of PAHs, decreasing the average PAH size and the overall abundance of PAHs. We additionally find that the central regions containing a high fraction of ionized PAHs with large sizes, e.g., those in MCG-05-23-016, are likely experiencing severe photo-ionization because of the radiative effects from the radiative shock precursor besides the AGN. The severe photo-ionization can contribute to the ionization of all PAHs and further destruction of small PAHs. Overall, different Seyferts, even different regions in the same galaxy, e.g., those in NGC\\,3081, can contain PAH populations of different properties. Specifically, Seyferts that exhibit similar PAH characteristics to ESO137-G034 and MCG-05-23-016 also tend to have similar emission line properties to them, suggesting that the explanations for PAH characteristics of ESO137-G034 and MCG-05-23-016 may also apply generally. These results have promising application in the era of JWST, especially in diagnosing different (i.e., radiative, and kinetic) AGN feedback modes.",
      "paper_authors": [
        "Lulu Zhang",
        "Ismael Garc\u00eda-Bernete",
        "Chris Packham",
        "Fergus R. Donnan",
        "Dimitra Rigopoulou",
        "Erin K. S. Hicks",
        "Ric I. Davies",
        "Taro T. Shimizu",
        "Almudena Alonso-Herrero",
        "Cristina Ramos Almeida",
        "Miguel Pereira-Santaella",
        "Claudio Ricci",
        "Andrew J. Bunker",
        "Mason T. Leist",
        "David J. Rosario",
        "Santiago Garc\u00eda-Burillo",
        "Laura Hermosa Mu\u00f1oz",
        "Francoise Combes",
        "Masatoshi Imanishi",
        "Alvaro Labiano",
        "Donaji Esparza-Arredondo",
        "Enrica Bellocchi",
        "Anelise Audibert",
        "Lindsay Fuller",
        "Omaira Gonz\u00e1lez-Mart\u00edn",
        "Sebastian H\u00f6nig",
        "Takuma Izumi",
        "Nancy A. Levenson",
        "Enrique L\u00f3pez-Rodr\u00edguez",
        "Daniel Rouan",
        "Marko Stalevski",
        "Martin J. Ward"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": "20 pages (6 pages in the appendix), 6 figures in the main text, ApJL\n  resubmitted after addressing referee's comments",
      "repo_url": "#"
    },
    "2409.09771": {
      "paper_id": "2409.09771v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09771v1",
      "paper_key": "2409.09771",
      "paper_title": "The Galaxy Activity, Torus, and Outflow Survey (GATOS). (IV): Exploring Ionized Gas Outflows in Central Kiloparsec Regions of GATOS Seyferts",
      "paper_url": "http://arxiv.org/abs/2409.09771v1",
      "paper_abstract": "Utilizing JWST MIRI/MRS IFU observations of the kiloparsec scale central regions, we showcase the diversity of ionized gas distributions and kinematics in six nearby Seyfert galaxies included in the GATOS survey. Specifically, we present spatially resolved flux distribution and velocity field maps of six ionized emission lines covering a large range of ionization potentials ($15.8-97.1$ eV). Based on these maps, we showcase the evidence of ionized gas outflows in the six targets, and find some highly disturbed regions in NGC\\,5728, NGC\\,5506, and ESO137-G034. We propose AGN-driven radio jets plausibly play an important role in triggering these highly disturbed regions. With the outflow rates estimated based on [Ne~{\\footnotesize V}] emission, we find the six targets tend to have ionized outflow rates converged to a narrower range than previous finding. These results have important implication for the outflow properties in AGN of comparable luminosity.",
      "paper_authors": [
        "Lulu Zhang",
        "Chris Packham",
        "Erin K. S. Hicks",
        "Ric I. Davies",
        "Taro T. Shimizu",
        "Almudena Alonso-Herrero",
        "Laura Hermosa Mu\u00f1oz",
        "Ismael Garc\u00eda-Bernete",
        "Miguel Pereira-Santaella",
        "Anelise Audibert",
        "Enrique L\u00f3pez-Rodr\u00edguez",
        "Enrica Bellocch",
        "Andrew J. Bunker",
        "Francoise Combes",
        "Tanio D\u00edaz-Santos",
        "Poshak Gandhi",
        "Santiago Garc\u00eda-Burillo",
        "Bego\u00f1a Garc\u00eda-Lorenzo",
        "Omaira Gonz\u00e1lez-Mart\u00edn",
        "Masatoshi Imanishi",
        "Alvaro Labiano",
        "Mason T. Leist",
        "Nancy A. Levenson",
        "Cristina Ramos Almeida",
        "Claudio Ricci",
        "Dimitra Rigopoulou",
        "David J. Rosario",
        "Marko Stalevski",
        "Martin J. Ward",
        "Donaji Esparza-Arredondo",
        "Dan Delaney",
        "Lindsay Fuller",
        "Houda Haidar",
        "Sebastian H\u00f6nig",
        "Takuma Izumi",
        "Daniel Rouan"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": "34 pages (11 pages in the appendix), 18 figures in the main text, ApJ\n  in press (accepted on July 26th)",
      "repo_url": "#"
    },
    "2409.09739": {
      "paper_id": "2409.09739v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09739v1",
      "paper_key": "2409.09739",
      "paper_title": "PersonaMark: Personalized LLM watermarking for model protection and user attribution",
      "paper_url": "http://arxiv.org/abs/2409.09739v1",
      "paper_abstract": "The rapid development of LLMs brings both convenience and potential threats. As costumed and private LLMs are widely applied, model copyright protection has become important. Text watermarking is emerging as a promising solution to AI-generated text detection and model protection issues. However, current text watermarks have largely ignored the critical need for injecting different watermarks for different users, which could help attribute the watermark to a specific individual. In this paper, we explore the personalized text watermarking scheme for LLM copyright protection and other scenarios, ensuring accountability and traceability in content generation. Specifically, we propose a novel text watermarking method PersonaMark that utilizes sentence structure as the hidden medium for the watermark information and optimizes the sentence-level generation algorithm to minimize disruption to the model's natural generation process. By employing a personalized hashing function to inject unique watermark signals for different users, personalized watermarked text can be obtained. Since our approach performs on sentence level instead of token probability, the text quality is highly preserved. The injection process of unique watermark signals for different users is time-efficient for a large number of users with the designed multi-user hashing function. As far as we know, we achieved personalized text watermarking for the first time through this. We conduct an extensive evaluation of four different LLMs in terms of perplexity, sentiment polarity, alignment, readability, etc. The results demonstrate that our method maintains performance with minimal perturbation to the model's behavior, allows for unbiased insertion of watermark information, and exhibits strong watermark recognition capabilities.",
      "paper_authors": [
        "Yuehan Zhang",
        "Peizhuo Lv",
        "Yinpeng Liu",
        "Yongqiang Ma",
        "Wei Lu",
        "Xiaofeng Wang",
        "Xiaozhong Liu",
        "Jiawei Liu"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": "Under review",
      "repo_url": "#"
    },
    "2409.09727": {
      "paper_id": "2409.09727v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09727v1",
      "paper_key": "2409.09727",
      "paper_title": "From Challenges and Pitfalls to Recommendations and Opportunities: Implementing Federated Learning in Healthcare",
      "paper_url": "http://arxiv.org/abs/2409.09727v1",
      "paper_abstract": "Federated learning holds great potential for enabling large-scale healthcare research and collaboration across multiple centres while ensuring data privacy and security are not compromised. Although numerous recent studies suggest or utilize federated learning based methods in healthcare, it remains unclear which ones have potential clinical utility. This review paper considers and analyzes the most recent studies up to May 2024 that describe federated learning based methods in healthcare. After a thorough review, we find that the vast majority are not appropriate for clinical use due to their methodological flaws and/or underlying biases which include but are not limited to privacy concerns, generalization issues, and communication costs. As a result, the effectiveness of federated learning in healthcare is significantly compromised. To overcome these challenges, we provide recommendations and promising opportunities that might be implemented to resolve these problems and improve the quality of model development in federated learning with healthcare.",
      "paper_authors": [
        "Ming Li",
        "Pengcheng Xu",
        "Junjie Hu",
        "Zeyu Tang",
        "Guang Yang"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": null,
      "repo_url": "#"
    },
    "2409.09726": {
      "paper_id": "2409.09726v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09726v1",
      "paper_key": "2409.09726",
      "paper_title": "High Definition Map Mapping and Update: A General Overview and Future Directions",
      "paper_url": "http://arxiv.org/abs/2409.09726v1",
      "paper_abstract": "Along with the rapid growth of autonomous vehicles (AVs), more and more demands are required for environment perception technology. Among others, HD mapping has become one of the more prominent roles in helping the vehicle realize essential tasks such as localization and path planning. While increasing research efforts have been directed toward HD Map development. However, a comprehensive overview of the overall HD map mapping and update framework is still lacking. This article introduces the development and current state of the algorithm involved in creating HD map mapping and its maintenance. As part of this study, the primary data preprocessing approach of processing raw data to information ready to feed for mapping and update purposes, semantic segmentation, and localization are also briefly reviewed. Moreover, the map taxonomy, ontology, and quality assessment are extensively discussed, the map data's general representation method is presented, and the mapping algorithm ranging from SLAM to transformers learning-based approaches are also discussed. The development of the HD map update algorithm, from change detection to the update methods, is also presented. Finally, the authors discuss possible future developments and the remaining challenges in HD map mapping and update technology. This paper simultaneously serves as a position paper and tutorial to those new to HD map mapping and update domains.",
      "paper_authors": [
        "Benny Wijaya",
        "Kun Jiang",
        "Mengmeng Yang",
        "Tuopu Wen",
        "Yunlong Wang",
        "Xuewei Tang",
        "Zheng Fu",
        "Taohua Zhou",
        "Diange Yang"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": "30 Pages, 13 figures",
      "repo_url": "#"
    },
    "2409.09704": {
      "paper_id": "2409.09704v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09704v1",
      "paper_key": "2409.09704",
      "paper_title": "AlpaPICO: Extraction of PICO Frames from Clinical Trial Documents Using LLMs",
      "paper_url": "http://arxiv.org/abs/2409.09704v1",
      "paper_abstract": "In recent years, there has been a surge in the publication of clinical trial reports, making it challenging to conduct systematic reviews. Automatically extracting Population, Intervention, Comparator, and Outcome (PICO) from clinical trial studies can alleviate the traditionally time-consuming process of manually scrutinizing systematic reviews. Existing approaches of PICO frame extraction involves supervised approach that relies on the existence of manually annotated data points in the form of BIO label tagging. Recent approaches, such as In-Context Learning (ICL), which has been shown to be effective for a number of downstream NLP tasks, require the use of labeled examples. In this work, we adopt ICL strategy by employing the pretrained knowledge of Large Language Models (LLMs), gathered during the pretraining phase of an LLM, to automatically extract the PICO-related terminologies from clinical trial documents in unsupervised set up to bypass the availability of large number of annotated data instances. Additionally, to showcase the highest effectiveness of LLM in oracle scenario where large number of annotated samples are available, we adopt the instruction tuning strategy by employing Low Rank Adaptation (LORA) to conduct the training of gigantic model in low resource environment for the PICO frame extraction task. Our empirical results show that our proposed ICL-based framework produces comparable results on all the version of EBM-NLP datasets and the proposed instruction tuned version of our framework produces state-of-the-art results on all the different EBM-NLP datasets. Our project is available at \\url{https://github.com/shrimonmuke0202/AlpaPICO.git}.",
      "paper_authors": [
        "Madhusudan Ghosh",
        "Shrimon Mukherjee",
        "Asmit Ganguly",
        "Partha Basuchowdhuri",
        "Sudip Kumar Naskar",
        "Debasis Ganguly"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": "Accepted at Methods",
      "repo_url": "https://github.com/shrimonmuke0202/alpapico"
    },
    "2409.09678": {
      "paper_id": "2409.09678v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09678v1",
      "paper_key": "2409.09678",
      "paper_title": "A Comprehensive Methodological Survey of Human Activity Recognition Across Divers Data Modalities",
      "paper_url": "http://arxiv.org/abs/2409.09678v1",
      "paper_abstract": "Human Activity Recognition (HAR) systems aim to understand human behaviour and assign a label to each action, attracting significant attention in computer vision due to their wide range of applications. HAR can leverage various data modalities, such as RGB images and video, skeleton, depth, infrared, point cloud, event stream, audio, acceleration, and radar signals. Each modality provides unique and complementary information suited to different application scenarios. Consequently, numerous studies have investigated diverse approaches for HAR using these modalities. This paper presents a comprehensive survey of the latest advancements in HAR from 2014 to 2024, focusing on machine learning (ML) and deep learning (DL) approaches categorized by input data modalities. We review both single-modality and multi-modality techniques, highlighting fusion-based and co-learning frameworks. Additionally, we cover advancements in hand-crafted action features, methods for recognizing human-object interactions, and activity detection. Our survey includes a detailed dataset description for each modality and a summary of the latest HAR systems, offering comparative results on benchmark datasets. Finally, we provide insightful observations and propose effective future research directions in HAR.",
      "paper_authors": [
        "Jungpil Shin",
        "Najmul Hassan",
        "Abu Saleh Musa Miah1",
        "Satoshi Nishimura"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": null,
      "repo_url": "#"
    },
    "2409.09660": {
      "paper_id": "2409.09660v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09660v1",
      "paper_key": "2409.09660",
      "paper_title": "On the Proofs of the Predictive Synthesis Formula",
      "paper_url": "http://arxiv.org/abs/2409.09660v1",
      "paper_abstract": "Bayesian predictive synthesis is useful in synthesizing multiple predictive distributions coherently. However, the proof for the fundamental equation of the synthesized predictive density has been missing. In this technical report, we review the series of research on predictive synthesis, then fill the gap between the known results and the equation used in modern applications. We provide two proofs and clarify the structure of predictive synthesis.",
      "paper_authors": [
        "Riku Masuda",
        "Kaoru Irie"
      ],
      "primary_category": "stat.ME",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": "11 pages, no figure, 1 table",
      "repo_url": "#"
    },
    "2409.09650": {
      "paper_id": "2409.09650v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09650v1",
      "paper_key": "2409.09650",
      "paper_title": "Conditional sampling within generative diffusion models",
      "paper_url": "http://arxiv.org/abs/2409.09650v1",
      "paper_abstract": "Generative diffusions are a powerful class of Monte Carlo samplers that leverage bridging Markov processes to approximate complex, high-dimensional distributions, such as those found in image processing and language models. Despite their success in these domains, an important open challenge remains: extending these techniques to sample from conditional distributions, as required in, for example, Bayesian inverse problems. In this paper, we present a comprehensive review of existing computational approaches to conditional sampling within generative diffusion models. Specifically, we highlight key methodologies that either utilise the joint distribution, or rely on (pre-trained) marginal distributions with explicit likelihoods, to construct conditional generative samplers.",
      "paper_authors": [
        "Zheng Zhao",
        "Ziwei Luo",
        "Jens Sj\u00f6lund",
        "Thomas B. Sch\u00f6n"
      ],
      "primary_category": "stat.ML",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": null,
      "repo_url": "https://github.com/spdes/gdcs"
    },
    "2409.09637": {
      "paper_id": "2409.09637v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09637v1",
      "paper_key": "2409.09637",
      "paper_title": "How Long Will the Quasar UV/Optical Flickering Be Damped? II. the Observational Test",
      "paper_url": "http://arxiv.org/abs/2409.09637v1",
      "paper_abstract": "The characteristic timescale at which the variability of active galactic nuclei (AGNs) turns from red noise to white noise can probe the accretion physics around supermassive black holes (SMBHs). A number of works have studied the characteristic timescale of quasars and obtained quite different scaling relations between the timescale and quasar physical properties. One possible reason for the discrepancies is that the characteristic timescale can be easily underestimated if the light curves are not long enough. In this work, we construct well-defined AGN samples to observationally test the relationships between the characteristic timescale and AGN properties obtained by previous works. Our samples eliminate the effects of insufficient light-curve lengths. We confirm that the timescale predictions \\citep{Zhou2024} of the Corona Heated Accretion disk Reprocessing model are consistent with our timescale measurements. The timescale predictions by empirically relations \\citep[e.g.,][]{Kelly2009} are systematically smaller than our measured ones. Our results provide further evidence that AGN variability is driven by thermal fluctuations in SMBH accretion disks. Future flagship time-domain surveys can critically test our conclusions and reveal the physical nature of AGN variability.",
      "paper_authors": [
        "Guowei Ren",
        "Shuying Zhou",
        "Mouyuan Sun",
        "Yongquan Xue"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": "20 pages, 7 figures, Accepted to ApJ",
      "repo_url": "#"
    },
    "2409.09601": {
      "paper_id": "2409.09601v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09601v1",
      "paper_key": "2409.09601",
      "paper_title": "A Survey of Foundation Models for Music Understanding",
      "paper_url": "http://arxiv.org/abs/2409.09601v1",
      "paper_abstract": "Music is essential in daily life, fulfilling emotional and entertainment needs, and connecting us personally, socially, and culturally. A better understanding of music can enhance our emotions, cognitive skills, and cultural connections. The rapid advancement of artificial intelligence (AI) has introduced new ways to analyze music, aiming to replicate human understanding of music and provide related services. While the traditional models focused on audio features and simple tasks, the recent development of large language models (LLMs) and foundation models (FMs), which excel in various fields by integrating semantic information and demonstrating strong reasoning abilities, could capture complex musical features and patterns, integrate music with language and incorporate rich musical, emotional and psychological knowledge. Therefore, they have the potential in handling complex music understanding tasks from a semantic perspective, producing outputs closer to human perception. This work, to our best knowledge, is one of the early reviews of the intersection of AI techniques and music understanding. We investigated, analyzed, and tested recent large-scale music foundation models in respect of their music comprehension abilities. We also discussed their limitations and proposed possible future directions, offering insights for researchers in this field.",
      "paper_authors": [
        "Wenjun Li",
        "Ying Cai",
        "Ziyang Wu",
        "Wenyi Zhang",
        "Yifan Chen",
        "Rundong Qi",
        "Mengqi Dong",
        "Peigen Chen",
        "Xiao Dong",
        "Fenghao Shi",
        "Lei Guo",
        "Junwei Han",
        "Bao Ge",
        "Tianming Liu",
        "Lin Gan",
        "Tuo Zhang"
      ],
      "primary_category": "cs.SD",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": "20 pages, 2 figures",
      "repo_url": "#"
    },
    "2409.09586": {
      "paper_id": "2409.09586v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09586v1",
      "paper_key": "2409.09586",
      "paper_title": "ValueCompass: A Framework of Fundamental Values for Human-AI Alignment",
      "paper_url": "http://arxiv.org/abs/2409.09586v1",
      "paper_abstract": "As AI systems become more advanced, ensuring their alignment with a diverse range of individuals and societal values becomes increasingly critical. But how can we capture fundamental human values and assess the degree to which AI systems align with them? We introduce ValueCompass, a framework of fundamental values, grounded in psychological theory and a systematic review, to identify and evaluate human-AI alignment. We apply ValueCompass to measure the value alignment of humans and language models (LMs) across four real-world vignettes: collaborative writing, education, public sectors, and healthcare. Our findings uncover risky misalignment between humans and LMs, such as LMs agreeing with values like \"Choose Own Goals\", which are largely disagreed by humans. We also observe values vary across vignettes, underscoring the necessity for context-aware AI alignment strategies. This work provides insights into the design space of human-AI alignment, offering foundations for developing AI that responsibly reflects societal values and ethics.",
      "paper_authors": [
        "Hua Shen",
        "Tiffany Knearem",
        "Reshmi Ghosh",
        "Yu-Ju Yang",
        "Tanushree Mitra",
        "Yun Huang"
      ],
      "primary_category": "cs.HC",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": null,
      "repo_url": "#"
    },
    "2409.09581": {
      "paper_id": "2409.09581v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09581v1",
      "paper_key": "2409.09581",
      "paper_title": "Possible anti-correlations between pulsation amplitudes and the disk growth of Be stars in giant-outbursting Be X-ray binaries",
      "paper_url": "http://arxiv.org/abs/2409.09581v1",
      "paper_abstract": "The mechanism of X-ray outbursts in Be X-ray binaries remains a mystery, and understanding their circumstellar disks is crucial for a solution of the mass-transfer problem. In particular, it is important to identify the Be star activities (e.g., pulsations) that cause mass ejection and, hence, disk formation. Therefore, we investigated the relationship between optical flux oscillations and the infrared (IR) excess in a sample of five Be X-ray binaries. Applying the Lomb-Scargle technique to high-cadence optical light curves from the Transiting Exoplanet Survey Satellite (TESS), we detected several significant oscillation modes in the 3 to 24 hour period range for each source. We also measured the IR excess (a proxy for disk growth) of those five sources, using J-band light curves from Palomar Gattini-IR. In four of the five sources, we found anti-correlations between the IR excess and the amplitude of the main flux oscillation modes. This result is inconsistent with the conventional idea that non-radial pulsations drive mass ejections. We propose an alternative scenario where internal temperature variations in the Be star cause transitions between pulsation-active and mass-ejection-active states.",
      "paper_authors": [
        "Masafumi Niwano",
        "Michael M. Fausnaugh",
        "Ryan M. Lau",
        "Kishalay De",
        "Roberto Soria",
        "George R. Ricker",
        "Roland Vanderspek",
        "Michael C. B. Ashley",
        "Nicholas Earley",
        "Matthew J. Hankins",
        "Mansi M. Kasliwal",
        "Anna M. Moore",
        "Jamie Soon",
        "Tony Travouillon",
        "Mahito Sasada",
        "Ichiro Takahashi",
        "Yoichi Yatsu",
        "Nobuyuki Kawai"
      ],
      "primary_category": "astro-ph.SR",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": "17 pages, 27 figures, 6 tables, accepted for publication in MNRAS",
      "repo_url": "#"
    },
    "2409.09574": {
      "paper_id": "2409.09574v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09574v1",
      "paper_key": "2409.09574",
      "paper_title": "Finding Passive Galaxies in HI-MaNGA: The Impact of Star-Formation Rate Indicator",
      "paper_url": "http://arxiv.org/abs/2409.09574v1",
      "paper_abstract": "HI-rich galaxies typically have high star-formation rates (SFR), but there exist interesting HI-rich and low star-forming (low-SF) galaxies. Previous work on a sample of these galaxies identified from HI-MaNGA (HI follow-up to the MaNGA survey) using an infrared indicator of specific-SFR (sSFR; namely W2-W3~<2) could find no single physical process to explain their unusual behaviour. The method by which galaxies are identified as low sSFR may be important in this conclusion. In this Research Note, we explore how an HI-rich, low sSFR sample of HI-MaNGA galaxies differs using H alpha, single stellar population, and ultraviolet estimators of SFR. We find that samples are statistically similar to each other so long as W2-W3~<2 is interpreted as corresponding to sSFR<10^{-11.15} yr^{-1}.",
      "paper_authors": [
        "Nora Salem",
        "Karen Masters",
        "David Stark",
        "Anubhav Sharma"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": null,
      "repo_url": "#"
    },
    "2409.09558": {
      "paper_id": "2409.09558v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09558v1",
      "paper_key": "2409.09558",
      "paper_title": "A Statistical Viewpoint on Differential Privacy: Hypothesis Testing, Representation and Blackwell's Theorem",
      "paper_url": "http://arxiv.org/abs/2409.09558v1",
      "paper_abstract": "Differential privacy is widely considered the formal privacy for privacy-preserving data analysis due to its robust and rigorous guarantees, with increasingly broad adoption in public services, academia, and industry. Despite originating in the cryptographic context, in this review paper we argue that, fundamentally, differential privacy can be considered a \\textit{pure} statistical concept. By leveraging a theorem due to David Blackwell, our focus is to demonstrate that the definition of differential privacy can be formally motivated from a hypothesis testing perspective, thereby showing that hypothesis testing is not merely convenient but also the right language for reasoning about differential privacy. This insight leads to the definition of $f$-differential privacy, which extends other differential privacy definitions through a representation theorem. We review techniques that render $f$-differential privacy a unified framework for analyzing privacy bounds in data analysis and machine learning. Applications of this differential privacy definition to private deep learning, private convex optimization, shuffled mechanisms, and U.S.~Census data are discussed to highlight the benefits of analyzing privacy bounds under this framework compared to existing alternatives.",
      "paper_authors": [
        "Weijie J. Su"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-09-14",
      "update_time": "2024-09-14",
      "comments": "To appear in Annual Review of Statistics and Its Application",
      "repo_url": "#"
    },
    "2409.09557": {
      "paper_id": "2409.09557v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09557v1",
      "paper_key": "2409.09557",
      "paper_title": "Adaptable, shape-conforming robotic endoscope",
      "paper_url": "http://arxiv.org/abs/2409.09557v1",
      "paper_abstract": "This paper introduces a size-adaptable robotic endoscope design, which aims to improve the efficiency and comfort of colonoscopy. The robotic endoscope proposed in this paper combines the expansion mechanism and the external drive system, which can adjust the shape according to the different pipe diameters, thus improving the stability and propulsion force during propulsion. As an actuator in the expansion mechanism, flexible bellows can provide a normal force of 3.89 N and an axial deformation of nearly 10mm at the maximum pressure, with a 53% expansion rate in the size of expandable tip. In the test of the locomotion performance of the prototype, we obtained the relationship with the propelling of the prototype by changing the friction coefficient of the pipe and the motor angular velocity. In the experiment with artificial bowel tissues, the prototype can generate a propelling force of 2.83 N, and the maximum linear speed is 29.29 m/s in average, and could produce effective propulsion when it passes through different pipe sizes. The results show that the prototype can realize the ability of shape adaptation in order to obtain more propulsion. The relationship between propelling force and traction force, structural optimization and miniaturization still need further exploration.",
      "paper_authors": [
        "Jiayang Du",
        "Lin Cao",
        "Sanja Dogramazi"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-09-14",
      "update_time": "2024-09-14",
      "comments": "Title: Adaptable, shape-conforming robotic endoscope Authors: Jiayang\n  Du, Lin Cao, Sanja Dogramazi Comments: 15 pages with 10 figures Subj-class:\n  robotic colonoscope This manuscript has been submitted to other journals and\n  is currently under review. Another manuscript borrowed some of the results of\n  this manuscript, so it is necessary to cite the reference",
      "repo_url": "#"
    },
    "2409.09502": {
      "paper_id": "2409.09502v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09502v1",
      "paper_key": "2409.09502",
      "paper_title": "One missing piece in Vision and Language: A Survey on Comics Understanding",
      "paper_url": "http://arxiv.org/abs/2409.09502v1",
      "paper_abstract": "Vision-language models have recently evolved into versatile systems capable of high performance across a range of tasks, such as document understanding, visual question answering, and grounding, often in zero-shot settings. Comics Understanding, a complex and multifaceted field, stands to greatly benefit from these advances. Comics, as a medium, combine rich visual and textual narratives, challenging AI models with tasks that span image classification, object detection, instance segmentation, and deeper narrative comprehension through sequential panels. However, the unique structure of comics -- characterized by creative variations in style, reading order, and non-linear storytelling -- presents a set of challenges distinct from those in other visual-language domains. In this survey, we present a comprehensive review of Comics Understanding from both dataset and task perspectives. Our contributions are fivefold: (1) We analyze the structure of the comics medium, detailing its distinctive compositional elements; (2) We survey the widely used datasets and tasks in comics research, emphasizing their role in advancing the field; (3) We introduce the Layer of Comics Understanding (LoCU) framework, a novel taxonomy that redefines vision-language tasks within comics and lays the foundation for future work; (4) We provide a detailed review and categorization of existing methods following the LoCU framework; (5) Finally, we highlight current research challenges and propose directions for future exploration, particularly in the context of vision-language models applied to comics. This survey is the first to propose a task-oriented framework for comics intelligence and aims to guide future research by addressing critical gaps in data availability and task definition. A project associated with this survey is available at https://github.com/emanuelevivoli/awesome-comics-understanding.",
      "paper_authors": [
        "Emanuele Vivoli",
        "Andrey Barsky",
        "Mohamed Ali Souibgui",
        "Artemis LLabres",
        "Marco Bertini",
        "Dimosthenis Karatzas"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-14",
      "update_time": "2024-09-14",
      "comments": "under review. project website:\n  https://github.com/emanuelevivoli/awesome-comics-understanding",
      "repo_url": "https://github.com/emanuelevivoli/awesome-comics-understanding"
    },
    "2409.09461": {
      "paper_id": "2409.09461v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09461v1",
      "paper_key": "2409.09461",
      "paper_title": "TX-Gen: Multi-Objective Optimization for Sparse Counterfactual Explanations for Time-Series Classification",
      "paper_url": "http://arxiv.org/abs/2409.09461v1",
      "paper_abstract": "In time-series classification, understanding model decisions is crucial for their application in high-stakes domains such as healthcare and finance. Counterfactual explanations, which provide insights by presenting alternative inputs that change model predictions, offer a promising solution. However, existing methods for generating counterfactual explanations for time-series data often struggle with balancing key objectives like proximity, sparsity, and validity. In this paper, we introduce TX-Gen, a novel algorithm for generating counterfactual explanations based on the Non-dominated Sorting Genetic Algorithm II (NSGA-II). TX-Gen leverages evolutionary multi-objective optimization to find a diverse set of counterfactuals that are both sparse and valid, while maintaining minimal dissimilarity to the original time series. By incorporating a flexible reference-guided mechanism, our method improves the plausibility and interpretability of the counterfactuals without relying on predefined assumptions. Extensive experiments on benchmark datasets demonstrate that TX-Gen outperforms existing methods in generating high-quality counterfactuals, making time-series models more transparent and interpretable.",
      "paper_authors": [
        "Qi Huang",
        "Sofoklis Kitharidis",
        "Thomas B\u00e4ck",
        "Niki van Stein"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-14",
      "update_time": "2024-09-14",
      "comments": "Preprint, under review",
      "repo_url": "#"
    },
    "2409.09459": {
      "paper_id": "2409.09459v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09459v1",
      "paper_key": "2409.09459",
      "paper_title": "Innovative schemes for Correlation Plenoptic Imaging",
      "paper_url": "http://arxiv.org/abs/2409.09459v1",
      "paper_abstract": "CPI is a novel imaging modality capable of addressing the intrinsic limitations of conventional plenoptic imaging - namely, the resolution loss and the sacrificed change of perspective, - while guaranteeing the typical advantages of plenotpic imaging: 3D imaging, refocusing of acquired pictures, in post-processing, and depth of field extension. In this work, we review a recently developed CPI scheme, named correlation plenoptic imaging between arbitrary planes, and derive the algorithm for image refocusing.",
      "paper_authors": [
        "Gianlorenzo Massaro",
        "Francesco Di Lena",
        "Augusto Garuccio",
        "Francesco V. Pepe",
        "Milena D'Angelo"
      ],
      "primary_category": "physics.optics",
      "publish_time": "2024-09-14",
      "update_time": "2024-09-14",
      "comments": "7 pages, 2 figures",
      "repo_url": "#"
    },
    "2409.09454": {
      "paper_id": "2409.09454v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09454v1",
      "paper_key": "2409.09454",
      "paper_title": "Dressed atom revisited: Hamiltonian-independent treatment of the radiative cascade",
      "paper_url": "http://arxiv.org/abs/2409.09454v1",
      "paper_abstract": "The dressed atom approach provides a tool to investigate the dynamics of an atom-laser system by fully retaining the quantum nature of the coherent mode. In its standard derivation, the internal atom-laser evolution is described within the rotating-wave approximation, which determines a doublet structure of the spectrum and the peculiar fluorescence triplet in the steady state. However, the rotating wave approximation may fail to apply to atomic systems subject to femtosecond light pulses, light-matter systems in the strong-coupling regime or sustaining permanent dipole moments. This work aims to demonstrate how the general features of the steady-state radiative cascade are affected by the interaction of the dressed atom with propagating radiation modes. Rather than focusing on a specific model, we analyze how these features depend on the parameters characterizing the dressed eigenstates in arbitrary atom-laser dynamics, given that a set of general hypotheses is satisfied. Our findings clarify the general conditions in which a description of the radiative cascade in terms of transition between dressed states is self-consistent. We provide a guideline to determine the properties of photon emission for any atom-laser interaction model, which can be particularly relevant when the model should be tailored to enhance a specific line. We apply the general results to the case in which a permanent dipole moment is a source of low-energy emission, whose frequency is of the order of the Rabi coupling.",
      "paper_authors": [
        "Francesco V. Pepe",
        "Karolina S\u0142owik"
      ],
      "primary_category": "quant-ph",
      "publish_time": "2024-09-14",
      "update_time": "2024-09-14",
      "comments": "13 pages, 4 figures",
      "repo_url": "#"
    },
    "2409.09441": {
      "paper_id": "2409.09441v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09441v2",
      "paper_key": "2409.09441",
      "paper_title": "PIP-Loco: A Proprioceptive Infinite Horizon Planning Framework for Quadrupedal Robot Locomotion",
      "paper_url": "http://arxiv.org/abs/2409.09441v2",
      "paper_abstract": "A core strength of Model Predictive Control (MPC) for quadrupedal locomotion has been its ability to enforce constraints and provide interpretability of the sequence of commands over the horizon. However, despite being able to plan, MPC struggles to scale with task complexity, often failing to achieve robust behavior on rapidly changing surfaces. On the other hand, model-free Reinforcement Learning (RL) methods have outperformed MPC on multiple terrains, showing emergent motions but inherently lack any ability to handle constraints or perform planning. To address these limitations, we propose a framework that integrates proprioceptive planning with RL, allowing for agile and safe locomotion behaviors through the horizon. Inspired by MPC, we incorporate an internal model that includes a velocity estimator and a Dreamer module. During training, the framework learns an expert policy and an internal model that are co-dependent, facilitating exploration for improved locomotion behaviors. During deployment, the Dreamer module solves an infinite-horizon MPC problem, adapting actions and velocity commands to respect the constraints. We validate the robustness of our training framework through ablation studies on internal model components and demonstrate improved robustness to training noise. Finally, we evaluate our approach across multi-terrain scenarios in both simulation and hardware.",
      "paper_authors": [
        "Aditya Shirwatkar",
        "Naman Saxena",
        "Kishore Chandra",
        "Shishir Kolathaya"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-09-14",
      "update_time": "2024-09-17",
      "comments": "Preprint under review",
      "repo_url": "#"
    },
    "2409.09403": {
      "paper_id": "2409.09403v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09403v1",
      "paper_key": "2409.09403",
      "paper_title": "AI-Driven Virtual Teacher for Enhanced Educational Efficiency: Leveraging Large Pretrain Models for Autonomous Error Analysis and Correction",
      "paper_url": "http://arxiv.org/abs/2409.09403v1",
      "paper_abstract": "Students frequently make mistakes while solving mathematical problems, and traditional error correction methods are both time-consuming and labor-intensive. This paper introduces an innovative \\textbf{V}irtual \\textbf{A}I \\textbf{T}eacher system designed to autonomously analyze and correct student \\textbf{E}rrors (VATE). Leveraging advanced large language models (LLMs), the system uses student drafts as a primary source for error analysis, which enhances understanding of the student's learning process. It incorporates sophisticated prompt engineering and maintains an error pool to reduce computational overhead. The AI-driven system also features a real-time dialogue component for efficient student interaction. Our approach demonstrates significant advantages over traditional and machine learning-based error correction methods, including reduced educational costs, high scalability, and superior generalizability. The system has been deployed on the Squirrel AI learning platform for elementary mathematics education, where it achieves 78.3\\% accuracy in error analysis and shows a marked improvement in student learning efficiency. Satisfaction surveys indicate a strong positive reception, highlighting the system's potential to transform educational practices.",
      "paper_authors": [
        "Tianlong Xu",
        "Yi-Fan Zhang",
        "Zhendong Chu",
        "Shen Wang",
        "Qingsong Wen"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-14",
      "update_time": "2024-09-14",
      "comments": null,
      "repo_url": "#"
    },
    "2409.09386": {
      "paper_id": "2409.09386v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09386v1",
      "paper_key": "2409.09386",
      "paper_title": "AMBER -- Advanced SegFormer for Multi-Band Image Segmentation: an application to Hyperspectral Imaging",
      "paper_url": "http://arxiv.org/abs/2409.09386v1",
      "paper_abstract": "Deep learning has revolutionized the field of hyperspectral image (HSI) analysis, enabling the extraction of complex and hierarchical features. While convolutional neural networks (CNNs) have been the backbone of HSI classification, their limitations in capturing global contextual features have led to the exploration of Vision Transformers (ViTs). This paper introduces AMBER, an advanced SegFormer specifically designed for multi-band image segmentation. AMBER enhances the original SegFormer by incorporating three-dimensional convolutions to handle hyperspectral data. Our experiments, conducted on the Indian Pines, Pavia University, and PRISMA datasets, show that AMBER outperforms traditional CNN-based methods in terms of Overall Accuracy, Kappa coefficient, and Average Accuracy on the first two datasets, and achieves state-of-the-art performance on the PRISMA dataset.",
      "paper_authors": [
        "Andrea Dosi",
        "Massimo Brescia",
        "Stefano Cavuoti",
        "Mariarca D'Aniello",
        "Michele Delli Veneri",
        "Carlo Donadio",
        "Adriano Ettari",
        "Giuseppe Longo",
        "Alvi Rownok",
        "Luca Sannino",
        "Maria Zampella"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-14",
      "update_time": "2024-09-14",
      "comments": "submitted to Neural Computing & Applications (Springer). Currently\n  under review",
      "repo_url": "#"
    },
    "2409.09378": {
      "paper_id": "2409.09378v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09378v1",
      "paper_key": "2409.09378",
      "paper_title": "Prevailing Research Areas for Music AI in the Era of Foundation Models",
      "paper_url": "http://arxiv.org/abs/2409.09378v1",
      "paper_abstract": "In tandem with the recent advancements in foundation model research, there has been a surge of generative music AI applications within the past few years. As the idea of AI-generated or AI-augmented music becomes more mainstream, many researchers in the music AI community may be wondering what avenues of research are left. With regards to music generative models, we outline the current areas of research with significant room for exploration. Firstly, we pose the question of foundational representation of these generative models and investigate approaches towards explainability. Next, we discuss the current state of music datasets and their limitations. We then overview different generative models, forms of evaluating these models, and their computational constraints/limitations. Subsequently, we highlight applications of these generative models towards extensions to multiple modalities and integration with artists' workflow as well as music education systems. Finally, we survey the potential copyright implications of generative music and discuss strategies for protecting the rights of musicians. While it is not meant to be exhaustive, our survey calls to attention a variety of research directions enabled by music foundation models.",
      "paper_authors": [
        "Megan Wei",
        "Mateusz Modrzejewski",
        "Aswin Sivaraman",
        "Dorien Herremans"
      ],
      "primary_category": "cs.SD",
      "publish_time": "2024-09-14",
      "update_time": "2024-09-14",
      "comments": null,
      "repo_url": "#"
    },
    "2409.09377": {
      "paper_id": "2409.09377v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09377v1",
      "paper_key": "2409.09377",
      "paper_title": "Asymptotic analysis in problems with fractional processes",
      "paper_url": "http://arxiv.org/abs/2409.09377v1",
      "paper_abstract": "Some problems in the theory and applications of stochastic processes can be reduced to solving integral equations. Such equations, however, rarely have explicit solutions. Useful information can be obtained by means of their asymptotic analysis with respect to relevant parameters. This paper is a brief survey of some recent progress in the study of such equations related to processes with fractional covariance structure.",
      "paper_authors": [
        "P. Chigansky",
        "M. Kleptsyna"
      ],
      "primary_category": "math.PR",
      "publish_time": "2024-09-14",
      "update_time": "2024-09-14",
      "comments": null,
      "repo_url": "#"
    },
    "2409.09363": {
      "paper_id": "2409.09363v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09363v2",
      "paper_key": "2409.09363",
      "paper_title": "Security and Privacy Perspectives of People Living in Shared Home Environments",
      "paper_url": "http://arxiv.org/abs/2409.09363v2",
      "paper_abstract": "Security and privacy perspectives of people in a multi-user home are a growing area of research, with many researchers reflecting on the complicated power imbalance and challenging access control issues of the devices involved. However, these studies primarily focused on the multi-user scenarios in traditional family home settings, leaving other types of multi-user home environments, such as homes shared by co-habitants without a familial relationship, under-studied. This paper closes this research gap via quantitative and qualitative analysis of results from an online survey and content analysis of sampled online posts on Reddit. It explores the complex roles of shared home users, which depend on various factors unique to the shared home environment, e.g., who owns what home devices, how home devices are used by multiple users, and more complicated relationships between the landlord and people in the shared home and among co-habitants. Half (50.7%) of our survey participants thought that devices in a shared home are less secure than in a traditional family home. This perception was found statistically significantly associated with factors such as the fear of devices being tampered with in their absence and (lack of) trust in other co-habitants and their visitors. Our study revealed new user types and relationships in a multi-user environment such as ExternalPrimary-InternalPrimary while analysing the landlord and shared home resident relationship with regard to shared home device use. We propose a threat actor model for shared home environments, which has a focus on possible malicious behaviours of current and past co-habitants of a shared home, as a special type of insider threat in a home environment. We also recommend further research to understand the complex roles co-habitants can play in navigating and adapting to a shared home environment's security and privacy landscape.",
      "paper_authors": [
        "Nandita Pattnaik",
        "Shujun Li",
        "Jason R. C. Nurse"
      ],
      "primary_category": "cs.HC",
      "publish_time": "2024-09-14",
      "update_time": "2024-09-19",
      "comments": "Published in Proceedings of the ACM on Human-Computer Interaction\n  (PACMHCI), Volume 8, Issue CSCW2, Article Number 368, 39 pages, ACM, 2024,\n  accepted to CSCW 2024 (27th ACM Conference on Computer-Supported Cooperative\n  Work and Social Computing) for presentation, https://doi.org/10.1145/3686907",
      "repo_url": "#"
    },
    "2409.09356": {
      "paper_id": "2409.09356v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09356v1",
      "paper_key": "2409.09356",
      "paper_title": "Towards Robust Detection of Open Source Software Supply Chain Poisoning Attacks in Industry Environments",
      "paper_url": "http://arxiv.org/abs/2409.09356v1",
      "paper_abstract": "The exponential growth of open-source package ecosystems, particularly NPM and PyPI, has led to an alarming increase in software supply chain poisoning attacks. Existing static analysis methods struggle with high false positive rates and are easily thwarted by obfuscation and dynamic code execution techniques. While dynamic analysis approaches offer improvements, they often suffer from capturing non-package behaviors and employing simplistic testing strategies that fail to trigger sophisticated malicious behaviors. To address these challenges, we present OSCAR, a robust dynamic code poisoning detection pipeline for NPM and PyPI ecosystems. OSCAR fully executes packages in a sandbox environment, employs fuzz testing on exported functions and classes, and implements aspect-based behavior monitoring with tailored API hook points. We evaluate OSCAR against six existing tools using a comprehensive benchmark dataset of real-world malicious and benign packages. OSCAR achieves an F1 score of 0.95 in NPM and 0.91 in PyPI, confirming that OSCAR is as effective as the current state-of-the-art technologies. Furthermore, for benign packages exhibiting characteristics typical of malicious packages, OSCAR reduces the false positive rate by an average of 32.06% in NPM (from 34.63% to 2.57%) and 39.87% in PyPI (from 41.10% to 1.23%), compared to other tools, significantly reducing the workload of manual reviews in real-world deployments. In cooperation with Ant Group, a leading financial technology company, we have deployed OSCAR on its NPM and PyPI mirrors since January 2023, identifying 10,404 malicious NPM packages and 1,235 malicious PyPI packages over 18 months. This work not only bridges the gap between academic research and industrial application in code poisoning detection but also provides a robust and practical solution that has been thoroughly tested in a real-world industrial setting.",
      "paper_authors": [
        "Xinyi Zheng",
        "Chen Wei",
        "Shenao Wang",
        "Yanjie Zhao",
        "Peiming Gao",
        "Yuanchao Zhang",
        "Kailong Wang",
        "Haoyu Wang"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-09-14",
      "update_time": "2024-09-14",
      "comments": "To appear in the 39th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE'24 Industry Showcase), October 27-November 1, 2024,\n  Sacramento, CA, USA",
      "repo_url": "#"
    },
    "2409.09343": {
      "paper_id": "2409.09343v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09343v1",
      "paper_key": "2409.09343",
      "paper_title": "Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study",
      "paper_url": "http://arxiv.org/abs/2409.09343v1",
      "paper_abstract": "Generative AI (GenAI), exemplified by Large Language Models (LLMs) such as OpenAI's ChatGPT, is revolutionizing various fields. Central to this transformation is Data Center Networking (DCN), which not only provides the computational power necessary for GenAI training and inference but also delivers GenAI-driven services to users. This article examines an interplay between GenAI and DCNs, highlighting their symbiotic relationship and mutual advancements. We begin by reviewing current challenges within DCNs and discuss how GenAI contributes to enhancing DCN capabilities through innovations, such as data augmentation, process automation, and domain transfer. We then focus on analyzing the distinctive characteristics of GenAI workloads on DCNs, gaining insights that catalyze the evolution of DCNs to more effectively support GenAI and LLMs. Moreover, to illustrate the seamless integration of GenAI with DCNs, we present a case study on full-lifecycle DCN digital twins. In this study, we employ LLMs equipped with Retrieval Augmented Generation (RAG) to formulate optimization problems for DCNs and adopt Diffusion-Deep Reinforcement Learning (DRL) for optimizing the RAG knowledge placement strategy. This approach not only demonstrates the application of advanced GenAI methods within DCNs but also positions the digital twin as a pivotal GenAI service operating on DCNs. We anticipate that this article can promote further research into enhancing the virtuous interaction between GenAI and DCNs.",
      "paper_authors": [
        "Yinqiu Liu",
        "Hongyang Du",
        "Dusit Niyato",
        "Jiawen Kang",
        "Zehui Xiong",
        "Yonggang Wen",
        "Dong In Kim"
      ],
      "primary_category": "cs.NI",
      "publish_time": "2024-09-14",
      "update_time": "2024-09-14",
      "comments": "9 pages",
      "repo_url": "#"
    },
    "2409.09340": {
      "paper_id": "2409.09340v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09340v1",
      "paper_key": "2409.09340",
      "paper_title": "Egocentric Speaker Classification in Child-Adult Dyadic Interactions: From Sensing to Computational Modeling",
      "paper_url": "http://arxiv.org/abs/2409.09340v1",
      "paper_abstract": "Autism spectrum disorder (ASD) is a neurodevelopmental condition characterized by challenges in social communication, repetitive behavior, and sensory processing. One important research area in ASD is evaluating children's behavioral changes over time during treatment. The standard protocol with this objective is BOSCC, which involves dyadic interactions between a child and clinicians performing a pre-defined set of activities. A fundamental aspect of understanding children's behavior in these interactions is automatic speech understanding, particularly identifying who speaks and when. Conventional approaches in this area heavily rely on speech samples recorded from a spectator perspective, and there is limited research on egocentric speech modeling. In this study, we design an experiment to perform speech sampling in BOSCC interviews from an egocentric perspective using wearable sensors and explore pre-training Ego4D speech samples to enhance child-adult speaker classification in dyadic interactions. Our findings highlight the potential of egocentric speech collection and pre-training to improve speaker classification accuracy.",
      "paper_authors": [
        "Tiantian Feng",
        "Anfeng Xu",
        "Xuan Shi",
        "Somer Bishop",
        "Shrikanth Narayanan"
      ],
      "primary_category": "cs.SD",
      "publish_time": "2024-09-14",
      "update_time": "2024-09-14",
      "comments": "pre-print under review",
      "repo_url": "#"
    },
    "2409.09339": {
      "paper_id": "2409.09339v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09339v1",
      "paper_key": "2409.09339",
      "paper_title": "Quantum data encoding as a distinct abstraction layer in the design of quantum circuits",
      "paper_url": "http://arxiv.org/abs/2409.09339v1",
      "paper_abstract": "Complex quantum circuits are constituted by combinations of quantum subroutines. The computation is possible as long as the quantum data encoding is consistent throughout the circuit. Despite its fundamental importance, the formalization of quantum data encoding has never been addressed systematically so far. We formalize the concept of quantum data encoding, namely the format providing a representation of a data set through a quantum state, as a distinct abstract layer with respect to the associated data loading circuit. We survey existing encoding methods and their respective strategies for classical-to-quantum exact and approximate data loading, for the quantum-to-classical extraction of information from states, and for quantum-to-quantum encoding conversion. Next, we show how major quantum algorithms find a natural interpretation in terms of data loading. For instance, the Quantum Fourier Transform is described as a quantum encoding converter, while the Quantum Amplitude Estimation as an extraction routine. The new conceptual framework is exemplified by considering its application to quantum-based Monte Carlo simulations, thus showcasing the power of the proposed formalism for the description of complex quantum circuits. Indeed, the approach clarifies the structure of complex quantum circuits and enables their efficient design.",
      "paper_authors": [
        "Gabriele Agliardi",
        "Enrico Prati"
      ],
      "primary_category": "cs.ET",
      "publish_time": "2024-09-14",
      "update_time": "2024-09-14",
      "comments": null,
      "repo_url": "#"
    },
    "2409.09310": {
      "paper_id": "2409.09310v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09310v1",
      "paper_key": "2409.09310",
      "paper_title": "Exact Posterior Mean and Covariance for Generalized Linear Mixed Models",
      "paper_url": "http://arxiv.org/abs/2409.09310v1",
      "paper_abstract": "A novel method is proposed for the exact posterior mean and covariance of the random effects given the response in a generalized linear mixed model (GLMM) when the response does not follow normal. The research solves a long-standing problem in Bayesian statistics when an intractable integral appears in the posterior distribution. It is well-known that the posterior distribution of the random effects given the response in a GLMM when the response does not follow normal contains intractable integrals. Previous methods rely on Monte Carlo simulations for the posterior distributions. They do not provide the exact posterior mean and covariance of the random effects given the response. The special integral computation (SIC) method is proposed to overcome the difficulty. The SIC method does not use the posterior distribution in the computation. It devises an optimization problem to reach the task. An advantage is that the computation of the posterior distribution is unnecessary. The proposed SIC avoids the main difficulty in Bayesian analysis when intractable integrals appear in the posterior distribution.",
      "paper_authors": [
        "Tonglin Zhang"
      ],
      "primary_category": "stat.ME",
      "publish_time": "2024-09-14",
      "update_time": "2024-09-14",
      "comments": "Manuscript under review",
      "repo_url": "#"
    },
    "2409.09309": {
      "paper_id": "2409.09309v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09309v1",
      "paper_key": "2409.09309",
      "paper_title": "Real-Time Stochastic Terrain Mapping and Processing for Autonomous Safe Landing",
      "paper_url": "http://arxiv.org/abs/2409.09309v1",
      "paper_abstract": "Onboard terrain sensing and mapping for safe planetary landings often suffer from missed hazardous features, e.g., small rocks, due to the large observational range and the limited resolution of the obtained terrain data. To this end, this paper develops a novel real-time stochastic terrain mapping algorithm that accounts for topographic uncertainty between the sampled points, or the uncertainty due to the sparse 3D terrain measurements. We introduce a Gaussian digital elevation map that is efficiently constructed using the combination of Delauney triangulation and local Gaussian process regression. The geometric investigation of the lander-terrain interaction is exploited to efficiently evaluate the marginally conservative local slope and roughness while avoiding the costly computation of the local plane. The conservativeness is proved in the paper. The developed real-time uncertainty quantification pipeline enables stochastic landing safety evaluation under challenging operational conditions, such as a large observational range or limited sensor capability, which is a critical stepping stone for the development of predictive guidance algorithms for safe autonomous planetary landing. Detailed reviews on background and related works are also presented.",
      "paper_authors": [
        "Kento Tomita",
        "Koki Ho"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2024-09-14",
      "update_time": "2024-09-14",
      "comments": null,
      "repo_url": "#"
    },
    "2409.09304": {
      "paper_id": "2409.09304v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09304v1",
      "paper_key": "2409.09304",
      "paper_title": "Consistent Spectral Clustering in Hyperbolic Spaces",
      "paper_url": "http://arxiv.org/abs/2409.09304v1",
      "paper_abstract": "Clustering, as an unsupervised technique, plays a pivotal role in various data analysis applications. Among clustering algorithms, Spectral Clustering on Euclidean Spaces has been extensively studied. However, with the rapid evolution of data complexity, Euclidean Space is proving to be inefficient for representing and learning algorithms. Although Deep Neural Networks on hyperbolic spaces have gained recent traction, clustering algorithms or non-deep machine learning models on non-Euclidean Spaces remain underexplored. In this paper, we propose a spectral clustering algorithm on Hyperbolic Spaces to address this gap. Hyperbolic Spaces offer advantages in representing complex data structures like hierarchical and tree-like structures, which cannot be embedded efficiently in Euclidean Spaces. Our proposed algorithm replaces the Euclidean Similarity Matrix with an appropriate Hyperbolic Similarity Matrix, demonstrating improved efficiency compared to clustering in Euclidean Spaces. Our contributions include the development of the spectral clustering algorithm on Hyperbolic Spaces and the proof of its weak consistency. We show that our algorithm converges at least as fast as Spectral Clustering on Euclidean Spaces. To illustrate the efficacy of our approach, we present experimental results on the Wisconsin Breast Cancer Dataset, highlighting the superior performance of Hyperbolic Spectral Clustering over its Euclidean counterpart. This work opens up avenues for utilizing non-Euclidean Spaces in clustering algorithms, offering new perspectives for handling complex data structures and improving clustering efficiency.",
      "paper_authors": [
        "Sagar Ghosh",
        "Swagatam Das"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-14",
      "update_time": "2024-09-14",
      "comments": "Currently under review in IEEE T-PAMI",
      "repo_url": "#"
    },
    "2409.09297": {
      "paper_id": "2409.09297v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09297v1",
      "paper_key": "2409.09297",
      "paper_title": "Bounding the probability of causality under ordinal outcomes",
      "paper_url": "http://arxiv.org/abs/2409.09297v1",
      "paper_abstract": "The probability of causation (PC) is often used in liability assessments. In a legal context, for example, where a patient suffered the side effect after taking a medication and sued the pharmaceutical company as a result, the value of the PC can help assess the likelihood that the side effect was caused by the medication, in other words, how likely it is that the patient will win the case. Beyond the issue of legal disputes, the PC plays an equally large role when one wants to go about explaining causal relationships between events that have already occurred in other areas. This article begins by reviewing the definitions and bounds of the probability of causality for binary outcomes, then generalizes them to ordinal outcomes. It demonstrates that incorporating additional mediator variable information in a complete mediation analysis provides a more refined bound compared to the simpler scenario where only exposure and outcome variables are considered.",
      "paper_authors": [
        "Hanmei Sun",
        "Chengfeng Shi",
        "Qiang Zhao"
      ],
      "primary_category": "math.ST",
      "publish_time": "2024-09-14",
      "update_time": "2024-09-14",
      "comments": "17 pges, 3 figures",
      "repo_url": "#"
    },
    "2409.09277": {
      "paper_id": "2409.09277v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09277v1",
      "paper_key": "2409.09277",
      "paper_title": "Symmetry operations and Critical Behaviour in Classical to Quantum Stochastic Processes",
      "paper_url": "http://arxiv.org/abs/2409.09277v1",
      "paper_abstract": "Construction of quantum analogs starting from classical stochastic processes have been previously introduced. In this paper, we generate a large class of self contained quantum extensions by symmetry operations. We show that the relaxation processes for different quantum extensions are different and that is supported by the measure of coherence, the the probability of reaching the equilibrium, decay of the domain walls and purity. However, the coherence measure based on the L1-norm does not capture the speed of the relaxation process. We also show that the finite size scaling of coherence exists for both short and long times.",
      "paper_authors": [
        "Gustavo Montes",
        "Soham Biswas",
        "Thomas Gorin"
      ],
      "primary_category": "quant-ph",
      "publish_time": "2024-09-14",
      "update_time": "2024-09-14",
      "comments": "7 pages, 8 figures : This is the first draft and will be edited\n  further before peer review",
      "repo_url": "#"
    },
    "2409.09227": {
      "paper_id": "2409.09227v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09227v2",
      "paper_key": "2409.09227",
      "paper_title": "Exploring Dimensionality Reduction of SDSS Spectral Abundances",
      "paper_url": "http://arxiv.org/abs/2409.09227v2",
      "paper_abstract": "High-resolution stellar spectra offer valuable insights into atmospheric parameters and chemical compositions. However, their inherent complexity and high-dimensionality present challenges in fully utilizing the information they contain. In this study, we utilize data from the Apache Point Observatory Galactic Evolution Experiment (APOGEE) within the Sloan Digital Sky Survey IV (SDSS-IV) to explore latent representations of chemical abundances by applying five dimensionality reduction techniques: PCA, t-SNE, UMAP, Autoencoder, and VAE. Through this exploration, we evaluate the preservation of information and compare reconstructed outputs with the original 19 chemical abundance data. Our findings reveal a performance ranking of PCA < UMAP < t-SNE < VAE < Autoencoder, through comparing their explained variance under optimized MSE. The performance of non-linear (Autoencoder and VAE) algorithms has approximately 10\\% improvement compared to linear (PCA) algorithm. This difference can be referred to as the \"non-linearity gap.\" Future work should focus on incorporating measurement errors into extension VAEs, thereby enhancing the reliability and interpretability of chemical abundance exploration in astronomical spectra.",
      "paper_authors": [
        "Qianyu Fan"
      ],
      "primary_category": "astro-ph.IM",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-19",
      "comments": null,
      "repo_url": "#"
    },
    "2409.09204": {
      "paper_id": "2409.09204v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09204v1",
      "paper_key": "2409.09204",
      "paper_title": "A Systematic Review on Process Mining for Curricular Analysis",
      "paper_url": "http://arxiv.org/abs/2409.09204v1",
      "paper_abstract": "Educational Process Mining (EPM) is a data analysis technique that is used to improve educational processes. It is based on Process Mining (PM), which involves gathering records (logs) of events to discover process models and analyze the data from a process-centric perspective. One specific application of EPM is curriculum mining, which focuses on understanding the learning program students follow to achieve educational goals. This is important for institutional curriculum decision-making and quality improvement. Therefore, academic institutions can benefit from organizing the existing techniques, capabilities, and limitations. We conducted a systematic literature review to identify works on applying PM to curricular analysis and provide insights for further research. From the analysis of 22 primary studies, we found that results can be classified into five categories concerning the objectives they pursue: the discovery of educational trajectories, the identification of deviations in the observed behavior of students, the analysis of bottlenecks, the analysis of stopout and dropout problems, and the generation of recommendation. Moreover, we identified some open challenges and opportunities, such as standardizing for replicating studies to perform cross-university curricular analysis and strengthening the connection between PM and data mining for improving curricular analysis.",
      "paper_authors": [
        "Daniel Calegari",
        "Andrea Delgado"
      ],
      "primary_category": "cs.DB",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": null,
      "repo_url": "#"
    },
    "2409.09190": {
      "paper_id": "2409.09190v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09190v1",
      "paper_key": "2409.09190",
      "paper_title": "Learnings from curating a trustworthy, well-annotated, and useful dataset of disordered English speech",
      "paper_url": "http://arxiv.org/abs/2409.09190v1",
      "paper_abstract": "Project Euphonia, a Google initiative, is dedicated to improving automatic speech recognition (ASR) of disordered speech. A central objective of the project is to create a large, high-quality, and diverse speech corpus. This report describes the project's latest advancements in data collection and annotation methodologies, such as expanding speaker diversity in the database, adding human-reviewed transcript corrections and audio quality tags to 350K (of the 1.2M total) audio recordings, and amassing a comprehensive set of metadata (including more than 40 speech characteristic labels) for over 75\\% of the speakers in the database. We report on the impact of transcript corrections on our machine-learning (ML) research, inter-rater variability of assessments of disordered speech patterns, and our rationale for gathering speech metadata. We also consider the limitations of using automated off-the-shelf annotation methods for assessing disordered speech.",
      "paper_authors": [
        "Pan-Pan Jiang",
        "Jimmy Tobin",
        "Katrin Tomanek",
        "Robert L. MacDonald",
        "Katie Seaver",
        "Richard Cave",
        "Marilyn Ladewig",
        "Rus Heywood",
        "Jordan R. Green"
      ],
      "primary_category": "eess.AS",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "Interspeech 2024",
      "repo_url": "#"
    },
    "2409.09186": {
      "paper_id": "2409.09186v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09186v1",
      "paper_key": "2409.09186",
      "paper_title": "Quantitative Insights into Language Model Usage and Trust in Academia: An Empirical Study",
      "paper_url": "http://arxiv.org/abs/2409.09186v1",
      "paper_abstract": "Language models (LMs) are revolutionizing knowledge retrieval and processing in academia. However, concerns regarding their misuse and erroneous outputs, such as hallucinations and fabrications, are reasons for distrust in LMs within academic communities. Consequently, there is a pressing need to deepen the understanding of how actual practitioners use and trust these models. There is a notable gap in quantitative evidence regarding the extent of LM usage, user trust in their outputs, and issues to prioritize for real-world development. This study addresses these gaps by providing data and analysis of LM usage and trust. Specifically, our study surveyed 125 individuals at a private school and secured 88 data points after pre-processing. Through both quantitative analysis and qualitative evidence, we found a significant variation in trust levels, which are strongly related to usage time and frequency. Additionally, we discover through a polling process that fact-checking is the most critical issue limiting usage. These findings inform several actionable insights: distrust can be overcome by providing exposure to the models, policies should be developed that prioritize fact-checking, and user trust can be enhanced by increasing engagement. By addressing these critical gaps, this research not only adds to the understanding of user experiences and trust in LMs but also informs the development of more effective LMs.",
      "paper_authors": [
        "Minseok Jung",
        "Aurora Zhang",
        "Junho Lee",
        "Paul Pu Liang"
      ],
      "primary_category": "cs.CY",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": null,
      "repo_url": "#"
    },
    "2409.09176": {
      "paper_id": "2409.09176v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09176v1",
      "paper_key": "2409.09176",
      "paper_title": "Negative charge transfer energy in correlated compounds",
      "paper_url": "http://arxiv.org/abs/2409.09176v1",
      "paper_abstract": "In correlated compounds containing cations in high formal oxidation states (assigned by assuming that anions attain full valence shells), the energy of ligand to cation charge transfer can become small or even negative. This yields compounds with a high degree of covalence and can lead to a self-doping of holes into the ligand states of the valence band. Such compounds are of particular topical interest, as highly studied perovskite oxides containing trivalent nickel or tetravalent iron are negative charge transfer systems, as are nickel-containing lithium ion battery cathode materials. In this report, we review the topic of negative charge transfer energy, with an emphasis on plots and diagrams as analysis tools, in the spirit of the celebrated Tanabe-Sugano diagrams which are the focus of this Special Topics Issue.",
      "paper_authors": [
        "Robert J. Green",
        "George A. Sawatzky"
      ],
      "primary_category": "cond-mat.str-el",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "Review article for Special Topics issue on 70 Years of Tanabe-Sugano\n  Diagrams",
      "repo_url": "#"
    },
    "2409.09174": {
      "paper_id": "2409.09174v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09174v1",
      "paper_key": "2409.09174",
      "paper_title": "Incorporation of Verifier Functionality in the Software for Operations and Network Attack Results Review and the Autonomous Penetration Testing System",
      "paper_url": "http://arxiv.org/abs/2409.09174v1",
      "paper_abstract": "The software for operations and network attack results review (SONARR) and the autonomous penetration testing system (APTS) use facts and common properties in digital twin networks to represent real-world entities. However, in some cases fact values will change regularly, making it difficult for objects in SONARR and APTS to consistently and accurately represent their real-world counterparts. This paper proposes and evaluates the addition of verifiers, which check real-world conditions and update network facts, to SONARR. This inclusion allows SONARR to retrieve fact values from its executing environment and update its network, providing a consistent method of ensuring that the operations and, therefore, the results align with the real-world systems being assessed. Verifiers allow arbitrary scripts and dynamic arguments to be added to normal SONARR operations. This provides a layer of flexibility and consistency that results in more reliable output from the software.",
      "paper_authors": [
        "Jordan Milbrath",
        "Jeremy Straub"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "The U.S. federal sponsor has requested that we not include funding\n  acknowledgement for this publication",
      "repo_url": "#"
    },
    "2409.09116": {
      "paper_id": "2409.09116v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09116v1",
      "paper_key": "2409.09116",
      "paper_title": "Characterizing the Molecular Gas in Infrared Bright Galaxies with CARMA",
      "paper_url": "http://arxiv.org/abs/2409.09116v1",
      "paper_abstract": "We present the CO(1-0) maps of 28 infrared-bright galaxies from the Great Observatories All-Sky Luminous Infrared Galaxy Survey (GOALS) taken with the Combined Array for Research in Millimeter Astronomy (CARMA). We detect 100GHz continuum in 16 of 28 galaxies, which trace both active galactic nuclei (AGNs) and compact star-forming cores. The GOALS galaxies show a variety of molecular gas morphologies, though in the majority of cases, the average velocity fields show a gradient consistent with rotation. We fit the full continuum SEDs of each of the source using either MAGPHYS or SED3FIT (if there are signs of an AGN) to derive the total stellar mass, dust mass, and star formation rates of each object. We adopt a value determined from luminous and ultraluminous infrared galaxies (LIRGs and ULIRGs) of $\\alpha_{\\rm CO}=1.5^{+1.3}_{-0.8}~M_\\odot$ (K km s$^{-1}$ pc$^2)^{-1}$, which leads to more physical values for $f_{\\rm mol}$ and the gas-to-dust ratio. Mergers tend to have the highest gas-to-dust ratios. We assume the cospatiality of the molecular gas and star formation, and plot the sample on the Schmidt-Kennicutt relation, we find that they preferentially lie above the line set by normal star-forming galaxies. This hyper-efficiency is likely due to the increased turbulence in these systems, which decreases the freefall time compared to star-forming galaxies, leading to \"enhanced\" star formation efficiency. Line wings are present in a non-negligible subsample (11/28) of the CARMA GOALS sources and are likely due to outflows driven by AGNs or star formation, gas inflows, or additional decoupled gas components.",
      "paper_authors": [
        "Katherine Alatalo",
        "Andreea O. Petric",
        "Lauranne Lanz",
        "Kate Rowlands",
        "Vivian U",
        "Kirsten L. Larson",
        "Lee Armus",
        "Loreto Barcos-Mu\u00f1oz",
        "Aaron S. Evans",
        "Jin Koda",
        "Yuanze Luo",
        "Anne M. Medling",
        "Kristina E. Nyland",
        "Justin A. Otter",
        "Pallavi Patil",
        "Fernando Pe\u00f1aloza",
        "Diane Salim",
        "David B. Sanders",
        "Elizaveta Sazonova",
        "Maya Skarbinski",
        "Yiqing Song",
        "Ezequiel Treister",
        "C. Meg Urry"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "29 pages, 4 tables, 11 figures, Accepted by the Astrophysical Journal",
      "repo_url": "#"
    },
    "2409.09106": {
      "paper_id": "2409.09106v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.09106v1",
      "paper_key": "2409.09106",
      "paper_title": "Recent Trends in Modelling the Continuous Time Series using Deep Learning: A Survey",
      "paper_url": "http://arxiv.org/abs/2409.09106v1",
      "paper_abstract": "Continuous-time series is essential for different modern application areas, e.g. healthcare, automobile, energy, finance, Internet of things (IoT) and other related areas. Different application needs to process as well as analyse a massive amount of data in time series structure in order to determine the data-driven result, for example, financial trend prediction, potential probability of the occurrence of a particular event occurrence identification, patient health record processing and so many more. However, modeling real-time data using a continuous-time series is challenging since the dynamical systems behind the data could be a differential equation. Several research works have tried to solve the challenges of modelling the continuous-time series using different neural network models and approaches for data processing and learning. The existing deep learning models are not free from challenges and limitations due to diversity among different attributes, behaviour, duration of steps, energy, and data sampling rate. This paper has described the general problem domain of time series and reviewed the challenges of modelling the continuous time series. We have presented a comparative analysis of recent developments in deep learning models and their contribution to solving different difficulties of modelling the continuous time series. We have also identified the limitations of the existing neural network model and open issues. The main goal of this review is to understand the recent trend of neural network models used in a different real-world application with continuous-time data.",
      "paper_authors": [
        "Mansura Habiba",
        "Barak A. Pearlmutter",
        "Mehrdad Maleki"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": null,
      "repo_url": "#"
    },
    "2409.11347": {
      "paper_id": "2409.11347v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11347v1",
      "paper_key": "2409.11347",
      "paper_title": "Hertzsprung gap stars in nearby galaxies and the Quest for Luminous Red Novae Progenitors",
      "paper_url": "http://arxiv.org/abs/2409.11347v1",
      "paper_abstract": "After the main sequence phase, stars more massive than 2.5 M$_\\odot$ rapidly evolve through the Hertzsprung gap as yellow giants and supergiants (YSG), before settling into the red giant branch. Identifying YSG in nearby galaxies is crucial for pinpointing progenitors of luminous red novae (LRNe) - astrophysical transients attributed to stellar mergers. In the era of extensive transient surveys like the Vera Rubin Observatory's LSST, this approach offers a new way to predict and select common envelope transients. This study investigates potential progenitors and precursors of LRNe by analysing Hubble Space Telescope (HST) photometry of stellar populations in galaxies within 20 Mpc to identify YSG candidates. Additionally, we use ZTF and MeerLICHT/BlackGEM to identify possible precursors, preparing for future observations by the LSST. We compiled a sample of 369 galaxies with HST exposures in the F475W, F555W, F606W, and F814W filters. We identified YSG candidates using MESA stellar evolution tracks and statistical analysis of color-magnitude diagrams (CMDs). Our sample includes 246,573 YSG candidates with masses between 3 and 20 $M_\\odot$ and is affected by various contaminants, such as foreground stars and extinguished main-sequence stars. After excluding foreground stars using Gaia proper motions, contamination is estimated at 1.7\\% from foreground stars and 20\\% from extinction affecting main-sequence stars. Combining our YSG candidates with time-domain catalogs yielded several interesting candidates. Notably, we identified 12 LRN precursor candidates for which followup is encouraged. We highlight the importance of monitoring future transients that match YSG candidates to avoid missing potential LRNe and other rare transients. LSST will be a game changer in the search for LRN progenitors and precursors, discovering over 300,000 new YSG and 100 precursors within 20 Mpc.",
      "paper_authors": [
        "Hugo Tranin",
        "Nadejda Blagorodnova",
        "Viraj Karambelkar",
        "Paul J. Groot",
        "Steven Bloemen",
        "Paul M. Vreeswijk",
        "Dani\u00eblle Pieterse",
        "Jan van Roestel"
      ],
      "primary_category": "astro-ph.SR",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "Submitted to A\\&A. 17 pages, 21 figures",
      "repo_url": "#"
    },
    "2409.11313": {
      "paper_id": "2409.11313v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11313v1",
      "paper_key": "2409.11313",
      "paper_title": "An Empirical Study of Sensitive Information in Logs",
      "paper_url": "http://arxiv.org/abs/2409.11313v1",
      "paper_abstract": "Software logs, generated during the runtime of software systems, are essential for various development and analysis activities, such as anomaly detection and failure diagnosis. However, the presence of sensitive information in these logs poses significant privacy concerns, particularly regarding Personally Identifiable Information (PII) and quasi-identifiers that could lead to re-identification risks. While general data privacy has been extensively studied, the specific domain of privacy in software logs remains underexplored, with inconsistent definitions of sensitivity and a lack of standardized guidelines for anonymization. To mitigate this gap, this study offers a comprehensive analysis of privacy in software logs from multiple perspectives. We start by performing an analysis of 25 publicly available log datasets to identify potentially sensitive attributes. Based on the result of this step, we focus on three perspectives: privacy regulations, research literature, and industry practices. We first analyze key data privacy regulations, such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA), to understand the legal requirements concerning sensitive information in logs. Second, we conduct a systematic literature review to identify common privacy attributes and practices in log anonymization, revealing gaps in existing approaches. Finally, we survey 45 industry professionals to capture practical insights on log anonymization practices. Our findings shed light on various perspectives of log privacy and reveal industry challenges, such as technical and efficiency issues while highlighting the need for standardized guidelines. By combining insights from regulatory, academic, and industry perspectives, our study aims to provide a clearer framework for identifying and protecting sensitive information in software logs.",
      "paper_authors": [
        "Roozbeh Aghili",
        "Heng Li",
        "Foutse Khomh"
      ],
      "primary_category": "cs.SE",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": null,
      "repo_url": "#"
    },
    "2409.11302": {
      "paper_id": "2409.11302v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11302v1",
      "paper_key": "2409.11302",
      "paper_title": "Beyond LoRA: Exploring Efficient Fine-Tuning Techniques for Time Series Foundational Models",
      "paper_url": "http://arxiv.org/abs/2409.11302v1",
      "paper_abstract": "Time Series Foundation Models (TSFMs) have recently garnered attention for their ability to model complex, large-scale time series data across domains such as retail, finance, and transportation. However, their application to sensitive, domain-specific fields like healthcare remains challenging, primarily due to the difficulty of fine-tuning these models for specialized, out-of-domain tasks with scarce publicly available datasets. In this work, we explore the use of Parameter-Efficient Fine-Tuning (PEFT) techniques to address these limitations, focusing on healthcare applications, particularly ICU vitals forecasting for sepsis patients. We introduce and evaluate two selective (BitFit and LayerNorm Tuning) and two additive (VeRA and FourierFT) PEFT techniques on multiple configurations of the Chronos TSFM for forecasting vital signs of sepsis patients. Our comparative analysis demonstrates that some of these PEFT methods outperform LoRA in terms of parameter efficiency and domain adaptation, establishing state-of-the-art (SOTA) results in ICU vital forecasting tasks. Interestingly, FourierFT applied to the Chronos (Tiny) variant surpasses the SOTA model while fine-tuning only 2,400 parameters compared to the 700K parameters of the benchmark.",
      "paper_authors": [
        "Divij Gupta",
        "Anubhav Bhatti",
        "Surajsinh Parmar"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "7 pages. Under review",
      "repo_url": "#"
    },
    "2409.11291": {
      "paper_id": "2409.11291v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11291v1",
      "paper_key": "2409.11291",
      "paper_title": "Features and prospects for Kilonova remnant detection with current and future surveys",
      "paper_url": "http://arxiv.org/abs/2409.11291v1",
      "paper_abstract": "We study the observable spectral and temporal properties of kilonova remnants analytically, and showcase quantitative differences with respect to supernova remnants. We provide detection prospects of kilonova remnants in the context of ongoing radio surveys. We find that there is a good chance to expect 10s of such objects in future surveys with a flux threshold of $\\sim 0.1$ mJy. Kilonova remnants from a postulated population of long lived supramassive neutron star remnants of neutron star mergers are even more likely to be detected as they are extremely bright and peak earlier. For ongoing survey with threshold of $\\sim$ mJy, we expect to find 10-100s of such objects if they are a significant fraction of total kilonova population. Considering that there are no promising such kilonovae candidates in current surveys, we constrain the fraction of such extreme kilonova to be no more than 30 percent of the overall kilonovae rate, depending on the details of ejecta mass and external density distribution.",
      "paper_authors": [
        "Sandeep Kumar Acharya",
        "Paz Beniamini",
        "Kenta Hotokezaka"
      ],
      "primary_category": "astro-ph.HE",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "Comments welcome",
      "repo_url": "#"
    },
    "2409.11266": {
      "paper_id": "2409.11266v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11266v1",
      "paper_key": "2409.11266",
      "paper_title": "Colour-Based Disentangling of Mira Variables and Ultra-Cool Dwarfs",
      "paper_url": "http://arxiv.org/abs/2409.11266v1",
      "paper_abstract": "Despite having different astronomical characteristics, the studies of mira variables and ultra-cool dwarfs frequently show similar red colors, which could cause leading to photometric misclassification. This study uses photometric data from the WISE, 2MASS, and Pan-STARRS surveys to construct color-based selection criteria for red dwarfs, brown dwarfs, and Mira variables. On analyzing the color indices, we developed empirical rules that separate these objects with an overall classification accuracy of approximately 91%-92%. While the differentiation between red dwarfs and both Mira variables and brown dwarfs is effective, challenges remain in distinguishing Mira variables from brown dwarfs due to overlapping color indices. The robustness of our classification technique was validated by a bootstrap analysis, highlighting the significance of color indices in large photometric surveys for stellar classification.",
      "paper_authors": [
        "Aleksandra Avdeeva",
        "Kefeng Tan",
        "Santosh Joshi",
        "Dana Kovaleva",
        "Harinder P. Singh",
        "Ali Luo",
        "Oleg Malkov"
      ],
      "primary_category": "astro-ph.SR",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "Accepted to Peremennye Zvezdy (Variable Stars), comments are welcomed",
      "repo_url": "#"
    },
    "2409.11259": {
      "paper_id": "2409.11259v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11259v1",
      "paper_key": "2409.11259",
      "paper_title": "Modeling a frustrated Ising square lattice with the D-Wave Quantum Annealer",
      "paper_url": "http://arxiv.org/abs/2409.11259v1",
      "paper_abstract": "The Ising model with nearest-neighbor interactions on a two-dimensional (2D) square lattice is one of the simplest models for studying ferro-magnetic to para-magnetic transitions. Extensive results are available in the literature for this model, which has become a paradigm for the study of magnetic phase transitions in materials, both theoretically and numerically. After a brief review of the main results obtained with a classical computer, we show how to implement on the D- Wave quantum annealer a more complex Ising model with the addition of competing antiferromagnetic interactions between the diagonal next-to-nearest neighbors with two coupling constants J1 and J2. The dynamics of this system, owing to frustration, are richer than those of the simple Ising model and exhibit a third striped (or antiferromagnetic) phase in addition to the ferro- and para-magnetic phases. In this work, we observed all three phases on the D-Wave hardware, studied the behavior of the solution with different annealing parameters, such as the chain strength and annealing time, and showed how to identify the phase transition by varying the ratio between the ferromagnetic and antiferromagnetic couplings. The same system is studied on a classical computer, with the possibility of taking into account the temperature (fixed on D-Wave) as a free parameter and to explore the full phase diagram: some comparative conclusions with D-Wave are drawn.",
      "paper_authors": [
        "C. Marin",
        "A. Fontana",
        "V. Bellani",
        "F. Pederiva",
        "A. Quaranta",
        "F. Rossella",
        "A. Salamon",
        "G. Salina"
      ],
      "primary_category": "physics.comp-ph",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": null,
      "repo_url": "#"
    },
    "2409.11224": {
      "paper_id": "2409.11224v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11224v1",
      "paper_key": "2409.11224",
      "paper_title": "A Human-Centered Risk Evaluation of Biometric Systems Using Conjoint Analysis",
      "paper_url": "http://arxiv.org/abs/2409.11224v1",
      "paper_abstract": "Biometric recognition systems, known for their convenience, are widely adopted across various fields. However, their security faces risks depending on the authentication algorithm and deployment environment. Current risk assessment methods faces significant challenges in incorporating the crucial factor of attacker's motivation, leading to incomplete evaluations. This paper presents a novel human-centered risk evaluation framework using conjoint analysis to quantify the impact of risk factors, such as surveillance cameras, on attacker's motivation. Our framework calculates risk values incorporating the False Acceptance Rate (FAR) and attack probability, allowing comprehensive comparisons across use cases. A survey of 600 Japanese participants demonstrates our method's effectiveness, showing how security measures influence attacker's motivation. This approach helps decision-makers customize biometric systems to enhance security while maintaining usability.",
      "paper_authors": [
        "Tetsushi Ohki",
        "Narishige Abe",
        "Hidetsugu Uchida",
        "Shigefumi Yamada"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": null,
      "repo_url": "#"
    },
    "2409.11209": {
      "paper_id": "2409.11209v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11209v1",
      "paper_key": "2409.11209",
      "paper_title": "Evidence for gravitational self-lensing of the central supermassive black hole binary in the Seyfert galaxy NGC 1566",
      "paper_url": "http://arxiv.org/abs/2409.11209v1",
      "paper_abstract": "It is generally accepted that all massive galaxies host supermassive black holes (BHs) in their center and that mergers of two galaxies lead to the formation of BH binaries. The most interesting among them comprise the mergers in their final state, that is to say with parsec (3.2 light years) or sub-parsec orbital separations. It is possible to detect these systems with binary self-lensing. Here we report the potential detection of a central supermassive BH binary in the active galaxy (AGN) NGC1566 based on a microlensing outburst. The light curve of the outburst - based on observations with the All Sky Automated Survey for SuperNovae - lasted from the beginning of 2017 until the beginning of 2020. The steep symmetric light curve as well as its shape look very different with respect to normal random variations in AGN. However, the observations could be easily reproduced with a best-fit standard microlensing light curve. Based on the light curve, we derived a characteristic timescale of 155 days. During the outburst, the continuum as well as the broad line intensities varied; however, the narrow emission lines did not. This is an indication that the lensing object orbits the AGN nucleus between the broad line region (BLR) and the narrow line region (NLR), that is, at a distance on the order of 250 light days. The light curve can be reproduced by a lens with a BH mass of 5*10^{5} M_solar. This implies a mass ratio to the central AGN on the order of 1 to 10.",
      "paper_authors": [
        "Wolfram Kollatschny",
        "Doron Chelouche"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "7 pages, 4 figures, Astronomy & Astrophysics Letters in press",
      "repo_url": "#"
    },
    "2409.11200": {
      "paper_id": "2409.11200v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11200v1",
      "paper_key": "2409.11200",
      "paper_title": "LoRa Communication for Agriculture 4.0: Opportunities, Challenges, and Future Directions",
      "paper_url": "http://arxiv.org/abs/2409.11200v1",
      "paper_abstract": "The emerging field of smart agriculture leverages the Internet of Things (IoT) to revolutionize farming practices. This paper investigates the transformative potential of Long Range (LoRa) technology as a key enabler of long-range wireless communication for agricultural IoT systems. By reviewing existing literature, we identify a gap in research specifically focused on LoRa's prospects and challenges from a communication perspective in smart agriculture. We delve into the details of LoRa-based agricultural networks, covering network architecture design, Physical Layer (PHY) considerations tailored to the agricultural environment, and channel modeling techniques that account for soil characteristics. The paper further explores relaying and routing mechanisms that address the challenges of extending network coverage and optimizing data transmission in vast agricultural landscapes. Transitioning to practical aspects, we discuss sensor deployment strategies and energy management techniques, offering insights for real-world deployments. A comparative analysis of LoRa with other wireless communication technologies employed in agricultural IoT applications highlights its strengths and weaknesses in this context. Furthermore, the paper outlines several future research directions to leverage the potential of LoRa-based agriculture 4.0. These include advancements in channel modeling for diverse farming environments, novel relay routing algorithms, integrating emerging sensor technologies like hyper-spectral imaging and drone-based sensing, on-device Artificial Intelligence (AI) models, and sustainable solutions. This survey can guide researchers, technologists, and practitioners to understand, implement, and propel smart agriculture initiatives using LoRa technology.",
      "paper_authors": [
        "Lameya Aldhaheri",
        "Noor Alshehhi",
        "Irfana Ilyas Jameela Manzil",
        "Ruhul Amin Khalil",
        "Shumaila Javaid",
        "Nasir Saeed",
        "Mohamed-Slim Alouini"
      ],
      "primary_category": "cs.NI",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": null,
      "repo_url": "#"
    },
    "2409.11192": {
      "paper_id": "2409.11192v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11192v1",
      "paper_key": "2409.11192",
      "paper_title": "Towards Ethical Personal AI Applications: Practical Considerations for AI Assistants with Long-Term Memory",
      "paper_url": "http://arxiv.org/abs/2409.11192v1",
      "paper_abstract": "One application area of long-term memory (LTM) capabilities with increasing traction is personal AI companions and assistants. With the ability to retain and contextualize past interactions and adapt to user preferences, personal AI companions and assistants promise a profound shift in how we interact with AI and are on track to become indispensable in personal and professional settings. However, this advancement introduces new challenges and vulnerabilities that require careful consideration regarding the deployment and widespread use of these systems. The goal of this paper is to explore the broader implications of building and deploying personal AI applications with LTM capabilities using a holistic evaluation approach. This will be done in three ways: 1) reviewing the technological underpinnings of LTM in Large Language Models, 2) surveying current personal AI companions and assistants, and 3) analyzing critical considerations and implications of deploying and using these applications.",
      "paper_authors": [
        "Eunhae Lee"
      ],
      "primary_category": "cs.CY",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": null,
      "repo_url": "#"
    },
    "2409.11163": {
      "paper_id": "2409.11163v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11163v1",
      "paper_key": "2409.11163",
      "paper_title": "Fast radio bursts as a probe of gravity on cosmological scales",
      "paper_url": "http://arxiv.org/abs/2409.11163v1",
      "paper_abstract": "We explore the potential for improving constraints on gravity by leveraging correlations in the dispersion measure derived from Fast Radio Bursts (FRBs) in combination with cosmic shear. Specifically, we focus on Horndeski gravity, inferring the kinetic braiding and Planck mass run rate from a stage-4 cosmic shear mock survey alongside a survey comprising $10^4$ FRBs. For the inference pipeline, we utilise hi_class to predict the linear matter power spectrum in modified gravity scenarios, while non-linear corrections are modelled with HMcode, including feedback mechanisms. Our findings indicate that FRBs can disentangle degeneracies between baryonic feedback and cosmological parameters, as well as the mass of massive neutrinos. Since these parameters are also degenerate with modified gravity parameters, the inclusion of FRBs can enhance constraints on Horndeski parameters by up to $40$ percent, despite being a less significant measurement. Additionally, we apply our model to current FRB data and use the uncertainty in the $\\mathrm{DM}-z$ relation to impose limits on gravity. However, due to the limited sample size of current data, constraints are predominantly influenced by theoretical priors. Despite this, our study demonstrates that FRBs will significantly augment the limited set of cosmological probes available, playing a critical role in providing alternative tests of feedback, cosmology, and gravity. All codes used in this work are made publically available.",
      "paper_authors": [
        "Dennis Neumann",
        "Robert Reischke",
        "Steffen Hagstotz",
        "Hendrik Hildebrandt"
      ],
      "primary_category": "astro-ph.CO",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "14 pages, 10 figures",
      "repo_url": "#"
    },
    "2409.11112": {
      "paper_id": "2409.11112v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11112v1",
      "paper_key": "2409.11112",
      "paper_title": "Strategic Insights in Human and Large Language Model Tactics at Word Guessing Games",
      "paper_url": "http://arxiv.org/abs/2409.11112v1",
      "paper_abstract": "At the beginning of 2022, a simplistic word-guessing game took the world by storm and was further adapted to many languages beyond the original English version. In this paper, we examine the strategies of daily word-guessing game players that have evolved during a period of over two years. A survey gathered from 25% of frequent players reveals their strategies and motivations for continuing the daily journey. We also explore the capability of several popular open-access large language model systems and open-source models at comprehending and playing the game in two different languages. Results highlight the struggles of certain models to maintain correct guess length and generate repetitions, as well as hallucinations of non-existent words and inflections.",
      "paper_authors": [
        "Mat\u012bss Rikters",
        "Sanita Reinsone"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "Published in the 4th Wordplay: When Language Meets Games Workshop @\n  ACL 2024",
      "repo_url": "#"
    },
    "2409.11067": {
      "paper_id": "2409.11067v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11067v1",
      "paper_key": "2409.11067",
      "paper_title": "Measurements of polarization and spin correlation and observation of entanglement in top quark pairs using lepton+jets events from proton-proton collisions at $\\sqrt{s}$ = 13 TeV",
      "paper_url": "http://arxiv.org/abs/2409.11067v1",
      "paper_abstract": "Measurements of the polarization and spin correlation in top quark pairs ($\\mathrm{t\\bar{t}}$) are presented using events with a single electron or muon and jets in the final state. The measurements are based on proton-proton collision data from the LHC at $\\sqrt{s}$ = 13 TeV collected by the CMS experiment, corresponding to an integrated luminosity of 138 fb$^{-1}$. All coefficients of the polarization vectors and the spin correlation matrix are extracted simultaneously by performing a binned likelihood fit to the data. The measurement is performed inclusively and in bins of additional observables, such as the mass of the $\\mathrm{t\\bar{t}}$ system and the top quark scattering angle in the $\\mathrm{t\\bar{t}}$ rest frame. The measured polarization and spin correlation are in agreement with the standard model. From the measured spin correlation, conclusions on the $\\mathrm{t\\bar{t}}$ spin entanglement are drawn by applying the Peres-Horodecki criterion. The standard model predicts entangled spins for $\\mathrm{t\\bar{t}}$ states at the production threshold and at high masses of the $\\mathrm{t\\bar{t}}$ system. Entanglement is observed for the first time in events at high $\\mathrm{t\\bar{t}}$ mass, where a large fraction of the $\\mathrm{t\\bar{t}}$ decays are space-like separated, with an expected and observed significance of above 5 standard deviations.",
      "paper_authors": [
        "CMS Collaboration"
      ],
      "primary_category": "hep-ex",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "Submitted to Physical Review D. All figures and tables can be found\n  at\n  http://cms-results.web.cern.ch/cms-results/public-results/publications/TOP-23-007\n  (CMS Public Pages)",
      "repo_url": "#"
    },
    "2409.11052": {
      "paper_id": "2409.11052v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11052v1",
      "paper_key": "2409.11052",
      "paper_title": "A logical alarm for misaligned binary classifiers",
      "paper_url": "http://arxiv.org/abs/2409.11052v1",
      "paper_abstract": "If two agents disagree in their decisions, we may suspect they are not both correct. This intuition is formalized for evaluating agents that have carried out a binary classification task. Their agreements and disagreements on a joint test allow us to establish the only group evaluations logically consistent with their responses. This is done by establishing a set of axioms (algebraic relations) that must be universally obeyed by all evaluations of binary responders. A complete set of such axioms are possible for each ensemble of size N. The axioms for $N = 1, 2$ are used to construct a fully logical alarm - one that can prove that at least one ensemble member is malfunctioning using only unlabeled data. The similarities of this approach to formal software verification and its utility for recent agendas of safe guaranteed AI are discussed.",
      "paper_authors": [
        "Andr\u00e9s Corrada-Emmanuel",
        "Ilya Parker",
        "Ramesh Bharadwaj"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "17 pages, 7 figures, under review",
      "repo_url": "#"
    },
    "2409.11017": {
      "paper_id": "2409.11017v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11017v1",
      "paper_key": "2409.11017",
      "paper_title": "Stretchable Electrohydraulic Artificial Muscle for Full Motion Ranges in Musculoskeletal Antagonistic Joints",
      "paper_url": "http://arxiv.org/abs/2409.11017v1",
      "paper_abstract": "Artificial muscles play a crucial role in musculoskeletal robotics and prosthetics to approximate the force-generating functionality of biological muscle. However, current artificial muscle systems are typically limited to either contraction or extension, not both. This limitation hinders the development of fully functional artificial musculoskeletal systems. We address this challenge by introducing an artificial antagonistic muscle system capable of both contraction and extension. Our design integrates non-stretchable electrohydraulic soft actuators (HASELs) with electrostatic clutches within an antagonistic musculoskeletal framework. This configuration enables an antagonistic joint to achieve a full range of motion without displacement loss due to tendon slack. We implement a synchronization method to coordinate muscle and clutch units, ensuring smooth motion profiles and speeds. This approach facilitates seamless transitions between antagonistic muscles at operational frequencies of up to 3.2 Hz. While our prototype utilizes electrohydraulic actuators, this muscle-clutch concept is adaptable to other non-stretchable artificial muscles, such as McKibben actuators, expanding their capability for extension and full range of motion in antagonistic setups. Our design represents a significant advancement in the development of fundamental components for more functional and efficient artificial musculoskeletal systems, bringing their capabilities closer to those of their biological counterparts.",
      "paper_authors": [
        "Amirhossein Kazemipour",
        "Ronan Hinchet",
        "Robert K. Katzschmann"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "This paper has been submitted to the IEEE International Conference on\n  Robotics and Automation (ICRA) 2025 and is under review",
      "repo_url": "#"
    },
    "2409.11013": {
      "paper_id": "2409.11013v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11013v1",
      "paper_key": "2409.11013",
      "paper_title": "CO spectra of the ISM in the Host Galaxies of the Most Luminous WISE-Selected AGNs",
      "paper_url": "http://arxiv.org/abs/2409.11013v1",
      "paper_abstract": "We present observations of mid-J J=4-3 or J=5-4 carbon monoxide (CO) emission lines and continuum emission from a sample of ten of the most luminous log(L/L_solar)~14 Hot Dust-Obscured Galaxies (Hot DOGs) discovered by the Wide-field Infrared Survey Explorer (WISE) with redshifts up to 4.6. We uncover broad spectral lines (FWHM~400 km/s) in these objects, suggesting a turbulent molecular interstellar medium (ISM) may be ubiquitous in Hot DOGs. A halo of molecular gas, extending out to a radius of 5 kpc is observed in W2305-0039, likely supplied by 940 km/s molecular outflows. W0831+0140 is plausibly the host of a merger between at least two galaxies, consistent with observations made using ionized gas. These CO(4-3) observations contrast with previous CO(1-0) studies of the same sources: the CO(4-3) to CO(1-0) luminosity ratios exceed 300 in each source, suggesting that the lowest excited states of CO are underluminous. These findings show that the molecular gas in Hot DOGs is consistently turbulent, plausibly a consequence of AGN feedback, triggered by galactic mergers.",
      "paper_authors": [
        "Lee R. Martin",
        "Andrew W. Blain",
        "Tanio D\u00edaz-Santos",
        "Roberto J. Assef",
        "Chao-Wei Tsai",
        "Hyunsung D. Jun",
        "Peter R. M. Eisenhardt",
        "Jingwen Wu",
        "Andrey Vayner",
        "Rom\u00e1n Fern\u00e1ndez Aranda"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "19 pages (16 main text & 3 in Appendix), 9 figures, plus 3 in\n  Appendix. MNRAS in press",
      "repo_url": "#"
    },
    "2409.11008": {
      "paper_id": "2409.11008v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11008v1",
      "paper_key": "2409.11008",
      "paper_title": "Latent mixed-effect models for high-dimensional longitudinal data",
      "paper_url": "http://arxiv.org/abs/2409.11008v1",
      "paper_abstract": "Modelling longitudinal data is an important yet challenging task. These datasets can be high-dimensional, contain non-linear effects and time-varying covariates. Gaussian process (GP) prior-based variational autoencoders (VAEs) have emerged as a promising approach due to their ability to model time-series data. However, they are costly to train and struggle to fully exploit the rich covariates characteristic of longitudinal data, making them difficult for practitioners to use effectively. In this work, we leverage linear mixed models (LMMs) and amortized variational inference to provide conditional priors for VAEs, and propose LMM-VAE, a scalable, interpretable and identifiable model. We highlight theoretical connections between it and GP-based techniques, providing a unified framework for this class of methods. Our proposal performs competitively compared to existing approaches across simulated and real-world datasets.",
      "paper_authors": [
        "Priscilla Ong",
        "Manuel Hau\u00dfmann",
        "Otto L\u00f6nnroth",
        "Harri L\u00e4hdesm\u00e4ki"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "Under review",
      "repo_url": "#"
    },
    "2409.11001": {
      "paper_id": "2409.11001v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11001v1",
      "paper_key": "2409.11001",
      "paper_title": "Foundations on k-contact geometry",
      "paper_url": "http://arxiv.org/abs/2409.11001v1",
      "paper_abstract": "k-Contact geometry appeared as a generalisation of contact geometry to analyse field theories. This work provides a new insightful approach to k-contact geometry by devising a theory of k-contact forms and proving that the kernel of a k-contact form is locally equivalent to a distribution of corank k that is distributionally maximally non-integrable and admits k commuting Lie symmetries: a so-called k-contact distribution. Compact manifolds admitting a global k-contact form are analysed, we give necessary topological conditions for their existence, k-contact Lie groups are defined and studied, we extend the Weinstein conjecture for the existence of closed orbits of Reeb vector fields in compact manifolds to the k-contact setting after studying compact low-dimensional manifolds endowed with a global k-contact form, and we provide some physical applications of some of our results. Polarisations for k-contact distributions are introduced and it is shown that a polarised k-contact distribution is locally diffeomorphic to the Cartan distribution of the first-order jet bundle over a fibre bundle of order k, which is a globally defined polarised k-contact distribution. Then, we relate k-contact manifolds to presymplectic and k-symplectic manifolds on fibre bundles of larger dimension and define for the first time types of submanifolds in k-contact geometry. We also review the theory of Hamiltonian k-vector fields, studying Hamilton-De Donder-Weyl equations in general and in Lie groups, which are here studied in an unprecedented manner. A theory of k-contact Hamiltonian vector fields is developed, which describes the theory of characteristics for Lie symmetries for first-order partial differential equations in a k-contact Hamiltonian manner. Our new Hamiltonian k-contact techniques are illustrated by analysing Hamilton-Jacobi and Dirac equations.",
      "paper_authors": [
        "Javier de Lucas",
        "Xavier Rivas",
        "Tomasz Sobczak"
      ],
      "primary_category": "math.DG",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "59 pp",
      "repo_url": "#"
    },
    "2409.10999": {
      "paper_id": "2409.10999v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10999v1",
      "paper_key": "2409.10999",
      "paper_title": "Enhancing Low-Resource Language and Instruction Following Capabilities of Audio Language Models",
      "paper_url": "http://arxiv.org/abs/2409.10999v1",
      "paper_abstract": "Audio language models can understand audio inputs and perform a range of audio-related tasks based on instructions, such as speech recognition and audio captioning, where the instructions are usually textual prompts. Audio language models are mostly initialized from pre-trained audio encoders and large language models (LLMs). Although these pre-trained components were developed to support multiple languages, audio-language models are trained predominantly on English data, which may limit their usability to only English instructions or English speech inputs. First, this paper examines the performance of existing audio language models in an underserved language using Thai as an example. This paper demonstrates that, despite being built on multilingual backbones, audio language models do not exhibit cross-lingual emergent abilities to low-resource languages. Second, this paper studies data mixture for developing audio language models that are optimized for a target language as well as English. In addition. this paper integrates audio comprehension and speech instruction-following capabilities into a single unified model. Our experiments provide insights into data mixture for enhancing instruction-following capabilities in both a low-resource language and English. Our model, Typhoon-Audio, outperforms existing open-source audio language models by a considerable margin, and it is comparable to state-of-the-art Gemini-1.5-Pro in both English and Thai languages.",
      "paper_authors": [
        "Potsawee Manakul",
        "Guangzhi Sun",
        "Warit Sirichotedumrong",
        "Kasima Tharnpipitchai",
        "Kunat Pipatanakul"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "5 pages. Preprint under review",
      "repo_url": "#"
    },
    "2409.10993": {
      "paper_id": "2409.10993v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10993v1",
      "paper_key": "2409.10993",
      "paper_title": "Multi-modal Generative Models in Recommendation System",
      "paper_url": "http://arxiv.org/abs/2409.10993v1",
      "paper_abstract": "Many recommendation systems limit user inputs to text strings or behavior signals such as clicks and purchases, and system outputs to a list of products sorted by relevance. With the advent of generative AI, users have come to expect richer levels of interactions. In visual search, for example, a user may provide a picture of their desired product along with a natural language modification of the content of the picture (e.g., a dress like the one shown in the picture but in red color). Moreover, users may want to better understand the recommendations they receive by visualizing how the product fits their use case, e.g., with a representation of how a garment might look on them, or how a furniture item might look in their room. Such advanced levels of interaction require recommendation systems that are able to discover both shared and complementary information about the product across modalities, and visualize the product in a realistic and informative way. However, existing systems often treat multiple modalities independently: text search is usually done by comparing the user query to product titles and descriptions, while visual search is typically done by comparing an image provided by the customer to product images. We argue that future recommendation systems will benefit from a multi-modal understanding of the products that leverages the rich information retailers have about both customers and products to come up with the best recommendations. In this chapter we review recommendation systems that use multiple data modalities simultaneously.",
      "paper_authors": [
        "Arnau Ramisa",
        "Rene Vidal",
        "Yashar Deldjoo",
        "Zhankui He",
        "Julian McAuley",
        "Anton Korikov",
        "Scott Sanner",
        "Mahesh Sathiamoorthy",
        "Atoosa Kasrizadeh",
        "Silvia Milano",
        "Francesco Ricci"
      ],
      "primary_category": "cs.IR",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "32 pages 5 figures",
      "repo_url": "#"
    },
    "2409.10963": {
      "paper_id": "2409.10963v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10963v1",
      "paper_key": "2409.10963",
      "paper_title": "JWST PRIMER: A lack of outshining in four normal z =4-6 galaxies from the ALMA-CRISTAL Survey",
      "paper_url": "http://arxiv.org/abs/2409.10963v1",
      "paper_abstract": "We present a spatially resolved analysis of four star-forming galaxies at $z = 4.44-5.64$ using data from the JWST PRIMER and ALMA-CRISTAL surveys to probe the stellar and inter-stellar medium properties on the sub-kpc scale. In the $1-5\\,\\mu{\\rm m}$ JWST NIRCam imaging we find that the galaxies are composed of multiple clumps (between $2$ and $\\sim 8$) separated by $\\simeq 5\\,{\\rm kpc}$, with comparable morphologies and sizes in the rest-frame UV and optical. Using BAGPIPES to perform pixel-by-pixel SED fitting to the JWST data we show that the SFR ($\\simeq 25\\,{\\rm M}_{\\odot}/{\\rm yr}$) and stellar mass (${\\rm log}_{10}(M_{\\star}/{\\rm M}_{\\odot}) \\simeq 9.5$) derived from the resolved analysis are in close ($ \\lesssim 0.3\\,{\\rm dex}$) agreement with those obtained by fitting the integrated photometry. In contrast to studies of lower-mass sources, we thus find a reduced impact of outshining of the older (more massive) stellar populations in these normal $z \\simeq 5$ galaxies. Our JWST analysis recovers bluer rest-frame UV slopes ($\\beta \\simeq -2.1$) and younger ages ($\\simeq 100\\,{\\rm Myr}$) than archival values. We find that the dust continuum from ALMA-CRISTAL seen in two of these galaxies correlates, as expected, with regions of redder rest-frame UV slopes and the SED-derived $A_{\\rm V}$, as well as the peak in the stellar mass map. We compute the resolved IRX-$\\beta$ relation, showing that the IRX is consistent with the local starburst attenuation curve and further demonstrating the presence of an inhomogeneous dust distribution within the galaxies. A comparison of the CRISTAL sources to those from the FirstLight zoom-in simulation of galaxies with the same $M_{\\star}$ and SFR reveals similar age and colour gradients, suggesting that major mergers may be important in the formation of clumpy galaxies at this epoch.",
      "paper_authors": [
        "N. E. P. Lines",
        "R. A. A. Bowler",
        "N. J. Adams",
        "R. Fisher",
        "R. G. Varadaraj",
        "Y. Nakazato",
        "M. Aravena",
        "R. J. Assef",
        "J. E. Birkin",
        "D. Ceverino",
        "E. da Cunha",
        "F. Cullen",
        "I. De Looze",
        "C. T. Donnan",
        "J. S. Dunlop",
        "A. Ferrara",
        "N. A. Grogin",
        "R. Herrera-Camus",
        "R. Ikeda",
        "A. M. Koekemoer",
        "M. Killi",
        "J. Li",
        "D. J. McLeod",
        "R. J. McLure",
        "I. Mitsuhashi",
        "P. G. P\u00e9rez-Gonz\u00e1lez",
        "M. Relano",
        "M. Solimano",
        "J. S. Spilker",
        "V. Villanueva",
        "N. Yoshida"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "16 pages, 8 figures, 3 tables, plus 4 page appendix. Submitted to\n  MNRAS",
      "repo_url": "#"
    },
    "2409.10961": {
      "paper_id": "2409.10961v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10961v1",
      "paper_key": "2409.10961",
      "paper_title": "The ALMA-CRISTAL Survey: Spatially-resolved Star Formation Activity and Dust Content in 4 < z < 6 Star-forming Galaxies",
      "paper_url": "http://arxiv.org/abs/2409.10961v1",
      "paper_abstract": "Using a combination of HST, JWST, and ALMA data, we perform spatially resolved spectral energy distributions (SED) fitting of fourteen 4<z<6 UV-selected main-sequence galaxies targeted by the [CII] Resolved ISM in Star-forming Galaxies with ALMA (CRISTAL) Large Program. We consistently model the emission from stars and dust in ~0.5-1kpc spatial bins to obtain maps of their physical properties. We find no offsets between the stellar masses (M*) and star formation rates (SFRs) derived from their global emission and those from adding up the values in our spatial bins, suggesting there is no bias of outshining by young stars on the derived global properties. We show that ALMA observations are important to derive robust parameter maps because they reduce the uncertainties in Ldust (hence Av and SFR). Using these maps we explore the resolved star-forming main sequence for z~5 galaxies, finding that this relation persists in typical star-forming galaxies in the early Universe. We find less obscured star formation where the M* (and SFR) surface densities are highest, typically in the central regions, contrary to the global relation between these parameters. We speculate this could be caused by feedback driving gas and dust out of these regions. However, more observations of infrared luminosities with ALMA are needed to verify this. Finally, we test empirical SFR prescriptions based on the UV+IR and [CII] line luminosity, finding they work well at the scales probed (~kpc). Our work demonstrates the usefulness of joint HST, JWST, and ALMA resolved SED modeling analyses at high redshift.",
      "paper_authors": [
        "Juno Li",
        "Elisabete Da Cunha",
        "Jorge Gonz\u00e1lez-L\u00f3pez",
        "Manuel Aravena",
        "Ilse De Looze",
        "N. M. F\u00f6rster Schreiber",
        "Rodrigo Herrera-Camus",
        "Justin Spilker",
        "Ken-ichi Tadaki",
        "Loreto Barcos-Munoz",
        "Andrew J. Battisti",
        "Jack E. Birkin",
        "Rebecca A. A. Bowler",
        "Rebecca Davies",
        "Tanio D\u00edaz-Santos",
        "Andrea Ferrara",
        "Deanne B. Fisher",
        "Jacqueline Hodge",
        "Ryota Ikeda",
        "Meghana Killi",
        "Lilian Lee",
        "Daizhong Liu",
        "Dieter Lutz",
        "Ikki Mitsuhashi",
        "Thorsten Naab",
        "Ana Posses",
        "Monica Rela\u00f1o",
        "Manuel Solimano",
        "Hannah \u00dcbler",
        "Stefan Anthony van der Giessen",
        "Vicente Villanueva"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "30 pages, 16 figures; re-submitted to ApJ",
      "repo_url": "#"
    },
    "2409.10959": {
      "paper_id": "2409.10959v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10959v1",
      "paper_key": "2409.10959",
      "paper_title": "Leveraging Reviewer Experience in Code Review Comment Generation",
      "paper_url": "http://arxiv.org/abs/2409.10959v1",
      "paper_abstract": "Modern code review is a ubiquitous software quality assurance process aimed at identifying potential issues within newly written code. Despite its effectiveness, the process demands large amounts of effort from the human reviewers involved. To help alleviate this workload, researchers have trained deep learning models to imitate human reviewers in providing natural language code reviews. Formally, this task is known as code review comment generation. Prior work has demonstrated improvements in this task by leveraging machine learning techniques and neural models, such as transfer learning and the transformer architecture. However, the quality of the model generated reviews remain sub-optimal due to the quality of the open-source code review data used in model training. This is in part due to the data obtained from open-source projects where code reviews are conducted in a public forum, and reviewers possess varying levels of software development experience, potentially affecting the quality of their feedback. To accommodate for this variation, we propose a suite of experience-aware training methods that utilise the reviewers' past authoring and reviewing experiences as signals for review quality. Specifically, we propose experience-aware loss functions (ELF), which use the reviewers' authoring and reviewing ownership of a project as weights in the model's loss function. Through this method, experienced reviewers' code reviews yield larger influence over the model's behaviour. Compared to the SOTA model, ELF was able to generate higher quality reviews in terms of accuracy, informativeness, and comment types generated. The key contribution of this work is the demonstration of how traditional software engineering concepts such as reviewer experience can be integrated into the design of AI-based automated code review models.",
      "paper_authors": [
        "Hong Yi Lin",
        "Patanamon Thongtanunam",
        "Christoph Treude",
        "Michael W. Godfrey",
        "Chunhua Liu",
        "Wachiraphan Charoenwet"
      ],
      "primary_category": "cs.SE",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": null,
      "repo_url": "#"
    },
    "2409.10940": {
      "paper_id": "2409.10940v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10940v1",
      "paper_key": "2409.10940",
      "paper_title": "RoadRunner M&M -- Learning Multi-range Multi-resolution Traversability Maps for Autonomous Off-road Navigation",
      "paper_url": "http://arxiv.org/abs/2409.10940v1",
      "paper_abstract": "Autonomous robot navigation in off-road environments requires a comprehensive understanding of the terrain geometry and traversability. The degraded perceptual conditions and sparse geometric information at longer ranges make the problem challenging especially when driving at high speeds. Furthermore, the sensing-to-mapping latency and the look-ahead map range can limit the maximum speed of the vehicle. Building on top of the recent work RoadRunner, in this work, we address the challenge of long-range (100 m) traversability estimation. Our RoadRunner (M&M) is an end-to-end learning-based framework that directly predicts the traversability and elevation maps at multiple ranges (50 m, 100 m) and resolutions (0.2 m, 0.8 m) taking as input multiple images and a LiDAR voxel map. Our method is trained in a self-supervised manner by leveraging the dense supervision signal generated by fusing predictions from an existing traversability estimation stack (X-Racer) in hindsight and satellite Digital Elevation Maps. RoadRunner M&M achieves a significant improvement of up to 50% for elevation mapping and 30% for traversability estimation over RoadRunner, and is able to predict in 30% more regions compared to X-Racer while achieving real-time performance. Experiments on various out-of-distribution datasets also demonstrate that our data-driven approach starts to generalize to novel unstructured environments. We integrate our proposed framework in closed-loop with the path planner to demonstrate autonomous high-speed off-road robotic navigation in challenging real-world environments. Project Page: https://leggedrobotics.github.io/roadrunner_mm/",
      "paper_authors": [
        "Manthan Patel",
        "Jonas Frey",
        "Deegan Atha",
        "Patrick Spieler",
        "Marco Hutter",
        "Shehryar Khattak"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "Under review for IEEE RA-L",
      "repo_url": "#"
    },
    "2409.10920": {
      "paper_id": "2409.10920v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10920v1",
      "paper_key": "2409.10920",
      "paper_title": "A review of a work by Raymond: Sturmian Hamiltonians with a large coupling constant -- periodic approximations and gap labels",
      "paper_url": "http://arxiv.org/abs/2409.10920v1",
      "paper_abstract": "We present a review of the work L. Raymond from 1995. The review aims at making this work more accessible and offers adaptations of some statements and proofs. In addition, this review forms an applicable framework for the complete solution of the Dry Ten Martini Problem for Sturmian Hamiltonians as appears in the work arXiv:2402.16703 by R. Band, S. Beckus and R. Loewy. A Sturmian Hamiltonian is a one-dimensional Schr\\\"odinger operator whose potential is a Sturmian sequence multiplied by a coupling constant, $V\\in\\mathbb{R}$. The spectrum of such an operator is commonly approximated by the spectra of designated periodic operators. If $V>4$, then the spectral bands of the periodic operators exhibit a particular combinatorial structure. This structure provides a formula for the integrated density of states. Employing this, it is shown that if $V>4$, then all the gaps, as predicted by the gap labelling theorem, are there.",
      "paper_authors": [
        "Ram Band",
        "Siegfried Beckus",
        "Barak Biber",
        "Laurent Raymond",
        "Yannik Thomas"
      ],
      "primary_category": "math-ph",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "63 pages, 5 figures",
      "repo_url": "#"
    },
    "2409.10903": {
      "paper_id": "2409.10903v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10903v1",
      "paper_key": "2409.10903",
      "paper_title": "Efficient Computation of Whole-Body Control Utilizing Simplified Whole-Body Dynamics via Centroidal Dynamics",
      "paper_url": "http://arxiv.org/abs/2409.10903v1",
      "paper_abstract": "In this study, we present a novel method for enhancing the computational efficiency of whole-body control for humanoid robots, a challenge accentuated by their high degrees of freedom. The reduced-dimension rigid body dynamics of a floating base robot is constructed by segmenting its kinematic chain into constrained and unconstrained chains, simplifying the dynamics of the unconstrained chain through the centroidal dynamics. The proposed dynamics model is possible to be applied to whole-body control methods, allowing the problem to be divided into two parts for more efficient computation. The efficiency of the framework is demonstrated by comparative experiments in simulations. The calculation results demonstrate a significant reduction in processing time, highlighting an improvement over the times reported in current methodologies. Additionally, the results also shows the computational efficiency increases as the degrees of freedom of robot model increases.",
      "paper_authors": [
        "Junewhee Ahn",
        "Jaesug Jung",
        "Yisoo Lee",
        "Hokyun Lee",
        "Sami Haddadin",
        "Jaeheung Park"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "submitted to RA-L, under review",
      "repo_url": "#"
    },
    "2409.10895": {
      "paper_id": "2409.10895v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10895v1",
      "paper_key": "2409.10895",
      "paper_title": "An Exploration of Effects of Dark Mode on University Students: A Human Computer Interface Analysis",
      "paper_url": "http://arxiv.org/abs/2409.10895v1",
      "paper_abstract": "This research dives into exploring the dark mode effects on students of a university. Research is carried out implementing the dark mode in e-Learning sites and its impact on behavior of the users. Students are spending more time in front of the screen for their studies especially after the pandemic. The blue light from the screen during late hours affects circadian rhythm of the body which negatively impacts the health of humans including eye strain and headache. The difficulty that students faced during the time of interacting with various e-Learning sites especially during late hours was analyzed using different techniques of HCI like survey, interview, evaluation methods and principles of design. Dark mode is an option which creates a pseudo inverted adaptable interface by changing brighter elements of UI into a dim-lit friendly environment. It is said that using dark mode will lessen the amount of blue light emitted and benefit students who suffer from eye strain. Students' interactions with dark mode were investigated using a survey, and an e-learning site with a dark mode theme was created. Based on the students' comments, researchers looked into the effects of dark mode on HCI in e-learning sites. The findings indicate that students have a clear preference for dark mode: 79.7% of survey participants preferred dark mode on their phones, and 61.7% said they would be interested in seeing this feature added to e-learning websites.",
      "paper_authors": [
        "Awan Shrestha",
        "Sabil Shrestha",
        "Biplov Paneru",
        "Bishwash Paneru",
        "Sansrit Paudel",
        "Ashish Adhikari",
        "Sanjog Chhetri Sapkota"
      ],
      "primary_category": "cs.HC",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "none",
      "repo_url": "#"
    },
    "2409.10893": {
      "paper_id": "2409.10893v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10893v1",
      "paper_key": "2409.10893",
      "paper_title": "Enhancing Security Testing Software for Systems that Cannot be Subjected to the Risks of Penetration Testing Through the Incorporation of Multi-threading and and Other Capabilities",
      "paper_url": "http://arxiv.org/abs/2409.10893v1",
      "paper_abstract": "The development of a system vulnerability analysis tool (SVAT) for complex mission critical systems (CMCS) produced the software for operation and network attack results review (SONARR). This software builds upon the Blackboard Architecture and uses its a rule-fact logic to assess model networks to identify potential pathways that an attacker might take through them via the exploitation of vulnerabilities within the network. The SONARR objects and algorithm were developed previously; however, performance was insufficient for analyzing large networks. This paper describes and analyzes the performance of a multi-threaded SONARR algorithm and other enhancements which were developed to increase SONARR's performance and facilitate the analysis of large networks.",
      "paper_authors": [
        "Matthew Tassava",
        "Cameron Kolodjski",
        "Jordan Milbrath",
        "Jeremy Straub"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "The U.S. federal sponsor has requested that we not include funding\n  acknowledgement for this publication",
      "repo_url": "#"
    },
    "2409.10892": {
      "paper_id": "2409.10892v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10892v1",
      "paper_key": "2409.10892",
      "paper_title": "Technical Upgrades to and Enhancements of a System Vulnerability Analysis Tool Based on the Blackboard Architecture",
      "paper_url": "http://arxiv.org/abs/2409.10892v1",
      "paper_abstract": "A system vulnerability analysis technique (SVAT) for the analysis of complex mission critical systems (CMCS) that cannot be taken offline or subjected to the risks posed by traditional penetration testing was previously developed. This system uses path-based analysis of vulnerabilities to identify potential threats to system security. Generalization logic building on the Blackboard Architecture's rule-fact paradigm was implemented in this system, the software for operation and network attack results review (SONARR). This paper presents an overview of additional functionality that has been added to this tool and the experimentation that was conducted to analyze their efficacy and the performance benefits of the new in-memory processing capabilities of the SONARR algorithm. The results of the performance tests and their relation to networks' architecture are discussed. The paper concludes with a discussion of avenues of future work, including the implementation of multithreading, additional analysis metrics like confidentiality, integrity, and availability, and improved heuristic development.",
      "paper_authors": [
        "Matthew Tassava",
        "Cameron Kolodjski",
        "Jeremy Straub"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "The U.S. federal sponsor has requested that we not include funding\n  acknowledgement for this publication",
      "repo_url": "#"
    },
    "2409.10877": {
      "paper_id": "2409.10877v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10877v1",
      "paper_key": "2409.10877",
      "paper_title": "A modified recursive transfer matrix algorithm for radiation and scattering computation of multilayer spheres",
      "paper_url": "http://arxiv.org/abs/2409.10877v1",
      "paper_abstract": "We discusses the electromagnetic scattering and radiation problems of multilayered spheres, reviewing the historical expansion of the Lorentz-Mie theory and the numerical stability issues encountered in handling multilayered spheres. By combining recursive methods with the transfer matrix method, we propose a modified transfer matrix algorithm designed for the stable and efficient calculation of electromagnetic scattering coefficients of multilayered spheres. The new algorithm simplifies the recursive formulas by introducing Debye potentials and logarithmic derivatives, effectively avoiding numerical overflow issues associated with Bessel functions under large complex variables. Numerical test results demonstrate that this algorithm offers superior stability and applicability when dealing with complex cases such as thin shells and strongly absorbing media.",
      "paper_authors": [
        "Jianing Zhang"
      ],
      "primary_category": "physics.comp-ph",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": null,
      "repo_url": "#"
    },
    "2409.10843": {
      "paper_id": "2409.10843v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10843v1",
      "paper_key": "2409.10843",
      "paper_title": "An Observer-Based View of Euclidean Geometry",
      "paper_url": "http://arxiv.org/abs/2409.10843v1",
      "paper_abstract": "Influence network of events is a view of the universe based on events that may be related to one another via influence. The network of events form a partially-ordered set which, when quantified consistently via a technique called chain projection, results in the emergence of spacetime and the Minkowski metric as well as the Lorentz transformation through changing an observer from one frame to another. Interestingly, using this approach, the motion of a free electron as well as the Dirac equation can be described. Indeed, the same approach can be employed to show how a discrete version of some of the features of Euclidean geometry, including directions, dimensions, subspaces, Pythagorean theorem, and geometric shapes can emerge.   In this paper, after reviewing the essentials of the influence network formalism, we build on some of our previous works to further develop aspects of Euclidean geometry. Specifically, we present the emergence of geometric shapes, a discrete version of the Parallel postulate, the dot product, and the outer (wedge product) in 2+1 dimensions. Finally, we show that the scalar quantification of two concatenated orthogonal intervals exhibits features that are similar to those of the well-known concept of geometric product in geometric Clifford algebras.",
      "paper_authors": [
        "Newshaw Bahreyni",
        "Carlo Cafaro",
        "Leonardo Rossetti"
      ],
      "primary_category": "math-ph",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "37 pages, 13 figures",
      "repo_url": "#"
    },
    "2409.10814": {
      "paper_id": "2409.10814v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10814v1",
      "paper_key": "2409.10814",
      "paper_title": "Advanced Halide Scintillators: From the Bulk to Nano",
      "paper_url": "http://arxiv.org/abs/2409.10814v1",
      "paper_abstract": "Halide scintillators play a crucial role in the detection of ionizing radiation since the discovery of scintillation in NaI:Tl in 1948. The discovery of NaI:Tl motivated the research and development (R&D) of halide scintillators resulting in the development of CsI:Tl, CsI:Na, CaF$_2$:Eu, etc. Later, the R&D shifted toward oxide materials due to their high mechanical and chemical stability, good scintillation properties, and relative ease of bulk single crystal growth. However, the development in crystal growth technology allowed for the growth of high-quality single crystals of hygroscopic and mechanically fragile materials including SrI$_2$ and LaBr$_3$. Scintillators based on these materials exhibit excellent performance and push the limits of inorganic scintillators. These results motivated intense research of a large variety of halide-based scintillators. Moreover, materials based on lead halide perovskites found application in the fields of photovoltaics, solid-state lighting, and lasers. The first studies show also the significant potential of lead halide perovskites as ultrafast scintillators in the form of NCs. The purpose of this review is to summarize the R&D in the field of halide scintillators during the last decade and to highlight perspectives for future development.",
      "paper_authors": [
        "Vojtech Vanecek",
        "Katerina Decka",
        "Eva Mihokova",
        "Vaclav Cuba",
        "Robert Kral",
        "Martin Nikl"
      ],
      "primary_category": "physics.app-ph",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "22 pgaes,16 figures, 355 references",
      "repo_url": "#"
    },
    "2409.10805": {
      "paper_id": "2409.10805v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10805v1",
      "paper_key": "2409.10805",
      "paper_title": "Opportunities and challenges of mRNA technologies in development of Dengue Virus Vaccine",
      "paper_url": "http://arxiv.org/abs/2409.10805v1",
      "paper_abstract": "Dengue virus (DENV) is a mosquito-borne virus with a significant human health concern. With 390 million infections annually and 96 million showing clinical symptoms, severe dengue can lead to life-threatening conditions like dengue hemorrhagic fever (DHF) and dengue shock syndrome (DSS). The only FDA-approved vaccine, Dengvaxia, has limitations due to antibody-dependent enhancement (ADE), necessitating careful administration. The recent pre-approval of TAK-003 by WHO in 2024 highlights ongoing efforts to improve vaccine options. This review explores recent advancements in dengue vaccine development, emphasizing potential utility of mRNA-based vaccines. By examining current clinical trial data and innovations, we aim to identify promising strategies to address the limitations of existing vaccines and enhance global dengue prevention efforts.",
      "paper_authors": [
        "Xiaoyang Liu",
        "Daniel Salmon"
      ],
      "primary_category": "q-bio.OT",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "16 pages, 1 figure, 1 table",
      "repo_url": "#"
    },
    "2409.10776": {
      "paper_id": "2409.10776v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10776v1",
      "paper_key": "2409.10776",
      "paper_title": "Research evolution of metal organic frameworks: A scientometric approach with human-in-the-loop",
      "paper_url": "http://arxiv.org/abs/2409.10776v1",
      "paper_abstract": "This paper reports on a scientometric analysis bolstered by human in the loop, domain experts, to examine the field of metal organic frameworks (MOFs) research. Scientometric analyses reveal the intellectual landscape of a field. The study engaged MOF scientists in the design and review of our research workflow. MOF materials are an essential component in next generation renewable energy storage and biomedical technologies. The research approach demonstrates how engaging experts, via human in the loop processes, can help develop a comprehensive view of a field research trends, influential works, and specialized topics.",
      "paper_authors": [
        "Xintong Zhao",
        "Kyle Langlois",
        "Jacob Furst",
        "Yuan An",
        "Xiaohua Hu",
        "Diego Gomez Gualdron",
        "Fernando Uribe-Romo",
        "Jane Greenberg"
      ],
      "primary_category": "cs.DL",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": null,
      "repo_url": "#"
    },
    "2409.10764": {
      "paper_id": "2409.10764v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10764v1",
      "paper_key": "2409.10764",
      "paper_title": "Federated Learning for Smart Grid: A Survey on Applications and Potential Vulnerabilities",
      "paper_url": "http://arxiv.org/abs/2409.10764v1",
      "paper_abstract": "The Smart Grid (SG) is a critical energy infrastructure that collects real-time electricity usage data to forecast future energy demands using information and communication technologies (ICT). Due to growing concerns about data security and privacy in SGs, federated learning (FL) has emerged as a promising training framework. FL offers a balance between privacy, efficiency, and accuracy in SGs by enabling collaborative model training without sharing private data from IoT devices. In this survey, we thoroughly review recent advancements in designing FL-based SG systems across three stages: generation, transmission and distribution, and consumption. Additionally, we explore potential vulnerabilities that may arise when implementing FL in these stages. Finally, we discuss the gap between state-of-the-art FL research and its practical applications in SGs and propose future research directions. These focus on potential attack and defense strategies for FL-based SG systems and the need to build a robust FL-based SG infrastructure. Unlike traditional surveys that address security issues in centralized machine learning methods for SG systems, this survey specifically examines the applications and security concerns in FL-based SG systems for the first time. Our aim is to inspire further research into applications and improvements in the robustness of FL-based SG systems.",
      "paper_authors": [
        "Zikai Zhang",
        "Suman Rath",
        "Jiaohao Xu",
        "Tingsong Xiao"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": null,
      "repo_url": "#"
    },
    "2409.10754": {
      "paper_id": "2409.10754v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10754v1",
      "paper_key": "2409.10754",
      "paper_title": "Impact Of Emotions on Information Seeking And Sharing Behaviors During Pandemic",
      "paper_url": "http://arxiv.org/abs/2409.10754v1",
      "paper_abstract": "We propose a novel approach to assess the public's coping behavior during the COVID-19 outbreak by examining the emotions. Specifically, we explore (1) changes in the public's emotions with the COVID-19 crisis progression and (2) the impacts of the public's emotions on their information-seeking, information-sharing behaviors, and compliance with stay-at-home policies. We base the study on the appraisal tendency framework, detect the public's emotions by fine-tuning a pre-trained RoBERTa model, and cross-analyze third-party behavioral data. We demonstrate the feasibility and reliability of our proposed approach in providing a large-scale examination of the publi's emotions and coping behaviors in a real-world crisis: COVID-19. The approach complements prior crisis communication research, mainly based on self-reported, small-scale experiments and survey data. Our results show that anger and fear are more prominent than other emotions experienced by the public at the pandemic's outbreak stage. Results also show that the extent of low certainty and passive emotions (e.g., sadness, fear) was related to increased information-seeking and information-sharing behaviors. Additionally, high-certainty (e.g., anger) and low-certainty (e.g., sadness, fear) emotions during the outbreak correlated to the public's compliance with stay-at-home orders.",
      "paper_authors": [
        "Smitha Muthya Sudheendra",
        "Hao Xu",
        "Jisu Huh",
        "Jaideep Srivastava"
      ],
      "primary_category": "cs.SI",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "Presented at 10th International Conference on Computational Social\n  Science (IC2S2) 2024",
      "repo_url": "#"
    },
    "2409.10735": {
      "paper_id": "2409.10735v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10735v1",
      "paper_key": "2409.10735",
      "paper_title": "Notas sobre Teor\u00eda de colas y algunas aplicaciones",
      "paper_url": "http://arxiv.org/abs/2409.10735v1",
      "paper_abstract": "This paper presents a comprehensive review of stochastic processes, with a particular focus on Markov chains and jump processes. The main results related to queuing systems are analyzed. Additionally, conditions that ensure the stability, or ergodicity, of such systems are presented. The paper also discusses stability results for queuing networks and their extension to visiting systems. Finally, key contributions concerning the Probability Generating Function, an essential tool in the analysis of the aforementioned processes, are introduced. The review is conducted from the perspective of queuing theory, grounded in the Kendall-Lee notation, emphasizing stability results and the computation of performance measures based on the specific characteristics of each process.",
      "paper_authors": [
        "Carlos E. Mart\u00ednez-Rodr\u00edguez"
      ],
      "primary_category": "math.PR",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "in Spanish language",
      "repo_url": "#"
    },
    "2409.10701": {
      "paper_id": "2409.10701v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10701v1",
      "paper_key": "2409.10701",
      "paper_title": "A Novel Optimal Transport-Based Approach for Interpolating Spectral Time Series: Paving the Way for Photometric Classification of Supernovae",
      "paper_url": "http://arxiv.org/abs/2409.10701v1",
      "paper_abstract": "This paper introduces a novel method for creating spectral time series, which can be used for generating synthetic light curves for photometric classification but also for applications like K-corrections and bolometric corrections. This approach is particularly valuable in the era of large astronomical surveys, where it can significantly enhance the analysis and understanding of an increasing number of SNe, even in the absence of extensive spectroscopic data. methods: By employing interpolations based on optimal transport theory, starting from a spectroscopic sequence, we derive weighted average spectra with high cadence. The weights incorporate an uncertainty factor, for penalizing interpolations between spectra with significant epoch differences and with poor match between the synthetic and observed photometry. results: Our analysis reveals that even with phase difference of up to 40 days between pairs of spectra, optical transport can generate interpolated spectral time series that closely resemble the original ones. Synthetic photometry extracted from these spectral time series aligns well with observed photometry. The best results are achieved in the V band, with relative residuals less than 10% for 87% and 84% of the data for type Ia and II, respectively. For the B, g, R and r bands the relative residuals are between 65% and 87% within the previously mentioned 10% threshold for both classes. The worse results correspond to the i and I bands where, in the case, of SN~Ia the values drop to 53% and 42%, respectively. conclusions: We introduce a new method to construct spectral time series for individual SN starting from a sparse spectroscopic sequence, demonstrating its capability to produce reliable light curves that can be used for photometric classification.",
      "paper_authors": [
        "M. Ramirez",
        "G. Pignata",
        "Francisco F\u00f6rster",
        "Santiago Gonz\u00e1les-Gait\u00e1n",
        "Claudia P. Guti\u00e9rrez",
        "B. Ayala",
        "Guillermo Cabrera-Vives",
        "M\u00e1rcio Catelan",
        "A. M. Mu\u00f1oz Arancibia",
        "J. Pineda-Garc\u00eda"
      ],
      "primary_category": "astro-ph.HE",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "Accepted for publication in A&A",
      "repo_url": "#"
    },
    "2409.10690": {
      "paper_id": "2409.10690v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10690v1",
      "paper_key": "2409.10690",
      "paper_title": "Centralization potential of automotive E/E architectures",
      "paper_url": "http://arxiv.org/abs/2409.10690v1",
      "paper_abstract": "Current automotive E/E architectures are subject to significant transformations: Computing-power-intensive advanced driver-assistance systems, bandwidth-hungry infotainment systems, the connection of the vehicle with the internet and the consequential need for cyber-security drives the centralization of E/E architectures. A centralized architecture is often seen as a key enabler to master those challenges. Available research focuses mostly on the different types of E/E architectures and contrasts their advantages and disadvantages. There is a research gap on guidelines for system designers and function developers to analyze the potential of their systems for centralization. The present paper aims to quantify centralization potential reviewing relevant literature and conducting qualitative interviews with industry practitioners. In literature, we identified seven key automotive system properties reaching limitations in current automotive architectures: busload, functional safety, computing power, feature dependencies, development and maintenance costs, error rate, modularity and flexibility. These properties serve as quantitative evaluation criteria to estimate whether centralization would enhance overall system performance. In the interviews, we have validated centralization and its fundament - the conceptual systems engineering - as capabilities to mitigate these limitations. By focusing on practical insights and lessons learned, this research provides system designers with actionable guidance to optimize their systems, addressing the outlined challenges while avoiding monolithic architecture. This paper bridges the gap between theoretical research and practical application, offering valuable takeaways for practitioners.",
      "paper_authors": [
        "Lucas Mauser",
        "Stefan Wagner"
      ],
      "primary_category": "cs.SE",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "In Practice track paper submitted as preprint to 'The Journal of\n  Systems & Software'. 1 page for highlights, 28 pages for content and 3 pages\n  for references. 5 figures, 10 tables",
      "repo_url": "#"
    },
    "2409.10679": {
      "paper_id": "2409.10679v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10679v1",
      "paper_key": "2409.10679",
      "paper_title": "SPORES-HWO. II. Limits on Planetary Companions of Future High-contrast Imaging Targets from $>$20 Years of HIRES and HARPS Radial Velocities",
      "paper_url": "http://arxiv.org/abs/2409.10679v1",
      "paper_abstract": "Future large, space-based observatories with starlight suppression technology, e.g., the Habitable Worlds Observatory (HWO), will directly image and characterize nearby Earth-like exoplanets. Prior limits on planet masses and system architectures from radial velocity (RV) measurements of potential exo-Earth hosts are critical to the success of HWO's science goals. Here, we present a uniform analysis of archival RVs from HIRES/Keck and HARPS/ESO of the most promising targets for the HWO exo-Earth survey. We analyze RVs and stellar activity indicators of 90 stars in the NASA ExEP Mission Star List and SPORES-HWO Catalog, finding 33 Keplerian signals associated with known planets and 12 signals associated with stellar activity. We also identify 5 new RV signals that we classify as either planet candidates or sources requiring confirmation, noting that the RV observations are biased toward cooler and less active stars. Assessing the sensitivity of the HIRES and HARPS data, we calculate RV limits ranging from $K_{\\rm RV} = 0.6 \\,{\\rm m\\,s}^{-1}$ (HD 10700) to $371 \\,{\\rm m\\,s}^{-1}$ (HD 17925) in the middle of the conservative habitable zone (HZ), corresponding to projected planet masses of $5.4 \\,{\\rm M_\\oplus}$ and $10.6 \\,{\\rm M_{Jup}}$ for those stars. The median HZ sensitivity limit of our sample is $M_{\\rm p} \\sin i \\simeq 66 \\,{\\rm M_\\oplus}$. This work demonstrates the need for future extreme precision radial velocity (EPRV) monitoring of high-priority targets for the next generation of DI missions that will search for habitable extrasolar systems. We advocate for the use of these results in developing future EPRV strategies.",
      "paper_authors": [
        "Caleb K. Harada",
        "Courtney D. Dressing",
        "Stephen R. Kane",
        "Sarah Blunt",
        "Jamie Dietrich",
        "Natalie R. Hinkel",
        "Zhexing Li",
        "Eric Mamajek",
        "Malena Rice",
        "Noah W. Tuchow",
        "Emma V. Turtelboom",
        "Robert A. Wittenmyer"
      ],
      "primary_category": "astro-ph.EP",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "55 pages, 12 figures, 9 tables, submitted to AAS Journals",
      "repo_url": "#"
    },
    "2409.10645": {
      "paper_id": "2409.10645v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10645v1",
      "paper_key": "2409.10645",
      "paper_title": "Excess entropy of strongly coupled Yukawa fluids",
      "paper_url": "http://arxiv.org/abs/2409.10645v1",
      "paper_abstract": "The entropy of strongly coupled Yukawa fluids is discussed from several perspectives. First, it is demonstrated that a vibrational paradigm of atomic dynamics in dense fluids can be used to obtain a simple and accurate estimate of the entropy without any adjustable parameters. Second, it is explained why a quasiuniversal value of the excess entropy of simple fluids at the freezing point should be expected, and it is demonstrated that a remaining very weak dependence of the freezing point entropy on the screening parameter in the Yukawa fluid can be described by a simple linear function. Third, a scaling of the excess entropy with the freezing temperature is examined, a modified form of the Rosenfeld-Tarazona scaling is put forward, and some consequences are briefly discussed. Fourth, the location of the Frenkel line on the phase diagram of Yukawa systems is discussed in terms of the excess entropy and compared with some predictions made in the literature. Fifth, the excess entropy scaling of the transport coefficients (self-diffusion, viscosity, and thermal conductivity) is reexamined using the contemporary datasets for the transport properties of Yukawa fluids. The results could be of particular interest in the context of complex (dusty) plasmas, colloidal suspensions, electrolytes, and other related systems with soft pairwise interactions.",
      "paper_authors": [
        "Sergey Khrapak"
      ],
      "primary_category": "cond-mat.soft",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "11 pages, 6 figures",
      "repo_url": "#"
    },
    "2409.10638": {
      "paper_id": "2409.10638v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10638v1",
      "paper_key": "2409.10638",
      "paper_title": "The Sun's Birth Environment: Context for Meteoritics",
      "paper_url": "http://arxiv.org/abs/2409.10638v1",
      "paper_abstract": "Meteorites trace planet formation in the Sun's protoplanetary disk, but they also record the influence of the Sun's birth environment. Whether the Sun formed in a region like Taurus-Auriga with ~10^2 stars, or a region like the Carina Nebula with ~10^6 stars, matters for how large the Sun's disk was, for how long and from how far away it accreted gas from the molecular cloud, and how it acquired radionuclides like 26Al. To provide context for the interpretation of meteoritic data, we review what is known about the Sun's birth environment. Based on an inferred gas disk outer radius ~50-90 AU, radial transport in the disk, and the abundances of noble gases in Jupiter's atmosphere, the Sun's molecular cloud and protoplanetary disk were exposed to an ultraviolet flux G0 ~30-3000 during its birth and first ~10 Myr of evolution. Based on the orbits of Kuiper Belt objects, the Solar System was subsequently exposed to a stellar density ~100 Msol/pc^3 for ~100 Myr, strongly implying formation in a bound cluster. These facts suggest formation in a region like the outskirts of the Orion Nebula, perhaps 2 pc from the center. The protoplanetary disk might have accreted gas for many Myr, but a few x10^5 yr seems more likely. It probably inherited radionuclides from its molecular cloud, enriched by inputs from supernovae and especially Wolf-Rayet star winds, and acquired a typical amount of 26Al.",
      "paper_authors": [
        "Steve Desch",
        "N\u00faria Miret-Roig"
      ],
      "primary_category": "astro-ph.EP",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "Accepted for publication in Space Science Reviews as part of\n  International Space Science Institute \"Evolution of the Solar System:\n  Constraints from Meteorites\" meeting, June 5-9, 2023",
      "repo_url": "#"
    },
    "2409.10632": {
      "paper_id": "2409.10632v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10632v1",
      "paper_key": "2409.10632",
      "paper_title": "SpectrAx: Spectral Search of Axion-Like Particles Using Multi-Band Observations of Galaxy Clusters from SKA, SO, CMB-S4 and eROSITA",
      "paper_url": "http://arxiv.org/abs/2409.10632v1",
      "paper_abstract": "The existence of axions or Axion-Like Particles (ALPs) has been predicted by various Beyond Standard Model (BSM) theories, and the proposed photon-ALP interaction is one of the ways to probe them. Such an interaction will lead to photon-ALP resonant conversion in galaxy clusters, resulting in a polarized spectral distortion in the CMB along the cluster line of sight. The estimation of this signal from galaxy clusters requires an estimation of the electron density and magnetic field in galaxy clusters. We have developed a new Bayesian framework \\texttt{SpectrAx} that can use observations from different electromagnetic bands such as radio, CMB, optical, and X-ray to infer the astrophysical properties of a galaxy cluster, such as cluster its redshift, electron density and magnetic field, along with the BSM physics such as ALPs. By using the simulated data for upcoming CMB surveys such as Simons Observatory (SO) and CMB-S4 in combination with Square Kilometer Array (SKA) and extended ROentgen Survey with an Imaging Telescope Array (eROSITA) we demonstrate the capability in accurately inferring the ALPs coupling strength along with the radial profile of electron density and magnetic field from galaxy clusters. The application of this framework to the data from future surveys by combining SKA+SO+eROSITA and SKA+CMB-S4+eROSITA will make it possible for the first time to explore both astrophysics and BSM physics from low-redshift galaxy clusters using a multi-band approach.",
      "paper_authors": [
        "Harsh Mehta",
        "Suvodip Mukherjee"
      ],
      "primary_category": "astro-ph.CO",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "31 pages, 11 figures, 6 tables, Submitted to JCAP",
      "repo_url": "#"
    },
    "2409.10631": {
      "paper_id": "2409.10631v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10631v1",
      "paper_key": "2409.10631",
      "paper_title": "Spatially Resolved Kinematics of SLACS Lens Galaxies. I: Data and Kinematic Classification",
      "paper_url": "http://arxiv.org/abs/2409.10631v1",
      "paper_abstract": "We obtain spatially resolved kinematics with the Keck Cosmic Web Imager (KCWI) integral-field spectrograph for a sample of 14 massive (11 < log10 M* < 12) lensing early-type galaxies (ETGs) at redshifts z=0.15-0.35 from the Sloan Lens ACS (SLACS) survey. We integrate within the galaxy effective radius and examine the rotational and dispersion velocities, showing that 11/14 are quantitatively classified as slow rotators in comparison with local galaxy surveys. Of key interest is the ability of this data to enable the precision required for cosmological inference with lensing time delays on scales of 1-2% uncertainty. The dataset is unprecedented for galaxy-scale lens galaxies, in terms of signal-to-noise ratio, sampling, and calibration. We test sources of systematic error and identify primary contributions from choice of stellar template library and wavelength range of the spectral fit. Systematics are quantified at the spatial bin level, resulting in systematic error at 3% and positive spatial covariance of 2%. We examine the effects of integration of the kinematic maps within circular apertures of different sizes and compare with SDSS single-aperture velocity dispersions. The most recent velocity dispersion estimates from SDSS spectra are found to be biased by a factor of 5.3% with respect to KCWI data, and to underestimate uncertainties. We examine correlations between scaling relations and show the correlations to agree with previous SLACS analysis with no statistically significant disagreement. A follow-up paper will present Jeans modeling and discuss the context of these observations within broader studies of galaxy evolution and cosmology.",
      "paper_authors": [
        "Shawn Knabel",
        "Tommaso Treu",
        "Michele Cappellari",
        "Anowar J. Shajib",
        "Chih-Fan Chen",
        "Vardha N. Bennert"
      ],
      "primary_category": "astro-ph.CO",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "33 pages, 14 figures, submitted for publication in ApJ",
      "repo_url": "#"
    },
    "2409.10629": {
      "paper_id": "2409.10629v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10629v1",
      "paper_key": "2409.10629",
      "paper_title": "Approximation by Fourier sums on the classes of generalized Poisson integrals",
      "paper_url": "http://arxiv.org/abs/2409.10629v1",
      "paper_abstract": "We present a survey of results related to the solution of Kolmogorov--Nikolsky problem for Fourier sums on the classes of generalized Poisson integrals $C^{\\alpha,r}_{\\beta,p}$, which consists in finding of asymptotic equalities for exact upper boundaries o f uniform norms of deviations of partial Fourier sums on the classes of $2\\pi$--periodic functions $C^{\\alpha,r}_{\\beta,p}$, which are defined as convolutions of the functions, which belong to the unit balls pf the spaces $L_{p}$, $1\\leq p\\leq \\infty$, with generalized Poisson kernels $$ P_{\\alpha,r,\\beta}(t)=\\sum\\limits_{k=1}^{\\infty}e^{-\\alpha k^{r}}\\cos \\big(kt-\\frac{\\beta\\pi}{2}\\big), \\ \\alpha>0, r>0, \\ \\beta\\in \\mathbb{R}.$$",
      "paper_authors": [
        "Anatoly Serdyuk",
        "Tetiana Stepaniuk"
      ],
      "primary_category": "math.CA",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "Survey article",
      "repo_url": "#"
    },
    "2409.10624": {
      "paper_id": "2409.10624v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10624v1",
      "paper_key": "2409.10624",
      "paper_title": "Physical Representations of Corner Symmetries",
      "paper_url": "http://arxiv.org/abs/2409.10624v1",
      "paper_abstract": "We give the full representation theory of the gravitational extended corner symmetry group in two-dimensions. This includes projective representations, which correspond to representations of the quantum corner symmetry group. We begin with a review of Mackey's theory of induced representations and then proceed to its application to the corner symmetries. The field representations, induced from the irreducible representations of the special linear group are worked out first. The little group method is then applied to the extended corner symmetry group to obtain the irreducible unitary representations. Finally, we focus on projective representations and their application to the description of local subsystems.",
      "paper_authors": [
        "Ludovic Varrin"
      ],
      "primary_category": "hep-th",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "24 pages",
      "repo_url": "#"
    },
    "2409.10621": {
      "paper_id": "2409.10621v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10621v1",
      "paper_key": "2409.10621",
      "paper_title": "Inferring stellar parameters and their uncertainties from high-resolution spectroscopy using invertible neural networks",
      "paper_url": "http://arxiv.org/abs/2409.10621v1",
      "paper_abstract": "Context: New spectroscopic surveys will increase the number of astronomical objects requiring characterization by over tenfold.. Machine learning tools are required to address this data deluge in a fast and accurate fashion. Most machine learning algorithms can not estimate error directly, making them unsuitable for reliable science.   Aims: We aim to train a supervised deep-learning algorithm tailored for high-resolution observational stellar spectra. This algorithm accurately infer precise estimates while providing coherent estimates of uncertainties by leveraging information from both the neural network and the spectra.   Methods: We train a conditional Invertible Neural Network (cINN) on observational spectroscopic data obtained from the GIRAFFE spectrograph (HR10 and HR21 setups) within the Gaia-ESO survey. A key features of cINN is its ability to produce the Bayesian posterior distribution of parameters for each spectrum. By analyzing this distribution, we inferred parameters and their uncertainties. Several tests have been applied to study how parameters and errors are estimated.   Results: We achieved an accuracy of 28K in $T_{\\text{eff}}$, 0.06 dex in $\\log g$, 0.03 dex in $[\\text{Fe/H}]$, and between 0.05 dex and 0.17 dex for the other abundances for high quality spectra. Accuracy remains stable with low signal-to-noise ratio spectra. The uncertainties obtained are well within the same order of magnitude. The network accurately reproduces astrophysical relationships both on the scale of the Milky Way and within smaller star clusters. We created a table containing the new parameters generated by our cINN.   Conclusion: This neural network represents a compelling proposition for future astronomical surveys. These coherent derived uncertainties make it possible to reuse these estimates in other works as Bayesian priors and thus present a solid basis for future work.",
      "paper_authors": [
        "Nils Candebat",
        "Giuseppe Germano Sacco",
        "Laura Magrini",
        "Francesco Belfiore",
        "Mathieu Van-der-Swaelmen",
        "Stefano Zibetti"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": null,
      "repo_url": "#"
    },
    "2409.10596": {
      "paper_id": "2409.10596v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10596v1",
      "paper_key": "2409.10596",
      "paper_title": "Unveiling the Diversity of Type IIn Supernovae via Systematic Light Curve Modeling",
      "paper_url": "http://arxiv.org/abs/2409.10596v1",
      "paper_abstract": "Type IIn supernovae (SNeIIn) are a highly heterogeneous subclass of core-collapse supernovae, spectroscopically characterized by signatures of interaction with a dense circumstellar medium (CSM). Here we systematically model the light curves of 142 archival SNeIIn using MOSFiT (the Modular Open Source Fitter for Transients). We find that the observed and inferred properties of SNIIn are diverse, but there are some trends. The typical SN CSM is dense ($\\sim$10$^{-12}$gcm$^{-3}$) with highly diverse CSM geometry, with a median CSM mass of $\\sim$1M$_\\odot$. The ejecta are typically massive ($\\gtrsim10$M$_\\odot$), suggesting massive progenitor systems. We find positive correlations between the CSM mass and the rise and fall times of SNeIIn. Furthermore there are positive correlations between the rise time and fall times and the $r$-band luminosity. We estimate the mass-loss rates of our sample (where spectroscopy is available) and find a high median mass-loss rate of $\\sim$10$^{-2}$M$_\\odot$yr$^{-1}$, with a range between 10$^{-4}$--1M$_\\odot$yr$^{-1}$. These mass-loss rates are most similar to the mass loss from great eruptions of luminous blue variables, consistent with the direct progenitor detections in the literature. We also discuss the role that binary interactions may play, concluding that at least some of our SNeIIn may be from massive binary systems. Finally, we estimate a detection rate of 1.6$\\times$10$^5$yr$^{-1}$ in the upcoming Legacy Survey of Space and Time at the Vera C. Rubin Observatory.",
      "paper_authors": [
        "C. L. Ransome",
        "V. A. Villar"
      ],
      "primary_category": "astro-ph.HE",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "55 pages, 19 figures, 6 tables",
      "repo_url": "#"
    },
    "2409.11184": {
      "paper_id": "2409.11184v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11184v1",
      "paper_key": "2409.11184",
      "paper_title": "LASERS: LAtent Space Encoding for Representations with Sparsity for Generative Modeling",
      "paper_url": "http://arxiv.org/abs/2409.11184v1",
      "paper_abstract": "Learning compact and meaningful latent space representations has been shown to be very useful in generative modeling tasks for visual data. One particular example is applying Vector Quantization (VQ) in variational autoencoders (VQ-VAEs, VQ-GANs, etc.), which has demonstrated state-of-the-art performance in many modern generative modeling applications. Quantizing the latent space has been justified by the assumption that the data themselves are inherently discrete in the latent space (like pixel values). In this paper, we propose an alternative representation of the latent space by relaxing the structural assumption than the VQ formulation. Specifically, we assume that the latent space can be approximated by a union of subspaces model corresponding to a dictionary-based representation under a sparsity constraint. The dictionary is learned/updated during the training process. We apply this approach to look at two models: Dictionary Learning Variational Autoencoders (DL-VAEs) and DL-VAEs with Generative Adversarial Networks (DL-GANs). We show empirically that our more latent space is more expressive and has leads to better representations than the VQ approach in terms of reconstruction quality at the expense of a small computational overhead for the latent space computation. Our results thus suggest that the true benefit of the VQ approach might not be from discretization of the latent space, but rather the lossy compression of the latent space. We confirm this hypothesis by showing that our sparse representations also address the codebook collapse issue as found common in VQ-family models.",
      "paper_authors": [
        "Xin Li",
        "Anand Sarwate"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "Preprint, under review. Submitted to 2025 IEEE/CVF Winter Conference\n  on Applications of Computer Vision (WACV)",
      "repo_url": "#"
    },
    "2409.10579": {
      "paper_id": "2409.10579v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10579v1",
      "paper_key": "2409.10579",
      "paper_title": "Recent advances in deep learning and language models for studying the microbiome",
      "paper_url": "http://arxiv.org/abs/2409.10579v1",
      "paper_abstract": "Recent advancements in deep learning, particularly large language models (LLMs), made a significant impact on how researchers study microbiome and metagenomics data. Microbial protein and genomic sequences, like natural languages, form a language of life, enabling the adoption of LLMs to extract useful insights from complex microbial ecologies. In this paper, we review applications of deep learning and language models in analyzing microbiome and metagenomics data. We focus on problem formulations, necessary datasets, and the integration of language modeling techniques. We provide an extensive overview of protein/genomic language modeling and their contributions to microbiome studies. We also discuss applications such as novel viromics language modeling, biosynthetic gene cluster prediction, and knowledge integration for metagenomics studies.",
      "paper_authors": [
        "Binghao Yan",
        "Yunbi Nam",
        "Lingyao Li",
        "Rebecca A. Deek",
        "Hongzhe Li",
        "Siyuan Ma"
      ],
      "primary_category": "q-bio.QM",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": null,
      "repo_url": "#"
    },
    "2409.10562": {
      "paper_id": "2409.10562v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.10562v1",
      "paper_key": "2409.10562",
      "paper_title": "Are Existing Road Design Guidelines Suitable for Autonomous Vehicles?",
      "paper_url": "http://arxiv.org/abs/2409.10562v1",
      "paper_abstract": "The emergence of Autonomous Vehicles (AVs) has spurred research into testing the resilience of their perception systems, i.e. to ensure they are not susceptible to making critical misjudgements. It is important that they are tested not only with respect to other vehicles on the road, but also those objects placed on the roadside. Trash bins, billboards, and greenery are all examples of such objects, typically placed according to guidelines that were developed for the human visual system, and which may not align perfectly with the needs of AVs. Existing tests, however, usually focus on adversarial objects with conspicuous shapes/patches, that are ultimately unrealistic given their unnatural appearances and the need for white box knowledge. In this work, we introduce a black box attack on the perception systems of AVs, in which the objective is to create realistic adversarial scenarios (i.e. satisfying road design guidelines) by manipulating the positions of common roadside objects, and without resorting to `unnatural' adversarial patches. In particular, we propose TrashFuzz , a fuzzing algorithm to find scenarios in which the placement of these objects leads to substantial misperceptions by the AV -- such as mistaking a traffic light's colour -- with overall the goal of causing it to violate traffic laws. To ensure the realism of these scenarios, they must satisfy several rules encoding regulatory guidelines about the placement of objects on public streets. We implemented and evaluated these attacks for the Apollo, finding that TrashFuzz induced it into violating 15 out of 24 different traffic laws.",
      "paper_authors": [
        "Yang Sun",
        "Christopher M. Poskitt",
        "Jun Sun"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-13",
      "update_time": "2024-09-13",
      "comments": "Currently under review by IEEE Transactions on Software Engineering\n  (TSE)",
      "repo_url": "#"
    },
    "2409.12187": {
      "paper_id": "2409.12187v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.12187v1",
      "paper_key": "2409.12187",
      "paper_title": "Exoplanet accretion monitoring spectroscopic survey (ENTROPY) I. Evidence for magnetospheric accretion in the young isolated planetary-mass object 2MASS J11151597+1937266",
      "paper_url": "http://arxiv.org/abs/2409.12187v1",
      "paper_abstract": "Accretion among planets is a poorly understood phenomenon, due to lack of both observational and theoretical studies. Detection of emission lines from accreting gas giants facilitate detailed investigations into this process. This work presents a detailed analysis of Balmer lines from one of the few known young, planetary-mass objects with observed emission, the isolated L2 dwarf 2MASS J11151597+1937266 with a mass 7-21 Mj and age 5-45 Myr, located at 45+-2 pc. We obtained the first high-resolution (R~50,000) spectrum of the target with VLT/UVES, a spectrograph in the near-UV to visible wavelengths (3200-6800 AA). We report resolved H3-H6 and He I (5875.6 AA) emission in the spectrum. Based on the asymmetric line profiles of H3 and H4, 10% width of H3 (199+-1 km/s), tentative He I 6678 AA emission and indications of a disk from MIR excess, we confirm ongoing accretion at this object. Using the Gaia update of the parallax, we revise its temperature to 1816+-63 K and radius to 1.5+-0.1 Rj. Analysis of observed H I profiles using 1D planet-surface shock model implies a pre-shock gas velocity of v0=120(+80,-40) km/s and a pre-shock density of log(n0/cm^-3)=14(+0,-5). Pre-shock velocity points to a mass of 6(+8,-4) Mj for the target. Combining the H I line luminosities and planetary Lline-Lacc scaling relations, we derive a mass accretion rate of 1.4(+2.8,-0.9)x10^-8 Mj/yr.",
      "paper_authors": [
        "Gayathri Viswanath",
        "Simon C. Ringqvist",
        "Dorian Demars",
        "Markus Janson",
        "Micka\u00ebl Bonnefoy",
        "Yuhiko Aoyama",
        "Gabriel-Dominique Marleau",
        "Catherine Dougados",
        "Judit Szul\u00e1gyi",
        "Thanawuth Thanathibodee"
      ],
      "primary_category": "astro-ph.EP",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "Accepted for publication at A&A on 2024-09-11. 15 pages, 9 figures, 7\n  tables",
      "repo_url": "#"
    },
    "2409.12185": {
      "paper_id": "2409.12185v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.12185v1",
      "paper_key": "2409.12185",
      "paper_title": "Disruption of a massive molecular cloud by a supernova in the Galactic Centre: Initial results from the ACES project",
      "paper_url": "http://arxiv.org/abs/2409.12185v1",
      "paper_abstract": "The Milky Way's Central Molecular Zone (CMZ) differs dramatically from our local solar neighbourhood, both in the extreme interstellar medium conditions it exhibits (e.g. high gas, stellar, and feedback density) and in the strong dynamics at play (e.g. due to shear and gas influx along the bar). Consequently, it is likely that there are large-scale physical structures within the CMZ that cannot form elsewhere in the Milky Way. In this paper, we present new results from the Atacama Large Millimeter/submillimeter Array (ALMA) large programme ACES (ALMA CMZ Exploration Survey) and conduct a multi-wavelength and kinematic analysis to determine the origin of the M0.8$-$0.2 ring, a molecular cloud with a distinct ring-like morphology. We estimate the projected inner and outer radii of the M0.8$-$0.2 ring to be 79\" and 154\", respectively (3.1 pc and 6.1 pc at an assumed Galactic Centre distance of 8.2 kpc) and calculate a mean gas density $> 10^{4}$ cm$^{-3}$, a mass of $\\sim$ $10^6$ M$_\\odot$, and an expansion speed of $\\sim$ 20 km s$^{-1}$, resulting in a high estimated kinetic energy ($> 10^{51}$ erg) and momentum ($> 10^7$ M$_\\odot$ km s$^{-1}$). We discuss several possible causes for the existence and expansion of the structure, including stellar feedback and large-scale dynamics. We propose that the most likely cause of the M0.8$-$0.2 ring is a single high-energy hypernova explosion. To viably explain the observed morphology and kinematics, such an explosion would need to have taken place inside a dense, very massive molecular cloud, the remnants of which we now see as the M0.8$-$0.2 ring. In this case, the structure provides an extreme example of how supernovae can affect molecular clouds.",
      "paper_authors": [
        "M. Nonhebel",
        "A. T. Barnes",
        "K. Immer",
        "J. Armijos-Abenda\u00f1o",
        "J. Bally",
        "C. Battersby",
        "M. G. Burton",
        "N. Butterfield",
        "L. Colzi",
        "P. Garc\u00eda",
        "A. Ginsburg",
        "J. D. Henshaw",
        "Y. Hu",
        "I. Jim\u00e9nez-Serra",
        "R. S. Klessen",
        "F. -H. Liang",
        "S. N. Longmore",
        "X. Lu",
        "S. Mart\u00edn",
        "F. Nogueras-Lara",
        "M. A. Petkova",
        "J. E. Pineda",
        "V. M. Rivilla",
        "\u00c1. S\u00e1nchez-Monge",
        "M. G. Santa-Maria",
        "H. A. Smith",
        "Y. Sofue",
        "M. C. Sormani",
        "V. Tolls",
        "D. L. Walker",
        "Q. D. Wang",
        "G. M. Williams",
        "F. -W. Xu"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "17 pages, 13 figures, and 2 tables. Accepted for publication in\n  Astronomy & Astrophysics",
      "repo_url": "#"
    },
    "2409.12147": {
      "paper_id": "2409.12147v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.12147v1",
      "paper_key": "2409.12147",
      "paper_title": "MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning",
      "paper_url": "http://arxiv.org/abs/2409.12147v1",
      "paper_abstract": "Large Language Models' (LLM) reasoning can be improved using test-time aggregation strategies, i.e., generating multiple samples and voting among generated samples. While these improve performance, they often reach a saturation point. Refinement offers an alternative by using LLM-generated feedback to improve solution quality. However, refinement introduces 3 key challenges: (1) Excessive refinement: Uniformly refining all instances can over-correct and reduce the overall performance. (2) Inability to localize and address errors: LLMs have a limited ability to self-correct and struggle to identify and correct their own mistakes. (3) Insufficient refinement: Deciding how many iterations of refinement are needed is non-trivial, and stopping too soon could leave errors unaddressed. To tackle these issues, we propose MAgICoRe, which avoids excessive refinement by categorizing problem difficulty as easy or hard, solving easy problems with coarse-grained aggregation and hard ones with fine-grained and iterative multi-agent refinement. To improve error localization, we incorporate external step-wise reward model (RM) scores. Moreover, to ensure effective refinement, we employ a multi-agent loop with three agents: Solver, Reviewer (which generates targeted feedback based on step-wise RM scores), and the Refiner (which incorporates feedback). To ensure sufficient refinement, we re-evaluate updated solutions, iteratively initiating further rounds of refinement. We evaluate MAgICoRe on Llama-3-8B and GPT-3.5 and show its effectiveness across 5 math datasets. Even one iteration of MAgICoRe beats Self-Consistency by 3.4%, Best-of-k by 3.2%, and Self-Refine by 4.0% while using less than half the samples. Unlike iterative refinement with baselines, MAgICoRe continues to improve with more iterations. Finally, our ablations highlight the importance of MAgICoRe's RMs and multi-agent communication.",
      "paper_authors": [
        "Justin Chih-Yao Chen",
        "Archiki Prasad",
        "Swarnadeep Saha",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "22 pages, code: https://github.com/dinobby/MAgICoRe",
      "repo_url": "https://github.com/dinobby/magicore"
    },
    "2409.12138": {
      "paper_id": "2409.12138v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.12138v1",
      "paper_key": "2409.12138",
      "paper_title": "Reporting Non-Consensual Intimate Media: An Audit Study of Deepfakes",
      "paper_url": "http://arxiv.org/abs/2409.12138v1",
      "paper_abstract": "Non-consensual intimate media (NCIM) inflicts significant harm. Currently, victim-survivors can use two mechanisms to report NCIM - as a non-consensual nudity violation or as copyright infringement. We conducted an audit study of takedown speed of NCIM reported to X (formerly Twitter) of both mechanisms. We uploaded 50 AI-generated nude images and reported half under X's \"non-consensual nudity\" reporting mechanism and half under its \"copyright infringement\" mechanism. The copyright condition resulted in successful image removal within 25 hours for all images (100% removal rate), while non-consensual nudity reports resulted in no image removal for over three weeks (0% removal rate). We stress the need for targeted legislation to regulate NCIM removal online. We also discuss ethical considerations for auditing NCIM on social platforms.",
      "paper_authors": [
        "Li Qiwei",
        "Shihui Zhang",
        "Andrew Timothy Kasper",
        "Joshua Ashkinaze",
        "Asia A. Eaton",
        "Sarita Schoenebeck",
        "Eric Gilbert"
      ],
      "primary_category": "cs.CY",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "under review",
      "repo_url": "#"
    },
    "2409.12118": {
      "paper_id": "2409.12118v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.12118v1",
      "paper_key": "2409.12118",
      "paper_title": "The Low-Redshift Lyman Continuum Survey: The Roles of Stellar Feedback and ISM Geometry in LyC Escape",
      "paper_url": "http://arxiv.org/abs/2409.12118v1",
      "paper_abstract": "One of the fundamental questions of cosmology is the origin and mechanism(s) responsible for the reionization of the Universe beyond $z\\sim6$. To address this question, many studies over the past decade have focused on local ($z\\sim0.3$) galaxies which leak ionizing radiation (Lyman continuum or LyC). However, line-of-sight effects and data quality have prohibited deeper insight into the nature of LyC escape. To circumvent these limitations, we analyze stacks of a consolidated sample of {\\it HST}/COS observations of the LyC in 89 galaxies at $z\\sim0.3$. From fitting of the continuum, we obtain information about the underlying stellar populations and neutral ISM geometry. We find that most LyC non-detections are not leaking appreciable LyC ($f_{esc}^{\\rm LyC}<1$\\%) but also that exceptional cases point to spatial variations in the LyC escape fraction $f_{esc}^{\\rm LyC}$. Stellar populations younger than 3 Myr lead to an increase in ionizing feedback, which in turn increases the isotropy of LyC escape. Moreover, mechanical feedback from supernovae in 8-10 Myr stellar populations is important for anisotropic gas distributions needed for LyC escape. While mechanical feedback is necessary for any LyC escape, high $f_{esc}^{\\rm LyC}$ ($>5$\\%) also requires a confluence of young stars and ionizing feedback. A two-stage burst of star formation could facilitate this optimal LyC escape scenario.",
      "paper_authors": [
        "Sophia R. Flury",
        "Anne E. Jaskot",
        "Alberto Saldana-Lopez",
        "M. S. Oey",
        "John Chisholm",
        "Ricardo Amor\u00edn",
        "Omkar Bait",
        "Sanchayeeta Borthakur",
        "Cody Carr",
        "Henry C. Ferguson",
        "Mauro Giavalisco",
        "Matthew Hayes",
        "Timothy Heckman",
        "Alaina Henry",
        "Zhiyuan Ji",
        "Lena Komarova",
        "Floriane Leclercq",
        "Alexandra Le Reste",
        "Stephan McCandliss",
        "Rui Marques-Chaves",
        "G\u00f6ran \u00d6stlin",
        "Laura Pentericci",
        "Swara Ravindranath",
        "Michael Rutkowski",
        "Claudia Scarlata",
        "Daniel Schaerer",
        "Trinh Thuan",
        "Maxime Trebitsch",
        "Eros Vanzella",
        "Anne Verhamme",
        "Bingjie Wang",
        "G\u00e1bor Worseck",
        "Xinfeng Xu"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "Submitted to AAS publications, 42 pages, 25 figures",
      "repo_url": "#"
    },
    "2409.12111": {
      "paper_id": "2409.12111v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.12111v1",
      "paper_key": "2409.12111",
      "paper_title": "Applications of Knowledge Distillation in Remote Sensing: A Survey",
      "paper_url": "http://arxiv.org/abs/2409.12111v1",
      "paper_abstract": "With the ever-growing complexity of models in the field of remote sensing (RS), there is an increasing demand for solutions that balance model accuracy with computational efficiency. Knowledge distillation (KD) has emerged as a powerful tool to meet this need, enabling the transfer of knowledge from large, complex models to smaller, more efficient ones without significant loss in performance. This review article provides an extensive examination of KD and its innovative applications in RS. KD, a technique developed to transfer knowledge from a complex, often cumbersome model (teacher) to a more compact and efficient model (student), has seen significant evolution and application across various domains. Initially, we introduce the fundamental concepts and historical progression of KD methods. The advantages of employing KD are highlighted, particularly in terms of model compression, enhanced computational efficiency, and improved performance, which are pivotal for practical deployments in RS scenarios. The article provides a comprehensive taxonomy of KD techniques, where each category is critically analyzed to demonstrate the breadth and depth of the alternative options, and illustrates specific case studies that showcase the practical implementation of KD methods in RS tasks, such as instance segmentation and object detection. Further, the review discusses the challenges and limitations of KD in RS, including practical constraints and prospective future directions, providing a comprehensive overview for researchers and practitioners in the field of RS. Through this organization, the paper not only elucidates the current state of research in KD but also sets the stage for future research opportunities, thereby contributing significantly to both academic research and real-world applications.",
      "paper_authors": [
        "Yassine Himeur",
        "Nour Aburaed",
        "Omar Elharrouss",
        "Iraklis Varlamis",
        "Shadi Atalla",
        "Wathiq Mansoor",
        "Hussain Al Ahmad"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "50 pages, 11 figures and 9 tables",
      "repo_url": "#"
    },
    "2409.12085": {
      "paper_id": "2409.12085v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.12085v1",
      "paper_key": "2409.12085",
      "paper_title": "Unveiling the Secrets of New Physics Through Top Quark Tagging",
      "paper_url": "http://arxiv.org/abs/2409.12085v1",
      "paper_abstract": "The ubiquity of top-rich final states in the context of beyond the Standard Model (BSM) searches has led to their status as extensively studied signatures at the LHC. Over the past decade, numerous endeavours have been undertaken in the literature to develop methods for efficiently distinguishing boosted top quark jets from QCD jets. Although cut-based strategies for boosted top tagging, which rely on substructure information from fat jets resulting from the hadronic decay of boosted top quarks, were introduced in the literature as early as 2008, recent years have witnessed a surge in the utilization of machine learning-based approaches for the classification of top-jets from QCD jets. The review focuses on the present status of boosted top tagging and its application for BSM searchers.",
      "paper_authors": [
        "Rameswar Sahu",
        "Saiyad Ashanujjaman",
        "Kirtiman Ghosh"
      ],
      "primary_category": "hep-ph",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "Accepted for publication in \"The European Physical Journal Special\n  Topics\", 28 pages, 20 figures",
      "repo_url": "#"
    },
    "2409.12081": {
      "paper_id": "2409.12081v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.12081v1",
      "paper_key": "2409.12081",
      "paper_title": "Optimising the Trade-Off Between Type I and Type II Errors: A Review and Extensions",
      "paper_url": "http://arxiv.org/abs/2409.12081v1",
      "paper_abstract": "In clinical studies upon which decisions are based there are two types of errors that can be made: a type I error arises when the decision is taken to declare a positive outcome when the truth is in fact negative, and a type II error arises when the decision is taken to declare a negative outcome when the truth is in fact positive. Commonly the primary analysis of such a study entails a two-sided hypothesis test with a type I error rate of 5% and the study is designed to have a sufficiently low type II error rate, for example 10% or 20%. These values are arbitrary and often do not reflect the clinical, or regulatory, context of the study and ignore both the relative costs of making either type of error and the sponsor's prior belief that the drug is superior to either placebo, or a standard of care if relevant. This simplistic approach has recently been challenged by numerous authors both from a frequentist and Bayesian perspective since when resources are constrained there will be a need to consider a trade-off between type I and type II errors. In this paper we review proposals to utilise the trade-off by formally acknowledging the costs to optimise the choice of error rates for simple, point null and alternative hypotheses and extend the results to composite, or interval hypotheses, showing links to the Probability of Success of a clinical study.",
      "paper_authors": [
        "Andrew P Grieve"
      ],
      "primary_category": "stat.ME",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": null,
      "repo_url": "#"
    },
    "2409.12069": {
      "paper_id": "2409.12069v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.12069v1",
      "paper_key": "2409.12069",
      "paper_title": "Probing the Possible Causes of the Transit Timing Variation for TrES-2b in TESS Era",
      "paper_url": "http://arxiv.org/abs/2409.12069v1",
      "paper_abstract": "Nowadays, transit timing variations (TTVs) are proving to be a very valuable tool in exoplanetary science to detect exoplanets by observing variations in transit times. To study the transit timing variation of the hot Jupiter, TrES-2b, we have combined 64 high-quality transit light curves from all seven sectors of NASA's Transiting Exoplanet Survey Satellite (TESS) along with 60 best-quality light curves from the ground-based facility Exoplanet Transit Database (ETD) and 106 mid-transit times from the previous works. From the precise transit timing analysis, we have observed a significant improvement in the orbital ephemerides, but we did not detect any short period TTVs that might result from an additional body. The inability to detect short-term TTVs further motivates us to investigate long-term TTVs, which might be caused by orbital decay, apsidal precession, Applegate mechanism, and $R{\\phi}$mer effect and the orbital decay appeared to be a better explanation for the observed TTV with $\\Delta BIC$ = 4.32. The orbital period of the hot Jupiter TrES-2b appears to be shrinking at a rate of $-5.58 \\pm 1.81$ ms/yr. Assuming this decay is primarily caused by tidal dissipation within the host star, we have subsequently calculated the stellar tidal quality factor value to be 9900, which is 2 to 3 orders of magnitude smaller than the theoretically predicted values for other hot-Jupiter systems and its low value indicates more efficient tidal dissipation within the host star. Additional precise photometric and radial velocity observations are required to pinpoint the cause of the change in the orbital period.",
      "paper_authors": [
        "Shraddha Biswas",
        "D. Bisht",
        "Ing-Guey Jiang",
        "Devesh P. Sariya",
        "Kaviya Parthasarathy"
      ],
      "primary_category": "astro-ph.EP",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "38 pages, accepted by AJ on 2nd August",
      "repo_url": "#"
    },
    "2409.12047": {
      "paper_id": "2409.12047v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.12047v1",
      "paper_key": "2409.12047",
      "paper_title": "A Survey-Based Quantitative Analysis of Stress Factors and Their Impacts Among Cybersecurity Professionals",
      "paper_url": "http://arxiv.org/abs/2409.12047v1",
      "paper_abstract": "This study investigates the prevalence and underlying causes of work-related stress and burnout among cybersecurity professionals using a quantitative survey approach guided by the Job Demands-Resources model. Analysis of responses from 50 cybersecurity practitioners reveals an alarming reality: 44% report experiencing severe work-related stress and burnout, while an additional 28% are uncertain about their condition. The demanding nature of cybersecurity roles, unrealistic expectations, and unsupportive organizational cultures emerge as primary factors fueling this crisis. Notably, 66% of respondents perceive cybersecurity jobs as more stressful than other IT positions, with 84% facing additional challenges due to the pandemic and recent high-profile breaches. The study finds that most cybersecurity experts are reluctant to report their struggles to management, perpetuating a cycle of silence and neglect. To address this critical issue, the paper recommends that organizations foster supportive work environments, implement mindfulness programs, and address systemic challenges. By prioritizing the mental health of cybersecurity professionals, organizations can cultivate a more resilient and effective workforce to protect against an ever-evolving threat landscape.",
      "paper_authors": [
        "Sunil Arora",
        "John D. Hastings"
      ],
      "primary_category": "cs.CR",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "10 pages, 8 figures, 6 tables",
      "repo_url": "#"
    },
    "2409.12034": {
      "paper_id": "2409.12034v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.12034v1",
      "paper_key": "2409.12034",
      "paper_title": "Multi-Sensor Deep Learning for Glacier Mapping",
      "paper_url": "http://arxiv.org/abs/2409.12034v1",
      "paper_abstract": "The more than 200,000 glaciers outside the ice sheets play a crucial role in our society by influencing sea-level rise, water resource management, natural hazards, biodiversity, and tourism. However, only a fraction of these glaciers benefit from consistent and detailed in-situ observations that allow for assessing their status and changes over time. This limitation can, in part, be overcome by relying on satellite-based Earth Observation techniques. Satellite-based glacier mapping applications have historically mainly relied on manual and semi-automatic detection methods, while recently, a fast and notable transition to deep learning techniques has started.   This chapter reviews how combining multi-sensor remote sensing data and deep learning allows us to better delineate (i.e. map) glaciers and detect their temporal changes. We explain how relying on deep learning multi-sensor frameworks to map glaciers benefits from the extensive availability of regional and global glacier inventories. We also analyse the rationale behind glacier mapping, the benefits of deep learning methodologies, and the inherent challenges in integrating multi-sensor earth observation data with deep learning algorithms.   While our review aims to provide a broad overview of glacier mapping efforts, we highlight a few setups where deep learning multi-sensor remote sensing applications have a considerable potential added value. This includes applications for debris-covered and rock glaciers that are visually difficult to distinguish from surroundings and for calving glaciers that are in contact with the ocean. These specific cases are illustrated through a series of visual imageries, highlighting some significant advantages and challenges when detecting glacier changes, including dealing with seasonal snow cover, changing debris coverage, and distinguishing glacier fronts from the surrounding sea ice.",
      "paper_authors": [
        "Codru\u0163-Andrei Diaconu",
        "Konrad Heidler",
        "Jonathan L. Bamber",
        "Harry Zekollari"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "This article will be a chapter of the book Deep Learning for\n  Multi-Sensor Earth Observation, to be published by Elsevier",
      "repo_url": "#"
    },
    "2409.12012": {
      "paper_id": "2409.12012v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.12012v1",
      "paper_key": "2409.12012",
      "paper_title": "Shannon Entropy is better Feature than Category and Sentiment in User Feedback Processing",
      "paper_url": "http://arxiv.org/abs/2409.12012v1",
      "paper_abstract": "App reviews in mobile app stores contain useful information which is used to improve applications and promote software evolution. This information is processed by automatic tools which prioritize reviews. In order to carry out this prioritization, reviews are decomposed into features like category and sentiment. Then, a weighted function assigns a weight to each feature and a review ranking is calculated. Unfortunately, in order to extract category and sentiment from reviews, its is required at least a classifier trained in an annotated corpus. Therefore this task is computational demanding. Thus, in this work, we propose Shannon Entropy as a simple feature which can replace standard features. Our results show that a Shannon Entropy based ranking is better than a standard ranking according to the NDCG metric. This result is promising even if we require fairness by means of algorithmic bias. Finally, we highlight a computational limit which appears in the search of the best ranking.",
      "paper_authors": [
        "Andres Rojas Paredes",
        "Brenda Mareco"
      ],
      "primary_category": "cs.SE",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "10 pages",
      "repo_url": "#"
    },
    "2409.12009": {
      "paper_id": "2409.12009v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.12009v1",
      "paper_key": "2409.12009",
      "paper_title": "Optimizing Redshift Distribution Inference through Joint Self-Calibration and Clustering-Redshift Synergy",
      "paper_url": "http://arxiv.org/abs/2409.12009v1",
      "paper_abstract": "Accurately characterizing the true redshift (true-$z$) distribution of a photometric redshift (photo-$z$) sample is critical for cosmological analyses in imaging surveys. Clustering-based techniques, which include clustering-redshift (CZ) and self-calibration (SC) methods--depending on whether external spectroscopic data are used--offer powerful tools for this purpose. In this study, we explore the joint inference of the true-$z$ distribution by combining SC and CZ (denoted as SC+CZ). We derive simple multiplicative update rules to perform the joint inference. By incorporating appropriate error weighting and an additional weighting function, our method shows significant improvement over previous algorithms. We validate our approach using a DES Y3 mock catalog. The true-$z$ distribution estimated through the combined SC+CZ method is generally more accurate than using SC or CZ alone. To account for the different constraining powers of these methods, we assign distinct weights to the SC and CZ contributions. The optimal weights, which minimize the distribution error, depend on the relative constraining strength of the SC and CZ data. Specifically, for a spectroscopic redshift sample that represents 1% of the photo-$z$ sample, the optimal combination reduces the total error by 20% (40%) compared to using CZ (SC) alone, and it keeps the bias in mean redshift [$\\Delta \\bar{z} / (1 + z) $] at the level of 0.3%. Furthermore, when CZ data is only available in the low-$z$ range and the high-$z$ range relies solely on SC data, SC+CZ enables consistent estimation of the true-$z$ distribution across the entire redshift range. Our findings demonstrate that SC+CZ is an effective tool for constraining the true-$z$ distribution, paving the way for clustering-based methods to be applied at $z\\gtrsim 1$.",
      "paper_authors": [
        "Weilun Zheng",
        "Kwan Chuen Chan",
        "Haojie Xu",
        "Le Zhang",
        "Ruiyu Song"
      ],
      "primary_category": "astro-ph.CO",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "17 pages, 13 figures",
      "repo_url": "#"
    },
    "2409.12001": {
      "paper_id": "2409.12001v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.12001v1",
      "paper_key": "2409.12001",
      "paper_title": "Putting Data at the Centre of Offline Multi-Agent Reinforcement Learning",
      "paper_url": "http://arxiv.org/abs/2409.12001v1",
      "paper_abstract": "Offline multi-agent reinforcement learning (MARL) is an exciting direction of research that uses static datasets to find optimal control policies for multi-agent systems. Though the field is by definition data-driven, efforts have thus far neglected data in their drive to achieve state-of-the-art results. We first substantiate this claim by surveying the literature, showing how the majority of works generate their own datasets without consistent methodology and provide sparse information about the characteristics of these datasets. We then show why neglecting the nature of the data is problematic, through salient examples of how tightly algorithmic performance is coupled to the dataset used, necessitating a common foundation for experiments in the field. In response, we take a big step towards improving data usage and data awareness in offline MARL, with three key contributions: (1) a clear guideline for generating novel datasets; (2) a standardisation of over 80 existing datasets, hosted in a publicly available repository, using a consistent storage format and easy-to-use API; and (3) a suite of analysis tools that allow us to understand these datasets better, aiding further development.",
      "paper_authors": [
        "Claude Formanek",
        "Louise Beyers",
        "Callum Rhys Tilbury",
        "Jonathan P. Shock",
        "Arnu Pretorius"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": null,
      "repo_url": "#"
    },
    "2409.11994": {
      "paper_id": "2409.11994v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11994v1",
      "paper_key": "2409.11994",
      "paper_title": "Black Hole Accretion is all about Sub-Keplerian Flows",
      "paper_url": "http://arxiv.org/abs/2409.11994v1",
      "paper_abstract": "We review the advantages of fitting with a Two Component Advective Flow (TCAF) which uses only four physical parameters. We then present the results of hydrodynamic simulations to highlight the fact that the primary component of a black hole accretion remains the sub-Keplerian or the low angular momentum flow independent of whether we have a high, intermediate or low mass X-ray binary. Every aspect of spectral and timing properties, including the disk-jet connection could be understood well only if such a component is present along with a Keplerian component of variable size and accretion rate.",
      "paper_authors": [
        "Sandip Kumar Chakrabarti"
      ],
      "primary_category": "astro-ph.HE",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "12 pages, 6 Figures, To be published in International Symposium on\n  Recent Developments in Relativistic Astrophysics",
      "repo_url": "#"
    },
    "2409.11992": {
      "paper_id": "2409.11992v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11992v1",
      "paper_key": "2409.11992",
      "paper_title": "Additive-feature-attribution methods: a review on explainable artificial intelligence for fluid dynamics and heat transfer",
      "paper_url": "http://arxiv.org/abs/2409.11992v1",
      "paper_abstract": "The use of data-driven methods in fluid mechanics has surged dramatically in recent years due to their capacity to adapt to the complex and multi-scale nature of turbulent flows, as well as to detect patterns in large-scale simulations or experimental tests. In order to interpret the relationships generated in the models during the training process, numerical attributions need to be assigned to the input features. One important example are the additive-feature-attribution methods. These explainability methods link the input features with the model prediction, providing an interpretation based on a linear formulation of the models. The SHapley Additive exPlanations (SHAP values) are formulated as the only possible interpretation that offers a unique solution for understanding the model. In this manuscript, the additive-feature-attribution methods are presented, showing four common implementations in the literature: kernel SHAP, tree SHAP, gradient SHAP, and deep SHAP. Then, the main applications of the additive-feature-attribution methods are introduced, dividing them into three main groups: turbulence modeling, fluid-mechanics fundamentals, and applied problems in fluid dynamics and heat transfer. This review shows thatexplainability techniques, and in particular additive-feature-attribution methods, are crucial for implementing interpretable and physics-compliant deep-learning models in the fluid-mechanics field.",
      "paper_authors": [
        "Andr\u00e9s Cremades",
        "Sergio Hoyas",
        "Ricardo Vinuesa"
      ],
      "primary_category": "physics.flu-dyn",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": null,
      "repo_url": "#"
    },
    "2409.11980": {
      "paper_id": "2409.11980v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11980v1",
      "paper_key": "2409.11980",
      "paper_title": "End-to-End Learning of Transmitter and Receiver Filters in Bandwidth Limited Fiber Optic Communication Systems",
      "paper_url": "http://arxiv.org/abs/2409.11980v1",
      "paper_abstract": "This paper investigates the application of end-to-end (E2E) learning for joint optimization of pulse-shaper and receiver filter to reduce intersymbol interference (ISI) in bandwidth-limited communication systems. We investigate this in two numerical simulation models: 1) an additive white Gaussian noise (AWGN) channel with bandwidth limitation and 2) an intensity modulated direct detection (IM/DD) link employing an electro-absorption modulator. For both simulation models, we implement a wavelength division multiplexing (WDM) scheme to ensure that the learned filters adhere to the bandwidth constraints of the WDM channels. Our findings reveal that E2E learning greatly surpasses traditional single-sided transmitter pulse-shaper or receiver filter optimization methods, achieving significant performance gains in terms of symbol error rate with shorter filter lengths. These results suggest that E2E learning can decrease the complexity and enhance the performance of future high-speed optical communication systems.",
      "paper_authors": [
        "S\u00f8ren F\u00f8ns Nielsen",
        "Francesco Da Ros",
        "Mikkel N. Schmidt",
        "Darko Zibar"
      ],
      "primary_category": "eess.SP",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "Under review",
      "repo_url": "#"
    },
    "2409.11965": {
      "paper_id": "2409.11965v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11965v1",
      "paper_key": "2409.11965",
      "paper_title": "SWEET-Cat: A view on the planetary mass-radius relation",
      "paper_url": "http://arxiv.org/abs/2409.11965v1",
      "paper_abstract": "SWEET-Cat (Stars With ExoplanETs Catalogue) was originally introduced in 2013, and since then, the number of confirmed exoplanets has increased significantly. A crucial step for a comprehensive understanding of these new worlds is the precise and homogeneous characterization of their host stars. We used a large number of high-resolution spectra to continue the addition of new stellar parameters for planet-host stars in SWEET-Cat following the new detection of exoplanets listed both at the Extrasolar Planets Encyclopedia and at the NASA exoplanet archive. We obtained high-resolution spectra for a significant number of these planet-host stars, either observed by our team or collected through public archives. For FGK stars, the spectroscopic stellar parameters were derived for the spectra following the same homogeneous process using ARES+MOOG as for the previous SWEET-Cat releases. The stellar properties are combined with the planet properties to study possible correlations that could shed more light into the star-planet connection studies. We increase the number of stars with homogeneous parameters by 232 ($\\sim$ 25\\% - from 959 to 1191). We then focus on the exoplanets with both mass and radius determined to review the mass-radius relation where we find consistent results with the ones previously reported in the literature. For the massive planets we also revisit the radius anomaly where we confirm a metallicity correlation for the radius anomaly already hinted in previous results.",
      "paper_authors": [
        "S. G. Sousa",
        "V. Adibekyan",
        "E. Delgado-Mena",
        "N. C. Santos",
        "B. Rojas-Ayala",
        "S. C. Barros",
        "O. D. S. Demangeon",
        "S. Hoyer",
        "G. Israelian",
        "A. Mortier",
        "B. M. T. Soares",
        "M. Tsantaki"
      ],
      "primary_category": "astro-ph.EP",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "8 pages, 5 figures, Accepted for A&A",
      "repo_url": "#"
    },
    "2409.11948": {
      "paper_id": "2409.11948v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11948v1",
      "paper_key": "2409.11948",
      "paper_title": "Research Citations Building Trust in Wikipedia",
      "paper_url": "http://arxiv.org/abs/2409.11948v1",
      "paper_abstract": "The use of Wikipedia citations in scholarly research has been the topic of much inquiry over the past decade. A cross-publisher study (Taylor & Francis and University of Michigan Press) convened by Digital Science was established in late 2022 to explore author sentiment towards Wikipedia as a trusted source of information. A short survey was designed to poll published authors about views and uses of Wikipedia and explore how the increased addition of research citations in Wikipedia might help combat misinformation in the context of increasing public engagement with and access to validated research sources. With 21,854 surveys sent, targeting 40,402 papers mentioned in Wikipedia, a total of 750 complete surveys from 60 countries were included in this analysis. In general, responses revealed a positive sentiment towards research citation in Wikipedia and the researcher engagement practices. However, our sub analysis revealed statistically significant differences when comparison articles vs books and across disciplines, but not open vs closed access. This study will open the door to further research and deepen our understanding of authors perceived trustworthiness of the representation of their research in Wikipedia.",
      "paper_authors": [
        "Michael Taylor",
        "Carlos Areia",
        "Kath Burton",
        "Charles Watkinson"
      ],
      "primary_category": "cs.DL",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": null,
      "repo_url": "#"
    },
    "2409.11945": {
      "paper_id": "2409.11945v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11945v1",
      "paper_key": "2409.11945",
      "paper_title": "Cyclic Segal Spaces",
      "paper_url": "http://arxiv.org/abs/2409.11945v1",
      "paper_abstract": "In this survey article, we review some conceptual approaches to the cyclic category $\\Lambda$, as well as its description as a crossed simplicial group. We then give a new proof of the model structure on cyclic sets, work through the details of the generalized Reedy structure on cyclic spaces, and introduce model structures for cyclic Segal spaces and cyclic 2-Segal spaces.",
      "paper_authors": [
        "Julia E. Bergner",
        "Walker H. Stern"
      ],
      "primary_category": "math.AT",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "46 pages, 16 figures",
      "repo_url": "#"
    },
    "2409.11925": {
      "paper_id": "2409.11925v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11925v1",
      "paper_key": "2409.11925",
      "paper_title": "Haptic-ACT: Bridging Human Intuition with Compliant Robotic Manipulation via Immersive VR",
      "paper_url": "http://arxiv.org/abs/2409.11925v1",
      "paper_abstract": "Robotic manipulation is essential for the widespread adoption of robots in industrial and home settings and has long been a focus within the robotics community. Advances in artificial intelligence have introduced promising learning-based methods to address this challenge, with imitation learning emerging as particularly effective. However, efficiently acquiring high-quality demonstrations remains a challenge. In this work, we introduce an immersive VR-based teleoperation setup designed to collect demonstrations from a remote human user. We also propose an imitation learning framework called Haptic Action Chunking with Transformers (Haptic-ACT). To evaluate the platform, we conducted a pick-and-place task and collected 50 demonstration episodes. Results indicate that the immersive VR platform significantly reduces demonstrator fingertip forces compared to systems without haptic feedback, enabling more delicate manipulation. Additionally, evaluations of the Haptic-ACT framework in both the MuJoCo simulator and on a real robot demonstrate its effectiveness in teaching robots more compliant manipulation compared to the original ACT. Additional materials are available at https://sites.google.com/view/hapticact.",
      "paper_authors": [
        "Kelin Li",
        "Shubham M Wagh",
        "Nitish Sharma",
        "Saksham Bhadani",
        "Wei Chen",
        "Chang Liu",
        "Petar Kormushev"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "This work is under review by ICRA 2025",
      "repo_url": "#"
    },
    "2409.11884": {
      "paper_id": "2409.11884v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11884v1",
      "paper_key": "2409.11884",
      "paper_title": "Recent Advances in OOD Detection: Problems and Approaches",
      "paper_url": "http://arxiv.org/abs/2409.11884v1",
      "paper_abstract": "Out-of-distribution (OOD) detection aims to detect test samples outside the training category space, which is an essential component in building reliable machine learning systems. Existing reviews on OOD detection primarily focus on method taxonomy, surveying the field by categorizing various approaches. However, many recent works concentrate on non-traditional OOD detection scenarios, such as test-time adaptation, multi-modal data sources and other novel contexts. In this survey, we uniquely review recent advances in OOD detection from the problem scenario perspective for the first time. According to whether the training process is completely controlled, we divide OOD detection methods into training-driven and training-agnostic. Besides, considering the rapid development of pre-trained models, large pre-trained model-based OOD detection is also regarded as an important category and discussed separately. Furthermore, we provide a discussion of the evaluation scenarios, a variety of applications, and several future research directions. We believe this survey with new taxonomy will benefit the proposal of new methods and the expansion of more practical scenarios. A curated list of related papers is provided in the Github repository: \\url{https://github.com/shuolucs/Awesome-Out-Of-Distribution-Detection}",
      "paper_authors": [
        "Shuo Lu",
        "YingSheng Wang",
        "LuJun Sheng",
        "AiHua Zheng",
        "LinXiao He",
        "Jian Liang"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "September 18, 2024",
      "repo_url": "#"
    },
    "2409.11881": {
      "paper_id": "2409.11881v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11881v1",
      "paper_key": "2409.11881",
      "paper_title": "Exascale Quantum Mechanical Simulations: Navigating the Shifting Sands of Hardware and Software",
      "paper_url": "http://arxiv.org/abs/2409.11881v1",
      "paper_abstract": "The era of exascale computing presents both exciting opportunities and unique challenges for quantum mechanical simulations. While the transition from petaflops to exascale computing has been marked by a steady increase in computational power, the shift towards heterogeneous architectures, particularly the dominant role of graphical processing units (GPUs), demands a fundamental shift in software development strategies. This review examines the changing landscape of hardware and software for exascale computing, highlighting the limitations of traditional algorithms and software implementations in light of the increasing use of heterogeneous architectures in high-end systems. We discuss the challenges of adapting quantum chemistry software to these new architectures, including the fragmentation of the software stack, the need for more efficient algorithms (including reduced precision versions) tailored for GPUs, and the importance of developing standardized libraries and programming models.",
      "paper_authors": [
        "Ravindra Shinde",
        "Claudia Filippi",
        "Anthony Scemama",
        "William Jalby"
      ],
      "primary_category": "physics.comp-ph",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "20 pages, 4 figures; Submitted to Nature Reviews Physics",
      "repo_url": "#"
    },
    "2409.11864": {
      "paper_id": "2409.11864v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11864v1",
      "paper_key": "2409.11864",
      "paper_title": "Motivations, Challenges, Best Practices, and Benefits for Bots and Conversational Agents in Software Engineering: A Multivocal Literature Review",
      "paper_url": "http://arxiv.org/abs/2409.11864v1",
      "paper_abstract": "Bots are software systems designed to support users by automating a specific process, task, or activity. When such systems implement a conversational component to interact with the users, they are also known as conversational agents. Bots, particularly in their conversation-oriented version and AI-powered, have seen their adoption increase over time for software development and engineering purposes. Despite their exciting potential, ulteriorly enhanced by the advent of Generative AI and Large Language Models, bots still need to be improved to develop and integrate into the development cycle since practitioners report that bots add additional challenges that may worsen rather than improve. In this work, we aim to provide a taxonomy for characterizing bots, as well as a series of challenges for their adoption for Software Engineering associated with potential mitigation strategies. To reach our objectives, we conducted a multivocal literature review, reviewing both research and practitioner's literature. Through such an approach, we hope to contribute to both researchers and practitioners by providing first, a series of future research routes to follow, second, a list of strategies to adopt for improving the use of bots for software engineering purposes, and third, enforce a technology and knowledge transfer from the research field to the practitioners one, that is one of the primary goal of multivocal literature reviews.",
      "paper_authors": [
        "Stefano Lambiase",
        "Gemma Catolino",
        "Fabio Palomba",
        "Filomena Ferrucci"
      ],
      "primary_category": "cs.SE",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": null,
      "repo_url": "#"
    },
    "2409.11845": {
      "paper_id": "2409.11845v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11845v1",
      "paper_key": "2409.11845",
      "paper_title": "Law-based and standards-oriented approach for privacy impact assessment in medical devices: a topic for lawyers, engineers and healthcare practitioners in MedTech",
      "paper_url": "http://arxiv.org/abs/2409.11845v1",
      "paper_abstract": "Background: The integration of the General Data Protection Regulation (GDPR) and the Medical Device Regulation (MDR) creates complexities in conducting Data Protection Impact Assessments (DPIAs) for medical devices. The adoption of non-binding standards like ISO and IEC can harmonize these processes by enhancing accountability and privacy by design. Methods: This study employs a multidisciplinary literature review, focusing on GDPR and MDR intersection in medical devices that process personal health data. It evaluates key standards, including ISO/IEC 29134 and IEC 62304, to propose a unified approach for DPIAs that aligns with legal and technical frameworks. Results: The analysis reveals the benefits of integrating ISO/IEC standards into DPIAs, which provide detailed guidance on implementing privacy by design, risk assessment, and mitigation strategies specific to medical devices. The proposed framework ensures that DPIAs are living documents, continuously updated to adapt to evolving data protection challenges. Conclusions: A unified approach combining European Union (EU) regulations and international standards offers a robust framework for conducting DPIAs in medical devices. This integration balances security, innovation, and privacy, enhancing compliance and fostering trust in medical technologies. The study advocates for leveraging both hard law and standards to systematically address privacy and safety in the design and operation of medical devices, thereby raising the maturity of the MedTech ecosystem.",
      "paper_authors": [
        "Yuri R. Ladeia",
        "David M. Pereira"
      ],
      "primary_category": "cs.CY",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "20 pages, 1 table",
      "repo_url": "#"
    },
    "2409.11818": {
      "paper_id": "2409.11818v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11818v1",
      "paper_key": "2409.11818",
      "paper_title": "Multimessenger astronomy",
      "paper_url": "http://arxiv.org/abs/2409.11818v1",
      "paper_abstract": "This brief review is based on a lecture given by one of the authors at the international youth conference AYSS-2023. It is devoted to multimessenger astronomy, which studies astrophysical objects and phenomena using various particles and waves that bring information from space. The messengers include electromagnetic and gravitational waves, neutrinos, and cosmic rays. We discuss new opportunities that open up with the combined use of several carriers of information. Combination of data obtained through various observation channels allows one to obtain more complete and accurate information about the processes occurring in the Universe, and even to use it for studying fundamental physics.",
      "paper_authors": [
        "V. Rozhkov",
        "S. Troitsky"
      ],
      "primary_category": "astro-ph.HE",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "14 pages, Lecture at AYSS-2023, to be published in Physics of\n  Particles and Nuclei",
      "repo_url": "#"
    },
    "2409.11810": {
      "paper_id": "2409.11810v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11810v1",
      "paper_key": "2409.11810",
      "paper_title": "Optical Label-Free Microscopy Characterization of Dielectric Nanoparticles",
      "paper_url": "http://arxiv.org/abs/2409.11810v1",
      "paper_abstract": "In order to relate nanoparticle properties to function, fast and detailed particle characterization, is needed. The ability to characterize nanoparticle samples using optical microscopy techniques has drastically improved over the past few decades; consequently, there are now numerous microscopy methods available for detailed characterization of particles with nanometric size. However, there is currently no ``one size fits all'' solution to the problem of nanoparticle characterization. Instead, since the available techniques have different detection limits and deliver related but different quantitative information, the measurement and analysis approaches need to be selected and adapted for the sample at hand. In this tutorial, we review the optical theory of single particle scattering and how it relates to the differences and similarities in the quantitative particle information obtained from commonly used microscopy techniques, with an emphasis on nanometric (submicron) sized dielectric particles. Particular emphasis is placed on how the optical signal relates to mass, size, structure, and material properties of the detected particles and to its combination with diffusivity-based particle sizing. We also discuss emerging opportunities in the wake of new technology development, with the ambition to guide the choice of measurement strategy based on various challenges related to different types of nanoparticle samples and associated analytical demands.",
      "paper_authors": [
        "Berenice Garcia Rodriguez",
        "Erik Ols\u00e9n",
        "Fredrik Sk\u00e4rberg",
        "Giovanni Volpe",
        "Fredrik H\u00f6\u00f6k",
        "Daniel Sund\u00e5s Midtvedt"
      ],
      "primary_category": "physics.optics",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": null,
      "repo_url": "#"
    },
    "2409.11778": {
      "paper_id": "2409.11778v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11778v1",
      "paper_key": "2409.11778",
      "paper_title": "From Group Psychology to Software Engineering Research to Automotive R&D: Measuring Team Development at Volvo Cars",
      "paper_url": "http://arxiv.org/abs/2409.11778v1",
      "paper_abstract": "From 2019 to 2022, Volvo Cars successfully translated our research discoveries regarding group dynamics within agile teams into widespread industrial practice. We wish to illuminate the insights gained through the process of garnering support, providing training, executing implementation, and sustaining a tool embraced by approximately 700 teams and 9,000 employees. This tool was designed to empower agile teams and propel their internal development. Our experiences underscore the necessity of comprehensive team training, the cultivation of a cadre of trainers across the organization, and the creation of a novel software solution. In essence, we deduce that an automated concise survey tool, coupled with a repository of actionable strategies, holds remarkable potential in fostering the maturation of agile teams, but we also share many of the challenges we encountered during the implementation.",
      "paper_authors": [
        "Lucas Gren",
        "Christian Jacobsson"
      ],
      "primary_category": "cs.SE",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": null,
      "repo_url": "#"
    },
    "2409.11756": {
      "paper_id": "2409.11756v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11756v1",
      "paper_key": "2409.11756",
      "paper_title": "Synthesizing Evolving Symbolic Representations for Autonomous Systems",
      "paper_url": "http://arxiv.org/abs/2409.11756v1",
      "paper_abstract": "Recently, AI systems have made remarkable progress in various tasks. Deep Reinforcement Learning(DRL) is an effective tool for agents to learn policies in low-level state spaces to solve highly complex tasks. Researchers have introduced Intrinsic Motivation(IM) to the RL mechanism, which simulates the agent's curiosity, encouraging agents to explore interesting areas of the environment. This new feature has proved vital in enabling agents to learn policies without being given specific goals. However, even though DRL intelligence emerges through a sub-symbolic model, there is still a need for a sort of abstraction to understand the knowledge collected by the agent. To this end, the classical planning formalism has been used in recent research to explicitly represent the knowledge an autonomous agent acquires and effectively reach extrinsic goals. Despite classical planning usually presents limited expressive capabilities, PPDDL demonstrated usefulness in reviewing the knowledge gathered by an autonomous system, making explicit causal correlations, and can be exploited to find a plan to reach any state the agent faces during its experience. This work presents a new architecture implementing an open-ended learning system able to synthesize from scratch its experience into a PPDDL representation and update it over time. Without a predefined set of goals and tasks, the system integrates intrinsic motivations to explore the environment in a self-directed way, exploiting the high-level knowledge acquired during its experience. The system explores the environment and iteratively: (a) discover options, (b) explore the environment using options, (c) abstract the knowledge collected and (d) plan. This paper proposes an alternative approach to implementing open-ended learning architectures exploiting low-level and high-level representations to extend its knowledge in a virtuous loop.",
      "paper_authors": [
        "Gabriele Sartor",
        "Angelo Oddi",
        "Riccardo Rasconi",
        "Vieri Giuliano Santucci",
        "Rosa Meo"
      ],
      "primary_category": "cs.AI",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": null,
      "repo_url": "https://github.com/gabrielesartor/discover_plan_act"
    },
    "2409.11736": {
      "paper_id": "2409.11736v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11736v1",
      "paper_key": "2409.11736",
      "paper_title": "Benchmarking the spectroscopic masses of 249 evolved stars using asteroseismology with TESS",
      "paper_url": "http://arxiv.org/abs/2409.11736v1",
      "paper_abstract": "One way to understand planet formation is through studying the correlations between planet occurrence rates and stellar mass. However, measuring stellar mass in the red giant regime is very difficult. In particular, the spectroscopic masses of certain evolved stars, often referred to as \"retired A-stars\", have been questioned in the literature. Efforts to resolve this mass controversy using spectroscopy, interferometry and asteroseismology have so far been inconclusive. A recent ensemble study found a mass-dependent mass offset, but the result was based on only 16 stars. With NASA's Transiting Exoplanet Survey Satellite (TESS), we expand the investigation of the mass discrepancy to a total of 92 low-luminosity stars, synonymous with the retired A-stars. We measure their characteristic oscillation frequency, $\\mathrm{\\nu}_{\\mathrm{max}}$, and the large frequency separation, $\\mathrm{\\Delta\\nu}$, from their TESS photometric time series. Using these measurements and asteroseismic scaling relations, we derive asteroseismic masses and compare them with spectroscopic masses from five surveys, to comprehensively study the alleged mass-dependent mass offset. We find a mass offset between spectroscopy and seismology that increases with stellar mass. However, we note that adopting the seismic mass scale does not have a significant effect on the planet occurrence-mass-metallicity correlation for the so-called retired A-stars. We also report seismic measurements and masses for 157 higher luminosity giants (mostly helium-core-burning) from the spectroscopic surveys.",
      "paper_authors": [
        "Sai Prathyusha Malla",
        "Dennis Stello",
        "Benjamin T. Monet",
        "Daniel Huber",
        "Marc Hon",
        "Timothy R. Bedding",
        "Claudia Reyes",
        "Daniel R. Hey"
      ],
      "primary_category": "astro-ph.SR",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "13 pages, 7 figues",
      "repo_url": "#"
    },
    "2409.11729": {
      "paper_id": "2409.11729v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11729v1",
      "paper_key": "2409.11729",
      "paper_title": "DETECLAP: Enhancing Audio-Visual Representation Learning with Object Information",
      "paper_url": "http://arxiv.org/abs/2409.11729v1",
      "paper_abstract": "Current audio-visual representation learning can capture rough object categories (e.g., ``animals'' and ``instruments''), but it lacks the ability to recognize fine-grained details, such as specific categories like ``dogs'' and ``flutes'' within animals and instruments. To address this issue, we introduce DETECLAP, a method to enhance audio-visual representation learning with object information. Our key idea is to introduce an audio-visual label prediction loss to the existing Contrastive Audio-Visual Masked AutoEncoder to enhance its object awareness. To avoid costly manual annotations, we prepare object labels from both audio and visual inputs using state-of-the-art language-audio models and object detectors. We evaluate the method of audio-visual retrieval and classification using the VGGSound and AudioSet20K datasets. Our method achieves improvements in recall@10 of +1.5% and +1.2% for audio-to-visual and visual-to-audio retrieval, respectively, and an improvement in accuracy of +0.6% for audio-visual classification.",
      "paper_authors": [
        "Shota Nakada",
        "Taichi Nishimura",
        "Hokuto Munakata",
        "Masayoshi Kondo",
        "Tatsuya Komatsu"
      ],
      "primary_category": "cs.MM",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "under review",
      "repo_url": "#"
    },
    "2409.11712": {
      "paper_id": "2409.11712v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11712v1",
      "paper_key": "2409.11712",
      "paper_title": "Revolutionizing Pharmaceutical Manufacturing: Advances and Challenges of 3D Printing System and Control",
      "paper_url": "http://arxiv.org/abs/2409.11712v1",
      "paper_abstract": "The advent of 3D printing has transformed the pharmaceutical industry, enabling precision drug manufacturing with controlled release profiles, dosing, and structural complexity. Additive manufacturing (AM) addresses the growing demand for personalized medicine, overcoming limitations of traditional methods. This technology facilitates tailored dosage forms, complex geometries, and real-time quality control. Recent advancements in drop-on-demand printing, UV curable inks, material science, and regulatory frameworks are discussed. Despite opportunities for cost reduction, flexibility, and decentralized manufacturing, challenges persist in scalability, reproducibility, and regulatory adaptation. This review provides an in-depth analysis of the current state of AM in pharmaceutical manufacturing, exploring recent developments, challenges, and future directions for mainstream integration.",
      "paper_authors": [
        "Rahul Kumar",
        "Vikram Singh",
        "Priya Gupta"
      ],
      "primary_category": "eess.SY",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "submitted to IMECE 2024",
      "repo_url": "#"
    },
    "2409.11708": {
      "paper_id": "2409.11708v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11708v1",
      "paper_key": "2409.11708",
      "paper_title": "Accretion Disc Outbursts and Stability Analysis",
      "paper_url": "http://arxiv.org/abs/2409.11708v1",
      "paper_abstract": "Accretion disc outbursts are re-occurring events observed in various astrophysical systems, including X-ray binaries and cataclysmic variables. These outbursts are characterized by a sudden increase in luminosity due to various instabilities in the accretion disc. We need to investigate the time-dependent accretion flow models to understand the mechanisms driving these outbursts. Time-dependent models incorporate the disc's time evolution and can capture the build-up of instabilities. This review aims to give a basic overview of accretion disc outburst and stability analysis. The paper highlights the necessity of considering the hierarchy of different timescales, dynamical, viscous, and thermal, when investigating the instabilities occurring in the accretion disc. The importance and observational implications of studying these accretion disc outbursts are also discussed.",
      "paper_authors": [
        "Liza Devi",
        "Asish Jyoti Boruah",
        "Biplob Sarkar"
      ],
      "primary_category": "astro-ph.HE",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "4 pages, 1 figure, The manuscript is accepted for publication as a\n  Conference paper in National Conference \"Physics Frontiers: Bridging Theory\n  and Experiment-2024\" organized by Bhawanipur Anchalik College, Assam, India",
      "repo_url": "#"
    },
    "2409.11683": {
      "paper_id": "2409.11683v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11683v1",
      "paper_key": "2409.11683",
      "paper_title": "k-mer-based approaches to bridging pangenomics and population genetics",
      "paper_url": "http://arxiv.org/abs/2409.11683v1",
      "paper_abstract": "Many commonly studied species now have more than one chromosome-scale genome assembly, revealing a large amount of genetic diversity previously missed by approaches that map short reads to a single reference. However, many species still lack multiple reference genomes and correctly aligning references to build pangenomes is challenging, limiting our ability to study this missing genomic variation in population genetics. Here, we argue that $k$-mers are a crucial stepping stone to bridging the reference-focused paradigms of population genetics with the reference-free paradigms of pangenomics. We review current literature on the uses of $k$-mers for performing three core components of most population genetics analyses: identifying, measuring, and explaining patterns of genetic variation. We also demonstrate how different $k$-mer-based measures of genetic variation behave in population genetic simulations according to the choice of $k$, depth of sequencing coverage, and degree of data compression. Overall, we find that $k$-mer-based measures of genetic diversity scale consistently with pairwise nucleotide diversity ($\\pi$) up to values of about $\\pi = 0.025$ ($R^2 = 0.97$) for neutrally evolving populations. For populations with even more variation, using shorter $k$-mers will maintain the scalability up to at least $\\pi = 0.1$. Furthermore, in our simulated populations, $k$-mer dissimilarity values can be reliably approximated from counting bloom filters, highlighting a potential avenue to decreasing the memory burden of $k$-mer based genomic dissimilarity analyses. For future studies, there is a great opportunity to further develop methods to identifying selected loci using $k$-mers.",
      "paper_authors": [
        "Miles D. Roberts",
        "Olivia Davis",
        "Emily B. Josephs",
        "Robert J. Williamson"
      ],
      "primary_category": "q-bio.PE",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "6 figures, 34 pages",
      "repo_url": "#"
    },
    "2409.11681": {
      "paper_id": "2409.11681v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11681v1",
      "paper_key": "2409.11681",
      "paper_title": "Gradient-Driven 3D Segmentation and Affordance Transfer in Gaussian Splatting Using 2D Masks",
      "paper_url": "http://arxiv.org/abs/2409.11681v1",
      "paper_abstract": "3D Gaussian Splatting has emerged as a powerful 3D scene representation technique, capturing fine details with high efficiency. In this paper, we introduce a novel voting-based method that extends 2D segmentation models to 3D Gaussian splats. Our approach leverages masked gradients, where gradients are filtered by input 2D masks, and these gradients are used as votes to achieve accurate segmentation. As a byproduct, we discovered that inference-time gradients can also be used to prune Gaussians, resulting in up to 21% compression. Additionally, we explore few-shot affordance transfer, allowing annotations from 2D images to be effectively transferred onto 3D Gaussian splats. The robust yet straightforward mathematical formulation underlying this approach makes it a highly effective tool for numerous downstream applications, such as augmented reality (AR), object editing, and robotics. The project code and additional resources are available at https://jojijoseph.github.io/3dgs-segmentation.",
      "paper_authors": [
        "Joji Joseph",
        "Bharadwaj Amrutur",
        "Shalabh Bhatnagar"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "Preprint, Under review for ICRA 2025",
      "repo_url": "https://github.com/JojiJoseph/3dgs-gradient-segmentation"
    },
    "2409.11668": {
      "paper_id": "2409.11668v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11668v2",
      "paper_key": "2409.11668",
      "paper_title": "WALLABY Pilot Survey: HI source-finding with a machine learning framework",
      "paper_url": "http://arxiv.org/abs/2409.11668v2",
      "paper_abstract": "The data volumes generated by the WALLABY atomic Hydrogen (HI) survey using the Australiian Square Kilometre Array Pathfinder (ASKAP) necessitate greater automation and reliable automation in the task of source-finding and cataloguing. To this end, we introduce and explore a novel deep learning framework for detecting low Signal-to-Noise Ratio (SNR) HI sources in an automated fashion. Specfically, our proposed method provides an automated process for separating true HI detections from false positives when used in combination with the Source Finding Application (SoFiA) output candidate catalogues. Leveraging the spatial and depth capabilities of 3D Convolutional Neural Networks (CNNs), our method is specifically designed to recognise patterns and features in three-dimensional space, making it uniquely suited for rejecting false positive sources in low SNR scenarios generated by conventional linear methods. As a result, our approach is significantly more accurate in source detection and results in considerably fewer false detections compared to previous linear statistics-based source finding algorithms. Performance tests using mock galaxies injected into real ASKAP data cubes reveal our method's capability to achieve near-100% completeness and reliability at a relatively low integrated SNR~3-5. An at-scale version of this tool will greatly maximise the science output from the upcoming widefield HI surveys.",
      "paper_authors": [
        "Li Wang",
        "O. Ivy Wong",
        "Tobias Westmeier",
        "Chandrashekar Murugeshan",
        "Karen Lee-Waddell",
        "Yuanzhi. Cai",
        "Xiu. Liu",
        "Austin Xiaofan Shen",
        "Jonghwan Rhee",
        "Helga D\u00e9nes",
        "Nathan Deg",
        "Peter Kamphuis",
        "Barbara Catinella"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-19",
      "comments": "14 pages, 12 figures, accepted for publication in the Publications of\n  the Astronomical Society of Australia",
      "repo_url": "#"
    },
    "2409.11653": {
      "paper_id": "2409.11653v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11653v1",
      "paper_key": "2409.11653",
      "paper_title": "Enhancing Semi-Supervised Learning via Representative and Diverse Sample Selection",
      "paper_url": "http://arxiv.org/abs/2409.11653v1",
      "paper_abstract": "Semi-Supervised Learning (SSL) has become a preferred paradigm in many deep learning tasks, which reduces the need for human labor. Previous studies primarily focus on effectively utilising the labelled and unlabeled data to improve performance. However, we observe that how to select samples for labelling also significantly impacts performance, particularly under extremely low-budget settings. The sample selection task in SSL has been under-explored for a long time. To fill in this gap, we propose a Representative and Diverse Sample Selection approach (RDSS). By adopting a modified Frank-Wolfe algorithm to minimise a novel criterion $\\alpha$-Maximum Mean Discrepancy ($\\alpha$-MMD), RDSS samples a representative and diverse subset for annotation from the unlabeled data. We demonstrate that minimizing $\\alpha$-MMD enhances the generalization ability of low-budget learning. Experimental results show that RDSS consistently improves the performance of several popular SSL frameworks and outperforms the state-of-the-art sample selection approaches used in Active Learning (AL) and Semi-Supervised Active Learning (SSAL), even with constrained annotation budgets.",
      "paper_authors": [
        "Qian Shao",
        "Jiangrui Kang",
        "Qiyuan Chen",
        "Zepeng Li",
        "Hongxia Xu",
        "Yiwen Cao",
        "Jiajuan Liang",
        "Jian Wu"
      ],
      "primary_category": "cs.LG",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "Under Review",
      "repo_url": "#"
    },
    "2409.11649": {
      "paper_id": "2409.11649v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11649v2",
      "paper_key": "2409.11649",
      "paper_title": "Second-Order Constrained Dynamic Optimization",
      "paper_url": "http://arxiv.org/abs/2409.11649v2",
      "paper_abstract": "This paper provides an overview, analysis, and comparison of second-order dynamic optimization algorithms, i.e., constrained Differential Dynamic Programming (DDP) and Sequential Quadratic Programming (SQP). Although a variety of these algorithms has been proposed and used successfully, there exists a gap in understanding the key differences and advantages, which we aim to provide in this work. For constrained DDP, we choose methods that incorporate nonlinear programming techniques to handle state and control constraints, including Augmented Lagrangian (AL), Interior Point, Primal Dual Augmented Lagrangian (PDAL), and Alternating Direction Method of Multipliers. Both DDP and SQP are provided in single- and multiple-shooting formulations, where constraints that arise from dynamics are encoded implicitly and explicitly, respectively. As a byproduct of the review, we also propose a single-shooting PDAL DDP which is robust to the growth of penalty parameters and performs better than the normal AL variant. We perform extensive numerical experiments on various systems with increasing complexity to investigate the quality of the solutions, the levels of constraint violation, iterations for convergence, and the sensitivity of final solutions with respect to initialization. The results show that DDP often has the advantage of finding better local minima, while SQP tends to achieve better constraint satisfaction. For multiple-shooting formulation, both DDP and SQP can enjoy informed initial guesses, while the latter appears to be more advantageous in complex systems. It is also worth highlighting that DDP provides favorable computational complexity and feedback gains as a byproduct of optimization.",
      "paper_authors": [
        "Yuichiro Aoyama",
        "Oswin So",
        "Augustinos D. Saravanos",
        "Evangelos A. Theodorou"
      ],
      "primary_category": "math.OC",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-20",
      "comments": "31 pages, 11 figures",
      "repo_url": "#"
    },
    "2409.11636": {
      "paper_id": "2409.11636v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11636v1",
      "paper_key": "2409.11636",
      "paper_title": "\"A Woman is More Culturally Knowledgeable than A Man?\": The Effect of Personas on Cultural Norm Interpretation in LLMs",
      "paper_url": "http://arxiv.org/abs/2409.11636v1",
      "paper_abstract": "As the deployment of large language models (LLMs) expands, there is an increasing demand for personalized LLMs. One method to personalize and guide the outputs of these models is by assigning a persona -- a role that describes the expected behavior of the LLM (e.g., a man, a woman, an engineer). This study investigates whether an LLM's understanding of social norms varies across assigned personas. Ideally, the perception of a social norm should remain consistent regardless of the persona, since acceptability of a social norm should be determined by the region the norm originates from, rather than by individual characteristics such as gender, body size, or race. A norm is universal within its cultural context. In our research, we tested 36 distinct personas from 12 sociodemographic categories (e.g., age, gender, beauty) across four different LLMs. We find that LLMs' cultural norm interpretation varies based on the persona used and the norm interpretation also varies within a sociodemographic category (e.g., a fat person and a thin person as in physical appearance group) where an LLM with the more socially desirable persona (e.g., a thin person) interprets social norms more accurately than with the less socially desirable persona (e.g., a fat person). We also discuss how different types of social biases may contribute to the results that we observe.",
      "paper_authors": [
        "Mahammed Kamruzzaman",
        "Hieu Nguyen",
        "Nazmul Hassan",
        "Gene Louis Kim"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "Preprint, Under Review",
      "repo_url": "#"
    },
    "2409.11635": {
      "paper_id": "2409.11635v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11635v1",
      "paper_key": "2409.11635",
      "paper_title": "PainDiffusion: Can robot express pain?",
      "paper_url": "http://arxiv.org/abs/2409.11635v1",
      "paper_abstract": "Pain is a more intuitive and user-friendly way of communicating problems, making it especially useful in rehabilitation nurse training robots. While most previous methods have focused on classifying or recognizing pain expressions, these approaches often result in unnatural, jiggling robot faces. We introduce PainDiffusion, a model that generates facial expressions in response to pain stimuli, with controllable pain expressiveness and emotion status. PainDiffusion leverages diffusion forcing to roll out predictions over arbitrary lengths using a conditioned temporal U-Net. It operates as a latent diffusion model within EMOCA's facial expression latent space, ensuring a compact data representation and quick rendering time. For training data, we process the BioVid Heatpain Database, extracting expression codes and subject identity configurations. We also propose a novel set of metrics to evaluate pain expressions, focusing on expressiveness, diversity, and the appropriateness of model-generated outputs. Finally, we demonstrate that PainDiffusion outperforms the autoregressive method, both qualitatively and quantitatively. Code, videos, and further analysis are available at: \\href{https://damtien444.github.io/paindf/}{https://damtien444.github.io/paindf/}.",
      "paper_authors": [
        "Quang Tien Dam",
        "Tri Tung Nguyen Nguyen",
        "Dinh Tuan Tran",
        "Joo-Ho Lee"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "Under reviewing",
      "repo_url": "#"
    },
    "2409.11613": {
      "paper_id": "2409.11613v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11613v1",
      "paper_key": "2409.11613",
      "paper_title": "Reionization relics in the cross-correlation between the Ly$\u03b1$ forest and 21 cm intensity mapping in the post-reionization era",
      "paper_url": "http://arxiv.org/abs/2409.11613v1",
      "paper_abstract": "The tumultuous effects of ultraviolet photons that source cosmic reionization, the subsequent compression and shock-heating of low-density regions, and the modulation of baryons in shallow potential wells induced by the passage of ionization fronts, collectively introduce perturbations to the evolution of the intergalactic medium in the post-reionization era. These enduring fluctuations persist deep into the post-reionization era, casting a challenge upon precision cosmology endeavors targeting tracers in this cosmic era. Simultaneously, these relics from reionization also present a unique opportunity to glean insights into the astrophysics that govern the epoch of reionization. In this work, we propose a first study of the cross-correlation of \\lya forest and 21 cm intensity mapping, accounting for the repercussions of inhomogeneous reionization in the post-reionization era. We investigate the ability of SKA $\\times$ DESI-like, SKA $\\times$ MUST-like, and PUMA $\\times$ MUST-like instrumental setups to achieve a high signal-to-noise ratio (SNR) in the redshift range $3.5 \\leq z \\leq 4$. Moreover, we assess how alterations in integration time, survey area, and reionization scenarios impact the SNR. Furthermore, we forecast the cross-correlation's potential to constrain cosmological parameters under varying assumptions: considering or disregarding reionization relics, marginalizing over reionization astrophysics, and assuming perfect knowledge of reionization. Notably, our findings underscore the remarkable capability of a futuristic PUMA $\\times$ MUST-like setup, with a modest 100-hour integration time over a 100 sq. deg. survey, to constrain the ionization efficiency error to $\\sigma_\\zeta = 3.42 $.",
      "paper_authors": [
        "Paulo Montero-Camacho",
        "Catalina Morales-Guti\u00e9rrez",
        "Yao Zhang",
        "Heyang Long",
        "Yi Mao"
      ],
      "primary_category": "astro-ph.CO",
      "publish_time": "2024-09-18",
      "update_time": "2024-09-18",
      "comments": "Comments welcome! (16 pages, 10 figures)",
      "repo_url": "#"
    },
    "2409.11564": {
      "paper_id": "2409.11564v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11564v1",
      "paper_key": "2409.11564",
      "paper_title": "Preference Tuning with Human Feedback on Language, Speech, and Vision Tasks: A Survey",
      "paper_url": "http://arxiv.org/abs/2409.11564v1",
      "paper_abstract": "Preference tuning is a crucial process for aligning deep generative models with human preferences. This survey offers a thorough overview of recent advancements in preference tuning and the integration of human feedback. The paper is organized into three main sections: 1) introduction and preliminaries: an introduction to reinforcement learning frameworks, preference tuning tasks, models, and datasets across various modalities: language, speech, and vision, as well as different policy approaches, 2) in-depth examination of each preference tuning approach: a detailed analysis of the methods used in preference tuning, and 3) applications, discussion, and future directions: an exploration of the applications of preference tuning in downstream tasks, including evaluation methods for different modalities, and an outlook on future research directions. Our objective is to present the latest methodologies in preference tuning and model alignment, enhancing the understanding of this field for researchers and practitioners. We hope to encourage further engagement and innovation in this area.",
      "paper_authors": [
        "Genta Indra Winata",
        "Hanyang Zhao",
        "Anirban Das",
        "Wenpin Tang",
        "David D. Yao",
        "Shi-Xiong Zhang",
        "Sambit Sahu"
      ],
      "primary_category": "cs.CL",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "Survey paper",
      "repo_url": "#"
    },
    "2409.11544": {
      "paper_id": "2409.11544v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11544v1",
      "paper_key": "2409.11544",
      "paper_title": "Interplay of Electron Trapping by Defect Midgap State and Quantum Confinement to Optimize Hot Carrier Effect in a Nanowire Structure",
      "paper_url": "http://arxiv.org/abs/2409.11544v1",
      "paper_abstract": "Hot carrier effect, a phenomenon where charge carriers generated by photon absorption remain energetic by not losing much energy, has been one of the leading strategies in increasing solar cell efficiency. Nanostructuring offers an effective approach to enhance hot carrier effect via the spatial confinement, as occurring in a nanowire structure. The recent experimental study by Esmaielpour et al. [ACS Applied Nano Materials 7, 2817 (2024)] reveals a fascinating non-monotonic dependence of the hot carrier effect in nanowire array on the diameter of the nanowire, contrary to what might be expected from quantum confinement alone. We show that this non-monotonic behavior can be explained by a simple model for electron energy loss that involves two principal mechanisms. First, electron-phonon scattering, that increases with nanowire diameter, leading to hot carrier effect that decreases with increasing diameter. Second, electron capture by a defect level within band gap, that is, a midgap state, that decreases with nanowire diameter, leading to hot carrier effect that increases with increasing diameter. The two mechanisms balance at a certain diameter corresponding to optimal hot carrier effect. Our result offers a guideline to optimize hot carrier effect in nanowire solar cells and ultimately their efficiency by adjusting the dimensions and micro-structural properties of nanowires.",
      "paper_authors": [
        "Imam Makhfudz",
        "Hamidreza Esmaielpour",
        "Yaser Hajati",
        "Gregor Koblm\u00fcller",
        "Nicolas Cavassilas"
      ],
      "primary_category": "cond-mat.mes-hall",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "Main text + Supplementary materials, accepted, to appear in Physical\n  Review B Letter (2024)",
      "repo_url": "#"
    },
    "2409.11543": {
      "paper_id": "2409.11543v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11543v1",
      "paper_key": "2409.11543",
      "paper_title": "Noise-aware Dynamic Image Denoising and Positron Range Correction for Rubidium-82 Cardiac PET Imaging via Self-supervision",
      "paper_url": "http://arxiv.org/abs/2409.11543v1",
      "paper_abstract": "Rb-82 is a radioactive isotope widely used for cardiac PET imaging. Despite numerous benefits of 82-Rb, there are several factors that limits its image quality and quantitative accuracy. First, the short half-life of 82-Rb results in noisy dynamic frames. Low signal-to-noise ratio would result in inaccurate and biased image quantification. Noisy dynamic frames also lead to highly noisy parametric images. The noise levels also vary substantially in different dynamic frames due to radiotracer decay and short half-life. Existing denoising methods are not applicable for this task due to the lack of paired training inputs/labels and inability to generalize across varying noise levels. Second, 82-Rb emits high-energy positrons. Compared with other tracers such as 18-F, 82-Rb travels a longer distance before annihilation, which negatively affect image spatial resolution. Here, the goal of this study is to propose a self-supervised method for simultaneous (1) noise-aware dynamic image denoising and (2) positron range correction for 82-Rb cardiac PET imaging. Tested on a series of PET scans from a cohort of normal volunteers, the proposed method produced images with superior visual quality. To demonstrate the improvement in image quantification, we compared image-derived input functions (IDIFs) with arterial input functions (AIFs) from continuous arterial blood samples. The IDIF derived from the proposed method led to lower AUC differences, decreasing from 11.09% to 7.58% on average, compared to the original dynamic frames. The proposed method also improved the quantification of myocardium blood flow (MBF), as validated against 15-O-water scans, with mean MBF differences decreased from 0.43 to 0.09, compared to the original dynamic frames. We also conducted a generalizability experiment on 37 patient scans obtained from a different country using a different scanner.",
      "paper_authors": [
        "Huidong Xie",
        "Liang Guo",
        "Alexandre Velo",
        "Zhao Liu",
        "Qiong Liu",
        "Xueqi Guo",
        "Bo Zhou",
        "Xiongchao Chen",
        "Yu-Jung Tsai",
        "Tianshun Miao",
        "Menghua Xia",
        "Yi-Hwa Liu",
        "Ian S. Armstrong",
        "Ge Wang",
        "Richard E. Carson",
        "Albert J. Sinusas",
        "Chi Liu"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "15 Pages, 10 Figures, 5 tables. Paper Under review. Oral Presentation\n  at IEEE MIC 2023",
      "repo_url": "#"
    },
    "2409.11540": {
      "paper_id": "2409.11540v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11540v1",
      "paper_key": "2409.11540",
      "paper_title": "What Does ChatGPT Make of Historical Stock Returns? Extrapolation and Miscalibration in LLM Stock Return Forecasts",
      "paper_url": "http://arxiv.org/abs/2409.11540v1",
      "paper_abstract": "We examine how large language models (LLMs) interpret historical stock returns and compare their forecasts with estimates from a crowd-sourced platform for ranking stocks. While stock returns exhibit short-term reversals, LLM forecasts over-extrapolate, placing excessive weight on recent performance similar to humans. LLM forecasts appear optimistic relative to historical and future realized returns. When prompted for 80% confidence interval predictions, LLM responses are better calibrated than survey evidence but are pessimistic about outliers, leading to skewed forecast distributions. The findings suggest LLMs manifest common behavioral biases when forecasting expected returns but are better at gauging risks than humans.",
      "paper_authors": [
        "Shuaiyu Chen",
        "T. Clifton Green",
        "Huseyin Gulen",
        "Dexin Zhou"
      ],
      "primary_category": "q-fin.GN",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": null,
      "repo_url": "#"
    },
    "2409.11533": {
      "paper_id": "2409.11533v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11533v1",
      "paper_key": "2409.11533",
      "paper_title": "A search for persistent radio sources toward repeating fast radio bursts discovered by CHIME/FRB",
      "paper_url": "http://arxiv.org/abs/2409.11533v1",
      "paper_abstract": "The identification of persistent radio sources (PRSs) coincident with two repeating fast radio bursts (FRBs) supports FRB theories requiring a compact central engine. However, deep non-detections in other cases highlight the diversity of repeating FRBs and their local environments. Here, we perform a systematic search for radio sources towards 37 CHIME/FRB repeaters using their arcminute localizations and a combination of archival surveys and targeted observations. Through multi-wavelength analysis of individual radio sources, we identify two (20181030A-S1 and 20190417A-S1) for which we disfavor an origin of either star formation or an active galactic nucleus in their host galaxies and thus consider them candidate PRSs. We do not find any associated PRSs for the majority of the repeating FRBs in our sample. For 8 FRB fields with Very Large Array imaging, we provide deep limits on the presence of PRSs that are 2--4 orders of magnitude fainter than the PRS associated with FRB\\,20121102A. Using Very Large Array Sky Survey imaging of all 37 fields, we constrain the rate of luminous ($\\gtrsim$10$^{40}$ erg s$^{-1}$) PRSs associated with repeating FRBs to be low. Within the context of FRB-PRS models, we find that 20181030A-S1 and 20190417A-S1 can be reasonably explained within the context of magnetar, hypernebulae, gamma-ray burst afterglow, or supernova ejecta models -- although we note that both sources follow the radio luminosity versus rotation measure relationship predicted in the nebula model framework. Future observations will be required to both further characterize and confirm the association of these PRS candidates with the FRBs.",
      "paper_authors": [
        "Adaeze L. Ibik",
        "Maria R. Drout",
        "Bryan M. Gaensler",
        "Paul Scholz",
        "Navin Sridhar",
        "Ben Margalit",
        "Tracy E. Clarke",
        "Shriharsh P. Tendulkar",
        "Daniele Michilli",
        "Tarraneh Eftekhari",
        "Mohit Bhardwaj",
        "Sarah Burke-Spolaor",
        "Shami Chatterjee",
        "Amanda M. Cook",
        "Jason W. T. Hessels",
        "Franz Kirsten",
        "Ronniy C. Joseph",
        "Victoria M. Kaspi",
        "Mattias Lazda",
        "Kiyoshi W. Masui",
        "Kenzie Nimmo",
        "Ayush Pandhi",
        "Aaron B. Pearlman",
        "Ziggy Pleunis",
        "Masoud Rafiei-Ravandi",
        "Kaitlyn Shin",
        "Kendrick M. Smith"
      ],
      "primary_category": "astro-ph.HE",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "37 pages, 12 figures",
      "repo_url": "#"
    },
    "2409.11532": {
      "paper_id": "2409.11532v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11532v1",
      "paper_key": "2409.11532",
      "paper_title": "Enhancing the Reliability of LiDAR Point Cloud Sampling: A Colorization and Super-Resolution Approach Based on LiDAR-Generated Images",
      "paper_url": "http://arxiv.org/abs/2409.11532v1",
      "paper_abstract": "In recent years, Light Detection and Ranging (LiDAR) technology, a critical sensor in robotics and autonomous systems, has seen significant advancements. These improvements include enhanced resolution of point clouds and the capability to provide 360{\\deg} low-resolution images. These images encode various data such as depth, reflectivity, and near-infrared light within the pixels. However, an excessive density of points and conventional point cloud sampling can be counterproductive, particularly in applications such as LiDAR odometry, where misleading points and degraded geometry information may induce drift errors. Currently, extensive research efforts are being directed towards leveraging LiDAR-generated images to improve situational awareness. This paper presents a comprehensive review of current deep learning (DL) techniques, including colorization and super-resolution, which are traditionally utilized in conventional computer vision tasks. These techniques are applied to LiDAR-generated images and are analyzed qualitatively. Based on this analysis, we have developed a novel approach that selectively integrates the most suited colorization and super-resolution methods with LiDAR imagery to sample reliable points from the LiDAR point cloud. This approach aims to not only improve the accuracy of point cloud registration but also avoid mismatching caused by lacking geometry information, thereby augmenting the utility and precision of LiDAR systems in practical applications. In our evaluation, the proposed approach demonstrates superior performance compared to our previous work, achieving lower translation and rotation errors with a reduced number of points.",
      "paper_authors": [
        "Sier Ha",
        "Honghao Du",
        "Xianjia Yu",
        "Jian Song",
        "Tomi Westerlund"
      ],
      "primary_category": "cs.RO",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "9 pages",
      "repo_url": "#"
    },
    "2409.11485": {
      "paper_id": "2409.11485v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11485v1",
      "paper_key": "2409.11485",
      "paper_title": "Survey of Orion Disks with ALMA (SODA) III: Disks in wide binary systems in L1641 and L1647",
      "paper_url": "http://arxiv.org/abs/2409.11485v1",
      "paper_abstract": "Aims. The goal of this work is to comprehensively characterize the impact of stellar multiplicity on Class II disks in the L1641 and L1647 regions of Orion A (~1-3 Myr), part of the Survey of Orion Disks with ALMA (SODA). We characterize the protostellar multiplicity using the Atacama Large Millimeter/submillimeter Array (ALMA), the ESO-VISTA, and Hubble Space telescopes. The resulting sample of 65 multiple systems represents the largest catalogue of wide binary systems to date (projected separation >1000 AU), allowing a more robust statistical characterization of the evolution and properties of protoplanetary disks. Methods. The disk population was observed in continuum with ALMA at 225 GHz, with a median rms of 1.5 Mearth. Combining these data (resolution ~1.1arcsec ) with the ESO-VISTA near-infrared survey of the Orion A cloud (resolution ~0.7arcsec ), multiple systems are assembled and selected by an iterative inside-out search in projected separation (>1000 AU). Results. We identify 61 binary systems, 3 triple systems, and one quadruple system. The separation range is between 1000 and 10^4 AU. The dust mass distributions inferred with the Kaplan-Meier estimator yield a median mass of 3.23+0.6-0.4 Mearth for primary disks and 3.88+0.3-0.3 Mearth for secondary disks.",
      "paper_authors": [
        "Giulia Ricciardi",
        "Sierk E. van Terwisga",
        "Veronica Roccatagliata",
        "Alvaro Hacar",
        "Thomas Henning",
        "Walter Del Pozzo"
      ],
      "primary_category": "astro-ph.EP",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "18 pages, 30 figures, comments welcome",
      "repo_url": "#"
    },
    "2409.11482": {
      "paper_id": "2409.11482v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11482v1",
      "paper_key": "2409.11482",
      "paper_title": "Female representation across mythologies",
      "paper_url": "http://arxiv.org/abs/2409.11482v1",
      "paper_abstract": "Social groups have been studied throughout history to understand how different configurations impact those within them. Along with this came the interest in investigating social groups of both fictional and mythological works. Over the last decade these social groups have been studied through the lens of network science allowing for a new level of comparison between these stories. We use this approach to focus on the attributes of the characters within these networks, specifically looking at their gender. With this we review how the female populations within various narratives and to some extent the societies they are based in are portrayed. Through this we find that although there is not a trend of all narratives of the same origin having similar levels of representation some are noticeably better than others. We also observe which narratives overall prioritise important female characters and which do not.",
      "paper_authors": [
        "M. Janickyj",
        "P. MacCarron",
        "J. Yose",
        "R. Kenna"
      ],
      "primary_category": "physics.soc-ph",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "12 pages, 4 tables",
      "repo_url": "#"
    },
    "2409.11481": {
      "paper_id": "2409.11481v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11481v1",
      "paper_key": "2409.11481",
      "paper_title": "The MDW H\u03b1 Sky Survey: Data Release 0",
      "paper_url": "http://arxiv.org/abs/2409.11481v1",
      "paper_abstract": "The Mittelman-di Cicco-Walker (MDW) H$\\alpha$ Sky Survey is an autonomously-operated and ongoing all-sky imaging survey in the narrowband H$\\alpha$ wavelength. The survey was founded by amateur astronomers, and is presented here in its first stage of refinement for rigorous scientific use. Each field is exposed through an H$\\alpha$ filter with a 3nm bandwidth for a total of four hours, with a pixel scale of 3.2 arcsec. Here, we introduce the first Data Release of the MDW H$\\alpha$ Survey (Data Release 0, or DR0), spanning 238 fields in the region of Orion (~3100 deg$^2$). DR0 includes: calibrated mean fields, star-removed mean fields, a point source catalog matched to Data Release 1 of the Panoramic Survey Telescope and Rapid Response System (Pan-STARRS1) and the INT Galactic Plane Survey (IGAPS), and mosaics.",
      "paper_authors": [
        "Noor Aftab",
        "Xunhe",
        "Zhang",
        "David R. Mittelman",
        "Dennis di Cicco",
        "Sean Walker",
        "David H. Sliski",
        "Julia Homa",
        "Colin Holm-Hansen",
        "Mary Putman",
        "David Schiminovich",
        "Arne Henden",
        "Gary Walker"
      ],
      "primary_category": "astro-ph.IM",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": null,
      "repo_url": "#"
    },
    "2409.11478": {
      "paper_id": "2409.11478v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11478v1",
      "paper_key": "2409.11478",
      "paper_title": "Hyperboloidal Approach to Quasinormal Modes",
      "paper_url": "http://arxiv.org/abs/2409.11478v1",
      "paper_abstract": "Oscillations of black hole spacetimes exhibit divergent behavior toward the bifurcation sphere and spatial infinity. This divergence can be understood as a consequence of the geometry in these spacetime regions. In contrast, black-hole oscillations are regular when evaluated toward the event horizon and null infinity. Hyperboloidal surfaces naturally connect these regions, providing a geometric regularization of time-harmonic oscillations called quasinormal modes (QNMs). This review traces the historical development of the hyperboloidal approach to QNMs. We discuss the physical motivation for the hyperboloidal approach and highlight current developments in the field.",
      "paper_authors": [
        "Rodrigo Panosso Macedo",
        "Anil Zenginoglu"
      ],
      "primary_category": "gr-qc",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "Mini-Review to topic collection \"Quasi-Normal Modes, Non-Selfadjoint\n  Operators and Pseudospectrum: an Interdisciplinary Approach\", Frontiers in\n  Physics",
      "repo_url": "#"
    },
    "2409.11476": {
      "paper_id": "2409.11476v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11476v1",
      "paper_key": "2409.11476",
      "paper_title": "A VLBI Calibrator Grid at 600MHz for Fast Radio Transient Localizations with CHIME/FRB Outriggers",
      "paper_url": "http://arxiv.org/abs/2409.11476v1",
      "paper_abstract": "The Canadian Hydrogen Intensity Mapping Experiment Fast Radio Burst (CHIME/FRB) Project has a new VLBI Outrigger at the Green Bank Observatory (GBO), which forms a 3300km baseline with CHIME operating at 400-800MHz. Using 100ms long full-array baseband \"snapshots\" collected commensally during FRB and pulsar triggers, we perform a shallow, wide-area VLBI survey covering a significant fraction of the Northern sky targeted at the positions of compact sources from the Radio Fundamental Catalog. In addition, our survey contains calibrators detected from two 1s long trial baseband snapshots for a deeper survey with CHIME and GBO. In this paper, we present the largest catalog of compact calibrators suitable for 30-milliarcsecond-scale VLBI observations at sub-GHz frequencies to date. Our catalog consists of 200 total calibrators in the Northern Hemisphere that are compact on 30-milliarcsecond scales with fluxes above 100mJy. This calibrator grid will enable the precise localization of hundreds of FRBs a year with CHIME/FRB-Outriggers.",
      "paper_authors": [
        "Shion Andrew",
        "Calvin Leung",
        "Alexander Li",
        "Kiyoshi W. Masui",
        "Bridget C. Andersen",
        "Kevin Bandura",
        "Alice P. Curtin",
        "Jane Kaczmarek",
        "Adam E. Lanman",
        "Mattias Lazda",
        "Juan Mena-Parra",
        "Daniele Michilli",
        "Kenzie Nimmo",
        "Aaron B. Pearlman",
        "Mubdi Rahman",
        "Vishwangi Shah",
        "Kaitlyn Shin",
        "Haochen Wang"
      ],
      "primary_category": "astro-ph.IM",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": null,
      "repo_url": "#"
    },
    "2409.11472": {
      "paper_id": "2409.11472v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11472v1",
      "paper_key": "2409.11472",
      "paper_title": "Testing the thermal Sunyaev-Zel'dovich power spectrum of a halo model using hydrodynamical simulations",
      "paper_url": "http://arxiv.org/abs/2409.11472v1",
      "paper_abstract": "Statistical properties of LSS serve as powerful tools to constrain the cosmological properties of our Universe. Tracing the gas pressure, the tSZ effect is a biased probe of mass distribution and can be used to test the physics of feedback or cosmological models. Therefore, it is crucial to develop robust modeling of hot gas pressure for applications to tSZ surveys. Since gas collapses into bound structures, it is expected that most of the tSZ signal is within halos produced by cosmic accretion shocks. Hence, simple empirical halo models can be used to predict the tSZ power spectra. In this study, we employed the HMx halo model to compare the tSZ power spectra with those of several hydrodynamical simulations: the Horizon suite and the Magneticum simulation. We examined various contributions to the tSZ power spectrum across different redshifts, including the one- and two-halo term decomposition, the amount of bound gas, the importance of different masses and the electron pressure profiles. Our comparison of the tSZ power spectrum reveals discrepancies that increase with redshift. We find a 20% to 50% difference between the measured and predicted tSZ angular power spectrum over the multipole range $\\ell=10^3-10^4$. Our analysis reveals that these differences are driven by the excess of power in the predicted two-halo term at low k and in the one-halo term at high k. At higher redshifts (z~3), simulations indicate that more power comes from outside the virial radius than from inside suggesting a limitation in the applicability of the halo model. We observe differences in the pressure profiles, despite the fair level of agreement on the tSZ power spectrum at low redshift with the default calibration of the halo model. In conclusion, our study suggests that the properties of the halo model need to be carefully controlled against real or mock data to be proven useful for cosmological purposes.",
      "paper_authors": [
        "Emma Ay\u00e7oberry",
        "Pranjal R. S.",
        "Karim Benabed",
        "Yohan Dubois",
        "Elisabeth Krause",
        "Tim Eifler"
      ],
      "primary_category": "astro-ph.CO",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "15 pages, 9 figures",
      "repo_url": "#"
    },
    "2409.11471": {
      "paper_id": "2409.11471v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11471v1",
      "paper_key": "2409.11471",
      "paper_title": "CuRIOS-ED: The Technology Demonstrator for the CubeSats for Rapid Infrared and Optical Surveys Mission",
      "paper_url": "http://arxiv.org/abs/2409.11471v1",
      "paper_abstract": "The rise of time-domain astronomy including electromagnetic counterparts to gravitational waves, gravitational microlensing, explosive phenomena, and even astrometry with Gaia, are showing the power and need for surveys with high-cadence, large area, and long time baselines to study the transient universe. A constellation of SmallSats or CubeSats providing wide, instantaneous sky coverage down to 21 Vega mag at optical wavelengths would be ideal for addressing this need. We are assembling CuRIOS-ED (CubeSats for Rapid Infrared and Optical Survey--Exploration Demo), an optical telescope payload which will act as a technology demonstrator for a larger constellation of several hundred 16U CubeSats known as CuRIOS. In preparation for CuRIOS, CuRIOS-ED will launch in late 2025 as part of the 12U Starspec InspireSat MVP payload. CuRIOS-ED will be used to demonstrate the StarSpec ADCS pointing capabilities to <1\" and to space-qualify a commercial camera package for use on the full CuRIOS payload. The CuRIOS-ED camera system will utilize a Sony IMX455 CMOS detector delivered in an off-the-shelf Atik apx60 package which we modified to be compatible with operations in vacuum as well as the CubeSat form factor, power, and thermal constraints. By qualifying this commercial camera solution, the cost of each CuRIOS satellite will be greatly decreased (~100x) when compared with current space-qualified cameras with IMX455 detectors. We discuss the CuRIOS-ED mission design with an emphasis on the disassembly, repackaging, and testing of the Atik apx60 for space-based missions. Characterization of the apx60's read noise, dark current, patterned noise, and thermal behavior are reported for a range of temperatures (-35 C to 40 C) and exposure times (0.001s to 30 s). Additionally, we comment on preliminary environmental testing results from a successful thermal vacuum test.",
      "paper_authors": [
        "Hannah Gulick",
        "Jessica R. Lu",
        "Aryan Sood",
        "Steven V. W. Beckwith",
        "Joshua S. Bloom",
        "Kodi Rider",
        "Dan Werthimer",
        "Wei Liu",
        "Guy Nir",
        "Harrison Lee",
        "Jeremy McCauley"
      ],
      "primary_category": "astro-ph.IM",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "17 pages, 10 figures, 1 table",
      "repo_url": "#"
    },
    "2409.11465": {
      "paper_id": "2409.11465v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11465v1",
      "paper_key": "2409.11465",
      "paper_title": "The LOFAR Two Metre Sky Survey Data Release 2: Probabilistic Spectral Source Classifications and Faint Radio Source Demographics",
      "paper_url": "http://arxiv.org/abs/2409.11465v1",
      "paper_abstract": "We present an analysis of 152,355 radio sources identified in the second data release of the LOFAR Two Metre Sky Survey (LoTSS-DR2) with Sloan Digital Sky Survey (SDSS) spectroscopic redshifts in the range 0.00 < z < 0.57. Using Monte Carlo simulations we determine the reliability of each source exhibiting an excess in radio luminosity relative to that predicted from their Ha emission, and, for a subset of 124,023 sources we combine this measurement with a full BPT analysis. Using these two independent diagnostics we determine the reliability of each source hosting a supermassive black hole of high or low Eddington-scaled accretion rate, and combine the measurements to determine the reliability of sources belonging to each of four physical classes of objects: star forming galaxies (SFGs), radio-quiet active galactic nuclei (RQAGN), and high- or low-excitation radio galaxies (HERGs or emission-line LERGs). The result is a catalogue which enables user-defined samples of radio sources with a reliability threshold suited to their science goal e.g. prioritising purity or completeness. Here we select high-confidence samples of radio sources (>90% reliability) to report: 38,588 radio-excess AGN in the LoTSS DR2 sample (362 HERGs, and 12,648 emission-line LERGs), together with 38,729 SFGs, and 18,726 RQAGN. We validate these results through comparison to literature using independent emission-line measurements, and to widely-adopted WISE photometric selection techniques. While our use of SDSS spectroscopy limits our current analysis to ~4 percent of the LoTSS-DR2 catalogue, our method is directly applicable to data from the forthcoming WEAVE-LOFAR survey which will obtain over a million spectra of 144 MHz selected sources.",
      "paper_authors": [
        "A. B. Drake",
        "D. J. B. Smith",
        "M. J. Hardcastle",
        "P. N. Best",
        "R. Kondapally",
        "M. I. Arnaudova",
        "S. Das",
        "S. Shenoy",
        "K. J. Duncan",
        "H. J. A. R\u00f6ttgering",
        "C. Tasse"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "Accepted for publication in MNRAS",
      "repo_url": "#"
    },
    "2409.11462": {
      "paper_id": "2409.11462v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11462v1",
      "paper_key": "2409.11462",
      "paper_title": "Equity considerations in COVID-19 vaccine allocation modelling: a literature review",
      "paper_url": "http://arxiv.org/abs/2409.11462v1",
      "paper_abstract": "We conducted a literature review of COVID-19 vaccine allocation modelling papers, specifically looking for publications that considered equity. We found that most models did not take equity into account, with the vast majority of publications presenting aggregated results and no results by any subgroup (e.g. age, race, geography, etc). We then give examples of how modelling can be useful to answer equity questions, and highlight some of the findings from the publications that did. Lastly, we describe seven considerations that seem important to consider when including equity in future vaccine allocation models.",
      "paper_authors": [
        "Eva Rumpler",
        "Marc Lipsitch"
      ],
      "primary_category": "stat.AP",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": null,
      "repo_url": "#"
    },
    "2409.11458": {
      "paper_id": "2409.11458v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11458v1",
      "paper_key": "2409.11458",
      "paper_title": "The VIRUS-dE Survey II: Cuspy and round halos in dwarf ellipticals -- A result of early assembly?",
      "paper_url": "http://arxiv.org/abs/2409.11458v1",
      "paper_abstract": "We analyze the dark matter (DM) halos of a sample of dwarf Ellitpicals (dE) and discuss cosmological and evolutionary implications. Using orbit modeling we recover their density slopes and, for the first time, the halo flattening. We find the `cusp-core' tension is mild, on average dEs have central slopes slightly below the Navarro Frenk White (NFW) predictions. However, the measured flattenings are still more spherical than cosmological simulations predict. Unlike brighter ETGs the total density slopes of dEs are shallower, and their average DM density does not follow their scaling relation with luminosity. Conversely, dE halos are denser and the densities steeper than in LTGs. We find average DM density and slope are strongly correlated with the environment and moderately with the angular momentum. Central, non-rotating dEs have dense and cuspy halos, whereas rotating dEs in Virgo's outskirts are more cored and less dense. This can be explained by a delayed formation of the dEs in the cluster outskirts, or alternatively, by the accumulated baryonic feedback the dEs in the outskirts have experienced during their very different star formation history. Our results suggest halo profiles are not universal (they depend on assembly conditions) and they evolve only mildly due to internal feedback. We conclude dEs in the local Universe have assembled at a higher redshift than local spirals. In these extreme conditions (e.g. star-formation, halo assembly) were very different, suggesting no new dEs are formed at present.",
      "paper_authors": [
        "Mathias Lipka",
        "Jens Thomas",
        "Roberto Saglia",
        "Ralf Bender",
        "Maximilian Fabricius",
        "Christian Partmann"
      ],
      "primary_category": "astro-ph.GA",
      "publish_time": "2024-09-17",
      "update_time": "2024-09-17",
      "comments": "Accepted for publication in ApJ, 46 pages, 16 figures, 1 table",
      "repo_url": "#"
    },
    "2409.11441": {
      "paper_id": "2409.11441v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11441v1",
      "paper_key": "2409.11441",
      "paper_title": "Continual Learning of Conjugated Visual Representations through Higher-order Motion Flows",
      "paper_url": "http://arxiv.org/abs/2409.11441v1",
      "paper_abstract": "Learning with neural networks from a continuous stream of visual information presents several challenges due to the non-i.i.d. nature of the data. However, it also offers novel opportunities to develop representations that are consistent with the information flow. In this paper we investigate the case of unsupervised continual learning of pixel-wise features subject to multiple motion-induced constraints, therefore named motion-conjugated feature representations. Differently from existing approaches, motion is not a given signal (either ground-truth or estimated by external modules), but is the outcome of a progressive and autonomous learning process, occurring at various levels of the feature hierarchy. Multiple motion flows are estimated with neural networks and characterized by different levels of abstractions, spanning from traditional optical flow to other latent signals originating from higher-level features, hence called higher-order motions. Continuously learning to develop consistent multi-order flows and representations is prone to trivial solutions, which we counteract by introducing a self-supervised contrastive loss, spatially-aware and based on flow-induced similarity. We assess our model on photorealistic synthetic streams and real-world videos, comparing to pre-trained state-of-the art feature extractors (also based on Transformers) and to recent unsupervised learning models, significantly outperforming these alternatives.",
      "paper_authors": [
        "Simone Marullo",
        "Matteo Tiezzi",
        "Marco Gori",
        "Stefano Melacci"
      ],
      "primary_category": "cs.CV",
      "publish_time": "2024-09-16",
      "update_time": "2024-09-16",
      "comments": "Currently under review",
      "repo_url": "#"
    },
    "2409.11438": {
      "paper_id": "2409.11438v2",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11438v2",
      "paper_key": "2409.11438",
      "paper_title": "Machine Learning for Analyzing Atomic Force Microscopy (AFM) Images Generated from Polymer Blends",
      "paper_url": "http://arxiv.org/abs/2409.11438v2",
      "paper_abstract": "In this paper we present a new machine learning workflow with unsupervised learning techniques to identify domains within atomic force microscopy images obtained from polymer films. The goal of the workflow is to identify the spatial location of the two types of polymer domains with little to no manual intervention and calculate the domain size distributions which in turn can help qualify the phase separated state of the material as macrophase or microphase ordered or disordered domains. We briefly review existing approaches used in other fields, computer vision and signal processing that can be applicable for the above tasks that happen frequently in the field of polymer science and engineering. We then test these approaches from computer vision and signal processing on the AFM image dataset to identify the strengths and limitations of each of these approaches for our first task. For our first domain segmentation task, we found that the workflow using discrete Fourier transform or discrete cosine transform with variance statistics as the feature works the best. The popular ResNet50 deep learning approach from computer vision field exhibited relatively poorer performance in the domain segmentation task for our AFM images as compared to the DFT and DCT based workflows. For the second task, for each of 144 input AFM images, we then used an existing porespy python package to calculate the domain size distribution from the output of that image from DFT based workflow. The information and open source codes we share in this paper can serve as a guide for researchers in the polymer and soft materials fields who need ML modeling and workflows for automated analyses of AFM images from polymer samples that may have crystalline or amorphous domains, sharp or rough interfaces between domains, or micro or macrophase separated domains.",
      "paper_authors": [
        "Aanish Paruchuri",
        "Yunfei Wang",
        "Xiaodan Gu",
        "Arthi Jayaraman"
      ],
      "primary_category": "eess.IV",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-20",
      "comments": "39 pages, 13 figures, 4 tables",
      "repo_url": "https://github.com/arthijayaraman-lab/automated-atomic-force-microscopy-image-analysis"
    },
    "2409.11436": {
      "paper_id": "2409.11436v1",
      "code_url": "https://arxiv.paperswithcode.com/api/v0/papers/2409.11436v1",
      "paper_key": "2409.11436",
      "paper_title": "Analysis of flexible traffic control method in SDN",
      "paper_url": "http://arxiv.org/abs/2409.11436v1",
      "paper_abstract": "The aim of this paper is to analyze methods of flexible control in SDN networks and to propose a self-developed solution that will enable intelligent adaptation of SDN controller performance. This work aims not only to review existing solutions, but also to develop an approach that will increase the efficiency and adaptability of network management. The project uses a modern type of machine learning, Reinforcement Learning, which allows autonomous decisions of a network that learns based on its choices in a dynamically changing environment, which is most similar to the way humans learn. The solution aims not only to improve the network's performance, but also its flexibility and real-time adaptability - flexible traffic control.",
      "paper_authors": [
        "Marta Szymczyk"
      ],
      "primary_category": "cs.NI",
      "publish_time": "2024-09-15",
      "update_time": "2024-09-15",
      "comments": null,
      "repo_url": "#"
    }
  }
}